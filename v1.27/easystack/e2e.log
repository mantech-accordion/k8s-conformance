  I0510 04:50:16.868271      22 e2e.go:117] Starting e2e run "f0d8353b-b148-477c-8f9b-38112bf35b15" on Ginkgo node 1
  May 10 04:50:16.898: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1683694216 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  May 10 04:50:17.067: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 04:50:17.068: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  May 10 04:50:17.090: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  May 10 04:50:17.092: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  May 10 04:50:17.092: INFO: e2e test version: v1.27.1
  May 10 04:50:17.093: INFO: kube-apiserver version: v1.27.1
  May 10 04:50:17.093: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 04:50:17.095: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.029 seconds]
------------------------------
S
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 05/10/23 04:50:17.274
  May 10 04:50:17.274: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename taint-single-pod @ 05/10/23 04:50:17.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 04:50:17.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 04:50:17.291
  May 10 04:50:17.292: INFO: Waiting up to 1m0s for all nodes to be ready
  May 10 04:51:17.310: INFO: Waiting for terminating namespaces to be deleted...
  May 10 04:51:17.312: INFO: Starting informer...
  STEP: Starting pod... @ 05/10/23 04:51:17.312
  May 10 04:51:17.523: INFO: Pod is running on node-02. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/10/23 04:51:17.523
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/10/23 04:51:17.537
  STEP: Waiting short time to make sure Pod is queued for deletion @ 05/10/23 04:51:17.539
  May 10 04:51:17.539: INFO: Pod wasn't evicted. Proceeding
  May 10 04:51:17.539: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/10/23 04:51:17.559
  STEP: Waiting some time to make sure that toleration time passed. @ 05/10/23 04:51:17.563
  May 10 04:52:32.564: INFO: Pod wasn't evicted. Test successful
  May 10 04:52:32.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-5014" for this suite. @ 05/10/23 04:52:32.568
• [135.298 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 05/10/23 04:52:32.573
  May 10 04:52:32.573: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename podtemplate @ 05/10/23 04:52:32.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 04:52:32.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 04:52:32.585
  STEP: Create set of pod templates @ 05/10/23 04:52:32.594
  May 10 04:52:32.598: INFO: created test-podtemplate-1
  May 10 04:52:32.602: INFO: created test-podtemplate-2
  May 10 04:52:32.608: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 05/10/23 04:52:32.608
  STEP: delete collection of pod templates @ 05/10/23 04:52:32.61
  May 10 04:52:32.610: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 05/10/23 04:52:32.624
  May 10 04:52:32.624: INFO: requesting list of pod templates to confirm quantity
  May 10 04:52:32.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1309" for this suite. @ 05/10/23 04:52:32.627
• [0.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 05/10/23 04:52:32.632
  May 10 04:52:32.632: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-probe @ 05/10/23 04:52:32.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 04:52:32.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 04:52:32.652
  STEP: Creating pod test-grpc-11de34d4-c43f-411a-94dc-f63d91e88736 in namespace container-probe-9024 @ 05/10/23 04:52:32.654
  May 10 04:52:34.668: INFO: Started pod test-grpc-11de34d4-c43f-411a-94dc-f63d91e88736 in namespace container-probe-9024
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/10/23 04:52:34.668
  May 10 04:52:34.669: INFO: Initial restart count of pod test-grpc-11de34d4-c43f-411a-94dc-f63d91e88736 is 0
  May 10 04:56:35.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 04:56:35.335
  STEP: Destroying namespace "container-probe-9024" for this suite. @ 05/10/23 04:56:35.347
• [242.720 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 05/10/23 04:56:35.353
  May 10 04:56:35.353: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pod-network-test @ 05/10/23 04:56:35.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 04:56:35.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 04:56:35.369
  STEP: Performing setup for networking test in namespace pod-network-test-4693 @ 05/10/23 04:56:35.371
  STEP: creating a selector @ 05/10/23 04:56:35.371
  STEP: Creating the service pods in kubernetes @ 05/10/23 04:56:35.371
  May 10 04:56:35.371: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/10/23 04:56:57.449
  May 10 04:56:59.466: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 10 04:56:59.466: INFO: Breadth first check of 100.67.79.147 on host 192.168.60.83...
  May 10 04:56:59.467: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.114.252.177:9080/dial?request=hostname&protocol=udp&host=100.67.79.147&port=8081&tries=1'] Namespace:pod-network-test-4693 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 04:56:59.467: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 04:56:59.468: INFO: ExecWithOptions: Clientset creation
  May 10 04:56:59.468: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4693/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.114.252.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.67.79.147%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 10 04:56:59.537: INFO: Waiting for responses: map[]
  May 10 04:56:59.537: INFO: reached 100.67.79.147 after 0/1 tries
  May 10 04:56:59.537: INFO: Breadth first check of 100.114.252.162 on host 192.168.60.6...
  May 10 04:56:59.645: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.114.252.177:9080/dial?request=hostname&protocol=udp&host=100.114.252.162&port=8081&tries=1'] Namespace:pod-network-test-4693 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 04:56:59.645: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 04:56:59.646: INFO: ExecWithOptions: Clientset creation
  May 10 04:56:59.646: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4693/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.114.252.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.114.252.162%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 10 04:56:59.720: INFO: Waiting for responses: map[]
  May 10 04:56:59.720: INFO: reached 100.114.252.162 after 0/1 tries
  May 10 04:56:59.720: INFO: Breadth first check of 100.74.79.25 on host 192.168.60.152...
  May 10 04:56:59.722: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.114.252.177:9080/dial?request=hostname&protocol=udp&host=100.74.79.25&port=8081&tries=1'] Namespace:pod-network-test-4693 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 04:56:59.722: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 04:56:59.723: INFO: ExecWithOptions: Clientset creation
  May 10 04:56:59.723: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4693/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.114.252.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.74.79.25%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 10 04:56:59.761: INFO: Waiting for responses: map[]
  May 10 04:56:59.761: INFO: reached 100.74.79.25 after 0/1 tries
  May 10 04:56:59.761: INFO: Going to retry 0 out of 3 pods....
  May 10 04:56:59.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4693" for this suite. @ 05/10/23 04:56:59.764
• [24.416 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 05/10/23 04:56:59.769
  May 10 04:56:59.769: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename subpath @ 05/10/23 04:56:59.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 04:56:59.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 04:56:59.781
  STEP: Setting up data @ 05/10/23 04:56:59.783
  STEP: Creating pod pod-subpath-test-configmap-9s8t @ 05/10/23 04:56:59.79
  STEP: Creating a pod to test atomic-volume-subpath @ 05/10/23 04:56:59.79
  STEP: Saw pod success @ 05/10/23 04:57:23.842
  May 10 04:57:23.843: INFO: Trying to get logs from node node-02 pod pod-subpath-test-configmap-9s8t container test-container-subpath-configmap-9s8t: <nil>
  STEP: delete the pod @ 05/10/23 04:57:23.848
  STEP: Deleting pod pod-subpath-test-configmap-9s8t @ 05/10/23 04:57:23.869
  May 10 04:57:23.869: INFO: Deleting pod "pod-subpath-test-configmap-9s8t" in namespace "subpath-4860"
  May 10 04:57:23.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4860" for this suite. @ 05/10/23 04:57:23.873
• [24.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 05/10/23 04:57:23.879
  May 10 04:57:23.879: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename containers @ 05/10/23 04:57:23.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 04:57:23.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 04:57:23.895
  STEP: Creating a pod to test override arguments @ 05/10/23 04:57:23.897
  STEP: Saw pod success @ 05/10/23 04:57:27.911
  May 10 04:57:27.913: INFO: Trying to get logs from node node-02 pod client-containers-389f182f-89c9-4fd8-95b3-99e0a9b144fd container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 04:57:27.918
  May 10 04:57:27.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3960" for this suite. @ 05/10/23 04:57:27.932
• [4.057 seconds]
------------------------------
S
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 05/10/23 04:57:27.935
  May 10 04:57:27.935: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename ingressclass @ 05/10/23 04:57:27.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 04:57:27.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 04:57:27.951
  STEP: getting /apis @ 05/10/23 04:57:27.953
  STEP: getting /apis/networking.k8s.io @ 05/10/23 04:57:27.957
  STEP: getting /apis/networking.k8s.iov1 @ 05/10/23 04:57:27.958
  STEP: creating @ 05/10/23 04:57:27.959
  STEP: getting @ 05/10/23 04:57:27.969
  STEP: listing @ 05/10/23 04:57:27.971
  STEP: watching @ 05/10/23 04:57:27.972
  May 10 04:57:27.972: INFO: starting watch
  STEP: patching @ 05/10/23 04:57:27.973
  STEP: updating @ 05/10/23 04:57:27.982
  May 10 04:57:27.986: INFO: waiting for watch events with expected annotations
  May 10 04:57:27.986: INFO: saw patched and updated annotations
  STEP: deleting @ 05/10/23 04:57:27.986
  STEP: deleting a collection @ 05/10/23 04:57:27.995
  May 10 04:57:28.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-3229" for this suite. @ 05/10/23 04:57:28.007
• [0.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 05/10/23 04:57:28.012
  May 10 04:57:28.012: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/10/23 04:57:28.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 04:57:28.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 04:57:28.026
  May 10 04:57:28.029: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/10/23 04:57:29.456
  May 10 04:57:29.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-3696 --namespace=crd-publish-openapi-3696 create -f -'
  May 10 04:57:30.380: INFO: stderr: ""
  May 10 04:57:30.380: INFO: stdout: "e2e-test-crd-publish-openapi-4913-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 10 04:57:30.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-3696 --namespace=crd-publish-openapi-3696 delete e2e-test-crd-publish-openapi-4913-crds test-cr'
  May 10 04:57:30.447: INFO: stderr: ""
  May 10 04:57:30.447: INFO: stdout: "e2e-test-crd-publish-openapi-4913-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  May 10 04:57:30.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-3696 --namespace=crd-publish-openapi-3696 apply -f -'
  May 10 04:57:30.714: INFO: stderr: ""
  May 10 04:57:30.714: INFO: stdout: "e2e-test-crd-publish-openapi-4913-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 10 04:57:30.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-3696 --namespace=crd-publish-openapi-3696 delete e2e-test-crd-publish-openapi-4913-crds test-cr'
  May 10 04:57:30.776: INFO: stderr: ""
  May 10 04:57:30.776: INFO: stdout: "e2e-test-crd-publish-openapi-4913-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/10/23 04:57:30.776
  May 10 04:57:30.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-3696 explain e2e-test-crd-publish-openapi-4913-crds'
  May 10 04:57:31.051: INFO: stderr: ""
  May 10 04:57:31.051: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-4913-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  May 10 04:57:32.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3696" for this suite. @ 05/10/23 04:57:32.484
• [4.500 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 05/10/23 04:57:32.513
  May 10 04:57:32.513: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-probe @ 05/10/23 04:57:32.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 04:57:32.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 04:57:32.571
  May 10 04:58:32.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-9563" for this suite. @ 05/10/23 04:58:32.586
• [60.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 05/10/23 04:58:32.591
  May 10 04:58:32.591: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename field-validation @ 05/10/23 04:58:32.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 04:58:32.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 04:58:32.606
  May 10 04:58:32.608: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  W0510 04:58:35.147495      22 warnings.go:70] unknown field "alpha"
  W0510 04:58:35.147513      22 warnings.go:70] unknown field "beta"
  W0510 04:58:35.147519      22 warnings.go:70] unknown field "delta"
  W0510 04:58:35.147524      22 warnings.go:70] unknown field "epsilon"
  W0510 04:58:35.147530      22 warnings.go:70] unknown field "gamma"
  May 10 04:58:35.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4960" for this suite. @ 05/10/23 04:58:35.166
• [2.585 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 05/10/23 04:58:35.177
  May 10 04:58:35.177: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename cronjob @ 05/10/23 04:58:35.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 04:58:35.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 04:58:35.19
  STEP: Creating a ReplaceConcurrent cronjob @ 05/10/23 04:58:35.192
  STEP: Ensuring a job is scheduled @ 05/10/23 04:58:35.2
  STEP: Ensuring exactly one is scheduled @ 05/10/23 04:59:01.204
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/10/23 04:59:01.205
  STEP: Ensuring the job is replaced with a new one @ 05/10/23 04:59:01.207
  STEP: Removing cronjob @ 05/10/23 05:00:01.211
  May 10 05:00:01.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5534" for this suite. @ 05/10/23 05:00:01.218
• [86.053 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 05/10/23 05:00:01.23
  May 10 05:00:01.230: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename gc @ 05/10/23 05:00:01.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:00:01.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:00:01.255
  STEP: create the deployment @ 05/10/23 05:00:01.258
  W0510 05:00:01.266125      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/10/23 05:00:01.266
  STEP: delete the deployment @ 05/10/23 05:00:01.771
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 05/10/23 05:00:01.776
  STEP: Gathering metrics @ 05/10/23 05:00:02.288
  May 10 05:00:02.358: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 10 05:00:02.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6443" for this suite. @ 05/10/23 05:00:02.362
• [1.143 seconds]
------------------------------
S
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 05/10/23 05:00:02.373
  May 10 05:00:02.374: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 05:00:02.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:00:02.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:00:02.387
  STEP: Creating projection with secret that has name secret-emptykey-test-46aa0f20-f9ca-4272-93d6-74fa071324e6 @ 05/10/23 05:00:02.389
  May 10 05:00:02.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5592" for this suite. @ 05/10/23 05:00:02.392
• [0.023 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 05/10/23 05:00:02.398
  May 10 05:00:02.398: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename field-validation @ 05/10/23 05:00:02.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:00:02.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:00:02.415
  May 10 05:00:02.417: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:00:04.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2139" for this suite. @ 05/10/23 05:00:04.981
• [2.588 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 05/10/23 05:00:04.987
  May 10 05:00:04.987: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename daemonsets @ 05/10/23 05:00:04.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:00:04.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:00:05.003
  STEP: Creating simple DaemonSet "daemon-set" @ 05/10/23 05:00:05.019
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/10/23 05:00:05.025
  May 10 05:00:05.046: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 05:00:05.046: INFO: Node node-01 is running 0 daemon pod, expected 1
  May 10 05:00:06.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 05:00:06.102: INFO: Node node-01 is running 0 daemon pod, expected 1
  May 10 05:00:07.051: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 10 05:00:07.051: INFO: Node ub-test is running 0 daemon pod, expected 1
  May 10 05:00:08.052: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 10 05:00:08.052: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 05/10/23 05:00:08.054
  May 10 05:00:08.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 10 05:00:08.064: INFO: Node ub-test is running 0 daemon pod, expected 1
  May 10 05:00:09.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 10 05:00:09.070: INFO: Node ub-test is running 0 daemon pod, expected 1
  May 10 05:00:10.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 10 05:00:10.070: INFO: Node ub-test is running 0 daemon pod, expected 1
  May 10 05:00:11.080: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 10 05:00:11.080: INFO: Node ub-test is running 0 daemon pod, expected 1
  May 10 05:00:12.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 10 05:00:12.071: INFO: Node ub-test is running 0 daemon pod, expected 1
  May 10 05:00:13.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 10 05:00:13.070: INFO: Node ub-test is running 0 daemon pod, expected 1
  May 10 05:00:14.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 10 05:00:14.070: INFO: Node ub-test is running 0 daemon pod, expected 1
  May 10 05:00:15.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 10 05:00:15.070: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/10/23 05:00:15.071
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6260, will wait for the garbage collector to delete the pods @ 05/10/23 05:00:15.071
  May 10 05:00:15.129: INFO: Deleting DaemonSet.extensions daemon-set took: 5.290231ms
  May 10 05:00:15.229: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.629594ms
  May 10 05:00:19.732: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 05:00:19.732: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 10 05:00:19.735: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1054045"},"items":null}

  May 10 05:00:19.737: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1054045"},"items":null}

  May 10 05:00:19.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6260" for this suite. @ 05/10/23 05:00:19.746
• [14.763 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 05/10/23 05:00:19.75
  May 10 05:00:19.750: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename crd-webhook @ 05/10/23 05:00:19.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:00:19.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:00:19.765
  STEP: Setting up server cert @ 05/10/23 05:00:19.767
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/10/23 05:00:19.969
  STEP: Deploying the custom resource conversion webhook pod @ 05/10/23 05:00:19.978
  STEP: Wait for the deployment to be ready @ 05/10/23 05:00:19.992
  May 10 05:00:20.003: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  May 10 05:00:22.010: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 0, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 0, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 0, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 0, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/10/23 05:00:24.013
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:00:24.028
  May 10 05:00:25.028: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 10 05:00:25.048: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Creating a v1 custom resource @ 05/10/23 05:00:27.606
  STEP: v2 custom resource should be converted @ 05/10/23 05:00:27.614
  May 10 05:00:27.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6346" for this suite. @ 05/10/23 05:00:28.176
• [8.435 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 05/10/23 05:00:28.185
  May 10 05:00:28.185: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename field-validation @ 05/10/23 05:00:28.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:00:28.199
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:00:28.203
  STEP: apply creating a deployment @ 05/10/23 05:00:28.205
  May 10 05:00:28.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5984" for this suite. @ 05/10/23 05:00:28.215
• [0.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 05/10/23 05:00:28.221
  May 10 05:00:28.221: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename sched-preemption @ 05/10/23 05:00:28.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:00:28.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:00:28.239
  May 10 05:00:28.260: INFO: Waiting up to 1m0s for all nodes to be ready
  May 10 05:01:28.281: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/10/23 05:01:28.283
  May 10 05:01:28.283: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/10/23 05:01:28.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:01:28.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:01:28.298
  STEP: Finding an available node @ 05/10/23 05:01:28.3
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/10/23 05:01:28.3
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/10/23 05:01:30.36
  May 10 05:01:30.385: INFO: found a healthy node: node-02
  May 10 05:01:42.440: INFO: pods created so far: [1 1 1]
  May 10 05:01:42.441: INFO: length of pods created so far: 3
  May 10 05:01:46.516: INFO: pods created so far: [2 2 1]
  May 10 05:01:53.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 10 05:01:53.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-2605" for this suite. @ 05/10/23 05:01:53.606
  STEP: Destroying namespace "sched-preemption-5084" for this suite. @ 05/10/23 05:01:53.611
• [85.395 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 05/10/23 05:01:53.616
  May 10 05:01:53.616: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-probe @ 05/10/23 05:01:53.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:01:53.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:01:53.632
  STEP: Creating pod liveness-2c235d7e-b2ea-4a44-ae27-9d217f22c0f6 in namespace container-probe-2277 @ 05/10/23 05:01:53.634
  May 10 05:01:55.649: INFO: Started pod liveness-2c235d7e-b2ea-4a44-ae27-9d217f22c0f6 in namespace container-probe-2277
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/10/23 05:01:55.649
  May 10 05:01:55.650: INFO: Initial restart count of pod liveness-2c235d7e-b2ea-4a44-ae27-9d217f22c0f6 is 0
  May 10 05:02:15.682: INFO: Restart count of pod container-probe-2277/liveness-2c235d7e-b2ea-4a44-ae27-9d217f22c0f6 is now 1 (20.031216258s elapsed)
  May 10 05:02:35.718: INFO: Restart count of pod container-probe-2277/liveness-2c235d7e-b2ea-4a44-ae27-9d217f22c0f6 is now 2 (40.06743571s elapsed)
  May 10 05:02:55.826: INFO: Restart count of pod container-probe-2277/liveness-2c235d7e-b2ea-4a44-ae27-9d217f22c0f6 is now 3 (1m0.175650333s elapsed)
  May 10 05:03:15.939: INFO: Restart count of pod container-probe-2277/liveness-2c235d7e-b2ea-4a44-ae27-9d217f22c0f6 is now 4 (1m20.288199523s elapsed)
  May 10 05:04:28.212: INFO: Restart count of pod container-probe-2277/liveness-2c235d7e-b2ea-4a44-ae27-9d217f22c0f6 is now 5 (2m32.561684381s elapsed)
  May 10 05:04:28.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 05:04:28.214
  STEP: Destroying namespace "container-probe-2277" for this suite. @ 05/10/23 05:04:28.226
• [154.614 seconds]
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 05/10/23 05:04:28.23
  May 10 05:04:28.230: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename replication-controller @ 05/10/23 05:04:28.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:04:28.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:04:28.243
  STEP: creating a ReplicationController @ 05/10/23 05:04:28.247
  STEP: waiting for RC to be added @ 05/10/23 05:04:28.251
  STEP: waiting for available Replicas @ 05/10/23 05:04:28.251
  STEP: patching ReplicationController @ 05/10/23 05:04:30.65
  STEP: waiting for RC to be modified @ 05/10/23 05:04:30.66
  STEP: patching ReplicationController status @ 05/10/23 05:04:30.66
  STEP: waiting for RC to be modified @ 05/10/23 05:04:30.669
  STEP: waiting for available Replicas @ 05/10/23 05:04:30.669
  STEP: fetching ReplicationController status @ 05/10/23 05:04:30.673
  STEP: patching ReplicationController scale @ 05/10/23 05:04:30.675
  STEP: waiting for RC to be modified @ 05/10/23 05:04:30.684
  STEP: waiting for ReplicationController's scale to be the max amount @ 05/10/23 05:04:30.684
  STEP: fetching ReplicationController; ensuring that it's patched @ 05/10/23 05:04:32.369
  STEP: updating ReplicationController status @ 05/10/23 05:04:32.37
  STEP: waiting for RC to be modified @ 05/10/23 05:04:32.375
  STEP: listing all ReplicationControllers @ 05/10/23 05:04:32.375
  STEP: checking that ReplicationController has expected values @ 05/10/23 05:04:32.376
  STEP: deleting ReplicationControllers by collection @ 05/10/23 05:04:32.376
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 05/10/23 05:04:32.386
  May 10 05:04:32.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0510 05:04:32.441340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-3535" for this suite. @ 05/10/23 05:04:32.443
• [4.216 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 05/10/23 05:04:32.447
  May 10 05:04:32.447: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 05:04:32.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:04:32.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:04:32.464
  STEP: creating service nodeport-test with type=NodePort in namespace services-2180 @ 05/10/23 05:04:32.466
  STEP: creating replication controller nodeport-test in namespace services-2180 @ 05/10/23 05:04:32.479
  I0510 05:04:32.489158      22 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-2180, replica count: 2
  E0510 05:04:33.441613      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:34.442344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:35.442644      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 05:04:35.539943      22 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 10 05:04:35.539: INFO: Creating new exec pod
  E0510 05:04:36.443300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:37.444028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:38.444104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:04:38.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-2180 exec execpodrws5x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May 10 05:04:38.828: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  May 10 05:04:38.829: INFO: stdout: "nodeport-test-hd6mq"
  May 10 05:04:38.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-2180 exec execpodrws5x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.49 80'
  May 10 05:04:38.968: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.2.49 80\nConnection to 10.96.2.49 80 port [tcp/http] succeeded!\n"
  May 10 05:04:38.968: INFO: stdout: "nodeport-test-fqkbn"
  May 10 05:04:38.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-2180 exec execpodrws5x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.60.152 30507'
  May 10 05:04:39.085: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.60.152 30507\nConnection to 192.168.60.152 30507 port [tcp/*] succeeded!\n"
  May 10 05:04:39.085: INFO: stdout: ""
  E0510 05:04:39.444750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:04:40.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-2180 exec execpodrws5x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.60.152 30507'
  May 10 05:04:40.210: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.60.152 30507\nConnection to 192.168.60.152 30507 port [tcp/*] succeeded!\n"
  May 10 05:04:40.210: INFO: stdout: "nodeport-test-fqkbn"
  May 10 05:04:40.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-2180 exec execpodrws5x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.60.6 30507'
  May 10 05:04:40.328: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.60.6 30507\nConnection to 192.168.60.6 30507 port [tcp/*] succeeded!\n"
  May 10 05:04:40.328: INFO: stdout: "nodeport-test-hd6mq"
  May 10 05:04:40.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2180" for this suite. @ 05/10/23 05:04:40.331
• [7.891 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 05/10/23 05:04:40.338
  May 10 05:04:40.338: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename resourcequota @ 05/10/23 05:04:40.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:04:40.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:04:40.351
  STEP: Creating a ResourceQuota with best effort scope @ 05/10/23 05:04:40.353
  STEP: Ensuring ResourceQuota status is calculated @ 05/10/23 05:04:40.357
  E0510 05:04:40.445706      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:41.445913      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 05/10/23 05:04:42.36
  STEP: Ensuring ResourceQuota status is calculated @ 05/10/23 05:04:42.366
  E0510 05:04:42.446607      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:43.446827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 05/10/23 05:04:44.369
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 05/10/23 05:04:44.39
  E0510 05:04:44.447539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:45.447755      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 05/10/23 05:04:46.392
  E0510 05:04:46.448170      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:47.448975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/10/23 05:04:48.395
  E0510 05:04:48.449686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota status released the pod usage @ 05/10/23 05:04:48.455
  E0510 05:04:49.450238      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:50.450425      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 05/10/23 05:04:50.458
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 05/10/23 05:04:50.469
  E0510 05:04:51.450755      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:52.451146      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 05/10/23 05:04:52.472
  E0510 05:04:53.451682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:54.451904      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/10/23 05:04:54.476
  STEP: Ensuring resource quota status released the pod usage @ 05/10/23 05:04:54.494
  E0510 05:04:55.452546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:56.452798      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:04:56.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9661" for this suite. @ 05/10/23 05:04:56.5
• [16.166 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 05/10/23 05:04:56.504
  May 10 05:04:56.504: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename server-version @ 05/10/23 05:04:56.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:04:56.52
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:04:56.523
  STEP: Request ServerVersion @ 05/10/23 05:04:56.525
  STEP: Confirm major version @ 05/10/23 05:04:56.526
  May 10 05:04:56.526: INFO: Major version: 1
  STEP: Confirm minor version @ 05/10/23 05:04:56.526
  May 10 05:04:56.526: INFO: cleanMinorVersion: 27
  May 10 05:04:56.526: INFO: Minor version: 27
  May 10 05:04:56.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-7783" for this suite. @ 05/10/23 05:04:56.528
• [0.028 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 05/10/23 05:04:56.533
  May 10 05:04:56.533: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 05:04:56.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:04:56.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:04:56.546
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-5773 @ 05/10/23 05:04:56.548
  STEP: changing the ExternalName service to type=ClusterIP @ 05/10/23 05:04:56.552
  STEP: creating replication controller externalname-service in namespace services-5773 @ 05/10/23 05:04:56.564
  I0510 05:04:56.571789      22 runners.go:194] Created replication controller with name: externalname-service, namespace: services-5773, replica count: 2
  E0510 05:04:57.453609      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:58.454131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:04:59.454649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 05:04:59.623175      22 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 10 05:04:59.623: INFO: Creating new exec pod
  E0510 05:05:00.455423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:01.455881      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:02.456381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:05:02.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-5773 exec execpod966nn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 10 05:05:02.786: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 10 05:05:02.786: INFO: stdout: "externalname-service-h7ghg"
  May 10 05:05:02.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-5773 exec execpod966nn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.1.123 80'
  May 10 05:05:02.907: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.1.123 80\nConnection to 10.96.1.123 80 port [tcp/http] succeeded!\n"
  May 10 05:05:02.907: INFO: stdout: "externalname-service-rrmrc"
  May 10 05:05:02.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 10 05:05:02.910: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-5773" for this suite. @ 05/10/23 05:05:02.92
• [6.392 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 05/10/23 05:05:02.925
  May 10 05:05:02.925: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename discovery @ 05/10/23 05:05:02.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:05:02.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:05:02.946
  STEP: Setting up server cert @ 05/10/23 05:05:02.949
  E0510 05:05:03.457148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:05:03.472: INFO: Checking APIGroup: apiregistration.k8s.io
  May 10 05:05:03.473: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  May 10 05:05:03.473: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  May 10 05:05:03.473: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  May 10 05:05:03.473: INFO: Checking APIGroup: apps
  May 10 05:05:03.474: INFO: PreferredVersion.GroupVersion: apps/v1
  May 10 05:05:03.474: INFO: Versions found [{apps/v1 v1}]
  May 10 05:05:03.474: INFO: apps/v1 matches apps/v1
  May 10 05:05:03.474: INFO: Checking APIGroup: events.k8s.io
  May 10 05:05:03.475: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  May 10 05:05:03.475: INFO: Versions found [{events.k8s.io/v1 v1}]
  May 10 05:05:03.475: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  May 10 05:05:03.475: INFO: Checking APIGroup: authentication.k8s.io
  May 10 05:05:03.476: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  May 10 05:05:03.476: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  May 10 05:05:03.476: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  May 10 05:05:03.476: INFO: Checking APIGroup: authorization.k8s.io
  May 10 05:05:03.477: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  May 10 05:05:03.477: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  May 10 05:05:03.477: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  May 10 05:05:03.477: INFO: Checking APIGroup: autoscaling
  May 10 05:05:03.477: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  May 10 05:05:03.477: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  May 10 05:05:03.477: INFO: autoscaling/v2 matches autoscaling/v2
  May 10 05:05:03.477: INFO: Checking APIGroup: batch
  May 10 05:05:03.478: INFO: PreferredVersion.GroupVersion: batch/v1
  May 10 05:05:03.478: INFO: Versions found [{batch/v1 v1}]
  May 10 05:05:03.478: INFO: batch/v1 matches batch/v1
  May 10 05:05:03.478: INFO: Checking APIGroup: certificates.k8s.io
  May 10 05:05:03.479: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  May 10 05:05:03.479: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  May 10 05:05:03.479: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  May 10 05:05:03.479: INFO: Checking APIGroup: networking.k8s.io
  May 10 05:05:03.479: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  May 10 05:05:03.479: INFO: Versions found [{networking.k8s.io/v1 v1}]
  May 10 05:05:03.479: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  May 10 05:05:03.479: INFO: Checking APIGroup: policy
  May 10 05:05:03.480: INFO: PreferredVersion.GroupVersion: policy/v1
  May 10 05:05:03.480: INFO: Versions found [{policy/v1 v1}]
  May 10 05:05:03.480: INFO: policy/v1 matches policy/v1
  May 10 05:05:03.480: INFO: Checking APIGroup: rbac.authorization.k8s.io
  May 10 05:05:03.481: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  May 10 05:05:03.481: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  May 10 05:05:03.481: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  May 10 05:05:03.481: INFO: Checking APIGroup: storage.k8s.io
  May 10 05:05:03.481: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  May 10 05:05:03.481: INFO: Versions found [{storage.k8s.io/v1 v1}]
  May 10 05:05:03.481: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  May 10 05:05:03.481: INFO: Checking APIGroup: admissionregistration.k8s.io
  May 10 05:05:03.482: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  May 10 05:05:03.482: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  May 10 05:05:03.482: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  May 10 05:05:03.482: INFO: Checking APIGroup: apiextensions.k8s.io
  May 10 05:05:03.483: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  May 10 05:05:03.483: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  May 10 05:05:03.483: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  May 10 05:05:03.483: INFO: Checking APIGroup: scheduling.k8s.io
  May 10 05:05:03.483: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  May 10 05:05:03.483: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  May 10 05:05:03.483: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  May 10 05:05:03.483: INFO: Checking APIGroup: coordination.k8s.io
  May 10 05:05:03.484: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  May 10 05:05:03.484: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  May 10 05:05:03.484: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  May 10 05:05:03.484: INFO: Checking APIGroup: node.k8s.io
  May 10 05:05:03.485: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  May 10 05:05:03.485: INFO: Versions found [{node.k8s.io/v1 v1}]
  May 10 05:05:03.485: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  May 10 05:05:03.485: INFO: Checking APIGroup: discovery.k8s.io
  May 10 05:05:03.485: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  May 10 05:05:03.485: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  May 10 05:05:03.485: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  May 10 05:05:03.485: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  May 10 05:05:03.486: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  May 10 05:05:03.486: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  May 10 05:05:03.486: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  May 10 05:05:03.486: INFO: Checking APIGroup: projectcalico.org
  May 10 05:05:03.487: INFO: PreferredVersion.GroupVersion: projectcalico.org/v3
  May 10 05:05:03.487: INFO: Versions found [{projectcalico.org/v3 v3}]
  May 10 05:05:03.487: INFO: projectcalico.org/v3 matches projectcalico.org/v3
  May 10 05:05:03.487: INFO: Checking APIGroup: crd.projectcalico.org
  May 10 05:05:03.487: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
  May 10 05:05:03.487: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
  May 10 05:05:03.487: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
  May 10 05:05:03.487: INFO: Checking APIGroup: operator.tigera.io
  May 10 05:05:03.488: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
  May 10 05:05:03.488: INFO: Versions found [{operator.tigera.io/v1 v1}]
  May 10 05:05:03.488: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
  May 10 05:05:03.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-9799" for this suite. @ 05/10/23 05:05:03.49
• [0.570 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 05/10/23 05:05:03.495
  May 10 05:05:03.495: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename hostport @ 05/10/23 05:05:03.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:05:03.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:05:03.515
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 05/10/23 05:05:03.518
  E0510 05:05:04.457188      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:05.457385      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.60.152 on the node which pod1 resides and expect scheduled @ 05/10/23 05:05:05.529
  E0510 05:05:06.458211      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:07.458992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:08.459697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:09.459965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.60.152 but use UDP protocol on the node which pod2 resides @ 05/10/23 05:05:09.54
  E0510 05:05:10.460900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:11.461232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:12.462250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:13.462478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 05/10/23 05:05:13.569
  May 10 05:05:13.569: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.60.152 http://127.0.0.1:54323/hostname] Namespace:hostport-2339 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:05:13.569: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:05:13.569: INFO: ExecWithOptions: Clientset creation
  May 10 05:05:13.569: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2339/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.60.152+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.60.152, port: 54323 @ 05/10/23 05:05:13.659
  May 10 05:05:13.659: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.60.152:54323/hostname] Namespace:hostport-2339 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:05:13.659: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:05:13.659: INFO: ExecWithOptions: Clientset creation
  May 10 05:05:13.659: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2339/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.60.152%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.60.152, port: 54323 UDP @ 05/10/23 05:05:13.705
  May 10 05:05:13.705: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.60.152 54323] Namespace:hostport-2339 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:05:13.705: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:05:13.705: INFO: ExecWithOptions: Clientset creation
  May 10 05:05:13.705: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2339/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.60.152+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0510 05:05:14.462698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:15.462912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:16.463222      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:17.463994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:18.464555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:05:18.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-2339" for this suite. @ 05/10/23 05:05:18.783
• [15.292 seconds]
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 05/10/23 05:05:18.788
  May 10 05:05:18.788: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 05:05:18.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:05:18.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:05:18.806
  STEP: Creating a pod to test emptydir volume type on node default medium @ 05/10/23 05:05:18.808
  E0510 05:05:19.465215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:20.465787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:21.466070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:22.466517      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:05:22.823
  May 10 05:05:22.825: INFO: Trying to get logs from node node-02 pod pod-cf254d76-a8b7-4236-906e-4983b5c74b28 container test-container: <nil>
  STEP: delete the pod @ 05/10/23 05:05:22.829
  May 10 05:05:22.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3418" for this suite. @ 05/10/23 05:05:22.855
• [4.072 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 05/10/23 05:05:22.86
  May 10 05:05:22.860: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename disruption @ 05/10/23 05:05:22.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:05:22.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:05:22.88
  STEP: Waiting for the pdb to be processed @ 05/10/23 05:05:22.887
  E0510 05:05:23.466739      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:24.467283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 05/10/23 05:05:25.036
  STEP: Waiting for all pods to be running @ 05/10/23 05:05:25.079
  May 10 05:05:25.082: INFO: running pods: 0 < 1
  E0510 05:05:25.467908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:26.468356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:05:27.086: INFO: running pods: 0 < 1
  E0510 05:05:27.468894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:28.469214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/10/23 05:05:29.085
  STEP: Waiting for the pdb to be processed @ 05/10/23 05:05:29.212
  STEP: Patching PodDisruptionBudget status @ 05/10/23 05:05:29.224
  STEP: Waiting for the pdb to be processed @ 05/10/23 05:05:29.236
  May 10 05:05:29.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9254" for this suite. @ 05/10/23 05:05:29.245
• [6.391 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 05/10/23 05:05:29.251
  May 10 05:05:29.251: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 05:05:29.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:05:29.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:05:29.27
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/10/23 05:05:29.272
  E0510 05:05:29.470181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:30.470550      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:31.471090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:32.471441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:05:33.287
  May 10 05:05:33.288: INFO: Trying to get logs from node node-02 pod pod-f92ed414-a272-4943-81cb-dc78db84d970 container test-container: <nil>
  STEP: delete the pod @ 05/10/23 05:05:33.292
  May 10 05:05:33.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1713" for this suite. @ 05/10/23 05:05:33.31
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 05/10/23 05:05:33.316
  May 10 05:05:33.316: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 05:05:33.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:05:33.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:05:33.342
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 05:05:33.345
  E0510 05:05:33.471710      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:34.472039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:35.472138      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:36.472336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:37.472479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:38.473429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:05:39.364
  May 10 05:05:39.366: INFO: Trying to get logs from node node-02 pod downwardapi-volume-cd0432a6-7954-4451-8bfc-a23478f22139 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 05:05:39.37
  May 10 05:05:39.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4040" for this suite. @ 05/10/23 05:05:39.386
• [6.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 05/10/23 05:05:39.391
  May 10 05:05:39.391: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:05:39.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:05:39.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:05:39.408
  STEP: Setting up server cert @ 05/10/23 05:05:39.425
  E0510 05:05:39.474040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:05:39.698
  STEP: Deploying the webhook pod @ 05/10/23 05:05:39.704
  STEP: Wait for the deployment to be ready @ 05/10/23 05:05:39.718
  May 10 05:05:39.722: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 05:05:40.474721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:41.475298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:05:41.729: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 5, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 5, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 5, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 5, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:05:42.475889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:43.476127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:05:43.733
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:05:43.741
  E0510 05:05:44.476255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:05:44.741: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 05/10/23 05:05:44.744
  STEP: create a configmap that should be updated by the webhook @ 05/10/23 05:05:44.768
  May 10 05:05:44.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4342" for this suite. @ 05/10/23 05:05:44.813
  STEP: Destroying namespace "webhook-markers-5072" for this suite. @ 05/10/23 05:05:44.822
• [5.437 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 05/10/23 05:05:44.828
  May 10 05:05:44.828: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/10/23 05:05:44.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:05:44.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:05:44.844
  May 10 05:05:44.846: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 05:05:45.477236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/10/23 05:05:46.331
  May 10 05:05:46.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-1643 --namespace=crd-publish-openapi-1643 create -f -'
  E0510 05:05:46.478260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:05:47.254: INFO: stderr: ""
  May 10 05:05:47.254: INFO: stdout: "e2e-test-crd-publish-openapi-7153-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 10 05:05:47.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-1643 --namespace=crd-publish-openapi-1643 delete e2e-test-crd-publish-openapi-7153-crds test-cr'
  May 10 05:05:47.328: INFO: stderr: ""
  May 10 05:05:47.328: INFO: stdout: "e2e-test-crd-publish-openapi-7153-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  May 10 05:05:47.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-1643 --namespace=crd-publish-openapi-1643 apply -f -'
  E0510 05:05:47.478980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:05:47.608: INFO: stderr: ""
  May 10 05:05:47.608: INFO: stdout: "e2e-test-crd-publish-openapi-7153-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 10 05:05:47.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-1643 --namespace=crd-publish-openapi-1643 delete e2e-test-crd-publish-openapi-7153-crds test-cr'
  May 10 05:05:47.668: INFO: stderr: ""
  May 10 05:05:47.668: INFO: stdout: "e2e-test-crd-publish-openapi-7153-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/10/23 05:05:47.668
  May 10 05:05:47.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-1643 explain e2e-test-crd-publish-openapi-7153-crds'
  May 10 05:05:47.927: INFO: stderr: ""
  May 10 05:05:47.927: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-7153-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0510 05:05:48.479650      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:05:49.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1643" for this suite. @ 05/10/23 05:05:49.366
• [4.542 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 05/10/23 05:05:49.375
  May 10 05:05:49.375: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 05:05:49.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:05:49.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:05:49.388
  STEP: Creating secret with name secret-test-80da324a-fadc-4301-9cab-a2bd23a28e4b @ 05/10/23 05:05:49.406
  STEP: Creating a pod to test consume secrets @ 05/10/23 05:05:49.409
  E0510 05:05:49.480670      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:50.480754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:51.481570      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:52.482061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:05:53.424
  May 10 05:05:53.426: INFO: Trying to get logs from node node-02 pod pod-secrets-16718a25-4bcf-4e57-93f8-d6d40edc895e container secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 05:05:53.43
  May 10 05:05:53.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2738" for this suite. @ 05/10/23 05:05:53.443
  STEP: Destroying namespace "secret-namespace-4110" for this suite. @ 05/10/23 05:05:53.45
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 05/10/23 05:05:53.455
  May 10 05:05:53.455: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pods @ 05/10/23 05:05:53.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:05:53.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:05:53.474
  STEP: creating pod @ 05/10/23 05:05:53.479
  E0510 05:05:53.482227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:54.482694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:55.482998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:05:55.497: INFO: Pod pod-hostip-25bff327-1e36-4289-91da-ac87b3f2ce84 has hostIP: 192.168.60.6
  May 10 05:05:55.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3608" for this suite. @ 05/10/23 05:05:55.5
• [2.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 05/10/23 05:05:55.506
  May 10 05:05:55.506: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 05:05:55.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:05:55.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:05:55.521
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 05:05:55.523
  E0510 05:05:56.483429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:57.483597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:58.484515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:05:59.484735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:05:59.536
  May 10 05:05:59.538: INFO: Trying to get logs from node node-02 pod downwardapi-volume-a6209372-3c6b-486e-b2c9-32ec7e48d189 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 05:05:59.542
  May 10 05:05:59.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6382" for this suite. @ 05/10/23 05:05:59.557
• [4.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 05/10/23 05:05:59.562
  May 10 05:05:59.562: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename field-validation @ 05/10/23 05:05:59.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:05:59.573
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:05:59.575
  May 10 05:05:59.576: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 05:06:00.484928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:01.485307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0510 05:06:02.115215      22 warnings.go:70] unknown field "alpha"
  W0510 05:06:02.115246      22 warnings.go:70] unknown field "beta"
  W0510 05:06:02.115253      22 warnings.go:70] unknown field "delta"
  W0510 05:06:02.115259      22 warnings.go:70] unknown field "epsilon"
  W0510 05:06:02.115266      22 warnings.go:70] unknown field "gamma"
  May 10 05:06:02.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6498" for this suite. @ 05/10/23 05:06:02.132
• [2.576 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 05/10/23 05:06:02.138
  May 10 05:06:02.138: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 05:06:02.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:06:02.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:06:02.156
  STEP: Creating secret with name secret-test-d3f48b39-276a-4c47-a881-fb6ebeed683f @ 05/10/23 05:06:02.158
  STEP: Creating a pod to test consume secrets @ 05/10/23 05:06:02.163
  E0510 05:06:02.486371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:03.486638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:04.486719      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:05.486994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:06:06.187
  May 10 05:06:06.189: INFO: Trying to get logs from node node-02 pod pod-secrets-7b3fb235-fe20-4de6-bbf5-85bb6b0b3a71 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 05:06:06.192
  May 10 05:06:06.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5315" for this suite. @ 05/10/23 05:06:06.211
• [4.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 05/10/23 05:06:06.216
  May 10 05:06:06.216: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-probe @ 05/10/23 05:06:06.216
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:06:06.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:06:06.232
  STEP: Creating pod busybox-8002deea-de85-4281-b425-225d635492e9 in namespace container-probe-406 @ 05/10/23 05:06:06.234
  E0510 05:06:06.487747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:07.488645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:06:08.244: INFO: Started pod busybox-8002deea-de85-4281-b425-225d635492e9 in namespace container-probe-406
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/10/23 05:06:08.244
  May 10 05:06:08.246: INFO: Initial restart count of pod busybox-8002deea-de85-4281-b425-225d635492e9 is 0
  E0510 05:06:08.489281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:09.489571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:10.490284      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:11.490559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:12.491395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:13.491606      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:14.492032      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:15.492264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:16.493281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:17.494107      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:18.495127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:19.495374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:20.495992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:21.497031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:22.497672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:23.497967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:24.498897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:25.499024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:26.499594      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:27.500110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:28.500241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:29.500362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:30.500796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:31.501020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:32.501920      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:33.502130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:34.502844      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:35.503213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:36.504325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:37.504947      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:38.505558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:39.505723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:40.505776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:41.506105      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:42.507109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:43.507329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:44.507984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:45.508270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:46.508366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:47.509075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:48.510079      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:49.510291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:50.511281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:51.511512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:52.511604      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:53.511958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:54.512063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:55.512207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:56.512335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:57.512864      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:58.513221      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:06:59.513339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:00.514224      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:01.514578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:02.514713      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:03.514948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:04.515548      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:05.515800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:06.515935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:07.516057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:08.516766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:09.516919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:10.517867      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:11.518000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:12.518953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:13.519126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:14.519193      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:15.519316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:16.519718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:17.519796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:18.519941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:19.520105      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:20.520401      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:21.520564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:22.521495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:23.521703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:24.522603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:25.522907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:26.523256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:27.523680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:28.524406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:29.524905      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:30.525808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:31.526021      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:32.527082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:33.527312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:34.527974      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:35.528186      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:36.528486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:37.529080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:38.530020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:39.530311      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:40.531252      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:41.531473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:42.532435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:43.532590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:44.533293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:45.533519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:46.534434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:47.535013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:48.535576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:49.535798      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:50.536524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:51.536748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:52.537709      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:53.538324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:54.539133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:55.539371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:56.539396      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:57.539919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:58.540036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:07:59.540256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:00.541203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:01.541400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:02.542281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:03.542483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:04.543555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:05.543737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:06.544382      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:07.544946      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:08.545841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:09.546032      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:10.546655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:11.546855      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:12.547860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:13.548062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:14.548558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:15.548826      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:16.549148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:17.549610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:18.550128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:19.550327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:20.551065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:21.551276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:22.552280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:23.552624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:24.553410      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:25.553651      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:26.554205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:27.554600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:28.555373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:29.555582      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:30.556217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:31.556435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:32.556667      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:33.557084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:34.557859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:35.558101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:36.558528      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:37.558846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:38.559929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:39.560303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:40.561097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:41.561310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:42.562301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:43.562502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:44.562785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:45.563004      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:46.563829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:47.564437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:48.565094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:49.565296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:50.566137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:51.566407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:52.566468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:53.566844      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:54.567590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:55.567808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:56.568249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:57.568835      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:58.569962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:08:59.570355      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:00.571387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:01.571610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:02.572130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:03.572353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:04.573399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:05.573961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:06.574923      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:07.575435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:08.576119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:09.576354      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:10.577103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:11.577330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:12.577527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:13.577745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:14.578484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:15.578700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:16.579749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:17.580316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:18.580966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:19.581176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:20.581652      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:21.581901      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:22.582934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:23.583153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:24.583768      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:25.583995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:26.584423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:27.585177      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:28.585379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:29.586158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:30.586364      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:31.587018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:32.587341      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:33.587510      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:34.587750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:35.588353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:36.588697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:37.589197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:38.589425      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:39.590205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:40.590414      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:41.591255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:42.591722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:43.592333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:44.592552      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:45.592898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:46.593115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:47.593208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:48.593493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:49.593526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:50.593760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:51.594611      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:52.595120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:53.595574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:54.595679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:55.596519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:56.596829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:57.597691      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:58.597935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:09:59.598098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:00.598217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:01.599062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:02.599337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:03.600255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:04.600356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:05.600987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:06.601122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:07.601255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:08.601498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:10:08.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 05:10:08.66
  STEP: Destroying namespace "container-probe-406" for this suite. @ 05/10/23 05:10:08.681
• [242.471 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 05/10/23 05:10:08.688
  May 10 05:10:08.688: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 05:10:08.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:10:08.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:10:08.702
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/10/23 05:10:08.704
  E0510 05:10:09.601938      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:10.602353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:11.602963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:12.603131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:13.603291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:14.603481      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:10:14.72
  May 10 05:10:14.722: INFO: Trying to get logs from node node-02 pod pod-b731cf97-adcc-40a2-8b6a-86c8f6837bad container test-container: <nil>
  STEP: delete the pod @ 05/10/23 05:10:14.728
  May 10 05:10:14.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7874" for this suite. @ 05/10/23 05:10:14.748
• [6.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 05/10/23 05:10:14.753
  May 10 05:10:14.753: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename svcaccounts @ 05/10/23 05:10:14.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:10:14.767
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:10:14.769
  STEP: creating a ServiceAccount @ 05/10/23 05:10:14.771
  STEP: watching for the ServiceAccount to be added @ 05/10/23 05:10:14.777
  STEP: patching the ServiceAccount @ 05/10/23 05:10:14.778
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 05/10/23 05:10:14.782
  STEP: deleting the ServiceAccount @ 05/10/23 05:10:14.783
  May 10 05:10:14.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7130" for this suite. @ 05/10/23 05:10:14.795
• [0.046 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 05/10/23 05:10:14.799
  May 10 05:10:14.799: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename deployment @ 05/10/23 05:10:14.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:10:14.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:10:14.814
  May 10 05:10:14.822: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0510 05:10:15.604353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:16.604692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:17.605243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:18.605483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:19.605800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:10:19.825: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/10/23 05:10:19.825
  May 10 05:10:19.825: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 05/10/23 05:10:19.831
  May 10 05:10:19.837: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9762  dcf2fee3-f24f-40f5-8dcf-5b0d5ddf1719 1057007 1 2023-05-10 05:10:19 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-10 05:10:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003db90e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  May 10 05:10:19.838: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  May 10 05:10:19.838: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  May 10 05:10:19.838: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9762  566bdcc2-0d4a-4d5b-821c-17c7b10b6dd4 1057008 1 2023-05-10 05:10:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment dcf2fee3-f24f-40f5-8dcf-5b0d5ddf1719 0xc003db940f 0xc003db9420}] [] [{e2e.test Update apps/v1 2023-05-10 05:10:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 05:10:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-10 05:10:19 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"dcf2fee3-f24f-40f5-8dcf-5b0d5ddf1719\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003db94d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 10 05:10:19.847: INFO: Pod "test-cleanup-controller-6hg8c" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-6hg8c test-cleanup-controller- deployment-9762  ad34bc76-4a8a-4c9b-8fd7-5ea8afd2e0e7 1056987 0 2023-05-10 05:10:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:deec559b5e3381ee244cedaff226f42ee3fe42f7b5cfaab2d64af5a21c1412fe cni.projectcalico.org/podIP:100.114.252.170/32 cni.projectcalico.org/podIPs:100.114.252.170/32] [{apps/v1 ReplicaSet test-cleanup-controller 566bdcc2-0d4a-4d5b-821c-17c7b10b6dd4 0xc003db97d7 0xc003db97d8}] [] [{kube-controller-manager Update v1 2023-05-10 05:10:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"566bdcc2-0d4a-4d5b-821c-17c7b10b6dd4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 05:10:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 05:10:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.114.252.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jtgsf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jtgsf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:10:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:10:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:10:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:100.114.252.170,StartTime:2023-05-10 05:10:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 05:10:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://464256b3474ab4e8113186b6191634b88c80e6a3437651b2fd14fd8c2e482e40,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.114.252.170,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 05:10:19.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9762" for this suite. @ 05/10/23 05:10:19.849
• [5.059 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 05/10/23 05:10:19.858
  May 10 05:10:19.858: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/10/23 05:10:19.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:10:19.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:10:19.885
  STEP: fetching the /apis discovery document @ 05/10/23 05:10:19.886
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 05/10/23 05:10:19.887
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 05/10/23 05:10:19.887
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 05/10/23 05:10:19.887
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 05/10/23 05:10:19.888
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 05/10/23 05:10:19.888
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 05/10/23 05:10:19.889
  May 10 05:10:19.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4252" for this suite. @ 05/10/23 05:10:19.893
• [0.039 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 05/10/23 05:10:19.898
  May 10 05:10:19.898: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 05:10:19.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:10:19.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:10:19.909
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3144 @ 05/10/23 05:10:19.911
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/10/23 05:10:19.918
  STEP: creating service externalsvc in namespace services-3144 @ 05/10/23 05:10:19.918
  STEP: creating replication controller externalsvc in namespace services-3144 @ 05/10/23 05:10:19.928
  I0510 05:10:19.933047      22 runners.go:194] Created replication controller with name: externalsvc, namespace: services-3144, replica count: 2
  E0510 05:10:20.606964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:21.607473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:22.607898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 05:10:22.984507      22 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 05/10/23 05:10:22.986
  May 10 05:10:23.001: INFO: Creating new exec pod
  E0510 05:10:23.608927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:24.609431      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:10:25.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-3144 exec execpod4q86d -- /bin/sh -x -c nslookup clusterip-service.services-3144.svc.cluster.local'
  E0510 05:10:25.609886      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:26.610947      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:27.611451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:10:27.972: INFO: stderr: "+ nslookup clusterip-service.services-3144.svc.cluster.local\n"
  May 10 05:10:27.972: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3144.svc.cluster.local\tcanonical name = externalsvc.services-3144.svc.cluster.local.\nName:\texternalsvc.services-3144.svc.cluster.local\nAddress: 10.96.0.72\n\n"
  May 10 05:10:27.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-3144, will wait for the garbage collector to delete the pods @ 05/10/23 05:10:27.976
  May 10 05:10:28.034: INFO: Deleting ReplicationController externalsvc took: 4.760591ms
  May 10 05:10:28.134: INFO: Terminating ReplicationController externalsvc pods took: 100.624462ms
  E0510 05:10:28.611791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:29.612251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:10:30.461: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-3144" for this suite. @ 05/10/23 05:10:30.474
• [10.584 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 05/10/23 05:10:30.482
  May 10 05:10:30.482: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 05:10:30.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:10:30.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:10:30.499
  STEP: Creating configMap configmap-6564/configmap-test-4700d9c1-416a-44b0-83d0-19943acb0ce9 @ 05/10/23 05:10:30.501
  STEP: Creating a pod to test consume configMaps @ 05/10/23 05:10:30.506
  E0510 05:10:30.612564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:31.612896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:32.612997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:33.613124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:10:34.527
  May 10 05:10:34.529: INFO: Trying to get logs from node node-02 pod pod-configmaps-ed8eb219-2b86-4f7a-a5e4-b9a2a135ab55 container env-test: <nil>
  STEP: delete the pod @ 05/10/23 05:10:34.533
  May 10 05:10:34.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6564" for this suite. @ 05/10/23 05:10:34.55
• [4.073 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 05/10/23 05:10:34.555
  May 10 05:10:34.555: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename svcaccounts @ 05/10/23 05:10:34.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:10:34.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:10:34.574
  May 10 05:10:34.594: INFO: created pod
  E0510 05:10:34.614182      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:35.614661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:36.615097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:37.615418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:38.615645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:39.615760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:40.615930      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:10:40.753
  E0510 05:10:41.616144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:42.616278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:43.616310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:44.616426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:45.616501      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:46.616681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:47.616788      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:48.616903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:49.617020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:50.617129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:51.617267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:52.617375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:53.618441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:54.618579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:55.619078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:56.619219      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:57.619360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:58.619523      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:10:59.619629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:00.619721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:01.619916      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:02.620009      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:03.620134      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:04.620343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:05.620491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:06.620589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:07.620755      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:08.620919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:09.621039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:10.621202      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:11:10.753: INFO: polling logs
  May 10 05:11:10.758: INFO: Pod logs: 
  I0510 05:10:35.637798       1 log.go:198] OK: Got token
  I0510 05:10:35.637839       1 log.go:198] validating with in-cluster discovery
  I0510 05:10:35.638120       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0510 05:10:35.638148       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9842:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683696034, NotBefore:1683695434, IssuedAt:1683695434, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9842", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"79899db3-9d50-48ac-ae46-0eb47f6ee569"}}}
  I0510 05:10:35.689755       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0510 05:10:35.697110       1 log.go:198] OK: Validated signature on JWT
  I0510 05:10:35.697229       1 log.go:198] OK: Got valid claims from token!
  I0510 05:10:35.697259       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9842:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683696034, NotBefore:1683695434, IssuedAt:1683695434, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9842", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"79899db3-9d50-48ac-ae46-0eb47f6ee569"}}}

  May 10 05:11:10.759: INFO: completed pod
  May 10 05:11:10.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9842" for this suite. @ 05/10/23 05:11:10.766
• [36.215 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 05/10/23 05:11:10.77
  May 10 05:11:10.770: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:11:10.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:11:10.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:11:10.783
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-c0c10616-61ba-40f5-9be7-1069aa793d30 @ 05/10/23 05:11:10.787
  STEP: Creating the pod @ 05/10/23 05:11:10.794
  E0510 05:11:11.621825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:12.622278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-c0c10616-61ba-40f5-9be7-1069aa793d30 @ 05/10/23 05:11:12.813
  STEP: waiting to observe update in volume @ 05/10/23 05:11:12.817
  E0510 05:11:13.622401      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:14.622677      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:11:14.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4702" for this suite. @ 05/10/23 05:11:14.83
• [4.065 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 05/10/23 05:11:14.835
  May 10 05:11:14.835: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:11:14.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:11:14.853
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:11:14.855
  STEP: Setting up server cert @ 05/10/23 05:11:14.87
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:11:15.271
  STEP: Deploying the webhook pod @ 05/10/23 05:11:15.279
  STEP: Wait for the deployment to be ready @ 05/10/23 05:11:15.292
  May 10 05:11:15.301: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 05:11:15.623496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:16.623861      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:11:17.308
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:11:17.315
  E0510 05:11:17.624488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:11:18.316: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/10/23 05:11:18.41
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/10/23 05:11:18.434
  STEP: Deleting the collection of validation webhooks @ 05/10/23 05:11:18.456
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/10/23 05:11:18.501
  May 10 05:11:18.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4858" for this suite. @ 05/10/23 05:11:18.544
  STEP: Destroying namespace "webhook-markers-7484" for this suite. @ 05/10/23 05:11:18.554
• [3.725 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 05/10/23 05:11:18.561
  May 10 05:11:18.561: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename namespaces @ 05/10/23 05:11:18.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:11:18.578
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:11:18.581
  STEP: Read namespace status @ 05/10/23 05:11:18.583
  May 10 05:11:18.586: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 05/10/23 05:11:18.586
  May 10 05:11:18.593: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 05/10/23 05:11:18.593
  May 10 05:11:18.600: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  May 10 05:11:18.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9607" for this suite. @ 05/10/23 05:11:18.603
• [0.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 05/10/23 05:11:18.618
  May 10 05:11:18.618: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename limitrange @ 05/10/23 05:11:18.619
  E0510 05:11:18.624876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:11:18.636
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:11:18.639
  STEP: Creating LimitRange "e2e-limitrange-hlmjx" in namespace "limitrange-8508" @ 05/10/23 05:11:18.641
  STEP: Creating another limitRange in another namespace @ 05/10/23 05:11:18.646
  May 10 05:11:18.657: INFO: Namespace "e2e-limitrange-hlmjx-7206" created
  May 10 05:11:18.657: INFO: Creating LimitRange "e2e-limitrange-hlmjx" in namespace "e2e-limitrange-hlmjx-7206"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-hlmjx" @ 05/10/23 05:11:18.662
  May 10 05:11:18.664: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-hlmjx" in "limitrange-8508" namespace @ 05/10/23 05:11:18.665
  May 10 05:11:18.669: INFO: LimitRange "e2e-limitrange-hlmjx" has been patched
  STEP: Delete LimitRange "e2e-limitrange-hlmjx" by Collection with labelSelector: "e2e-limitrange-hlmjx=patched" @ 05/10/23 05:11:18.67
  STEP: Confirm that the limitRange "e2e-limitrange-hlmjx" has been deleted @ 05/10/23 05:11:18.674
  May 10 05:11:18.674: INFO: Requesting list of LimitRange to confirm quantity
  May 10 05:11:18.676: INFO: Found 0 LimitRange with label "e2e-limitrange-hlmjx=patched"
  May 10 05:11:18.676: INFO: LimitRange "e2e-limitrange-hlmjx" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-hlmjx" @ 05/10/23 05:11:18.677
  May 10 05:11:18.681: INFO: Found 1 limitRange
  May 10 05:11:18.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-8508" for this suite. @ 05/10/23 05:11:18.683
  STEP: Destroying namespace "e2e-limitrange-hlmjx-7206" for this suite. @ 05/10/23 05:11:18.687
• [0.073 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 05/10/23 05:11:18.692
  May 10 05:11:18.692: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename security-context @ 05/10/23 05:11:18.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:11:18.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:11:18.708
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/10/23 05:11:18.711
  E0510 05:11:19.625329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:20.625492      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:21.625630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:22.626196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:23.626468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:24.626901      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:11:24.728
  May 10 05:11:24.729: INFO: Trying to get logs from node node-02 pod security-context-3f70a6b1-0ae6-4d38-a6ee-a35ed4a529fc container test-container: <nil>
  STEP: delete the pod @ 05/10/23 05:11:24.733
  May 10 05:11:24.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9354" for this suite. @ 05/10/23 05:11:24.747
• [6.064 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 05/10/23 05:11:24.758
  May 10 05:11:24.758: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 05:11:24.759
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:11:24.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:11:24.771
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 05:11:24.773
  E0510 05:11:25.627081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:26.627339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:27.627898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:28.628043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:29.628374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:30.628595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:11:30.794
  May 10 05:11:30.796: INFO: Trying to get logs from node node-02 pod downwardapi-volume-4bcfb502-05b2-40d4-854b-8fa3ca42b4e2 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 05:11:30.8
  May 10 05:11:30.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1575" for this suite. @ 05/10/23 05:11:30.819
• [6.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 05/10/23 05:11:30.825
  May 10 05:11:30.825: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename daemonsets @ 05/10/23 05:11:30.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:11:30.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:11:30.836
  May 10 05:11:30.855: INFO: Create a RollingUpdate DaemonSet
  May 10 05:11:30.859: INFO: Check that daemon pods launch on every node of the cluster
  May 10 05:11:30.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 05:11:30.863: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 05:11:31.628742      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:11:31.869: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 05:11:31.869: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 05:11:32.628793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:11:32.869: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 10 05:11:32.869: INFO: Node ub-test is running 0 daemon pod, expected 1
  E0510 05:11:33.629672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:11:33.869: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 10 05:11:33.869: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  May 10 05:11:33.869: INFO: Update the DaemonSet to trigger a rollout
  May 10 05:11:33.876: INFO: Updating DaemonSet daemon-set
  E0510 05:11:34.630885      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:35.630805      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:11:35.884: INFO: Roll back the DaemonSet before rollout is complete
  May 10 05:11:35.893: INFO: Updating DaemonSet daemon-set
  May 10 05:11:35.893: INFO: Make sure DaemonSet rollback is complete
  May 10 05:11:35.895: INFO: Wrong image for pod: daemon-set-tnvvt. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  May 10 05:11:35.895: INFO: Pod daemon-set-tnvvt is not available
  E0510 05:11:36.630949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:37.631478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:38.631808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:39.632007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:40.633042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:11:40.900: INFO: Pod daemon-set-ngp85 is not available
  STEP: Deleting DaemonSet "daemon-set" @ 05/10/23 05:11:40.906
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2631, will wait for the garbage collector to delete the pods @ 05/10/23 05:11:40.906
  May 10 05:11:40.963: INFO: Deleting DaemonSet.extensions daemon-set took: 4.366707ms
  May 10 05:11:41.063: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.635743ms
  E0510 05:11:41.634001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:42.634030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:11:43.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 05:11:43.166: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 10 05:11:43.167: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1057818"},"items":null}

  May 10 05:11:43.169: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1057818"},"items":null}

  May 10 05:11:43.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2631" for this suite. @ 05/10/23 05:11:43.177
• [12.357 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 05/10/23 05:11:43.184
  May 10 05:11:43.184: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-probe @ 05/10/23 05:11:43.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:11:43.199
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:11:43.201
  STEP: Creating pod test-webserver-fb79e20b-b8ef-4134-a241-3dea640135e1 in namespace container-probe-7450 @ 05/10/23 05:11:43.203
  E0510 05:11:43.634810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:44.635131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:11:45.213: INFO: Started pod test-webserver-fb79e20b-b8ef-4134-a241-3dea640135e1 in namespace container-probe-7450
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/10/23 05:11:45.213
  May 10 05:11:45.216: INFO: Initial restart count of pod test-webserver-fb79e20b-b8ef-4134-a241-3dea640135e1 is 0
  E0510 05:11:45.636243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:46.636480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:47.636684      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:48.636914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:49.637277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:50.637516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:51.638106      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:52.638274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:53.638593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:54.638610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:55.639377      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:56.639539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:57.640591      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:58.640819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:11:59.641578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:00.641702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:01.642283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:02.642703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:03.643350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:04.643563      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:05.643872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:06.644081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:07.644837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:08.644994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:09.645061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:10.645311      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:11.645954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:12.646088      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:13.646822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:14.647029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:15.647253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:16.647470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:17.648008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:18.648236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:19.648434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:20.648640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:21.649603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:22.650008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:23.650272      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:24.650390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:25.650600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:26.650798      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:27.651561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:28.651795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:29.652106      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:30.652256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:31.653179      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:32.653331      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:33.653972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:34.654167      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:35.655101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:36.655310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:37.655384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:38.655536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:39.655598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:40.655733      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:41.656596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:42.656929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:43.657638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:44.657778      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:45.658197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:46.658397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:47.659131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:48.659270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:49.659504      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:50.659689      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:51.660254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:52.660632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:53.661205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:54.661423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:55.661775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:56.661992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:57.662432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:58.662637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:12:59.663268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:00.663452      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:01.663588      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:02.663973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:03.664091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:04.664614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:05.664965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:06.665074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:07.665930      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:08.666172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:09.666991      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:10.667162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:11.667858      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:12.668282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:13.668985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:14.669122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:15.669261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:16.669451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:17.669664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:18.669806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:19.670228      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:20.670438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:21.670871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:22.671328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:23.671841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:24.671970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:25.672119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:26.672352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:27.673390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:28.673607      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:29.674692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:30.674846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:31.674894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:32.675371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:33.675438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:34.675800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:35.676117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:36.676342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:37.676594      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:38.676717      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:39.677573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:40.677778      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:41.677931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:42.678299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:43.678730      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:44.678931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:45.679869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:46.680085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:47.681118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:48.681336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:49.682323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:50.682535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:51.683288      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:52.683440      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:53.683723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:54.683963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:55.684963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:56.685124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:57.685435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:58.685676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:13:59.686223      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:00.686476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:01.687414      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:02.687832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:03.688463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:04.688689      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:05.689496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:06.689704      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:07.689765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:08.689973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:09.690147      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:10.690376      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:11.691129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:12.691508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:13.692292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:14.692481      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:15.692628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:16.692810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:17.693069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:18.693306      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:19.694018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:20.694288      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:21.695245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:22.695388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:23.695646      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:24.695867      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:25.696439      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:26.696661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:27.697023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:28.697171      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:29.698202      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:30.699118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:31.699770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:32.699907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:33.700539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:34.700584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:35.701633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:36.701888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:37.702692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:38.702870      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:39.703711      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:40.703995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:41.704540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:42.704656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:43.705168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:44.705378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:45.705503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:46.705713      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:47.706140      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:48.706404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:49.707004      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:50.707199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:51.707446      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:52.707859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:53.708280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:54.708627      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:55.709623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:56.709716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:57.710806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:58.711030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:14:59.711218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:00.711428      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:01.712488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:02.712857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:03.713594      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:04.714542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:05.715456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:06.715637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:07.716057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:08.716422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:09.716530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:10.716659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:11.717716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:12.717894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:13.718832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:14.719071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:15.719258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:16.719354      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:17.720245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:18.720456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:19.721425      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:20.721565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:21.721649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:22.722074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:23.722521      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:24.722759      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:25.723034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:26.723258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:27.724055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:28.724284      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:29.724809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:30.725255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:31.726209      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:32.726576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:33.726705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:34.727103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:35.727187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:36.727410      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:37.727470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:38.727698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:39.727767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:40.727961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:41.728569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:42.728953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:43.729619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:44.729814      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:15:45.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 05:15:45.612
  STEP: Destroying namespace "container-probe-7450" for this suite. @ 05/10/23 05:15:45.625
• [242.447 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 05/10/23 05:15:45.631
  May 10 05:15:45.631: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename deployment @ 05/10/23 05:15:45.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:15:45.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:15:45.647
  STEP: creating a Deployment @ 05/10/23 05:15:45.651
  STEP: waiting for Deployment to be created @ 05/10/23 05:15:45.658
  STEP: waiting for all Replicas to be Ready @ 05/10/23 05:15:45.659
  May 10 05:15:45.660: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 10 05:15:45.660: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 10 05:15:45.671: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 10 05:15:45.671: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 10 05:15:45.684: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 10 05:15:45.684: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 10 05:15:45.709: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 10 05:15:45.709: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0510 05:15:45.730042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:15:46.681: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May 10 05:15:46.681: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  E0510 05:15:46.730084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:15:47.077: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 05/10/23 05:15:47.077
  W0510 05:15:47.086313      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 10 05:15:47.087: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 05/10/23 05:15:47.087
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 0
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:47.088: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:47.105: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:47.105: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:47.123: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:47.123: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:47.131: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  May 10 05:15:47.131: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  May 10 05:15:47.139: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  May 10 05:15:47.139: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  E0510 05:15:47.730300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:48.730514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:15:49.093: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:49.093: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:49.115: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  STEP: listing Deployments @ 05/10/23 05:15:49.115
  May 10 05:15:49.117: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 05/10/23 05:15:49.117
  May 10 05:15:49.129: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 05/10/23 05:15:49.129
  May 10 05:15:49.133: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 10 05:15:49.140: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 10 05:15:49.170: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 10 05:15:49.193: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 10 05:15:49.199: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0510 05:15:49.730515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:15:50.617: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0510 05:15:50.731503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:15:51.129: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  May 10 05:15:51.147: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 10 05:15:51.170: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0510 05:15:51.732467      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:15:52.694: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 05/10/23 05:15:52.717
  STEP: fetching the DeploymentStatus @ 05/10/23 05:15:52.722
  May 10 05:15:52.726: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  May 10 05:15:52.726: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  May 10 05:15:52.726: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  May 10 05:15:52.726: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  May 10 05:15:52.726: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 1
  May 10 05:15:52.726: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:52.726: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 3
  May 10 05:15:52.726: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:52.726: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 2
  May 10 05:15:52.726: INFO: observed Deployment test-deployment in namespace deployment-6651 with ReadyReplicas 3
  STEP: deleting the Deployment @ 05/10/23 05:15:52.726
  E0510 05:15:52.733051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:15:53.278: INFO: observed event type MODIFIED
  May 10 05:15:53.278: INFO: observed event type MODIFIED
  May 10 05:15:53.278: INFO: observed event type MODIFIED
  May 10 05:15:53.278: INFO: observed event type MODIFIED
  May 10 05:15:53.278: INFO: observed event type MODIFIED
  May 10 05:15:53.278: INFO: observed event type MODIFIED
  May 10 05:15:53.278: INFO: observed event type MODIFIED
  May 10 05:15:53.278: INFO: observed event type MODIFIED
  May 10 05:15:53.278: INFO: observed event type MODIFIED
  May 10 05:15:53.278: INFO: observed event type MODIFIED
  May 10 05:15:53.278: INFO: observed event type MODIFIED
  May 10 05:15:53.363: INFO: Log out all the ReplicaSets if there is no deployment created
  May 10 05:15:53.366: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-6651  b857f4ea-ddf3-41c3-a64d-fcc874408d74 1058633 3 2023-05-10 05:15:45 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 78af6186-9b12-453c-9b5f-d82d4456f6d6 0xc002d53e97 0xc002d53e98}] [] [{kube-controller-manager Update apps/v1 2023-05-10 05:15:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78af6186-9b12-453c-9b5f-d82d4456f6d6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 05:15:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d53f20 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May 10 05:15:53.368: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-6651  0012f9cb-8a96-46f8-9bba-840d6ac4addd 1058749 4 2023-05-10 05:15:47 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 78af6186-9b12-453c-9b5f-d82d4456f6d6 0xc002d53f87 0xc002d53f88}] [] [{kube-controller-manager Update apps/v1 2023-05-10 05:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78af6186-9b12-453c-9b5f-d82d4456f6d6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 05:15:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005002010 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May 10 05:15:53.416: INFO: pod: "test-deployment-5b5dcbcd95-khpdc":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-khpdc test-deployment-5b5dcbcd95- deployment-6651  b0ed177f-d10d-4671-a5eb-8579c0047fdc 1058752 0 2023-05-10 05:15:47 +0000 UTC 2023-05-10 05:15:52 +0000 UTC 0xc005002618 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:6777fdb82a2d19a7c5500a0b14c040969b3bf17d5a57b585cf9522f8f8ecfaa8 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 0012f9cb-8a96-46f8-9bba-840d6ac4addd 0xc005002647 0xc005002648}] [] [{kube-controller-manager Update v1 2023-05-10 05:15:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0012f9cb-8a96-46f8-9bba-840d6ac4addd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 05:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 05:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.114.252.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7qrpw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7qrpw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Succeeded,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:47 +0000 UTC,Reason:PodCompleted,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:52 +0000 UTC,Reason:PodCompleted,Message:,},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:52 +0000 UTC,Reason:PodCompleted,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:100.114.252.171,StartTime:2023-05-10 05:15:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:nil,Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-05-10 05:15:48 +0000 UTC,FinishedAt:2023-05-10 05:15:51 +0000 UTC,ContainerID:containerd://34e61795a530291d7900b5a2f6a61353caa175437579367cbb5afb42c23c51b2,},},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:sealos.hub:5000/pause:3.9,ImageID:sealos.hub:5000/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://34e61795a530291d7900b5a2f6a61353caa175437579367cbb5afb42c23c51b2,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.114.252.171,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 10 05:15:53.416: INFO: pod: "test-deployment-5b5dcbcd95-vfmp9":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-vfmp9 test-deployment-5b5dcbcd95- deployment-6651  76bc963c-0672-43f8-9c7b-63d2f8da79cc 1058744 0 2023-05-10 05:15:49 +0000 UTC 2023-05-10 05:15:53 +0000 UTC 0xc005002878 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:5ffc2e7ce9c19332a5488feff84d33ee7ed4e94efc9a53a71bbc9ca2acf7ff7d cni.projectcalico.org/podIP:100.74.79.7/32 cni.projectcalico.org/podIPs:100.74.79.7/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 0012f9cb-8a96-46f8-9bba-840d6ac4addd 0xc0050028c7 0xc0050028c8}] [] [{calico Update v1 2023-05-10 05:15:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-10 05:15:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0012f9cb-8a96-46f8-9bba-840d6ac4addd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 05:15:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.74.79.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zs88z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zs88z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.152,PodIP:100.74.79.7,StartTime:2023-05-10 05:15:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 05:15:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:sealos.hub:5000/pause:3.9,ImageID:sealos.hub:5000/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://a5297992eaf9d656352556f064f8b4d08d8d6a6327c8a378cd2265d2fb8ddbe5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.74.79.7,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 10 05:15:53.416: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-6651  28f638c0-8175-45e2-857c-75d3e4831b0b 1058741 2 2023-05-10 05:15:49 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 78af6186-9b12-453c-9b5f-d82d4456f6d6 0xc005002077 0xc005002078}] [] [{kube-controller-manager Update apps/v1 2023-05-10 05:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78af6186-9b12-453c-9b5f-d82d4456f6d6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 05:15:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005002100 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  May 10 05:15:53.422: INFO: pod: "test-deployment-6fc78d85c6-9vjns":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-9vjns test-deployment-6fc78d85c6- deployment-6651  97fa7668-0d2a-4ac4-ace0-caff7f8ad80b 1058703 0 2023-05-10 05:15:49 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:e91303f8705056e5044ca698950032d54a5ea715550609d7f2a9a99713d60052 cni.projectcalico.org/podIP:100.114.252.178/32 cni.projectcalico.org/podIPs:100.114.252.178/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 28f638c0-8175-45e2-857c-75d3e4831b0b 0xc004ba9647 0xc004ba9648}] [] [{kube-controller-manager Update v1 2023-05-10 05:15:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28f638c0-8175-45e2-857c-75d3e4831b0b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 05:15:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 05:15:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.114.252.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plhbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plhbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:100.114.252.178,StartTime:2023-05-10 05:15:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 05:15:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://72508fbb3e3ccab8c6a97a5aeab4619cbaca85f7843162345cd13618d5d738bc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.114.252.178,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 10 05:15:53.422: INFO: pod: "test-deployment-6fc78d85c6-n6tqs":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-n6tqs test-deployment-6fc78d85c6- deployment-6651  d6a02bd5-dbd4-4b40-9a02-41095a250803 1058740 0 2023-05-10 05:15:51 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:d5e904210e8b3e299a235424a837e1a4f8b450296819cc7a2e8f743ff9372f08 cni.projectcalico.org/podIP:100.67.79.189/32 cni.projectcalico.org/podIPs:100.67.79.189/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 28f638c0-8175-45e2-857c-75d3e4831b0b 0xc004ba9897 0xc004ba9898}] [] [{calico Update v1 2023-05-10 05:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-10 05:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28f638c0-8175-45e2-857c-75d3e4831b0b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 05:15:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.67.79.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttkb5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttkb5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:15:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.83,PodIP:100.67.79.189,StartTime:2023-05-10 05:15:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 05:15:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://d0d396a61516c822082b70030523d0fbf34c7113dc9b8f5f2b515231dab09e2f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.67.79.189,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 10 05:15:53.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6651" for this suite. @ 05/10/23 05:15:53.424
• [7.813 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 05/10/23 05:15:53.445
  May 10 05:15:53.445: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename gc @ 05/10/23 05:15:53.445
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:15:53.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:15:53.456
  STEP: create the rc1 @ 05/10/23 05:15:53.46
  STEP: create the rc2 @ 05/10/23 05:15:53.468
  E0510 05:15:53.733425      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:54.733647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:55.734430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:56.734639      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 05/10/23 05:15:57.504
  E0510 05:15:57.735673      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:58.735969      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:15:59.736672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 05/10/23 05:15:59.786
  STEP: wait for the rc to be deleted @ 05/10/23 05:15:59.836
  E0510 05:16:00.736788      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:01.736903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:02.737033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:03.737152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:04.737367      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:16:05.283: INFO: 75 pods remaining
  May 10 05:16:05.284: INFO: 73 pods has nil DeletionTimestamp
  May 10 05:16:05.284: INFO: 
  E0510 05:16:05.737507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:06.737821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:07.738292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:08.738510      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:09.738718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:16:10.584: INFO: 53 pods remaining
  May 10 05:16:10.584: INFO: 50 pods has nil DeletionTimestamp
  May 10 05:16:10.584: INFO: 
  E0510 05:16:10.739775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:11.739916      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:12.740304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:13.740364      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:14.741440      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/10/23 05:16:14.846
  May 10 05:16:15.141: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 10 05:16:15.141: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jt9w" in namespace "gc-1193"
  May 10 05:16:15.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-2p2p8" in namespace "gc-1193"
  May 10 05:16:15.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-47mf7" in namespace "gc-1193"
  May 10 05:16:15.209: INFO: Deleting pod "simpletest-rc-to-be-deleted-4chx6" in namespace "gc-1193"
  May 10 05:16:15.223: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mgrb" in namespace "gc-1193"
  May 10 05:16:15.239: INFO: Deleting pod "simpletest-rc-to-be-deleted-4r5hg" in namespace "gc-1193"
  May 10 05:16:15.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-4smlp" in namespace "gc-1193"
  May 10 05:16:15.260: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vtgs" in namespace "gc-1193"
  May 10 05:16:15.272: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wx92" in namespace "gc-1193"
  May 10 05:16:15.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-52sb2" in namespace "gc-1193"
  May 10 05:16:15.294: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cvs5" in namespace "gc-1193"
  May 10 05:16:15.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-5kmbr" in namespace "gc-1193"
  May 10 05:16:15.320: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qlxv" in namespace "gc-1193"
  May 10 05:16:15.330: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h4hn" in namespace "gc-1193"
  May 10 05:16:15.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h5s6" in namespace "gc-1193"
  May 10 05:16:15.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p7g2" in namespace "gc-1193"
  May 10 05:16:15.378: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rq28" in namespace "gc-1193"
  May 10 05:16:15.398: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sbbw" in namespace "gc-1193"
  May 10 05:16:15.415: INFO: Deleting pod "simpletest-rc-to-be-deleted-8948z" in namespace "gc-1193"
  E0510 05:16:15.741484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:16.741877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:17.742480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:16:17.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-8b6nl" in namespace "gc-1193"
  May 10 05:16:18.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jwdf" in namespace "gc-1193"
  E0510 05:16:18.742536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:16:19.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-92czp" in namespace "gc-1193"
  May 10 05:16:19.085: INFO: Deleting pod "simpletest-rc-to-be-deleted-96jsq" in namespace "gc-1193"
  May 10 05:16:19.509: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mvx8" in namespace "gc-1193"
  May 10 05:16:19.528: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vsr7" in namespace "gc-1193"
  May 10 05:16:19.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-bs2nl" in namespace "gc-1193"
  May 10 05:16:19.588: INFO: Deleting pod "simpletest-rc-to-be-deleted-bszxr" in namespace "gc-1193"
  E0510 05:16:19.743210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:16:20.243: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmsrz" in namespace "gc-1193"
  May 10 05:16:20.268: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnxvk" in namespace "gc-1193"
  May 10 05:16:20.291: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2l9j" in namespace "gc-1193"
  E0510 05:16:20.744274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:16:20.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-dg72f" in namespace "gc-1193"
  May 10 05:16:20.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhmn5" in namespace "gc-1193"
  May 10 05:16:20.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkvrv" in namespace "gc-1193"
  May 10 05:16:21.021: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwb96" in namespace "gc-1193"
  May 10 05:16:21.030: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8v6x" in namespace "gc-1193"
  May 10 05:16:21.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjbpk" in namespace "gc-1193"
  E0510 05:16:21.744351      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:16:21.895: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkbs8" in namespace "gc-1193"
  May 10 05:16:21.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-fngzl" in namespace "gc-1193"
  May 10 05:16:21.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftr49" in namespace "gc-1193"
  May 10 05:16:21.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-gh6wm" in namespace "gc-1193"
  May 10 05:16:22.000: INFO: Deleting pod "simpletest-rc-to-be-deleted-gj5wx" in namespace "gc-1193"
  May 10 05:16:22.014: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtfbq" in namespace "gc-1193"
  May 10 05:16:22.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtmnr" in namespace "gc-1193"
  E0510 05:16:22.744826      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:16:23.433: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwktm" in namespace "gc-1193"
  May 10 05:16:23.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7746" in namespace "gc-1193"
  E0510 05:16:23.745476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:16:23.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-hc66h" in namespace "gc-1193"
  May 10 05:16:24.006: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhbl9" in namespace "gc-1193"
  E0510 05:16:24.746546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:16:25.088: INFO: Deleting pod "simpletest-rc-to-be-deleted-hppcr" in namespace "gc-1193"
  May 10 05:16:25.146: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvs5g" in namespace "gc-1193"
  May 10 05:16:25.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6xrq" in namespace "gc-1193"
  May 10 05:16:25.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1193" for this suite. @ 05/10/23 05:16:25.668
• [32.230 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 05/10/23 05:16:25.675
  May 10 05:16:25.675: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename endpointslice @ 05/10/23 05:16:25.676
  E0510 05:16:25.747340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:16:26.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:16:26.208
  E0510 05:16:26.747354      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:27.748336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:28.748448      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:29.748558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:30.748676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 05/10/23 05:16:31.311
  E0510 05:16:31.748856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:32.749256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:33.749527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:34.749842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:35.750045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 05/10/23 05:16:36.324
  E0510 05:16:36.750776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:37.751351      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:38.751584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:39.751793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:40.752052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 05/10/23 05:16:41.329
  E0510 05:16:41.752986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:42.753466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:43.753758      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:44.754039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:45.754523      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 05/10/23 05:16:46.336
  May 10 05:16:46.348: INFO: EndpointSlice for Service endpointslice-3156/example-named-port not found
  E0510 05:16:46.754684      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:47.755234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:48.755441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:49.755632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:50.755957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:51.756821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:52.757177      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:53.757371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:54.757556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:55.757785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:16:56.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3156" for this suite. @ 05/10/23 05:16:56.357
• [30.689 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 05/10/23 05:16:56.365
  May 10 05:16:56.365: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/10/23 05:16:56.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:16:56.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:16:56.379
  May 10 05:16:56.383: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 05:16:56.757840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:16:57.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-224" for this suite. @ 05/10/23 05:16:57.433
• [1.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 05/10/23 05:16:57.448
  May 10 05:16:57.448: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:16:57.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:16:57.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:16:57.463
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 05:16:57.465
  E0510 05:16:57.758585      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:58.758865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:16:59.759349      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:00.759589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:17:01.491
  May 10 05:17:01.494: INFO: Trying to get logs from node node-02 pod downwardapi-volume-77c749d3-d46e-4db0-8948-c2560915ee8a container client-container: <nil>
  STEP: delete the pod @ 05/10/23 05:17:01.502
  May 10 05:17:01.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-16" for this suite. @ 05/10/23 05:17:01.524
• [4.080 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 05/10/23 05:17:01.529
  May 10 05:17:01.529: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:17:01.53
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:17:01.54
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:17:01.542
  STEP: Setting up server cert @ 05/10/23 05:17:01.561
  E0510 05:17:01.760572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:17:01.911
  STEP: Deploying the webhook pod @ 05/10/23 05:17:01.917
  STEP: Wait for the deployment to be ready @ 05/10/23 05:17:01.935
  May 10 05:17:01.944: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 05:17:02.760799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:03.761104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:17:03.952: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 17, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 17, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 17, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 17, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:17:04.761230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:05.761307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:17:05.955
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:17:05.963
  E0510 05:17:06.761836      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:17:06.964: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 05/10/23 05:17:06.967
  STEP: create a pod that should be updated by the webhook @ 05/10/23 05:17:06.985
  May 10 05:17:07.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2258" for this suite. @ 05/10/23 05:17:07.054
  STEP: Destroying namespace "webhook-markers-5571" for this suite. @ 05/10/23 05:17:07.062
• [5.539 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 05/10/23 05:17:07.068
  May 10 05:17:07.068: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:17:07.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:17:07.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:17:07.086
  STEP: Setting up server cert @ 05/10/23 05:17:07.112
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:17:07.431
  STEP: Deploying the webhook pod @ 05/10/23 05:17:07.436
  STEP: Wait for the deployment to be ready @ 05/10/23 05:17:07.453
  May 10 05:17:07.458: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 05:17:07.762020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:08.762268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:17:09.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 17, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 17, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 17, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 17, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:17:09.763111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:10.763154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:17:11.468
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:17:11.476
  E0510 05:17:11.763317      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:17:12.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 05/10/23 05:17:12.479
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/10/23 05:17:12.479
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 05/10/23 05:17:12.5
  E0510 05:17:12.763974      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 05/10/23 05:17:13.509
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/10/23 05:17:13.509
  E0510 05:17:13.764737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 05/10/23 05:17:14.707
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/10/23 05:17:14.707
  E0510 05:17:14.765273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:15.765453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:16.765543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:17.766164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:18.766404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 05/10/23 05:17:19.763
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/10/23 05:17:19.763
  E0510 05:17:19.767038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:20.767206      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:21.767486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:22.767891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:23.768010      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:24.768249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:17:24.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7487" for this suite. @ 05/10/23 05:17:24.887
  STEP: Destroying namespace "webhook-markers-2370" for this suite. @ 05/10/23 05:17:24.909
• [17.847 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 05/10/23 05:17:24.916
  May 10 05:17:24.916: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename watch @ 05/10/23 05:17:24.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:17:24.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:17:24.937
  STEP: getting a starting resourceVersion @ 05/10/23 05:17:24.94
  STEP: starting a background goroutine to produce watch events @ 05/10/23 05:17:24.942
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 05/10/23 05:17:24.942
  E0510 05:17:25.769077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:26.769929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:17:27.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0510 05:17:27.770808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "watch-4890" for this suite. @ 05/10/23 05:17:27.837
• [2.928 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 05/10/23 05:17:27.845
  May 10 05:17:27.845: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:17:27.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:17:27.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:17:27.865
  STEP: Creating configMap with name projected-configmap-test-volume-map-d1083a76-1114-42e1-b1d9-6e317fb87fa1 @ 05/10/23 05:17:27.867
  STEP: Creating a pod to test consume configMaps @ 05/10/23 05:17:27.872
  E0510 05:17:28.771971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:29.772489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:30.772748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:31.773065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:32.773485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:33.773873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:17:33.895
  May 10 05:17:33.897: INFO: Trying to get logs from node node-02 pod pod-projected-configmaps-2bd7cd06-ce54-4605-931f-b81d78517d2b container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 05:17:33.901
  May 10 05:17:33.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2172" for this suite. @ 05/10/23 05:17:33.926
• [6.086 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 05/10/23 05:17:33.931
  May 10 05:17:33.931: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename field-validation @ 05/10/23 05:17:33.931
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:17:33.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:17:33.949
  STEP: apply creating a deployment @ 05/10/23 05:17:33.951
  May 10 05:17:33.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5925" for this suite. @ 05/10/23 05:17:33.962
• [0.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 05/10/23 05:17:33.969
  May 10 05:17:33.969: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename dns @ 05/10/23 05:17:33.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:17:33.986
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:17:33.988
  STEP: Creating a test headless service @ 05/10/23 05:17:33.99
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-693.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-693.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-693.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-693.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-693.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-693.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 63.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.63_udp@PTR;check="$$(dig +tcp +noall +answer +search 63.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.63_tcp@PTR;sleep 1; done
   @ 05/10/23 05:17:34.013
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-693.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-693.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-693.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-693.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-693.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-693.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 63.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.63_udp@PTR;check="$$(dig +tcp +noall +answer +search 63.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.63_tcp@PTR;sleep 1; done
   @ 05/10/23 05:17:34.013
  STEP: creating a pod to probe DNS @ 05/10/23 05:17:34.013
  STEP: submitting the pod to kubernetes @ 05/10/23 05:17:34.013
  E0510 05:17:34.774457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:35.775278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:36.775400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:37.776224      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/10/23 05:17:38.04
  STEP: looking for the results for each expected name from probers @ 05/10/23 05:17:38.042
  May 10 05:17:38.045: INFO: Unable to read wheezy_udp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:38.047: INFO: Unable to read wheezy_tcp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:38.049: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:38.051: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:38.060: INFO: Unable to read jessie_udp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:38.062: INFO: Unable to read jessie_tcp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:38.064: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:38.067: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:38.075: INFO: Lookups using dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9 failed for: [wheezy_udp@dns-test-service.dns-693.svc.cluster.local wheezy_tcp@dns-test-service.dns-693.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-693.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local jessie_udp@dns-test-service.dns-693.svc.cluster.local jessie_tcp@dns-test-service.dns-693.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-693.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local]

  E0510 05:17:38.776871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:39.777087      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:40.778112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:41.778306      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:42.778438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:17:43.079: INFO: Unable to read wheezy_udp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:43.081: INFO: Unable to read wheezy_tcp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:43.083: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:43.086: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:43.096: INFO: Unable to read jessie_udp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:43.098: INFO: Unable to read jessie_tcp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:43.101: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:43.111: INFO: Lookups using dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9 failed for: [wheezy_udp@dns-test-service.dns-693.svc.cluster.local wheezy_tcp@dns-test-service.dns-693.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-693.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local jessie_udp@dns-test-service.dns-693.svc.cluster.local jessie_tcp@dns-test-service.dns-693.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local]

  E0510 05:17:43.778713      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:44.778931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:45.779457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:46.779660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:47.779757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:17:48.078: INFO: Unable to read wheezy_udp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:48.080: INFO: Unable to read wheezy_tcp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:48.082: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:48.084: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:48.110: INFO: Lookups using dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9 failed for: [wheezy_udp@dns-test-service.dns-693.svc.cluster.local wheezy_tcp@dns-test-service.dns-693.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-693.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local]

  E0510 05:17:48.779925      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:49.780223      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:50.780465      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:51.780587      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:52.780968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:17:53.084: INFO: Unable to read wheezy_tcp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:53.088: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:53.114: INFO: Lookups using dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9 failed for: [wheezy_tcp@dns-test-service.dns-693.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local]

  E0510 05:17:53.781100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:54.781300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:55.781526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:56.781723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:57.782292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:17:58.081: INFO: Unable to read wheezy_tcp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:58.085: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:17:58.109: INFO: Lookups using dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9 failed for: [wheezy_tcp@dns-test-service.dns-693.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local]

  E0510 05:17:58.782856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:17:59.783086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:00.783227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:01.783474      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:02.783758      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:18:03.080: INFO: Unable to read wheezy_tcp@dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:18:03.084: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local from pod dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9: the server could not find the requested resource (get pods dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9)
  May 10 05:18:03.108: INFO: Lookups using dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9 failed for: [wheezy_tcp@dns-test-service.dns-693.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-693.svc.cluster.local]

  E0510 05:18:03.784776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:04.785019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:05.785246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:06.785444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:07.785536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:18:08.108: INFO: DNS probes using dns-693/dns-test-3363cd7f-b42c-46c0-baf5-a429fd07cde9 succeeded

  May 10 05:18:08.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 05:18:08.11
  STEP: deleting the test service @ 05/10/23 05:18:08.131
  STEP: deleting the test headless service @ 05/10/23 05:18:08.154
  STEP: Destroying namespace "dns-693" for this suite. @ 05/10/23 05:18:08.17
• [34.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 05/10/23 05:18:08.182
  May 10 05:18:08.182: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 05:18:08.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:18:08.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:18:08.196
  STEP: Creating a pod to test downward api env vars @ 05/10/23 05:18:08.199
  E0510 05:18:08.785576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:09.786012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:10.786515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:11.786714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:18:12.213
  May 10 05:18:12.214: INFO: Trying to get logs from node ub-test pod downward-api-a9c8c320-2348-4fe0-91b1-7c2ffa416e75 container dapi-container: <nil>
  STEP: delete the pod @ 05/10/23 05:18:12.218
  May 10 05:18:12.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5587" for this suite. @ 05/10/23 05:18:12.241
• [4.063 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 05/10/23 05:18:12.245
  May 10 05:18:12.245: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 05:18:12.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:18:12.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:18:12.261
  STEP: Creating Pod @ 05/10/23 05:18:12.263
  E0510 05:18:12.787115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:13.787864      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 05/10/23 05:18:14.277
  May 10 05:18:14.277: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8302 PodName:pod-sharedvolume-6eba16a4-3728-4285-a7cd-2abe04009391 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:18:14.277: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:18:14.277: INFO: ExecWithOptions: Clientset creation
  May 10 05:18:14.277: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8302/pods/pod-sharedvolume-6eba16a4-3728-4285-a7cd-2abe04009391/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  May 10 05:18:14.556: INFO: Exec stderr: ""
  May 10 05:18:14.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8302" for this suite. @ 05/10/23 05:18:14.559
• [2.325 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 05/10/23 05:18:14.572
  May 10 05:18:14.572: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/10/23 05:18:14.573
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:18:14.584
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:18:14.586
  STEP: creating @ 05/10/23 05:18:14.587
  STEP: getting @ 05/10/23 05:18:14.604
  STEP: listing @ 05/10/23 05:18:14.607
  STEP: deleting @ 05/10/23 05:18:14.608
  May 10 05:18:14.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-8376" for this suite. @ 05/10/23 05:18:14.626
• [0.059 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 05/10/23 05:18:14.631
  May 10 05:18:14.631: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-runtime @ 05/10/23 05:18:14.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:18:14.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:18:14.647
  STEP: create the container @ 05/10/23 05:18:14.649
  W0510 05:18:14.654578      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 05/10/23 05:18:14.654
  E0510 05:18:14.788857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:15.789848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:16.789965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:17.790120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/10/23 05:18:18.669
  STEP: the container should be terminated @ 05/10/23 05:18:18.671
  STEP: the termination message should be set @ 05/10/23 05:18:18.671
  May 10 05:18:18.671: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/10/23 05:18:18.671
  May 10 05:18:18.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3105" for this suite. @ 05/10/23 05:18:18.688
• [4.061 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 05/10/23 05:18:18.693
  May 10 05:18:18.694: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename containers @ 05/10/23 05:18:18.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:18:18.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:18:18.708
  E0510 05:18:18.790881      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:19.791180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:18:20.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1641" for this suite. @ 05/10/23 05:18:20.729
• [2.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 05/10/23 05:18:20.744
  May 10 05:18:20.744: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename replication-controller @ 05/10/23 05:18:20.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:18:20.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:18:20.76
  STEP: Creating ReplicationController "e2e-rc-hgzd6" @ 05/10/23 05:18:20.762
  May 10 05:18:20.776: INFO: Get Replication Controller "e2e-rc-hgzd6" to confirm replicas
  E0510 05:18:20.791705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:18:21.779: INFO: Get Replication Controller "e2e-rc-hgzd6" to confirm replicas
  May 10 05:18:21.780: INFO: Found 1 replicas for "e2e-rc-hgzd6" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-hgzd6" @ 05/10/23 05:18:21.78
  STEP: Updating a scale subresource @ 05/10/23 05:18:21.782
  STEP: Verifying replicas where modified for replication controller "e2e-rc-hgzd6" @ 05/10/23 05:18:21.787
  May 10 05:18:21.787: INFO: Get Replication Controller "e2e-rc-hgzd6" to confirm replicas
  E0510 05:18:21.792587      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:18:22.789: INFO: Get Replication Controller "e2e-rc-hgzd6" to confirm replicas
  May 10 05:18:22.792: INFO: Found 2 replicas for "e2e-rc-hgzd6" replication controller
  May 10 05:18:22.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0510 05:18:22.793122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-3755" for this suite. @ 05/10/23 05:18:22.794
• [2.055 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 05/10/23 05:18:22.799
  May 10 05:18:22.799: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:18:22.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:18:22.815
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:18:22.818
  STEP: Creating configMap with name projected-configmap-test-volume-eb4bbaf8-4bfa-493f-80bb-6e19bd6a76c2 @ 05/10/23 05:18:22.82
  STEP: Creating a pod to test consume configMaps @ 05/10/23 05:18:22.826
  E0510 05:18:23.793630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:24.793867      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:25.794417      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:26.795418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:27.795573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:28.795748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:18:28.94
  May 10 05:18:28.943: INFO: Trying to get logs from node node-02 pod pod-projected-configmaps-f944698b-c761-4ade-9534-2964d365efe1 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 05:18:28.985
  May 10 05:18:29.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8222" for this suite. @ 05/10/23 05:18:29.002
• [6.208 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 05/10/23 05:18:29.007
  May 10 05:18:29.007: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 05:18:29.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:18:29.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:18:29.034
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/10/23 05:18:29.036
  E0510 05:18:29.796237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:30.796593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:31.796860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:32.797362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:33.798305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:34.798567      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:18:35.06
  May 10 05:18:35.062: INFO: Trying to get logs from node node-02 pod pod-7265f64c-0171-4df3-a713-8b5dfb3b9493 container test-container: <nil>
  STEP: delete the pod @ 05/10/23 05:18:35.066
  May 10 05:18:35.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8987" for this suite. @ 05/10/23 05:18:35.082
• [6.081 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 05/10/23 05:18:35.088
  May 10 05:18:35.088: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename deployment @ 05/10/23 05:18:35.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:18:35.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:18:35.106
  May 10 05:18:35.108: INFO: Creating deployment "test-recreate-deployment"
  May 10 05:18:35.119: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  May 10 05:18:35.129: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0510 05:18:35.799002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:36.799962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:18:37.134: INFO: Waiting deployment "test-recreate-deployment" to complete
  May 10 05:18:37.136: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  May 10 05:18:37.145: INFO: Updating deployment test-recreate-deployment
  May 10 05:18:37.145: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  May 10 05:18:37.242: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-864  e1708557-fbd3-4576-8442-3eaf4176be0b 1062187 2 2023-05-10 05:18:35 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-10 05:18:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 05:18:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005403fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-10 05:18:37 +0000 UTC,LastTransitionTime:2023-05-10 05:18:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-05-10 05:18:37 +0000 UTC,LastTransitionTime:2023-05-10 05:18:35 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  May 10 05:18:37.243: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-864  7a6555cf-1f58-4ebc-95e4-06e41f13262a 1062186 1 2023-05-10 05:18:37 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment e1708557-fbd3-4576-8442-3eaf4176be0b 0xc002f2e9a7 0xc002f2e9a8}] [] [{kube-controller-manager Update apps/v1 2023-05-10 05:18:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1708557-fbd3-4576-8442-3eaf4176be0b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 05:18:37 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f2ea68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 10 05:18:37.243: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  May 10 05:18:37.243: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-864  1af40e69-de2f-464f-b5f7-6f2ed66e8cc4 1062175 2 2023-05-10 05:18:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment e1708557-fbd3-4576-8442-3eaf4176be0b 0xc002f2ead7 0xc002f2ead8}] [] [{kube-controller-manager Update apps/v1 2023-05-10 05:18:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1708557-fbd3-4576-8442-3eaf4176be0b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 05:18:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f2eb88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 10 05:18:37.245: INFO: Pod "test-recreate-deployment-54757ffd6c-pnfvt" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-pnfvt test-recreate-deployment-54757ffd6c- deployment-864  b0be2499-6b1b-4715-aa55-5c3bc7b32e19 1062185 0 2023-05-10 05:18:37 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 7a6555cf-1f58-4ebc-95e4-06e41f13262a 0xc002f2f017 0xc002f2f018}] [] [{kube-controller-manager Update v1 2023-05-10 05:18:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a6555cf-1f58-4ebc-95e4-06e41f13262a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 05:18:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pftvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pftvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:18:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:18:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:18:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 05:18:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:,StartTime:2023-05-10 05:18:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 05:18:37.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-864" for this suite. @ 05/10/23 05:18:37.247
• [2.163 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 05/10/23 05:18:37.251
  May 10 05:18:37.252: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 05:18:37.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:18:37.261
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:18:37.265
  STEP: Creating configMap with name configmap-test-volume-map-c723f89d-55ec-4c4f-a408-3016fab49474 @ 05/10/23 05:18:37.266
  STEP: Creating a pod to test consume configMaps @ 05/10/23 05:18:37.27
  E0510 05:18:37.800702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:38.800910      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:39.801187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:40.801423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:18:41.283
  May 10 05:18:41.285: INFO: Trying to get logs from node node-01 pod pod-configmaps-7c26bbaf-69ab-468b-84e9-ddd6cadaf7ce container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 05:18:41.289
  May 10 05:18:41.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5626" for this suite. @ 05/10/23 05:18:41.314
• [4.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 05/10/23 05:18:41.323
  May 10 05:18:41.323: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename var-expansion @ 05/10/23 05:18:41.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:18:41.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:18:41.35
  STEP: creating the pod with failed condition @ 05/10/23 05:18:41.353
  E0510 05:18:41.801864      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:42.802640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:43.803659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:44.804069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:45.804910      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:46.805733      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:47.806078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:48.806284      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:49.807075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:50.807349      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:51.808058      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:52.808478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:53.809412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:54.809657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:55.810693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:56.810874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:57.811953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:58.812161      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:18:59.812936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:00.813187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:01.813795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:02.814551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:03.815612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:04.815836      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:05.816541      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:06.817438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:07.817845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:08.818042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:09.819122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:10.819349      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:11.820326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:12.820768      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:13.821775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:14.822018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:15.822962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:16.823164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:17.823464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:18.823698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:19.824604      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:20.824924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:21.825876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:22.826329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:23.827226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:24.827449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:25.827809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:26.828161      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:27.828148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:28.828452      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:29.829229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:30.829455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:31.829893      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:32.830418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:33.831050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:34.831367      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:35.832164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:36.832310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:37.833388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:38.833501      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:39.834319      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:40.834554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:41.834642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:42.834765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:43.835683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:44.835929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:45.836835      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:46.837104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:47.837244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:48.837359      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:49.838405      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:50.838622      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:51.839319      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:52.839721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:53.840518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:54.840664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:55.841464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:56.841679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:57.841830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:58.841995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:19:59.842664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:00.843695      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:01.843812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:02.843939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:03.844059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:04.844292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:05.844310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:06.844536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:07.845171      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:08.845373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:09.846291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:10.846494      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:11.846985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:12.847477      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:13.848283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:14.848784      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:15.849536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:16.849744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:17.850047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:18.850292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:19.850884      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:20.851101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:21.852059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:22.852247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:23.853179      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:24.853397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:25.853519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:26.854501      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:27.855313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:28.855443      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:29.856286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:30.856411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:31.856982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:32.857097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:33.857575      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:34.857868      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:35.858510      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:36.858680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:37.858809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:38.859039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:39.859852      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:40.860122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 05/10/23 05:20:41.358
  E0510 05:20:41.860777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:20:41.868: INFO: Successfully updated pod "var-expansion-ce44a7e5-351f-4604-8070-98222643be57"
  STEP: waiting for pod running @ 05/10/23 05:20:41.868
  E0510 05:20:42.861242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:43.861474      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 05/10/23 05:20:43.873
  May 10 05:20:43.873: INFO: Deleting pod "var-expansion-ce44a7e5-351f-4604-8070-98222643be57" in namespace "var-expansion-5924"
  May 10 05:20:43.880: INFO: Wait up to 5m0s for pod "var-expansion-ce44a7e5-351f-4604-8070-98222643be57" to be fully deleted
  E0510 05:20:44.862157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:45.862423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:46.862538      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:47.863077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:48.863572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:49.863786      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:50.863900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:51.864302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:52.864335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:53.864522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:54.865173      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:55.865299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:56.866143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:57.866272      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:58.866911      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:20:59.867167      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:00.867321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:01.867533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:02.868096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:03.868399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:04.869282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:05.869487      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:06.869546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:07.869604      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:08.870406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:09.870633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:10.870742      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:11.871147      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:12.871819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:13.872081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:14.872569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:15.872765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:21:15.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5924" for this suite. @ 05/10/23 05:21:15.933
• [154.616 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 05/10/23 05:21:15.94
  May 10 05:21:15.940: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename csistoragecapacity @ 05/10/23 05:21:15.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:21:15.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:21:15.961
  STEP: getting /apis @ 05/10/23 05:21:15.962
  STEP: getting /apis/storage.k8s.io @ 05/10/23 05:21:15.966
  STEP: getting /apis/storage.k8s.io/v1 @ 05/10/23 05:21:15.967
  STEP: creating @ 05/10/23 05:21:15.968
  STEP: watching @ 05/10/23 05:21:15.978
  May 10 05:21:15.978: INFO: starting watch
  STEP: getting @ 05/10/23 05:21:15.982
  STEP: listing in namespace @ 05/10/23 05:21:15.984
  STEP: listing across namespaces @ 05/10/23 05:21:15.985
  STEP: patching @ 05/10/23 05:21:15.987
  STEP: updating @ 05/10/23 05:21:15.99
  May 10 05:21:15.996: INFO: waiting for watch events with expected annotations in namespace
  May 10 05:21:15.996: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 05/10/23 05:21:15.996
  STEP: deleting a collection @ 05/10/23 05:21:16.004
  May 10 05:21:16.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-7164" for this suite. @ 05/10/23 05:21:16.015
• [0.079 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 05/10/23 05:21:16.019
  May 10 05:21:16.019: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:21:16.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:21:16.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:21:16.032
  STEP: Setting up server cert @ 05/10/23 05:21:16.05
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:21:16.678
  STEP: Deploying the webhook pod @ 05/10/23 05:21:16.684
  STEP: Wait for the deployment to be ready @ 05/10/23 05:21:16.698
  May 10 05:21:16.701: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0510 05:21:16.873750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:17.873944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:21:18.707
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:21:18.714
  E0510 05:21:18.874454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:21:19.714: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 10 05:21:19.716: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 05:21:19.875307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 05/10/23 05:21:20.227
  STEP: Creating a custom resource that should be denied by the webhook @ 05/10/23 05:21:20.246
  E0510 05:21:20.875363      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:21.875965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 05/10/23 05:21:22.264
  STEP: Updating the custom resource with disallowed data should be denied @ 05/10/23 05:21:22.27
  STEP: Deleting the custom resource should be denied @ 05/10/23 05:21:22.276
  STEP: Remove the offending key and value from the custom resource data @ 05/10/23 05:21:22.28
  STEP: Deleting the updated custom resource should be successful @ 05/10/23 05:21:22.287
  May 10 05:21:22.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1722" for this suite. @ 05/10/23 05:21:22.85
  STEP: Destroying namespace "webhook-markers-8892" for this suite. @ 05/10/23 05:21:22.855
• [6.846 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 05/10/23 05:21:22.865
  May 10 05:21:22.865: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:21:22.866
  E0510 05:21:22.876511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:21:22.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:21:22.878
  STEP: Setting up server cert @ 05/10/23 05:21:22.897
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:21:23.325
  STEP: Deploying the webhook pod @ 05/10/23 05:21:23.33
  STEP: Wait for the deployment to be ready @ 05/10/23 05:21:23.344
  May 10 05:21:23.355: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 05:21:23.877309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:24.877478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:21:25.363
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:21:25.371
  E0510 05:21:25.877902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:21:26.371: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/10/23 05:21:26.373
  STEP: create a pod @ 05/10/23 05:21:26.39
  E0510 05:21:26.878395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:27.878703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 05/10/23 05:21:28.4
  May 10 05:21:28.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=webhook-1964 attach --namespace=webhook-1964 to-be-attached-pod -i -c=container1'
  May 10 05:21:28.470: INFO: rc: 1
  May 10 05:21:28.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1964" for this suite. @ 05/10/23 05:21:28.527
  STEP: Destroying namespace "webhook-markers-3197" for this suite. @ 05/10/23 05:21:28.534
• [5.674 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 05/10/23 05:21:28.539
  May 10 05:21:28.539: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pod-network-test @ 05/10/23 05:21:28.54
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:21:28.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:21:28.561
  STEP: Performing setup for networking test in namespace pod-network-test-8455 @ 05/10/23 05:21:28.563
  STEP: creating a selector @ 05/10/23 05:21:28.564
  STEP: Creating the service pods in kubernetes @ 05/10/23 05:21:28.564
  May 10 05:21:28.564: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0510 05:21:28.879138      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:29.880092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:30.880247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:31.880678      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:32.881224      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:33.881355      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:34.881609      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:35.882035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:36.882350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:37.882558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:38.883063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:39.883295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:40.884068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:41.884590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:42.885509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:43.885719      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:44.886137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:45.886266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:46.886888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:47.887113      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:48.887232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:49.887592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/10/23 05:21:50.773
  E0510 05:21:50.888669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:51.889130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:21:52.794: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 10 05:21:52.794: INFO: Going to poll 100.67.79.137 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May 10 05:21:52.796: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.67.79.137:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8455 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:21:52.796: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:21:52.797: INFO: ExecWithOptions: Clientset creation
  May 10 05:21:52.797: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8455/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.67.79.137%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 10 05:21:52.854: INFO: Found all 1 expected endpoints: [netserver-0]
  May 10 05:21:52.854: INFO: Going to poll 100.114.252.170 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May 10 05:21:52.857: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.114.252.170:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8455 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:21:52.857: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:21:52.858: INFO: ExecWithOptions: Clientset creation
  May 10 05:21:52.858: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8455/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.114.252.170%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0510 05:21:52.889919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:21:52.927: INFO: Found all 1 expected endpoints: [netserver-1]
  May 10 05:21:52.927: INFO: Going to poll 100.74.79.37 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May 10 05:21:52.929: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.74.79.37:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8455 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:21:52.929: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:21:52.930: INFO: ExecWithOptions: Clientset creation
  May 10 05:21:52.930: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8455/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.74.79.37%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 10 05:21:52.975: INFO: Found all 1 expected endpoints: [netserver-2]
  May 10 05:21:52.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8455" for this suite. @ 05/10/23 05:21:52.978
• [24.444 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 05/10/23 05:21:52.985
  May 10 05:21:52.985: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename var-expansion @ 05/10/23 05:21:52.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:21:52.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:21:53.001
  E0510 05:21:53.891023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:54.891461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:21:55.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 10 05:21:55.346: INFO: Deleting pod "var-expansion-d431db65-931c-48f5-ad43-b1a202c7e44a" in namespace "var-expansion-7530"
  May 10 05:21:55.354: INFO: Wait up to 5m0s for pod "var-expansion-d431db65-931c-48f5-ad43-b1a202c7e44a" to be fully deleted
  E0510 05:21:55.891594      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:56.892152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-7530" for this suite. @ 05/10/23 05:21:57.359
• [4.378 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 05/10/23 05:21:57.364
  May 10 05:21:57.364: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename endpointslicemirroring @ 05/10/23 05:21:57.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:21:57.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:21:57.381
  STEP: mirroring a new custom Endpoint @ 05/10/23 05:21:57.389
  May 10 05:21:57.394: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0510 05:21:57.892310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:21:58.892512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 05/10/23 05:21:59.397
  May 10 05:21:59.407: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0510 05:21:59.892843      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:00.893046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 05/10/23 05:22:01.41
  May 10 05:22:01.416: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0510 05:22:01.893627      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:02.893842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:03.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-6641" for this suite. @ 05/10/23 05:22:03.422
• [6.064 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 05/10/23 05:22:03.429
  May 10 05:22:03.429: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename namespaces @ 05/10/23 05:22:03.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:22:03.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:22:03.441
  STEP: Creating a test namespace @ 05/10/23 05:22:03.443
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:22:03.456
  STEP: Creating a pod in the namespace @ 05/10/23 05:22:03.459
  STEP: Waiting for the pod to have running status @ 05/10/23 05:22:03.468
  E0510 05:22:03.894697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:04.895034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 05/10/23 05:22:05.548
  STEP: Waiting for the namespace to be removed. @ 05/10/23 05:22:05.552
  E0510 05:22:05.896146      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:06.896530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:07.897208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:08.897671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:09.897823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:10.898375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:11.899096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:12.899271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:13.899433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:14.899503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:15.900390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 05/10/23 05:22:16.555
  STEP: Verifying there are no pods in the namespace @ 05/10/23 05:22:16.57
  May 10 05:22:16.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2689" for this suite. @ 05/10/23 05:22:16.575
  STEP: Destroying namespace "nsdeletetest-9858" for this suite. @ 05/10/23 05:22:16.58
  May 10 05:22:16.581: INFO: Namespace nsdeletetest-9858 was already deleted
  STEP: Destroying namespace "nsdeletetest-1807" for this suite. @ 05/10/23 05:22:16.581
• [13.166 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 05/10/23 05:22:16.595
  May 10 05:22:16.595: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename watch @ 05/10/23 05:22:16.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:22:16.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:22:16.609
  STEP: creating a watch on configmaps @ 05/10/23 05:22:16.612
  STEP: creating a new configmap @ 05/10/23 05:22:16.613
  STEP: modifying the configmap once @ 05/10/23 05:22:16.617
  STEP: closing the watch once it receives two notifications @ 05/10/23 05:22:16.638
  May 10 05:22:16.638: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6093  217dae59-a15f-4c68-97f0-2d145a569d78 1063333 0 2023-05-10 05:22:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-10 05:22:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:22:16.638: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6093  217dae59-a15f-4c68-97f0-2d145a569d78 1063334 0 2023-05-10 05:22:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-10 05:22:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 05/10/23 05:22:16.639
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 05/10/23 05:22:16.644
  STEP: deleting the configmap @ 05/10/23 05:22:16.645
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 05/10/23 05:22:16.65
  May 10 05:22:16.650: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6093  217dae59-a15f-4c68-97f0-2d145a569d78 1063335 0 2023-05-10 05:22:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-10 05:22:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:22:16.651: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6093  217dae59-a15f-4c68-97f0-2d145a569d78 1063336 0 2023-05-10 05:22:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-10 05:22:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:22:16.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6093" for this suite. @ 05/10/23 05:22:16.653
• [0.064 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 05/10/23 05:22:16.659
  May 10 05:22:16.659: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 05:22:16.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:22:16.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:22:16.674
  STEP: creating all guestbook components @ 05/10/23 05:22:16.68
  May 10 05:22:16.680: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  May 10 05:22:16.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-4154 create -f -'
  E0510 05:22:16.900389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:17.863: INFO: stderr: ""
  May 10 05:22:17.863: INFO: stdout: "service/agnhost-replica created\n"
  May 10 05:22:17.863: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  May 10 05:22:17.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-4154 create -f -'
  E0510 05:22:17.901190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:18.189: INFO: stderr: ""
  May 10 05:22:18.189: INFO: stdout: "service/agnhost-primary created\n"
  May 10 05:22:18.190: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  May 10 05:22:18.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-4154 create -f -'
  May 10 05:22:18.553: INFO: stderr: ""
  May 10 05:22:18.553: INFO: stdout: "service/frontend created\n"
  May 10 05:22:18.553: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  May 10 05:22:18.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-4154 create -f -'
  May 10 05:22:18.840: INFO: stderr: ""
  May 10 05:22:18.840: INFO: stdout: "deployment.apps/frontend created\n"
  May 10 05:22:18.840: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 10 05:22:18.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-4154 create -f -'
  E0510 05:22:18.902060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:19.142: INFO: stderr: ""
  May 10 05:22:19.142: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  May 10 05:22:19.142: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 10 05:22:19.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-4154 create -f -'
  May 10 05:22:19.573: INFO: stderr: ""
  May 10 05:22:19.573: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 05/10/23 05:22:19.574
  May 10 05:22:19.574: INFO: Waiting for all frontend pods to be Running.
  E0510 05:22:19.902540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:20.902972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:21.903580      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:22.904449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:23.905073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:24.624: INFO: Waiting for frontend to serve content.
  May 10 05:22:24.632: INFO: Trying to add a new entry to the guestbook.
  May 10 05:22:24.640: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 05/10/23 05:22:24.646
  May 10 05:22:24.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-4154 delete --grace-period=0 --force -f -'
  May 10 05:22:24.713: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 10 05:22:24.714: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 05/10/23 05:22:24.714
  May 10 05:22:24.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-4154 delete --grace-period=0 --force -f -'
  May 10 05:22:24.782: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 10 05:22:24.782: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/10/23 05:22:24.782
  May 10 05:22:24.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-4154 delete --grace-period=0 --force -f -'
  May 10 05:22:24.860: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 10 05:22:24.860: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/10/23 05:22:24.86
  May 10 05:22:24.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-4154 delete --grace-period=0 --force -f -'
  E0510 05:22:24.905908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:24.921: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 10 05:22:24.921: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/10/23 05:22:24.921
  May 10 05:22:24.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-4154 delete --grace-period=0 --force -f -'
  May 10 05:22:25.717: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 10 05:22:25.717: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/10/23 05:22:25.717
  May 10 05:22:25.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-4154 delete --grace-period=0 --force -f -'
  E0510 05:22:25.906486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:25.951: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 10 05:22:25.951: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  May 10 05:22:25.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4154" for this suite. @ 05/10/23 05:22:25.965
• [9.312 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 05/10/23 05:22:25.971
  May 10 05:22:25.971: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 05:22:25.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:22:26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:22:26.006
  May 10 05:22:26.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8218" for this suite. @ 05/10/23 05:22:26.426
• [0.461 seconds]
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 05/10/23 05:22:26.433
  May 10 05:22:26.433: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename podtemplate @ 05/10/23 05:22:26.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:22:26.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:22:26.445
  STEP: Create a pod template @ 05/10/23 05:22:26.451
  STEP: Replace a pod template @ 05/10/23 05:22:26.456
  May 10 05:22:26.475: INFO: Found updated podtemplate annotation: "true"

  May 10 05:22:26.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2745" for this suite. @ 05/10/23 05:22:26.478
• [0.256 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 05/10/23 05:22:26.689
  May 10 05:22:26.689: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename aggregator @ 05/10/23 05:22:26.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:22:26.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:22:26.719
  May 10 05:22:26.721: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Registering the sample API server. @ 05/10/23 05:22:26.722
  E0510 05:22:26.907089      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:26.958: INFO: Found ClusterRoles; assuming RBAC is enabled.
  May 10 05:22:27.004: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0510 05:22:27.907771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:28.907948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:29.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:22:29.908304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:30.908511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:31.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:22:31.908658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:32.908858      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:33.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:22:33.909548      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:34.909756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:35.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:22:35.909891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:36.910808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:37.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:22:37.911603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:38.911762      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:39.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:22:39.911918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:40.912237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:41.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:22:41.912756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:42.913009      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:43.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:22:43.913280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:44.913438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:45.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:22:45.913565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:46.914250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:47.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:22:47.914530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:48.914751      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:49.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 22, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:22:49.914891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:50.915126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:51.163: INFO: Waited 109.614838ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 05/10/23 05:22:51.202
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 05/10/23 05:22:51.204
  STEP: List APIServices @ 05/10/23 05:22:51.212
  May 10 05:22:51.216: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 05/10/23 05:22:51.216
  May 10 05:22:51.223: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 05/10/23 05:22:51.223
  May 10 05:22:51.235: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.May, 10, 5, 22, 51, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 05/10/23 05:22:51.236
  May 10 05:22:51.237: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-05-10 05:22:51 +0000 UTC Passed all checks passed}
  May 10 05:22:51.237: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 10 05:22:51.237: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 05/10/23 05:22:51.237
  May 10 05:22:51.251: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-225759317" @ 05/10/23 05:22:51.251
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 05/10/23 05:22:51.268
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 05/10/23 05:22:51.278
  STEP: Patch APIService Status @ 05/10/23 05:22:51.28
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 05/10/23 05:22:51.289
  May 10 05:22:51.291: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-05-10 05:22:51 +0000 UTC Passed all checks passed}
  May 10 05:22:51.291: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 10 05:22:51.291: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  May 10 05:22:51.291: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 05/10/23 05:22:51.291
  STEP: Confirm that the generated APIService has been deleted @ 05/10/23 05:22:51.298
  May 10 05:22:51.298: INFO: Requesting list of APIServices to confirm quantity
  May 10 05:22:51.302: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  May 10 05:22:51.302: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  May 10 05:22:51.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-6454" for this suite. @ 05/10/23 05:22:51.39
• [24.710 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 05/10/23 05:22:51.4
  May 10 05:22:51.400: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename events @ 05/10/23 05:22:51.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:22:51.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:22:51.415
  STEP: Create set of events @ 05/10/23 05:22:51.418
  STEP: get a list of Events with a label in the current namespace @ 05/10/23 05:22:51.434
  STEP: delete a list of events @ 05/10/23 05:22:51.437
  May 10 05:22:51.437: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/10/23 05:22:51.452
  May 10 05:22:51.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-3008" for this suite. @ 05/10/23 05:22:51.456
• [0.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 05/10/23 05:22:51.49
  May 10 05:22:51.490: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename events @ 05/10/23 05:22:51.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:22:51.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:22:51.508
  STEP: creating a test event @ 05/10/23 05:22:51.51
  STEP: listing all events in all namespaces @ 05/10/23 05:22:51.514
  STEP: patching the test event @ 05/10/23 05:22:51.517
  STEP: fetching the test event @ 05/10/23 05:22:51.523
  STEP: updating the test event @ 05/10/23 05:22:51.525
  STEP: getting the test event @ 05/10/23 05:22:51.533
  STEP: deleting the test event @ 05/10/23 05:22:51.535
  STEP: listing all events in all namespaces @ 05/10/23 05:22:51.539
  May 10 05:22:51.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9945" for this suite. @ 05/10/23 05:22:51.544
• [0.057 seconds]
------------------------------
S
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 05/10/23 05:22:51.548
  May 10 05:22:51.548: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 05:22:51.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:22:51.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:22:51.563
  STEP: creating a secret @ 05/10/23 05:22:51.565
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 05/10/23 05:22:51.569
  STEP: patching the secret @ 05/10/23 05:22:51.573
  STEP: deleting the secret using a LabelSelector @ 05/10/23 05:22:51.579
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 05/10/23 05:22:51.585
  May 10 05:22:51.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9125" for this suite. @ 05/10/23 05:22:51.598
• [0.056 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 05/10/23 05:22:51.605
  May 10 05:22:51.606: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename replicaset @ 05/10/23 05:22:51.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:22:51.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:22:51.619
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 05/10/23 05:22:51.624
  May 10 05:22:51.630: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0510 05:22:51.915992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:52.916307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:53.916462      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:54.916692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:55.916982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:22:56.632: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/10/23 05:22:56.632
  STEP: getting scale subresource @ 05/10/23 05:22:56.632
  STEP: updating a scale subresource @ 05/10/23 05:22:56.634
  STEP: verifying the replicaset Spec.Replicas was modified @ 05/10/23 05:22:56.639
  STEP: Patch a scale subresource @ 05/10/23 05:22:56.641
  May 10 05:22:56.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8463" for this suite. @ 05/10/23 05:22:56.664
• [5.071 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 05/10/23 05:22:56.676
  May 10 05:22:56.676: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:22:56.677
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:22:56.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:22:56.694
  STEP: Creating projection with secret that has name projected-secret-test-map-35907447-4aaa-46cd-9f98-6c028e1b4610 @ 05/10/23 05:22:56.696
  STEP: Creating a pod to test consume secrets @ 05/10/23 05:22:56.704
  E0510 05:22:56.917559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:57.917924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:58.918243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:22:59.918477      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:00.919238      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:01.919673      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:23:02.733
  May 10 05:23:02.735: INFO: Trying to get logs from node node-02 pod pod-projected-secrets-5c518e7e-8aa3-42c6-b578-f6ecc5490242 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 05:23:02.74
  May 10 05:23:02.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6279" for this suite. @ 05/10/23 05:23:02.808
• [6.139 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 05/10/23 05:23:02.816
  May 10 05:23:02.816: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename resourcequota @ 05/10/23 05:23:02.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:23:02.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:23:02.828
  STEP: Creating a ResourceQuota with terminating scope @ 05/10/23 05:23:02.83
  STEP: Ensuring ResourceQuota status is calculated @ 05/10/23 05:23:02.838
  E0510 05:23:02.920008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:03.920326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 05/10/23 05:23:04.841
  STEP: Ensuring ResourceQuota status is calculated @ 05/10/23 05:23:04.846
  E0510 05:23:04.921436      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:05.921571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 05/10/23 05:23:06.849
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 05/10/23 05:23:06.863
  E0510 05:23:06.922023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:07.922317      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 05/10/23 05:23:08.866
  E0510 05:23:08.923372      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:09.923588      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/10/23 05:23:10.869
  STEP: Ensuring resource quota status released the pod usage @ 05/10/23 05:23:10.887
  E0510 05:23:10.924292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:11.924645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 05/10/23 05:23:12.891
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 05/10/23 05:23:12.906
  E0510 05:23:12.925727      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:13.925799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 05/10/23 05:23:14.909
  E0510 05:23:14.925927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:15.926154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/10/23 05:23:16.913
  E0510 05:23:16.926302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota status released the pod usage @ 05/10/23 05:23:16.928
  E0510 05:23:17.926554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:18.926677      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:23:18.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5823" for this suite. @ 05/10/23 05:23:18.935
• [16.124 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 05/10/23 05:23:18.94
  May 10 05:23:18.940: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 05:23:18.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:23:18.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:23:18.957
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/10/23 05:23:18.959
  E0510 05:23:19.926876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:20.927026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:21.927570      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:22.928198      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:23:22.985
  May 10 05:23:22.986: INFO: Trying to get logs from node node-02 pod pod-c87d9c38-83db-4862-9fa4-123bef4d6b91 container test-container: <nil>
  STEP: delete the pod @ 05/10/23 05:23:22.991
  May 10 05:23:23.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1091" for this suite. @ 05/10/23 05:23:23.014
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 05/10/23 05:23:23.021
  May 10 05:23:23.021: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 05:23:23.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:23:23.033
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:23:23.035
  STEP: creating service in namespace services-8048 @ 05/10/23 05:23:23.037
  STEP: creating service affinity-clusterip in namespace services-8048 @ 05/10/23 05:23:23.037
  STEP: creating replication controller affinity-clusterip in namespace services-8048 @ 05/10/23 05:23:23.056
  I0510 05:23:23.061506      22 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-8048, replica count: 3
  E0510 05:23:23.929099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:24.930141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:25.930595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 05:23:26.112512      22 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 10 05:23:26.116: INFO: Creating new exec pod
  E0510 05:23:26.930992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:27.931192      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:28.931877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:23:29.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-8048 exec execpod-affinityk6qcc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  May 10 05:23:29.267: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  May 10 05:23:29.267: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 05:23:29.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-8048 exec execpod-affinityk6qcc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.137 80'
  May 10 05:23:29.380: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.2.137 80\nConnection to 10.96.2.137 80 port [tcp/http] succeeded!\n"
  May 10 05:23:29.380: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 05:23:29.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-8048 exec execpod-affinityk6qcc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.2.137:80/ ; done'
  May 10 05:23:29.561: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.137:80/\n"
  May 10 05:23:29.561: INFO: stdout: "\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn\naffinity-clusterip-kvlwn"
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Received response from host: affinity-clusterip-kvlwn
  May 10 05:23:29.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 10 05:23:29.564: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-8048, will wait for the garbage collector to delete the pods @ 05/10/23 05:23:29.583
  May 10 05:23:29.644: INFO: Deleting ReplicationController affinity-clusterip took: 8.2723ms
  May 10 05:23:29.745: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.716413ms
  E0510 05:23:29.932848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:30.932977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:31.933555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-8048" for this suite. @ 05/10/23 05:23:32.766
• [9.750 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 05/10/23 05:23:32.772
  May 10 05:23:32.772: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename gc @ 05/10/23 05:23:32.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:23:32.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:23:32.791
  May 10 05:23:32.821: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"6c9e3355-02de-445a-b0ec-1049e9956ca1", Controller:(*bool)(0xc00526059a), BlockOwnerDeletion:(*bool)(0xc00526059b)}}
  May 10 05:23:32.829: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"059660b2-a86a-4a89-9204-c706f88a145c", Controller:(*bool)(0xc0048992e2), BlockOwnerDeletion:(*bool)(0xc0048992e3)}}
  May 10 05:23:32.851: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c27124f5-c5fb-4886-9693-8f52a63321d0", Controller:(*bool)(0xc0052607e2), BlockOwnerDeletion:(*bool)(0xc0052607e3)}}
  E0510 05:23:32.934419      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:33.935005      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:34.935251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:35.935475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:36.935969      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:23:37.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7340" for this suite. @ 05/10/23 05:23:37.87
• [5.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 05/10/23 05:23:37.876
  May 10 05:23:37.876: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename replicaset @ 05/10/23 05:23:37.876
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:23:37.889
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:23:37.891
  STEP: Create a Replicaset @ 05/10/23 05:23:37.895
  STEP: Verify that the required pods have come up. @ 05/10/23 05:23:37.902
  May 10 05:23:37.903: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0510 05:23:37.936709      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:38.937359      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:39.937587      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:40.937911      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:41.938393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:23:42.905: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/10/23 05:23:42.905
  STEP: Getting /status @ 05/10/23 05:23:42.905
  May 10 05:23:42.907: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 05/10/23 05:23:42.907
  May 10 05:23:42.918: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 05/10/23 05:23:42.918
  May 10 05:23:42.920: INFO: Observed &ReplicaSet event: ADDED
  May 10 05:23:42.920: INFO: Observed &ReplicaSet event: MODIFIED
  May 10 05:23:42.920: INFO: Observed &ReplicaSet event: MODIFIED
  May 10 05:23:42.920: INFO: Observed &ReplicaSet event: MODIFIED
  May 10 05:23:42.920: INFO: Found replicaset test-rs in namespace replicaset-9327 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 10 05:23:42.920: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 05/10/23 05:23:42.921
  May 10 05:23:42.921: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 10 05:23:42.925: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 05/10/23 05:23:42.925
  May 10 05:23:42.927: INFO: Observed &ReplicaSet event: ADDED
  May 10 05:23:42.927: INFO: Observed &ReplicaSet event: MODIFIED
  May 10 05:23:42.927: INFO: Observed &ReplicaSet event: MODIFIED
  May 10 05:23:42.927: INFO: Observed &ReplicaSet event: MODIFIED
  May 10 05:23:42.927: INFO: Observed replicaset test-rs in namespace replicaset-9327 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 10 05:23:42.927: INFO: Observed &ReplicaSet event: MODIFIED
  May 10 05:23:42.927: INFO: Found replicaset test-rs in namespace replicaset-9327 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  May 10 05:23:42.928: INFO: Replicaset test-rs has a patched status
  May 10 05:23:42.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9327" for this suite. @ 05/10/23 05:23:42.931
• [5.061 seconds]
------------------------------
SSSS  E0510 05:23:42.938521      22 retrywatcher.go:130] "Watch failed" err="context canceled"
SS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 05/10/23 05:23:42.938
  May 10 05:23:42.938: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename init-container @ 05/10/23 05:23:42.939
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:23:42.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:23:42.954
  STEP: creating the pod @ 05/10/23 05:23:42.957
  May 10 05:23:42.957: INFO: PodSpec: initContainers in spec.initContainers
  E0510 05:23:43.938835      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:44.939778      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:45.940593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:46.941475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:23:47.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2392" for this suite. @ 05/10/23 05:23:47.59
• [4.662 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 05/10/23 05:23:47.601
  May 10 05:23:47.601: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename resourcequota @ 05/10/23 05:23:47.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:23:47.612
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:23:47.615
  STEP: Creating a ResourceQuota @ 05/10/23 05:23:47.617
  STEP: Getting a ResourceQuota @ 05/10/23 05:23:47.62
  STEP: Updating a ResourceQuota @ 05/10/23 05:23:47.622
  STEP: Verifying a ResourceQuota was modified @ 05/10/23 05:23:47.634
  STEP: Deleting a ResourceQuota @ 05/10/23 05:23:47.635
  STEP: Verifying the deleted ResourceQuota @ 05/10/23 05:23:47.64
  May 10 05:23:47.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2175" for this suite. @ 05/10/23 05:23:47.648
• [0.057 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 05/10/23 05:23:47.658
  May 10 05:23:47.658: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename subpath @ 05/10/23 05:23:47.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:23:47.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:23:47.673
  STEP: Setting up data @ 05/10/23 05:23:47.676
  STEP: Creating pod pod-subpath-test-secret-8bxv @ 05/10/23 05:23:47.688
  STEP: Creating a pod to test atomic-volume-subpath @ 05/10/23 05:23:47.688
  E0510 05:23:47.942128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:48.942536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:49.943251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:50.943481      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:51.943509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:52.943739      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:53.944397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:54.944617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:55.944757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:56.945306      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:57.946062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:58.946297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:23:59.947259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:00.947429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:01.948171      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:02.948327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:03.948486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:04.948676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:05.949465      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:06.950085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:07.950692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:08.950923      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:09.951088      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:10.951276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:24:11.734
  May 10 05:24:11.736: INFO: Trying to get logs from node node-01 pod pod-subpath-test-secret-8bxv container test-container-subpath-secret-8bxv: <nil>
  STEP: delete the pod @ 05/10/23 05:24:11.74
  STEP: Deleting pod pod-subpath-test-secret-8bxv @ 05/10/23 05:24:11.753
  May 10 05:24:11.753: INFO: Deleting pod "pod-subpath-test-secret-8bxv" in namespace "subpath-3021"
  May 10 05:24:11.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3021" for this suite. @ 05/10/23 05:24:11.757
• [24.106 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 05/10/23 05:24:11.764
  May 10 05:24:11.764: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 05:24:11.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:24:11.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:24:11.792
  STEP: Creating configMap that has name configmap-test-emptyKey-465c6523-017c-4156-88cd-ee8f3bf3992c @ 05/10/23 05:24:11.794
  May 10 05:24:11.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8871" for this suite. @ 05/10/23 05:24:11.798
• [0.038 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 05/10/23 05:24:11.803
  May 10 05:24:11.803: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename var-expansion @ 05/10/23 05:24:11.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:24:11.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:24:11.816
  STEP: Creating a pod to test substitution in container's command @ 05/10/23 05:24:11.82
  E0510 05:24:11.951737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:12.952060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:13.952771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:14.953002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:24:15.834
  May 10 05:24:15.836: INFO: Trying to get logs from node node-02 pod var-expansion-f83e5e74-8137-40d0-a4bc-18f8bcde7bbc container dapi-container: <nil>
  STEP: delete the pod @ 05/10/23 05:24:15.84
  May 10 05:24:15.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1727" for this suite. @ 05/10/23 05:24:15.86
• [4.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 05/10/23 05:24:15.866
  May 10 05:24:15.866: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename cronjob @ 05/10/23 05:24:15.867
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:24:15.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:24:15.891
  STEP: Creating a cronjob @ 05/10/23 05:24:15.894
  STEP: Ensuring more than one job is running at a time @ 05/10/23 05:24:15.9
  E0510 05:24:15.954088      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:16.954604      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:17.955344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:18.955509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:19.955676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:20.955914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:21.956596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:22.956832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:23.957046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:24.957280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:25.958104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:26.958595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:27.959251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:28.959490      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:29.960123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:30.960319      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:31.960807      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:32.960948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:33.962038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:34.962260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:35.963000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:36.963688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:37.964353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:38.965225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:39.965999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:40.966227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:41.966767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:42.966979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:43.967984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:44.968252      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:45.969139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:46.969662      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:47.970381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:48.970679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:49.970955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:50.971230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:51.972308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:52.972543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:53.973071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:54.973362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:55.974407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:56.974810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:57.975494      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:58.975727      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:24:59.976130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:00.976374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:01.976677      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:02.976904      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:03.977562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:04.977756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:05.978399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:06.979071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:07.979986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:08.980202      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:09.980937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:10.981229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:11.982093      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:12.982301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:13.983216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:14.983415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:15.984255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:16.984966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:17.985567      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:18.985780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:19.986308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:20.986511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:21.987471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:22.987802      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:23.987999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:24.988316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:25.988687      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:26.988530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:27.989544      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:28.989800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:29.990430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:30.990781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:31.991566      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:32.991790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:33.992260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:34.992484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:35.992515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:36.993106      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:37.993979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:38.994873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:39.995781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:40.995998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:41.996842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:42.997074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:43.997984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:44.998245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:45.998685      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:46.999260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:48.000039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:49.000250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:50.000742      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:51.000994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:52.001775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:53.002027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:54.002372      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:55.002651      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:56.003454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:57.004027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:58.004675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:25:59.004872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:00.005491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:01.005695      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 05/10/23 05:26:01.903
  STEP: Removing cronjob @ 05/10/23 05:26:01.905
  May 10 05:26:01.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1117" for this suite. @ 05/10/23 05:26:01.91
• [106.050 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 05/10/23 05:26:01.916
  May 10 05:26:01.916: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/10/23 05:26:01.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:26:01.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:26:01.943
  STEP: create the container to handle the HTTPGet hook request. @ 05/10/23 05:26:01.949
  E0510 05:26:02.006556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:03.007005      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:04.007781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:05.008000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/10/23 05:26:05.975
  E0510 05:26:06.008516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:07.009098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 05/10/23 05:26:07.985
  STEP: delete the pod with lifecycle hook @ 05/10/23 05:26:07.989
  E0510 05:26:08.009752      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:09.009987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:10.009975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:11.010120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:12.010697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:26:12.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7346" for this suite. @ 05/10/23 05:26:12.145
• [10.237 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 05/10/23 05:26:12.153
  May 10 05:26:12.153: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:26:12.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:26:12.162
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:26:12.165
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 05:26:12.166
  E0510 05:26:13.011592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:14.011851      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:15.011873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:16.012283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:26:16.181
  May 10 05:26:16.186: INFO: Trying to get logs from node node-01 pod downwardapi-volume-5a7721cf-1708-44c1-95db-0bb9bf6bdeb4 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 05:26:16.191
  May 10 05:26:16.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2726" for this suite. @ 05/10/23 05:26:16.206
• [4.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 05/10/23 05:26:16.211
  May 10 05:26:16.211: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 05:26:16.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:26:16.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:26:16.223
  STEP: creating a ConfigMap @ 05/10/23 05:26:16.225
  STEP: fetching the ConfigMap @ 05/10/23 05:26:16.228
  STEP: patching the ConfigMap @ 05/10/23 05:26:16.23
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 05/10/23 05:26:16.233
  STEP: deleting the ConfigMap by collection with a label selector @ 05/10/23 05:26:16.235
  STEP: listing all ConfigMaps in test namespace @ 05/10/23 05:26:16.239
  May 10 05:26:16.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5206" for this suite. @ 05/10/23 05:26:16.242
• [0.039 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 05/10/23 05:26:16.251
  May 10 05:26:16.251: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename subpath @ 05/10/23 05:26:16.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:26:16.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:26:16.262
  STEP: Setting up data @ 05/10/23 05:26:16.264
  STEP: Creating pod pod-subpath-test-downwardapi-m55n @ 05/10/23 05:26:16.275
  STEP: Creating a pod to test atomic-volume-subpath @ 05/10/23 05:26:16.275
  E0510 05:26:17.012992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:18.013451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:19.013694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:20.013958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:21.014090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:22.014552      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:23.014775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:24.015006      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:25.016032      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:26.016274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:27.016674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:28.016872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:29.017234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:30.017402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:31.017576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:32.018002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:33.018426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:34.018617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:35.019479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:36.019655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:37.020733      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:38.020946      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:39.021039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:40.021110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:26:40.429
  May 10 05:26:40.431: INFO: Trying to get logs from node node-02 pod pod-subpath-test-downwardapi-m55n container test-container-subpath-downwardapi-m55n: <nil>
  STEP: delete the pod @ 05/10/23 05:26:40.436
  STEP: Deleting pod pod-subpath-test-downwardapi-m55n @ 05/10/23 05:26:40.45
  May 10 05:26:40.450: INFO: Deleting pod "pod-subpath-test-downwardapi-m55n" in namespace "subpath-2063"
  May 10 05:26:40.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2063" for this suite. @ 05/10/23 05:26:40.454
• [24.207 seconds]
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 05/10/23 05:26:40.458
  May 10 05:26:40.458: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubelet-test @ 05/10/23 05:26:40.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:26:40.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:26:40.473
  E0510 05:26:41.021273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:42.021822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:43.022035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:44.022249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:26:44.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6190" for this suite. @ 05/10/23 05:26:44.489
• [4.036 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 05/10/23 05:26:44.494
  May 10 05:26:44.494: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 05:26:44.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:26:44.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:26:44.512
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 05:26:44.513
  E0510 05:26:45.023201      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:46.023356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:47.023659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:48.023909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:26:48.528
  May 10 05:26:48.530: INFO: Trying to get logs from node node-02 pod downwardapi-volume-632aa48f-14c5-4207-a56f-ef5ed163d5f4 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 05:26:48.534
  May 10 05:26:48.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1083" for this suite. @ 05/10/23 05:26:48.55
• [4.061 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 05/10/23 05:26:48.555
  May 10 05:26:48.555: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-probe @ 05/10/23 05:26:48.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:26:48.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:26:48.567
  E0510 05:26:49.024927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:50.025532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:51.026196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:52.026761      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:53.027122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:54.027311      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:55.028034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:56.028768      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:57.028820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:58.029007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:26:59.030034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:00.030217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:01.030417      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:02.030800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:03.031842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:04.032356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:05.032648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:06.032712      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:07.033477      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:08.033821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:09.034700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:10.034952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:27:10.618: INFO: Container started at 2023-05-10 05:26:49 +0000 UTC, pod became ready at 2023-05-10 05:27:08 +0000 UTC
  May 10 05:27:10.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6636" for this suite. @ 05/10/23 05:27:10.62
• [22.073 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 05/10/23 05:27:10.627
  May 10 05:27:10.627: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 05:27:10.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:27:10.636
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:27:10.638
  STEP: creating service multi-endpoint-test in namespace services-1975 @ 05/10/23 05:27:10.64
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1975 to expose endpoints map[] @ 05/10/23 05:27:10.648
  May 10 05:27:10.650: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  E0510 05:27:11.035309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:27:11.682: INFO: successfully validated that service multi-endpoint-test in namespace services-1975 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-1975 @ 05/10/23 05:27:11.682
  E0510 05:27:12.035815      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:13.035857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1975 to expose endpoints map[pod1:[100]] @ 05/10/23 05:27:13.698
  May 10 05:27:13.704: INFO: successfully validated that service multi-endpoint-test in namespace services-1975 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-1975 @ 05/10/23 05:27:13.704
  E0510 05:27:14.036389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:15.036485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1975 to expose endpoints map[pod1:[100] pod2:[101]] @ 05/10/23 05:27:15.714
  May 10 05:27:15.722: INFO: successfully validated that service multi-endpoint-test in namespace services-1975 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 05/10/23 05:27:15.722
  May 10 05:27:15.722: INFO: Creating new exec pod
  E0510 05:27:16.037329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:17.037946      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:18.039039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:19.039252      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:20.039331      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:27:20.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-1975 exec execpodgwqtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  May 10 05:27:20.855: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  May 10 05:27:20.855: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 05:27:20.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-1975 exec execpodgwqtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.125 80'
  May 10 05:27:20.988: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.2.125 80\nConnection to 10.96.2.125 80 port [tcp/http] succeeded!\n"
  May 10 05:27:20.988: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 05:27:20.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-1975 exec execpodgwqtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  E0510 05:27:21.039999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:27:21.102: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  May 10 05:27:21.102: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 05:27:21.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-1975 exec execpodgwqtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.125 81'
  May 10 05:27:21.212: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.2.125 81\nConnection to 10.96.2.125 81 port [tcp/*] succeeded!\n"
  May 10 05:27:21.212: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-1975 @ 05/10/23 05:27:21.212
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1975 to expose endpoints map[pod2:[101]] @ 05/10/23 05:27:21.235
  May 10 05:27:21.249: INFO: successfully validated that service multi-endpoint-test in namespace services-1975 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-1975 @ 05/10/23 05:27:21.249
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1975 to expose endpoints map[] @ 05/10/23 05:27:21.26
  May 10 05:27:21.272: INFO: successfully validated that service multi-endpoint-test in namespace services-1975 exposes endpoints map[]
  May 10 05:27:21.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1975" for this suite. @ 05/10/23 05:27:21.285
• [10.666 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 05/10/23 05:27:21.294
  May 10 05:27:21.294: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 05:27:21.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:27:21.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:27:21.31
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/10/23 05:27:21.313
  E0510 05:27:22.040118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:23.040515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:24.041179      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:25.041387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:26.041643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:27.042056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:27:27.365
  May 10 05:27:27.366: INFO: Trying to get logs from node node-02 pod pod-9295416d-6a07-4dc1-8717-0cbe1b06d070 container test-container: <nil>
  STEP: delete the pod @ 05/10/23 05:27:27.371
  May 10 05:27:27.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4619" for this suite. @ 05/10/23 05:27:27.466
• [6.185 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 05/10/23 05:27:27.479
  May 10 05:27:27.479: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:27:27.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:27:27.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:27:27.492
  STEP: Setting up server cert @ 05/10/23 05:27:27.52
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:27:27.904
  STEP: Deploying the webhook pod @ 05/10/23 05:27:27.909
  STEP: Wait for the deployment to be ready @ 05/10/23 05:27:27.928
  May 10 05:27:27.931: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0510 05:27:28.043041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:29.043295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:27:30.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 27, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 27, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 27, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 27, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:27:30.044253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:31.044506      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:27:32.042
  E0510 05:27:32.044497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:27:32.056
  E0510 05:27:33.045525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:27:33.056: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 05/10/23 05:27:33.059
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 05/10/23 05:27:33.06
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 05/10/23 05:27:33.06
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 05/10/23 05:27:33.06
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 05/10/23 05:27:33.061
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/10/23 05:27:33.061
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/10/23 05:27:33.062
  May 10 05:27:33.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6975" for this suite. @ 05/10/23 05:27:33.097
  STEP: Destroying namespace "webhook-markers-6978" for this suite. @ 05/10/23 05:27:33.109
• [5.640 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 05/10/23 05:27:33.12
  May 10 05:27:33.120: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename replication-controller @ 05/10/23 05:27:33.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:27:33.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:27:33.138
  STEP: Creating replication controller my-hostname-basic-b3b3e521-6880-4d87-b784-53c75678031b @ 05/10/23 05:27:33.141
  May 10 05:27:33.152: INFO: Pod name my-hostname-basic-b3b3e521-6880-4d87-b784-53c75678031b: Found 0 pods out of 1
  E0510 05:27:34.045774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:35.046136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:36.046337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:37.046925      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:38.047115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:27:38.154: INFO: Pod name my-hostname-basic-b3b3e521-6880-4d87-b784-53c75678031b: Found 1 pods out of 1
  May 10 05:27:38.154: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b3b3e521-6880-4d87-b784-53c75678031b" are running
  May 10 05:27:38.156: INFO: Pod "my-hostname-basic-b3b3e521-6880-4d87-b784-53c75678031b-dx2fc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-10 05:27:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-10 05:27:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-10 05:27:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-10 05:27:33 +0000 UTC Reason: Message:}])
  May 10 05:27:38.156: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/10/23 05:27:38.156
  May 10 05:27:38.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1267" for this suite. @ 05/10/23 05:27:38.163
• [5.049 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 05/10/23 05:27:38.17
  May 10 05:27:38.170: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubelet-test @ 05/10/23 05:27:38.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:27:38.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:27:38.182
  May 10 05:27:38.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8200" for this suite. @ 05/10/23 05:27:38.215
• [0.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 05/10/23 05:27:38.225
  May 10 05:27:38.225: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:27:38.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:27:38.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:27:38.239
  STEP: Creating projection with secret that has name projected-secret-test-f58ce41c-2519-4393-943a-5d72bcd18a17 @ 05/10/23 05:27:38.242
  STEP: Creating a pod to test consume secrets @ 05/10/23 05:27:38.245
  E0510 05:27:39.047832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:40.048203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:41.048596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:42.048770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:43.049432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:44.049712      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:27:44.263
  May 10 05:27:44.265: INFO: Trying to get logs from node node-02 pod pod-projected-secrets-c31c9d29-8d58-431e-8c18-fe6b0da1c154 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 05:27:44.268
  May 10 05:27:44.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1007" for this suite. @ 05/10/23 05:27:44.285
• [6.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 05/10/23 05:27:44.289
  May 10 05:27:44.289: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename dns @ 05/10/23 05:27:44.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:27:44.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:27:44.305
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-818.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-818.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 05/10/23 05:27:44.307
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-818.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-818.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 05/10/23 05:27:44.307
  STEP: creating a pod to probe /etc/hosts @ 05/10/23 05:27:44.307
  STEP: submitting the pod to kubernetes @ 05/10/23 05:27:44.307
  E0510 05:27:45.050714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:46.051106      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:47.052194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:48.052407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/10/23 05:27:48.325
  STEP: looking for the results for each expected name from probers @ 05/10/23 05:27:48.326
  May 10 05:27:48.333: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.dns-818.svc.cluster.local from pod dns-818/dns-test-cc7210b7-237a-4984-9f57-3dec4cb27e24: the server could not find the requested resource (get pods dns-test-cc7210b7-237a-4984-9f57-3dec4cb27e24)
  May 10 05:27:48.334: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-818/dns-test-cc7210b7-237a-4984-9f57-3dec4cb27e24: the server could not find the requested resource (get pods dns-test-cc7210b7-237a-4984-9f57-3dec4cb27e24)
  May 10 05:27:48.335: INFO: Lookups using dns-818/dns-test-cc7210b7-237a-4984-9f57-3dec4cb27e24 failed for: [jessie_hosts@dns-querier-1.dns-test-service.dns-818.svc.cluster.local jessie_hosts@dns-querier-1]

  E0510 05:27:49.052885      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:50.053094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:51.053325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:52.053720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:53.054287      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:27:53.345: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-818/dns-test-cc7210b7-237a-4984-9f57-3dec4cb27e24: the server could not find the requested resource (get pods dns-test-cc7210b7-237a-4984-9f57-3dec4cb27e24)
  May 10 05:27:53.345: INFO: Lookups using dns-818/dns-test-cc7210b7-237a-4984-9f57-3dec4cb27e24 failed for: [jessie_hosts@dns-querier-1]

  E0510 05:27:54.055061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:55.055349      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:56.055763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:57.056424      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:27:58.056749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:27:58.344: INFO: DNS probes using dns-818/dns-test-cc7210b7-237a-4984-9f57-3dec4cb27e24 succeeded

  May 10 05:27:58.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 05:27:58.347
  STEP: Destroying namespace "dns-818" for this suite. @ 05/10/23 05:27:58.362
• [14.083 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 05/10/23 05:27:58.373
  May 10 05:27:58.373: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename subpath @ 05/10/23 05:27:58.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:27:58.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:27:58.393
  STEP: Setting up data @ 05/10/23 05:27:58.396
  STEP: Creating pod pod-subpath-test-projected-9zl2 @ 05/10/23 05:27:58.406
  STEP: Creating a pod to test atomic-volume-subpath @ 05/10/23 05:27:58.406
  E0510 05:27:59.057766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:00.057984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:01.058566      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:02.058976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:03.059134      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:04.059409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:05.060213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:06.060342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:07.061291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:08.061681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:09.061937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:10.062142      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:11.062293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:12.062731      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:13.062859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:14.062959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:15.063866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:16.064077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:17.064955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:18.065247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:19.066101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:20.066921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:21.067739      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:22.068190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:23.068687      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:24.068947      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:28:24.455
  May 10 05:28:24.457: INFO: Trying to get logs from node node-02 pod pod-subpath-test-projected-9zl2 container test-container-subpath-projected-9zl2: <nil>
  STEP: delete the pod @ 05/10/23 05:28:24.461
  STEP: Deleting pod pod-subpath-test-projected-9zl2 @ 05/10/23 05:28:24.477
  May 10 05:28:24.477: INFO: Deleting pod "pod-subpath-test-projected-9zl2" in namespace "subpath-7383"
  May 10 05:28:24.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7383" for this suite. @ 05/10/23 05:28:24.482
• [26.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 05/10/23 05:28:24.487
  May 10 05:28:24.487: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename sched-pred @ 05/10/23 05:28:24.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:28:24.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:28:24.501
  May 10 05:28:24.505: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 10 05:28:24.509: INFO: Waiting for terminating namespaces to be deleted...
  May 10 05:28:24.511: INFO: 
  Logging pods the apiserver thinks is on node node-01 before test
  May 10 05:28:24.516: INFO: calico-node-hrprp from calico-system started at 2023-05-08 03:40:50 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.516: INFO: 	Container calico-node ready: true, restart count 0
  May 10 05:28:24.516: INFO: csi-node-driver-wrcs7 from calico-system started at 2023-05-08 03:41:03 +0000 UTC (2 container statuses recorded)
  May 10 05:28:24.516: INFO: 	Container calico-csi ready: true, restart count 0
  May 10 05:28:24.516: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 10 05:28:24.516: INFO: kube-proxy-jtvxj from kube-system started at 2023-05-08 03:40:50 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.516: INFO: 	Container kube-proxy ready: true, restart count 0
  May 10 05:28:24.517: INFO: kube-sealos-lvscare-node-01 from kube-system started at 2023-05-08 03:41:06 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.517: INFO: 	Container kube-sealos-lvscare ready: true, restart count 0
  May 10 05:28:24.517: INFO: sonobuoy from sonobuoy started at 2023-05-10 04:50:14 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.517: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 10 05:28:24.517: INFO: sonobuoy-e2e-job-9c188259183c425e from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 05:28:24.517: INFO: 	Container e2e ready: true, restart count 0
  May 10 05:28:24.517: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 05:28:24.517: INFO: sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-xwr24 from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 05:28:24.517: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 05:28:24.517: INFO: 	Container systemd-logs ready: true, restart count 0
  May 10 05:28:24.517: INFO: 
  Logging pods the apiserver thinks is on node node-02 before test
  May 10 05:28:24.523: INFO: calico-node-96glz from calico-system started at 2023-05-08 05:47:14 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.523: INFO: 	Container calico-node ready: true, restart count 0
  May 10 05:28:24.523: INFO: calico-typha-5777d458f7-g7bn9 from calico-system started at 2023-05-08 05:47:15 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.523: INFO: 	Container calico-typha ready: true, restart count 0
  May 10 05:28:24.523: INFO: csi-node-driver-8wkxf from calico-system started at 2023-05-10 04:51:20 +0000 UTC (2 container statuses recorded)
  May 10 05:28:24.523: INFO: 	Container calico-csi ready: true, restart count 0
  May 10 05:28:24.523: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 10 05:28:24.523: INFO: kube-proxy-96kwf from kube-system started at 2023-05-08 05:47:14 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.523: INFO: 	Container kube-proxy ready: true, restart count 0
  May 10 05:28:24.523: INFO: kube-sealos-lvscare-node-02 from kube-system started at 2023-05-08 05:47:31 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.523: INFO: 	Container kube-sealos-lvscare ready: true, restart count 0
  May 10 05:28:24.523: INFO: sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-254cd from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 05:28:24.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 05:28:24.523: INFO: 	Container systemd-logs ready: true, restart count 0
  May 10 05:28:24.523: INFO: 
  Logging pods the apiserver thinks is on node ub-test before test
  May 10 05:28:24.530: INFO: calico-apiserver-7f745d8f87-78trq from calico-apiserver started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container calico-apiserver ready: true, restart count 0
  May 10 05:28:24.530: INFO: calico-apiserver-7f745d8f87-gk9nt from calico-apiserver started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container calico-apiserver ready: true, restart count 0
  May 10 05:28:24.530: INFO: calico-kube-controllers-6dfbf88686-x7jtc from calico-system started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 10 05:28:24.530: INFO: calico-node-rwtc6 from calico-system started at 2023-05-06 03:21:16 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container calico-node ready: true, restart count 2
  May 10 05:28:24.530: INFO: calico-typha-5777d458f7-jvrx5 from calico-system started at 2023-05-06 03:21:16 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container calico-typha ready: true, restart count 2
  May 10 05:28:24.530: INFO: csi-node-driver-fgjb5 from calico-system started at 2023-05-06 09:37:06 +0000 UTC (2 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container calico-csi ready: true, restart count 0
  May 10 05:28:24.530: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 10 05:28:24.530: INFO: coredns-5d78c9869d-d9nxz from kube-system started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container coredns ready: true, restart count 0
  May 10 05:28:24.530: INFO: coredns-5d78c9869d-w9ngw from kube-system started at 2023-05-06 09:37:01 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container coredns ready: true, restart count 0
  May 10 05:28:24.530: INFO: etcd-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container etcd ready: true, restart count 2
  May 10 05:28:24.530: INFO: kube-apiserver-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container kube-apiserver ready: true, restart count 2
  May 10 05:28:24.530: INFO: kube-controller-manager-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container kube-controller-manager ready: true, restart count 6
  May 10 05:28:24.530: INFO: kube-proxy-hqbzr from kube-system started at 2023-05-06 03:18:37 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container kube-proxy ready: true, restart count 2
  May 10 05:28:24.530: INFO: kube-scheduler-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container kube-scheduler ready: true, restart count 6
  May 10 05:28:24.530: INFO: sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-44wmx from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 05:28:24.530: INFO: 	Container systemd-logs ready: true, restart count 0
  May 10 05:28:24.530: INFO: tigera-operator-7bf7458-fcvpq from tigera-operator started at 2023-05-06 03:21:10 +0000 UTC (1 container statuses recorded)
  May 10 05:28:24.530: INFO: 	Container tigera-operator ready: true, restart count 6
  STEP: verifying the node has the label node node-01 @ 05/10/23 05:28:24.548
  STEP: verifying the node has the label node node-02 @ 05/10/23 05:28:24.562
  STEP: verifying the node has the label node ub-test @ 05/10/23 05:28:24.573
  May 10 05:28:24.582: INFO: Pod calico-apiserver-7f745d8f87-78trq requesting resource cpu=0m on Node ub-test
  May 10 05:28:24.582: INFO: Pod calico-apiserver-7f745d8f87-gk9nt requesting resource cpu=0m on Node ub-test
  May 10 05:28:24.582: INFO: Pod calico-kube-controllers-6dfbf88686-x7jtc requesting resource cpu=0m on Node ub-test
  May 10 05:28:24.582: INFO: Pod calico-node-96glz requesting resource cpu=0m on Node node-02
  May 10 05:28:24.582: INFO: Pod calico-node-hrprp requesting resource cpu=0m on Node node-01
  May 10 05:28:24.582: INFO: Pod calico-node-rwtc6 requesting resource cpu=0m on Node ub-test
  May 10 05:28:24.582: INFO: Pod calico-typha-5777d458f7-g7bn9 requesting resource cpu=0m on Node node-02
  May 10 05:28:24.583: INFO: Pod calico-typha-5777d458f7-jvrx5 requesting resource cpu=0m on Node ub-test
  May 10 05:28:24.583: INFO: Pod csi-node-driver-8wkxf requesting resource cpu=0m on Node node-02
  May 10 05:28:24.583: INFO: Pod csi-node-driver-fgjb5 requesting resource cpu=0m on Node ub-test
  May 10 05:28:24.583: INFO: Pod csi-node-driver-wrcs7 requesting resource cpu=0m on Node node-01
  May 10 05:28:24.583: INFO: Pod coredns-5d78c9869d-d9nxz requesting resource cpu=100m on Node ub-test
  May 10 05:28:24.583: INFO: Pod coredns-5d78c9869d-w9ngw requesting resource cpu=100m on Node ub-test
  May 10 05:28:24.583: INFO: Pod etcd-ub-test requesting resource cpu=100m on Node ub-test
  May 10 05:28:24.583: INFO: Pod kube-apiserver-ub-test requesting resource cpu=250m on Node ub-test
  May 10 05:28:24.583: INFO: Pod kube-controller-manager-ub-test requesting resource cpu=200m on Node ub-test
  May 10 05:28:24.583: INFO: Pod kube-proxy-96kwf requesting resource cpu=0m on Node node-02
  May 10 05:28:24.583: INFO: Pod kube-proxy-hqbzr requesting resource cpu=0m on Node ub-test
  May 10 05:28:24.583: INFO: Pod kube-proxy-jtvxj requesting resource cpu=0m on Node node-01
  May 10 05:28:24.583: INFO: Pod kube-scheduler-ub-test requesting resource cpu=100m on Node ub-test
  May 10 05:28:24.583: INFO: Pod kube-sealos-lvscare-node-01 requesting resource cpu=0m on Node node-01
  May 10 05:28:24.583: INFO: Pod kube-sealos-lvscare-node-02 requesting resource cpu=0m on Node node-02
  May 10 05:28:24.583: INFO: Pod sonobuoy requesting resource cpu=0m on Node node-01
  May 10 05:28:24.583: INFO: Pod sonobuoy-e2e-job-9c188259183c425e requesting resource cpu=0m on Node node-01
  May 10 05:28:24.583: INFO: Pod sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-254cd requesting resource cpu=0m on Node node-02
  May 10 05:28:24.583: INFO: Pod sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-44wmx requesting resource cpu=0m on Node ub-test
  May 10 05:28:24.583: INFO: Pod sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-xwr24 requesting resource cpu=0m on Node node-01
  May 10 05:28:24.583: INFO: Pod tigera-operator-7bf7458-fcvpq requesting resource cpu=0m on Node ub-test
  STEP: Starting Pods to consume most of the cluster CPU. @ 05/10/23 05:28:24.583
  May 10 05:28:24.583: INFO: Creating a pod which consumes cpu=5600m on Node node-01
  May 10 05:28:24.593: INFO: Creating a pod which consumes cpu=5600m on Node node-02
  May 10 05:28:24.598: INFO: Creating a pod which consumes cpu=10605m on Node ub-test
  E0510 05:28:25.069141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:26.069364      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 05/10/23 05:28:26.62
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-5e663a2f-0bc6-4a05-b545-0803a230cb5a.175db10ce30b1e66], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5711/filler-pod-5e663a2f-0bc6-4a05-b545-0803a230cb5a to ub-test] @ 05/10/23 05:28:26.622
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-5e663a2f-0bc6-4a05-b545-0803a230cb5a.175db10d1ccaed4e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/10/23 05:28:26.622
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-5e663a2f-0bc6-4a05-b545-0803a230cb5a.175db10d2c865af6], Reason = [Created], Message = [Created container filler-pod-5e663a2f-0bc6-4a05-b545-0803a230cb5a] @ 05/10/23 05:28:26.622
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-5e663a2f-0bc6-4a05-b545-0803a230cb5a.175db10d333e006f], Reason = [Started], Message = [Started container filler-pod-5e663a2f-0bc6-4a05-b545-0803a230cb5a] @ 05/10/23 05:28:26.622
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-9d4220ec-7c31-4296-95e9-691cdfc8c1f7.175db10ce1eeb173], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5711/filler-pod-9d4220ec-7c31-4296-95e9-691cdfc8c1f7 to node-01] @ 05/10/23 05:28:26.622
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-9d4220ec-7c31-4296-95e9-691cdfc8c1f7.175db10d178309c7], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/10/23 05:28:26.622
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-9d4220ec-7c31-4296-95e9-691cdfc8c1f7.175db10d27efbe86], Reason = [Created], Message = [Created container filler-pod-9d4220ec-7c31-4296-95e9-691cdfc8c1f7] @ 05/10/23 05:28:26.622
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-9d4220ec-7c31-4296-95e9-691cdfc8c1f7.175db10d2dc217bb], Reason = [Started], Message = [Started container filler-pod-9d4220ec-7c31-4296-95e9-691cdfc8c1f7] @ 05/10/23 05:28:26.622
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ba691835-de46-40d0-abd8-8aad27b06fa0.175db10ce2556578], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5711/filler-pod-ba691835-de46-40d0-abd8-8aad27b06fa0 to node-02] @ 05/10/23 05:28:26.622
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ba691835-de46-40d0-abd8-8aad27b06fa0.175db10d1a5a8f87], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/10/23 05:28:26.622
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ba691835-de46-40d0-abd8-8aad27b06fa0.175db10d2a86c017], Reason = [Created], Message = [Created container filler-pod-ba691835-de46-40d0-abd8-8aad27b06fa0] @ 05/10/23 05:28:26.622
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ba691835-de46-40d0-abd8-8aad27b06fa0.175db10d2fa890d3], Reason = [Started], Message = [Started container filler-pod-ba691835-de46-40d0-abd8-8aad27b06fa0] @ 05/10/23 05:28:26.622
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.175db10d5a9a47bf], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 05/10/23 05:28:26.632
  E0510 05:28:27.070406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node node-02 @ 05/10/23 05:28:27.632
  STEP: verifying the node doesn't have the label node @ 05/10/23 05:28:27.649
  STEP: removing the label node off the node ub-test @ 05/10/23 05:28:27.651
  STEP: verifying the node doesn't have the label node @ 05/10/23 05:28:27.665
  STEP: removing the label node off the node node-01 @ 05/10/23 05:28:27.668
  STEP: verifying the node doesn't have the label node @ 05/10/23 05:28:27.681
  May 10 05:28:27.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5711" for this suite. @ 05/10/23 05:28:27.686
• [3.204 seconds]
------------------------------
S
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 05/10/23 05:28:27.692
  May 10 05:28:27.692: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename runtimeclass @ 05/10/23 05:28:27.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:28:27.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:28:27.715
  STEP: getting /apis @ 05/10/23 05:28:27.718
  STEP: getting /apis/node.k8s.io @ 05/10/23 05:28:27.723
  STEP: getting /apis/node.k8s.io/v1 @ 05/10/23 05:28:27.724
  STEP: creating @ 05/10/23 05:28:27.725
  STEP: watching @ 05/10/23 05:28:27.741
  May 10 05:28:27.741: INFO: starting watch
  STEP: getting @ 05/10/23 05:28:27.75
  STEP: listing @ 05/10/23 05:28:27.752
  STEP: patching @ 05/10/23 05:28:27.754
  STEP: updating @ 05/10/23 05:28:27.759
  May 10 05:28:27.763: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 05/10/23 05:28:27.763
  STEP: deleting a collection @ 05/10/23 05:28:27.771
  May 10 05:28:27.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1041" for this suite. @ 05/10/23 05:28:27.783
• [0.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 05/10/23 05:28:27.79
  May 10 05:28:27.790: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:28:27.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:28:27.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:28:27.811
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 05:28:27.813
  E0510 05:28:28.071080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:29.071746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:30.072111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:31.072302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:32.073043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:33.073432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:28:33.855
  May 10 05:28:33.856: INFO: Trying to get logs from node ub-test pod downwardapi-volume-d6a4fc8d-60b0-4668-9133-e49de1ee80f7 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 05:28:33.86
  May 10 05:28:33.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2320" for this suite. @ 05/10/23 05:28:33.876
• [6.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 05/10/23 05:28:33.889
  May 10 05:28:33.889: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 05:28:33.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:28:33.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:28:33.907
  STEP: Creating configMap configmap-5588/configmap-test-094e2c4d-433d-4952-9f20-1b138ca40b96 @ 05/10/23 05:28:33.908
  STEP: Creating a pod to test consume configMaps @ 05/10/23 05:28:33.915
  E0510 05:28:34.073445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:35.073724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:36.074475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:37.075142      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:28:37.927
  May 10 05:28:37.934: INFO: Trying to get logs from node node-01 pod pod-configmaps-f51b7cbb-cf51-48f7-b3bd-760fbd754019 container env-test: <nil>
  STEP: delete the pod @ 05/10/23 05:28:37.938
  May 10 05:28:37.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5588" for this suite. @ 05/10/23 05:28:37.952
• [4.068 seconds]
------------------------------
S
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 05/10/23 05:28:37.957
  May 10 05:28:37.957: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 05:28:37.958
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:28:37.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:28:37.971
  STEP: fetching services @ 05/10/23 05:28:37.973
  May 10 05:28:37.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8263" for this suite. @ 05/10/23 05:28:37.977
• [0.024 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 05/10/23 05:28:37.982
  May 10 05:28:37.982: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 05:28:37.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:28:37.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:28:37.992
  STEP: Creating secret with name secret-test-93214af7-5cfa-4d01-94a4-da96438d4941 @ 05/10/23 05:28:37.997
  STEP: Creating a pod to test consume secrets @ 05/10/23 05:28:38
  E0510 05:28:38.075968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:39.076189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:40.077185      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:41.077397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:28:42.012
  May 10 05:28:42.014: INFO: Trying to get logs from node node-02 pod pod-secrets-a23ac463-8252-40ee-8782-d1f4da2c8783 container secret-env-test: <nil>
  STEP: delete the pod @ 05/10/23 05:28:42.02
  May 10 05:28:42.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1674" for this suite. @ 05/10/23 05:28:42.042
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 05/10/23 05:28:42.047
  May 10 05:28:42.047: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename resourcequota @ 05/10/23 05:28:42.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:28:42.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:28:42.063
  E0510 05:28:42.078258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:43.078460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:44.078746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:45.079788      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:46.080596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:47.081074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:48.081290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:49.082380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:50.082996      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:51.083927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:52.084582      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:53.085463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:54.085876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:55.086216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:56.086544      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:57.087135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:28:58.087956      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 05/10/23 05:28:59.068
  E0510 05:28:59.088657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:00.089320      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:01.089967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:02.091021      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:03.091197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/10/23 05:29:04.07
  STEP: Ensuring resource quota status is calculated @ 05/10/23 05:29:04.075
  E0510 05:29:04.091527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:05.091778      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 05/10/23 05:29:06.077
  STEP: Ensuring resource quota status captures configMap creation @ 05/10/23 05:29:06.088
  E0510 05:29:06.091863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:07.092438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:08.092924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 05/10/23 05:29:08.094
  STEP: Ensuring resource quota status released usage @ 05/10/23 05:29:08.098
  E0510 05:29:09.093641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:10.094562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:29:10.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2166" for this suite. @ 05/10/23 05:29:10.103
• [28.060 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 05/10/23 05:29:10.108
  May 10 05:29:10.108: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename var-expansion @ 05/10/23 05:29:10.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:29:10.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:29:10.127
  STEP: Creating a pod to test substitution in container's args @ 05/10/23 05:29:10.129
  E0510 05:29:11.094958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:12.095480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:13.095998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:14.096264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:29:14.15
  May 10 05:29:14.151: INFO: Trying to get logs from node node-02 pod var-expansion-7c4cd44b-92db-4c0f-a841-d0f5c298006b container dapi-container: <nil>
  STEP: delete the pod @ 05/10/23 05:29:14.155
  May 10 05:29:14.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1284" for this suite. @ 05/10/23 05:29:14.18
• [4.078 seconds]
------------------------------
S
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 05/10/23 05:29:14.185
  May 10 05:29:14.185: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename cronjob @ 05/10/23 05:29:14.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:29:14.2
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:29:14.202
  STEP: Creating a suspended cronjob @ 05/10/23 05:29:14.204
  STEP: Ensuring no jobs are scheduled @ 05/10/23 05:29:14.21
  E0510 05:29:15.096670      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:16.096981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:17.097787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:18.098056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:19.098181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:20.098370      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:21.098619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:22.099028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:23.099130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:24.099375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:25.099474      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:26.099660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:27.100500      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:28.100645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:29.101373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:30.101600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:31.102284      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:32.102808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:33.103364      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:34.103579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:35.103702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:36.103929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:37.104962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:38.105157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:39.105743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:40.105948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:41.106820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:42.107280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:43.107399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:44.107592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:45.107952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:46.108079      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:47.108966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:48.109178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:49.109907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:50.110139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:51.110257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:52.110729      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:53.111344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:54.111456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:55.111574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:56.111782      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:57.112666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:58.112846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:29:59.113396      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:00.113478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:01.113561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:02.113732      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:03.113781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:04.113990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:05.114103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:06.114327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:07.115138      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:08.115421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:09.115515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:10.115719      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:11.115874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:12.116457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:13.116572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:14.116818      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:15.116921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:16.117124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:17.117952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:18.118168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:19.118834      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:20.119067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:21.119542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:22.119941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:23.120686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:24.120938      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:25.121435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:26.121641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:27.121759      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:28.122162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:29.122392      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:30.122633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:31.123759      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:32.124190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:33.124278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:34.124350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:35.125061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:36.125307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:37.125414      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:38.126280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:39.127014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:40.127253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:41.127343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:42.127892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:43.128585      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:44.128810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:45.129558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:46.129754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:47.130763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:48.130985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:49.131643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:50.131888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:51.132514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:52.132983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:53.134075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:54.134329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:55.135125      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:56.135335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:57.136013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:58.136267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:30:59.136901      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:00.137119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:01.137561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:02.137862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:03.138082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:04.138312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:05.139008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:06.139325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:07.140125      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:08.140352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:09.140823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:10.141087      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:11.141990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:12.142402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:13.143008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:14.143433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:15.143833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:16.144040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:17.145083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:18.145302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:19.146070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:20.146304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:21.146820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:22.147835      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:23.148524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:24.148773      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:25.149761      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:26.150131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:27.151148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:28.151362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:29.151543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:30.151753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:31.152380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:32.152834      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:33.153245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:34.153473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:35.153717      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:36.153984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:37.154761      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:38.155001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:39.155448      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:40.155659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:41.156286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:42.156902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:43.157400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:44.157611      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:45.157803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:46.158006      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:47.158620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:48.158865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:49.158994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:50.159198      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:51.159662      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:52.159963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:53.160613      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:54.160936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:55.161848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:56.162083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:57.163149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:58.163441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:31:59.163919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:00.164197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:01.165125      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:02.165447      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:03.165902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:04.166102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:05.166954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:06.167207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:07.167402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:08.167697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:09.168614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:10.168857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:11.169532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:12.170145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:13.170275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:14.170598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:15.171312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:16.171542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:17.171878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:18.172094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:19.173031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:20.173242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:21.173707      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:22.174030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:23.174222      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:24.174447      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:25.175013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:26.175189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:27.176031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:28.176262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:29.177034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:30.177240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:31.177662      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:32.178038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:33.178617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:34.179015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:35.179169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:36.179369      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:37.180227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:38.180554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:39.181127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:40.181332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:41.181771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:42.182113      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:43.182514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:44.182735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:45.183456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:46.183545      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:47.184562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:48.184775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:49.185076      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:50.185877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:51.186554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:52.186854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:53.187292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:54.187962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:55.188051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:56.188267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:57.188546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:58.188775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:32:59.188882      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:00.189090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:01.189522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:02.189805      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:03.190833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:04.191040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:05.191762      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:06.191971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:07.192681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:08.193751      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:09.194017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:10.195062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:11.195224      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:12.195753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:13.195977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:14.196199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:15.196892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:16.197104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:17.198067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:18.198254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:19.198856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:20.199057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:21.199937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:22.200263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:23.200560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:24.200772      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:25.201429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:26.201685      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:27.202236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:28.202565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:29.203499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:30.204598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:31.205007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:32.205336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:33.206210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:34.206508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:35.207129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:36.207409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:37.208314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:38.208717      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:39.209731      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:40.209957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:41.210459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:42.210829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:43.211082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:44.211302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:45.211830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:46.212658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:47.213040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:48.213339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:49.214242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:50.214696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:51.214953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:52.214973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:53.215185      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:54.215803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:55.216000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:56.216304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:57.216716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:58.216890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:33:59.217207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:00.217633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:01.217850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:02.218442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:03.218661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:04.219685      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:05.219900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:06.220899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:07.220998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:08.221911      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:09.222154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:10.223018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:11.223234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:12.223833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:13.223983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 05/10/23 05:34:14.215
  STEP: Removing cronjob @ 05/10/23 05:34:14.216
  May 10 05:34:14.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6602" for this suite. @ 05/10/23 05:34:14.223
  E0510 05:34:14.223988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
• [300.041 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 05/10/23 05:34:14.227
  May 10 05:34:14.227: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/10/23 05:34:14.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:34:14.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:34:14.239
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 05/10/23 05:34:14.241
  May 10 05:34:14.241: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 05:34:15.224872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:34:15.694: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 05:34:16.225989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:17.226823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:18.227255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:19.227493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:20.227783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:21.228953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:34:21.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6695" for this suite. @ 05/10/23 05:34:21.789
• [7.566 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 05/10/23 05:34:21.794
  May 10 05:34:21.794: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 05:34:21.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:34:21.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:34:21.806
  STEP: Creating the pod @ 05/10/23 05:34:21.814
  E0510 05:34:22.229343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:23.229916      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:24.230150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:34:24.346: INFO: Successfully updated pod "annotationupdate35e6c29e-ae11-43ba-a9c3-4ffaa318d7be"
  E0510 05:34:25.230889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:26.231197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:27.232129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:28.232365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:34:28.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5508" for this suite. @ 05/10/23 05:34:28.365
• [6.576 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 05/10/23 05:34:28.37
  May 10 05:34:28.370: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 05:34:28.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:34:28.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:34:28.388
  STEP: Creating secret with name secret-test-1bf617d8-07e1-48f2-a3d7-ccd6ced40549 @ 05/10/23 05:34:28.391
  STEP: Creating a pod to test consume secrets @ 05/10/23 05:34:28.395
  E0510 05:34:29.233144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:30.233537      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:31.233905      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:32.234307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:34:32.414
  May 10 05:34:32.416: INFO: Trying to get logs from node node-02 pod pod-secrets-89fa249b-db4d-4657-b725-73a99586376e container secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 05:34:32.42
  May 10 05:34:32.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8904" for this suite. @ 05/10/23 05:34:32.439
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 05/10/23 05:34:32.444
  May 10 05:34:32.444: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 05:34:32.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:34:32.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:34:32.46
  STEP: creating Agnhost RC @ 05/10/23 05:34:32.462
  May 10 05:34:32.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5698 create -f -'
  May 10 05:34:32.774: INFO: stderr: ""
  May 10 05:34:32.774: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/10/23 05:34:32.774
  E0510 05:34:33.235057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:34:33.777: INFO: Selector matched 1 pods for map[app:agnhost]
  May 10 05:34:33.777: INFO: Found 0 / 1
  E0510 05:34:34.235282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:34:34.778: INFO: Selector matched 1 pods for map[app:agnhost]
  May 10 05:34:34.778: INFO: Found 0 / 1
  E0510 05:34:35.235596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:34:35.777: INFO: Selector matched 1 pods for map[app:agnhost]
  May 10 05:34:35.777: INFO: Found 1 / 1
  May 10 05:34:35.778: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 05/10/23 05:34:35.778
  May 10 05:34:35.779: INFO: Selector matched 1 pods for map[app:agnhost]
  May 10 05:34:35.779: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 10 05:34:35.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5698 patch pod agnhost-primary-8675r -p {"metadata":{"annotations":{"x":"y"}}}'
  May 10 05:34:35.848: INFO: stderr: ""
  May 10 05:34:35.848: INFO: stdout: "pod/agnhost-primary-8675r patched\n"
  STEP: checking annotations @ 05/10/23 05:34:35.848
  May 10 05:34:35.850: INFO: Selector matched 1 pods for map[app:agnhost]
  May 10 05:34:35.850: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 10 05:34:35.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5698" for this suite. @ 05/10/23 05:34:35.853
• [3.413 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 05/10/23 05:34:35.858
  May 10 05:34:35.858: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename security-context @ 05/10/23 05:34:35.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:34:35.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:34:35.869
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/10/23 05:34:35.871
  E0510 05:34:36.236157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:37.237028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:38.237168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:39.237479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:34:39.89
  May 10 05:34:39.892: INFO: Trying to get logs from node node-02 pod security-context-e6faa651-d711-4932-a387-7355efb9623b container test-container: <nil>
  STEP: delete the pod @ 05/10/23 05:34:39.897
  May 10 05:34:39.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9314" for this suite. @ 05/10/23 05:34:39.916
• [4.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 05/10/23 05:34:39.922
  May 10 05:34:39.922: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pods @ 05/10/23 05:34:39.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:34:39.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:34:39.937
  E0510 05:34:40.237546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:41.237899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:42.238557      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:43.238710      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:44.239806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:45.239893      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:46.240040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:47.240123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:34:47.981
  May 10 05:34:47.982: INFO: Trying to get logs from node node-02 pod client-envvars-88e36f26-f587-449e-996f-984f0cd2aa51 container env3cont: <nil>
  STEP: delete the pod @ 05/10/23 05:34:47.987
  May 10 05:34:48.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5512" for this suite. @ 05/10/23 05:34:48.009
• [8.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 05/10/23 05:34:48.015
  May 10 05:34:48.015: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename dns @ 05/10/23 05:34:48.015
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:34:48.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:34:48.027
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 05/10/23 05:34:48.029
  May 10 05:34:48.037: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1539  529c442d-a563-4fa9-993d-d978701f373e 1067495 0 2023-05-10 05:34:48 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-10 05:34:48 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5rzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5rzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0510 05:34:48.241153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:49.241693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 05/10/23 05:34:50.043
  May 10 05:34:50.043: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1539 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:34:50.043: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:34:50.043: INFO: ExecWithOptions: Clientset creation
  May 10 05:34:50.043: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1539/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 05/10/23 05:34:50.121
  May 10 05:34:50.121: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1539 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:34:50.121: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:34:50.122: INFO: ExecWithOptions: Clientset creation
  May 10 05:34:50.122: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1539/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 10 05:34:50.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 10 05:34:50.172: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-1539" for this suite. @ 05/10/23 05:34:50.19
• [2.179 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 05/10/23 05:34:50.194
  May 10 05:34:50.194: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename field-validation @ 05/10/23 05:34:50.195
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:34:50.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:34:50.212
  May 10 05:34:50.214: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 05:34:50.241963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:51.242681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:52.243100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0510 05:34:52.755430      22 warnings.go:70] unknown field "alpha"
  W0510 05:34:52.755477      22 warnings.go:70] unknown field "beta"
  W0510 05:34:52.755485      22 warnings.go:70] unknown field "delta"
  W0510 05:34:52.755530      22 warnings.go:70] unknown field "epsilon"
  W0510 05:34:52.755535      22 warnings.go:70] unknown field "gamma"
  May 10 05:34:52.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2060" for this suite. @ 05/10/23 05:34:52.778
• [2.589 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 05/10/23 05:34:52.783
  May 10 05:34:52.783: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 05:34:52.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:34:52.798
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:34:52.8
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-2897 @ 05/10/23 05:34:52.803
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/10/23 05:34:52.815
  STEP: creating service externalsvc in namespace services-2897 @ 05/10/23 05:34:52.815
  STEP: creating replication controller externalsvc in namespace services-2897 @ 05/10/23 05:34:52.826
  I0510 05:34:52.834929      22 runners.go:194] Created replication controller with name: externalsvc, namespace: services-2897, replica count: 2
  E0510 05:34:53.243771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:54.244421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:55.244514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 05:34:55.886330      22 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 05/10/23 05:34:55.888
  May 10 05:34:55.904: INFO: Creating new exec pod
  E0510 05:34:56.244620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:57.245369      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:58.245685      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:34:59.245914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:34:59.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-2897 exec execpodsn46p -- /bin/sh -x -c nslookup nodeport-service.services-2897.svc.cluster.local'
  E0510 05:35:00.245921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:01.246110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:35:02.095: INFO: stderr: "+ nslookup nodeport-service.services-2897.svc.cluster.local\n"
  May 10 05:35:02.095: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-2897.svc.cluster.local\tcanonical name = externalsvc.services-2897.svc.cluster.local.\nName:\texternalsvc.services-2897.svc.cluster.local\nAddress: 10.96.3.41\n\n"
  May 10 05:35:02.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-2897, will wait for the garbage collector to delete the pods @ 05/10/23 05:35:02.098
  May 10 05:35:02.155: INFO: Deleting ReplicationController externalsvc took: 5.434154ms
  E0510 05:35:02.246592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:35:02.255: INFO: Terminating ReplicationController externalsvc pods took: 100.392819ms
  E0510 05:35:03.247450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:04.248154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:35:05.171: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-2897" for this suite. @ 05/10/23 05:35:05.178
• [12.405 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 05/10/23 05:35:05.188
  May 10 05:35:05.188: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename sched-preemption @ 05/10/23 05:35:05.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:35:05.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:35:05.206
  E0510 05:35:05.248458      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:35:05.307: INFO: Waiting up to 1m0s for all nodes to be ready
  E0510 05:35:06.248673      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:07.249226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:08.249366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:09.249503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:10.250383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:11.250858      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:12.251441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:13.252472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:14.252593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:15.253402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:16.253515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:17.254099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:18.254330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:19.254597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:20.255215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:21.255475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:22.256085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:23.256278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:24.256914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:25.257102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:26.257901      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:27.258412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:28.259160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:29.259461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:30.260541      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:31.261393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:32.262590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:33.262825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:34.263684      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:35.263961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:36.264061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:37.264106      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:38.264316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:39.264344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:40.264469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:41.264599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:42.264910      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:43.265207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:44.265367      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:45.265590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:46.266278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:47.267022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:48.267407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:49.267539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:50.267718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:51.267809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:52.267973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:53.268208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:54.268449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:55.268592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:56.268739      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:57.269362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:58.269531      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:35:59.269775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:00.270387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:01.270521      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:02.270568      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:03.270805      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:04.271232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:05.271379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:36:05.327: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/10/23 05:36:05.329
  May 10 05:36:05.345: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 10 05:36:05.352: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May 10 05:36:05.370: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 10 05:36:05.382: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  May 10 05:36:05.400: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  May 10 05:36:05.408: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/10/23 05:36:05.408
  E0510 05:36:06.271538      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:07.271647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:08.272387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:09.272614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 05/10/23 05:36:09.425
  E0510 05:36:10.272739      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:11.272890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:12.273017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:13.273289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:36:13.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1889" for this suite. @ 05/10/23 05:36:13.501
• [68.317 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 05/10/23 05:36:13.507
  May 10 05:36:13.507: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 05:36:13.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:36:13.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:36:13.526
  May 10 05:36:13.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4295" for this suite. @ 05/10/23 05:36:13.532
• [0.029 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 05/10/23 05:36:13.536
  May 10 05:36:13.536: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 05:36:13.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:36:13.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:36:13.555
  STEP: creating service in namespace services-9896 @ 05/10/23 05:36:13.557
  STEP: creating service affinity-nodeport-transition in namespace services-9896 @ 05/10/23 05:36:13.557
  STEP: creating replication controller affinity-nodeport-transition in namespace services-9896 @ 05/10/23 05:36:13.567
  I0510 05:36:13.575555      22 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-9896, replica count: 3
  E0510 05:36:14.273520      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:15.273909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:16.274340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 05:36:16.626948      22 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 10 05:36:16.633: INFO: Creating new exec pod
  E0510 05:36:17.275064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:18.275422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:19.275610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:36:19.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-9896 exec execpod-affinityn2h4m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  May 10 05:36:20.019: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  May 10 05:36:20.019: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 05:36:20.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-9896 exec execpod-affinityn2h4m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.92 80'
  E0510 05:36:20.275696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:36:20.286: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.2.92 80\nConnection to 10.96.2.92 80 port [tcp/http] succeeded!\n"
  May 10 05:36:20.286: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 05:36:20.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-9896 exec execpod-affinityn2h4m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.60.152 32097'
  May 10 05:36:20.418: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.60.152 32097\nConnection to 192.168.60.152 32097 port [tcp/*] succeeded!\n"
  May 10 05:36:20.418: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 05:36:20.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-9896 exec execpod-affinityn2h4m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.60.83 32097'
  May 10 05:36:20.549: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.60.83 32097\nConnection to 192.168.60.83 32097 port [tcp/*] succeeded!\n"
  May 10 05:36:20.549: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 05:36:20.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-9896 exec execpod-affinityn2h4m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.60.83:32097/ ; done'
  May 10 05:36:20.748: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n"
  May 10 05:36:20.749: INFO: stdout: "\naffinity-nodeport-transition-thj7p\naffinity-nodeport-transition-nd6wz\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-thj7p\naffinity-nodeport-transition-nd6wz\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-thj7p\naffinity-nodeport-transition-nd6wz\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-thj7p\naffinity-nodeport-transition-nd6wz\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-thj7p\naffinity-nodeport-transition-nd6wz\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-thj7p"
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-thj7p
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-nd6wz
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-thj7p
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-nd6wz
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-thj7p
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-nd6wz
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-thj7p
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-nd6wz
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-thj7p
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-nd6wz
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.749: INFO: Received response from host: affinity-nodeport-transition-thj7p
  May 10 05:36:20.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-9896 exec execpod-affinityn2h4m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.60.83:32097/ ; done'
  May 10 05:36:20.945: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:32097/\n"
  May 10 05:36:20.945: INFO: stdout: "\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9\naffinity-nodeport-transition-mpvl9"
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Received response from host: affinity-nodeport-transition-mpvl9
  May 10 05:36:20.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 10 05:36:20.947: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9896, will wait for the garbage collector to delete the pods @ 05/10/23 05:36:20.962
  May 10 05:36:21.019: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.72749ms
  May 10 05:36:21.120: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.723486ms
  E0510 05:36:21.276080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:22.276999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:23.277701      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-9896" for this suite. @ 05/10/23 05:36:23.741
• [10.210 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 05/10/23 05:36:23.746
  May 10 05:36:23.746: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename dns @ 05/10/23 05:36:23.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:36:23.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:36:23.766
  STEP: Creating a test headless service @ 05/10/23 05:36:23.768
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9074.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9074.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 05/10/23 05:36:23.772
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9074.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9074.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 05/10/23 05:36:23.772
  STEP: creating a pod to probe DNS @ 05/10/23 05:36:23.772
  STEP: submitting the pod to kubernetes @ 05/10/23 05:36:23.772
  E0510 05:36:24.278385      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:25.278577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:26.278656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:27.278769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/10/23 05:36:27.796
  STEP: looking for the results for each expected name from probers @ 05/10/23 05:36:27.798
  May 10 05:36:27.808: INFO: DNS probes using dns-9074/dns-test-a388ce47-d068-476b-aebc-a5ea240ab2ba succeeded

  May 10 05:36:27.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 05:36:27.811
  STEP: deleting the test headless service @ 05/10/23 05:36:27.83
  STEP: Destroying namespace "dns-9074" for this suite. @ 05/10/23 05:36:27.839
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 05/10/23 05:36:27.845
  May 10 05:36:27.845: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:36:27.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:36:27.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:36:27.863
  STEP: Setting up server cert @ 05/10/23 05:36:27.883
  E0510 05:36:28.278893      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:36:28.351
  STEP: Deploying the webhook pod @ 05/10/23 05:36:28.356
  STEP: Wait for the deployment to be ready @ 05/10/23 05:36:28.371
  May 10 05:36:28.377: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 05:36:29.279186      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:30.280419      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:36:30.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 10, 5, 36, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 36, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 5, 36, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 5, 36, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 05:36:31.281275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:32.281664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:36:32.388
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:36:32.403
  E0510 05:36:33.282139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:36:33.403: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/10/23 05:36:33.47
  STEP: Creating a configMap that should be mutated @ 05/10/23 05:36:33.482
  STEP: Deleting the collection of validation webhooks @ 05/10/23 05:36:33.504
  STEP: Creating a configMap that should not be mutated @ 05/10/23 05:36:33.543
  May 10 05:36:33.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1809" for this suite. @ 05/10/23 05:36:33.585
  STEP: Destroying namespace "webhook-markers-6997" for this suite. @ 05/10/23 05:36:33.594
• [5.754 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 05/10/23 05:36:33.599
  May 10 05:36:33.599: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:36:33.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:36:33.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:36:33.616
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 05:36:33.618
  E0510 05:36:34.282649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:35.282992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:36.283314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:37.283913      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:38.284571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:39.284786      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:36:39.637
  May 10 05:36:39.639: INFO: Trying to get logs from node node-02 pod downwardapi-volume-4947331e-44ef-4be0-ac15-6027e0079041 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 05:36:39.643
  May 10 05:36:39.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8134" for this suite. @ 05/10/23 05:36:39.658
• [6.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 05/10/23 05:36:39.664
  May 10 05:36:39.664: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename watch @ 05/10/23 05:36:39.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:36:39.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:36:39.678
  STEP: creating a watch on configmaps with a certain label @ 05/10/23 05:36:39.68
  STEP: creating a new configmap @ 05/10/23 05:36:39.682
  STEP: modifying the configmap once @ 05/10/23 05:36:39.685
  STEP: changing the label value of the configmap @ 05/10/23 05:36:39.69
  STEP: Expecting to observe a delete notification for the watched object @ 05/10/23 05:36:39.694
  May 10 05:36:39.694: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9268  c3ccbada-6a58-42c7-bde2-ee3d9b49a112 1068496 0 2023-05-10 05:36:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-10 05:36:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:36:39.694: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9268  c3ccbada-6a58-42c7-bde2-ee3d9b49a112 1068497 0 2023-05-10 05:36:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-10 05:36:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:36:39.694: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9268  c3ccbada-6a58-42c7-bde2-ee3d9b49a112 1068498 0 2023-05-10 05:36:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-10 05:36:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 05/10/23 05:36:39.694
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 05/10/23 05:36:39.702
  E0510 05:36:40.285817      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:41.286215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:42.286583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:43.286820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:44.287114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:45.287357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:46.287602      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:47.288154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:48.288337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:49.288540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 05/10/23 05:36:49.703
  STEP: modifying the configmap a third time @ 05/10/23 05:36:49.709
  STEP: deleting the configmap @ 05/10/23 05:36:49.714
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 05/10/23 05:36:49.717
  May 10 05:36:49.717: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9268  c3ccbada-6a58-42c7-bde2-ee3d9b49a112 1068536 0 2023-05-10 05:36:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-10 05:36:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:36:49.717: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9268  c3ccbada-6a58-42c7-bde2-ee3d9b49a112 1068537 0 2023-05-10 05:36:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-10 05:36:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:36:49.717: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9268  c3ccbada-6a58-42c7-bde2-ee3d9b49a112 1068538 0 2023-05-10 05:36:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-10 05:36:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:36:49.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9268" for this suite. @ 05/10/23 05:36:49.72
• [10.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 05/10/23 05:36:49.728
  May 10 05:36:49.728: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/10/23 05:36:49.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:36:49.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:36:49.738
  STEP: create the container to handle the HTTPGet hook request. @ 05/10/23 05:36:49.743
  E0510 05:36:50.289133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:51.289655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/10/23 05:36:51.824
  E0510 05:36:52.289727      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:53.289955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 05/10/23 05:36:53.835
  E0510 05:36:54.290200      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:55.290273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:56.290645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:57.290709      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:36:58.290882      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 05/10/23 05:36:58.566
  May 10 05:36:58.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6742" for this suite. @ 05/10/23 05:36:58.574
• [8.859 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 05/10/23 05:36:58.588
  May 10 05:36:58.588: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pods @ 05/10/23 05:36:58.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:36:58.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:36:58.602
  STEP: creating a Pod with a static label @ 05/10/23 05:36:58.608
  STEP: watching for Pod to be ready @ 05/10/23 05:36:58.617
  May 10 05:36:58.619: INFO: observed Pod pod-test in namespace pods-2018 in phase Pending with labels: map[test-pod-static:true] & conditions []
  May 10 05:36:58.625: INFO: observed Pod pod-test in namespace pods-2018 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:36:58 +0000 UTC  }]
  May 10 05:36:58.641: INFO: observed Pod pod-test in namespace pods-2018 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:36:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:36:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:36:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:36:58 +0000 UTC  }]
  May 10 05:36:59.165: INFO: observed Pod pod-test in namespace pods-2018 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:36:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:36:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:36:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:36:58 +0000 UTC  }]
  E0510 05:36:59.291349      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:00.291502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:37:00.331: INFO: Found Pod pod-test in namespace pods-2018 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:36:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:37:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:37:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 05:36:58 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 05/10/23 05:37:00.333
  STEP: getting the Pod and ensuring that it's patched @ 05/10/23 05:37:00.342
  STEP: replacing the Pod's status Ready condition to False @ 05/10/23 05:37:00.344
  STEP: check the Pod again to ensure its Ready conditions are False @ 05/10/23 05:37:00.354
  STEP: deleting the Pod via a Collection with a LabelSelector @ 05/10/23 05:37:00.354
  STEP: watching for the Pod to be deleted @ 05/10/23 05:37:00.359
  May 10 05:37:00.360: INFO: observed event type MODIFIED
  E0510 05:37:01.291644      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:37:02.044: INFO: observed event type MODIFIED
  E0510 05:37:02.292381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:37:03.236: INFO: observed event type MODIFIED
  E0510 05:37:03.292577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:37:03.360: INFO: observed event type MODIFIED
  E0510 05:37:04.292731      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:37:04.460: INFO: observed event type MODIFIED
  May 10 05:37:04.470: INFO: observed event type MODIFIED
  May 10 05:37:04.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2018" for this suite. @ 05/10/23 05:37:04.478
• [5.894 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 05/10/23 05:37:04.482
  May 10 05:37:04.482: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/10/23 05:37:04.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:37:04.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:37:04.5
  STEP: Creating 50 configmaps @ 05/10/23 05:37:04.502
  STEP: Creating RC which spawns configmap-volume pods @ 05/10/23 05:37:04.736
  May 10 05:37:04.846: INFO: Pod name wrapped-volume-race-142ab0c6-eaaf-4298-aa6e-ed29b45b80be: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/10/23 05:37:04.846
  E0510 05:37:05.293218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:06.293455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:07.293924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:08.294075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:09.295012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:10.295154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:11.295765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:12.296004      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:13.296988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:14.297144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:15.298030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:16.298154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:17.298276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:18.298440      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 05/10/23 05:37:18.938
  May 10 05:37:18.950: INFO: Pod name wrapped-volume-race-b7ab7e99-67cf-4c97-9373-066766f1e02c: Found 0 pods out of 5
  E0510 05:37:19.299279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:20.299415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:21.299624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:22.299815      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:23.299900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:37:23.956: INFO: Pod name wrapped-volume-race-b7ab7e99-67cf-4c97-9373-066766f1e02c: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/10/23 05:37:23.956
  E0510 05:37:24.300425      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:25.300764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:26.301571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:27.301673      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:28.302589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:29.303693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:30.304678      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:31.305379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:32.306234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:33.307160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 05/10/23 05:37:33.984
  May 10 05:37:33.997: INFO: Pod name wrapped-volume-race-1b0148ec-7321-44bc-8a62-a54ee225501f: Found 0 pods out of 5
  E0510 05:37:34.308065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:35.308386      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:36.308641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:37.309347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:38.309645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:37:39.004: INFO: Pod name wrapped-volume-race-1b0148ec-7321-44bc-8a62-a54ee225501f: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/10/23 05:37:39.004
  E0510 05:37:39.309794      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:40.310254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:41.310504      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:42.311112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:43.311877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:44.311995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:45.313134      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:46.313366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:47.314254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:48.314545      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:37:49.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-1b0148ec-7321-44bc-8a62-a54ee225501f in namespace emptydir-wrapper-7275, will wait for the garbage collector to delete the pods @ 05/10/23 05:37:49.034
  May 10 05:37:49.092: INFO: Deleting ReplicationController wrapped-volume-race-1b0148ec-7321-44bc-8a62-a54ee225501f took: 5.950104ms
  May 10 05:37:49.193: INFO: Terminating ReplicationController wrapped-volume-race-1b0148ec-7321-44bc-8a62-a54ee225501f pods took: 100.819671ms
  E0510 05:37:49.315304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:50.316045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:51.316692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:52.317055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:53.318011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:54.318933      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-b7ab7e99-67cf-4c97-9373-066766f1e02c in namespace emptydir-wrapper-7275, will wait for the garbage collector to delete the pods @ 05/10/23 05:37:55.094
  May 10 05:37:55.152: INFO: Deleting ReplicationController wrapped-volume-race-b7ab7e99-67cf-4c97-9373-066766f1e02c took: 5.694112ms
  May 10 05:37:55.253: INFO: Terminating ReplicationController wrapped-volume-race-b7ab7e99-67cf-4c97-9373-066766f1e02c pods took: 101.040735ms
  E0510 05:37:55.319192      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:56.319899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:57.320731      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:58.321773      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:37:59.322615      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-142ab0c6-eaaf-4298-aa6e-ed29b45b80be in namespace emptydir-wrapper-7275, will wait for the garbage collector to delete the pods @ 05/10/23 05:37:59.654
  May 10 05:37:59.715: INFO: Deleting ReplicationController wrapped-volume-race-142ab0c6-eaaf-4298-aa6e-ed29b45b80be took: 6.876341ms
  May 10 05:37:59.815: INFO: Terminating ReplicationController wrapped-volume-race-142ab0c6-eaaf-4298-aa6e-ed29b45b80be pods took: 100.151574ms
  E0510 05:38:00.323143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:01.323715      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:02.324404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:03.325299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:04.325617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 05/10/23 05:38:04.816
  STEP: Destroying namespace "emptydir-wrapper-7275" for this suite. @ 05/10/23 05:38:05.06
• [60.585 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 05/10/23 05:38:05.067
  May 10 05:38:05.067: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename resourcequota @ 05/10/23 05:38:05.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:38:05.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:38:05.085
  STEP: Counting existing ResourceQuota @ 05/10/23 05:38:05.088
  E0510 05:38:05.326527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:06.327438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:07.327975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:08.328205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:09.329194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/10/23 05:38:10.091
  STEP: Ensuring resource quota status is calculated @ 05/10/23 05:38:10.095
  E0510 05:38:10.329962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:11.330181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 05/10/23 05:38:12.098
  STEP: Ensuring resource quota status captures replicaset creation @ 05/10/23 05:38:12.112
  E0510 05:38:12.331193      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:13.331452      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 05/10/23 05:38:14.115
  STEP: Ensuring resource quota status released usage @ 05/10/23 05:38:14.12
  E0510 05:38:14.331863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:15.332006      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:38:16.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6093" for this suite. @ 05/10/23 05:38:16.125
• [11.062 seconds]
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 05/10/23 05:38:16.129
  May 10 05:38:16.129: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename taint-multiple-pods @ 05/10/23 05:38:16.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:38:16.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:38:16.146
  May 10 05:38:16.148: INFO: Waiting up to 1m0s for all nodes to be ready
  E0510 05:38:16.332126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:17.332482      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:18.333359      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:19.333535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:20.333929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:21.334040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:22.334184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:23.334380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:24.335232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:25.335470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:26.335914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:27.336577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:28.336668      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:29.336899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:30.336994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:31.337193      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:32.338027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:33.338263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:34.338819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:35.339837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:36.340632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:37.341278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:38.341477      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:39.341672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:40.342314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:41.342524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:42.343250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:43.343462      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:44.343537      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:45.343739      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:46.344563      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:47.345334      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:48.345846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:49.345979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:50.346399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:51.346605      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:52.346757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:53.346908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:54.347438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:55.347536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:56.348225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:57.348323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:58.348738      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:38:59.348871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:00.349403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:01.349561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:02.349675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:03.349813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:04.349909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:05.350180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:06.350293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:07.350857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:08.350990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:09.351722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:10.352323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:11.353376      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:12.353735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:13.353883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:14.354066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:15.354700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:39:16.167: INFO: Waiting for terminating namespaces to be deleted...
  May 10 05:39:16.169: INFO: Starting informer...
  STEP: Starting pods... @ 05/10/23 05:39:16.17
  E0510 05:39:16.355137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:39:16.389: INFO: Pod1 is running on node-02. Tainting Node
  E0510 05:39:17.355891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:18.356591      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:19.357646      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:20.357845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:39:20.610: INFO: Pod2 is running on node-02. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/10/23 05:39:20.61
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/10/23 05:39:20.624
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 05/10/23 05:39:20.626
  E0510 05:39:21.357939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:22.358656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:23.358872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:24.359039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:25.359156      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:26.359860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:39:26.667: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0510 05:39:27.360213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:28.360316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:29.360340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:30.360476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:31.360797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:32.361112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:33.361259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:34.361605      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:35.361831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:36.361965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:37.362448      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:38.362669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:39.363530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:40.364633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:41.364770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:42.364879      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:43.365072      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:44.365301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:45.365560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:46.366648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:39:46.704: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  May 10 05:39:46.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/10/23 05:39:46.72
  STEP: Destroying namespace "taint-multiple-pods-7642" for this suite. @ 05/10/23 05:39:46.722
• [90.598 seconds]
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 05/10/23 05:39:46.728
  May 10 05:39:46.728: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename dns @ 05/10/23 05:39:46.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:39:46.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:39:46.75
  STEP: Creating a test headless service @ 05/10/23 05:39:46.752
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local;sleep 1; done
   @ 05/10/23 05:39:46.759
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8127.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local;sleep 1; done
   @ 05/10/23 05:39:46.76
  STEP: creating a pod to probe DNS @ 05/10/23 05:39:46.76
  STEP: submitting the pod to kubernetes @ 05/10/23 05:39:46.76
  E0510 05:39:47.367693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:48.368489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/10/23 05:39:48.784
  STEP: looking for the results for each expected name from probers @ 05/10/23 05:39:48.787
  May 10 05:39:48.789: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:48.791: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:48.793: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:48.795: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:48.797: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:48.798: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:48.800: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:48.801: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:48.801: INFO: Lookups using dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local]

  E0510 05:39:49.368604      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:50.368835      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:51.369120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:52.369453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:53.369631      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:39:53.805: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:53.807: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:53.809: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:53.811: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:53.813: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:53.815: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:53.817: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:53.819: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:53.819: INFO: Lookups using dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local]

  E0510 05:39:54.369890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:55.370141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:56.370307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:57.370819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:39:58.370968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:39:58.805: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:58.807: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:58.809: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:58.811: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:58.813: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:58.815: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:58.817: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:58.818: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:39:58.818: INFO: Lookups using dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local]

  E0510 05:39:59.371645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:00.371893      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:01.372232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:02.372764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:03.372938      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:40:03.805: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:03.807: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:03.809: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:03.810: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:03.812: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:03.814: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:03.815: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:03.817: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:03.817: INFO: Lookups using dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local]

  E0510 05:40:04.373498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:05.373747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:06.373981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:07.374540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:08.374815      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:40:08.806: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:08.808: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:08.810: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:08.812: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:08.814: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:08.816: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:08.818: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:08.820: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:08.820: INFO: Lookups using dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local]

  E0510 05:40:09.375683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:10.375892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:11.376090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:12.376212      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:13.376491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:40:13.805: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:13.807: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:13.809: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:13.810: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:13.812: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:13.814: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:13.816: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:13.818: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:13.818: INFO: Lookups using dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8127.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8127.svc.cluster.local jessie_udp@dns-test-service-2.dns-8127.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8127.svc.cluster.local]

  E0510 05:40:14.376812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:15.377026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:16.377167      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:17.377897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:18.378073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:40:18.812: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local from pod dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2: the server could not find the requested resource (get pods dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2)
  May 10 05:40:18.820: INFO: Lookups using dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2 failed for: [wheezy_tcp@dns-test-service-2.dns-8127.svc.cluster.local]

  E0510 05:40:19.379084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:20.379315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:21.379577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:22.380049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:23.380305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:40:23.823: INFO: DNS probes using dns-8127/dns-test-5ef9e0bb-f327-4412-9304-74578c0c55b2 succeeded

  May 10 05:40:23.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 05:40:23.826
  STEP: deleting the test headless service @ 05/10/23 05:40:23.843
  STEP: Destroying namespace "dns-8127" for this suite. @ 05/10/23 05:40:23.855
• [37.132 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 05/10/23 05:40:23.86
  May 10 05:40:23.860: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename statefulset @ 05/10/23 05:40:23.86
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:40:23.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:40:23.879
  STEP: Creating service test in namespace statefulset-5015 @ 05/10/23 05:40:23.881
  STEP: Creating statefulset ss in namespace statefulset-5015 @ 05/10/23 05:40:23.885
  May 10 05:40:23.898: INFO: Found 0 stateful pods, waiting for 1
  E0510 05:40:24.380740      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:25.380980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:26.381299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:27.382038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:28.382249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:29.382448      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:30.382680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:31.382855      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:32.383118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:33.383302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:40:33.901: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 05/10/23 05:40:33.904
  STEP: updating a scale subresource @ 05/10/23 05:40:33.906
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/10/23 05:40:33.915
  STEP: Patch a scale subresource @ 05/10/23 05:40:33.917
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/10/23 05:40:33.925
  May 10 05:40:33.930: INFO: Deleting all statefulset in ns statefulset-5015
  May 10 05:40:33.931: INFO: Scaling statefulset ss to 0
  E0510 05:40:34.384289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:35.384475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:36.384532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:37.384675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:38.384800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:39.385057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:40.385269      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:41.385464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:42.385849      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:43.386094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:40:43.947: INFO: Waiting for statefulset status.replicas updated to 0
  May 10 05:40:43.949: INFO: Deleting statefulset ss
  May 10 05:40:43.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5015" for this suite. @ 05/10/23 05:40:43.994
• [20.146 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 05/10/23 05:40:44.006
  May 10 05:40:44.006: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename crd-watch @ 05/10/23 05:40:44.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:40:44.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:40:44.023
  May 10 05:40:44.034: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 05:40:44.386894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:45.387746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:46.387933      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 05/10/23 05:40:46.581
  May 10 05:40:46.596: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-10T05:40:46Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-10T05:40:46Z]] name:name1 resourceVersion:1070532 uid:e7813836-c54d-4650-bd9f-0489ed7572a4] num:map[num1:9223372036854775807 num2:1000000]]}
  E0510 05:40:47.388900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:48.389166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:49.389354      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:50.389614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:51.389738      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:52.389825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:53.390051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:54.390161      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:55.390294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:56.390405      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 05/10/23 05:40:56.597
  May 10 05:40:56.603: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-10T05:40:56Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-10T05:40:56Z]] name:name2 resourceVersion:1070584 uid:910e0a8c-d302-40d0-a871-62ed03258242] num:map[num1:9223372036854775807 num2:1000000]]}
  E0510 05:40:57.391398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:58.391650      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:40:59.391784      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:00.391984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:01.392216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:02.392334      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:03.392548      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:04.393409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:05.393536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:06.393683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 05/10/23 05:41:06.604
  May 10 05:41:06.609: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-10T05:40:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-10T05:41:06Z]] name:name1 resourceVersion:1070611 uid:e7813836-c54d-4650-bd9f-0489ed7572a4] num:map[num1:9223372036854775807 num2:1000000]]}
  E0510 05:41:07.393802      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:08.394091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:09.394758      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:10.394766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:11.395112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:12.395516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:13.395777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:14.395973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:15.396187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:16.396301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 05/10/23 05:41:16.61
  May 10 05:41:16.620: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-10T05:40:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-10T05:41:16Z]] name:name2 resourceVersion:1070640 uid:910e0a8c-d302-40d0-a871-62ed03258242] num:map[num1:9223372036854775807 num2:1000000]]}
  E0510 05:41:17.397360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:18.397569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:19.397783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:20.398000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:21.398213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:22.398362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:23.398583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:24.398817      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:25.398930      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:26.399137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 05/10/23 05:41:26.62
  May 10 05:41:26.626: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-10T05:40:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-10T05:41:06Z]] name:name1 resourceVersion:1070669 uid:e7813836-c54d-4650-bd9f-0489ed7572a4] num:map[num1:9223372036854775807 num2:1000000]]}
  E0510 05:41:27.399735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:28.399922      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:29.400065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:30.400315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:31.400505      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:32.401069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:33.401316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:34.402309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:35.402492      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:36.402699      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 05/10/23 05:41:36.627
  May 10 05:41:36.633: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-10T05:40:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-10T05:41:16Z]] name:name2 resourceVersion:1070696 uid:910e0a8c-d302-40d0-a871-62ed03258242] num:map[num1:9223372036854775807 num2:1000000]]}
  E0510 05:41:37.402902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:38.403174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:39.403399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:40.403571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:41.403782      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:42.404144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:43.404365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:44.404479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:45.404742      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:46.404953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:41:47.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-26" for this suite. @ 05/10/23 05:41:47.146
• [63.144 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 05/10/23 05:41:47.15
  May 10 05:41:47.150: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename runtimeclass @ 05/10/23 05:41:47.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:41:47.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:41:47.162
  E0510 05:41:47.405020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:48.405336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:41:49.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3892" for this suite. @ 05/10/23 05:41:49.196
• [2.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 05/10/23 05:41:49.202
  May 10 05:41:49.202: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename resourcequota @ 05/10/23 05:41:49.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:41:49.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:41:49.214
  STEP: Creating resourceQuota "e2e-rq-status-2sk2r" @ 05/10/23 05:41:49.221
  May 10 05:41:49.228: INFO: Resource quota "e2e-rq-status-2sk2r" reports spec: hard cpu limit of 500m
  May 10 05:41:49.228: INFO: Resource quota "e2e-rq-status-2sk2r" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-2sk2r" /status @ 05/10/23 05:41:49.228
  STEP: Confirm /status for "e2e-rq-status-2sk2r" resourceQuota via watch @ 05/10/23 05:41:49.234
  May 10 05:41:49.235: INFO: observed resourceQuota "e2e-rq-status-2sk2r" in namespace "resourcequota-3889" with hard status: v1.ResourceList(nil)
  May 10 05:41:49.235: INFO: Found resourceQuota "e2e-rq-status-2sk2r" in namespace "resourcequota-3889" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 10 05:41:49.235: INFO: ResourceQuota "e2e-rq-status-2sk2r" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 05/10/23 05:41:49.237
  May 10 05:41:49.240: INFO: Resource quota "e2e-rq-status-2sk2r" reports spec: hard cpu limit of 1
  May 10 05:41:49.240: INFO: Resource quota "e2e-rq-status-2sk2r" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-2sk2r" /status @ 05/10/23 05:41:49.24
  STEP: Confirm /status for "e2e-rq-status-2sk2r" resourceQuota via watch @ 05/10/23 05:41:49.255
  May 10 05:41:49.256: INFO: observed resourceQuota "e2e-rq-status-2sk2r" in namespace "resourcequota-3889" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 10 05:41:49.256: INFO: Found resourceQuota "e2e-rq-status-2sk2r" in namespace "resourcequota-3889" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  May 10 05:41:49.256: INFO: ResourceQuota "e2e-rq-status-2sk2r" /status was patched
  STEP: Get "e2e-rq-status-2sk2r" /status @ 05/10/23 05:41:49.256
  May 10 05:41:49.259: INFO: Resourcequota "e2e-rq-status-2sk2r" reports status: hard cpu of 1
  May 10 05:41:49.259: INFO: Resourcequota "e2e-rq-status-2sk2r" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-2sk2r" /status before checking Spec is unchanged @ 05/10/23 05:41:49.261
  May 10 05:41:49.267: INFO: Resourcequota "e2e-rq-status-2sk2r" reports status: hard cpu of 2
  May 10 05:41:49.267: INFO: Resourcequota "e2e-rq-status-2sk2r" reports status: hard memory of 2Gi
  May 10 05:41:49.268: INFO: Found resourceQuota "e2e-rq-status-2sk2r" in namespace "resourcequota-3889" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0510 05:41:49.405767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:50.406243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:51.406452      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:52.407197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:53.407509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:54.407953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:55.408810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:56.408998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:57.409423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:58.409698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:41:59.410422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:00.410663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:01.410896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:02.411304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:03.411524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:04.412502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:05.412788      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:06.413376      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:07.413925      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:08.414140      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:09.414984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:10.415205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:11.415577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:12.416016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:13.416269      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:14.416883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:15.417103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:16.417316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:17.417876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:18.418098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:19.418445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:20.418667      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:21.418887      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:22.419019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:23.419274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:24.419576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:25.419792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:26.420003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:27.420601      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:28.420809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:29.421600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:30.421809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:31.422022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:32.422348      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:33.422566      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:34.422855      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:35.423063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:36.423278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:37.423833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:38.424066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:39.424898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:40.425129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:41.425336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:42.425721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:43.426624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:44.427152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:45.427402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:46.427632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:47.428237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:48.428476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:49.428563      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:50.428777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:51.429014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:52.429393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:53.429705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:54.429889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:55.430128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:56.430347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:57.430858      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:58.431092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:42:59.431971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:00.432241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:01.432479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:02.432887      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:03.433094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:43:04.274: INFO: ResourceQuota "e2e-rq-status-2sk2r" Spec was unchanged and /status reset
  May 10 05:43:04.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3889" for this suite. @ 05/10/23 05:43:04.276
• [75.079 seconds]
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 05/10/23 05:43:04.28
  May 10 05:43:04.280: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-runtime @ 05/10/23 05:43:04.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:43:04.293
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:43:04.295
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 05/10/23 05:43:04.306
  E0510 05:43:04.433821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:05.433960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:06.434308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:07.435335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:08.435653      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:09.436231      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:10.436764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:11.436897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:12.437322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:13.438184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:14.439210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:15.439694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:16.440678      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:17.441015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:18.442132      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:19.442432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 05/10/23 05:43:20.354
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 05/10/23 05:43:20.361
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 05/10/23 05:43:20.365
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 05/10/23 05:43:20.365
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 05/10/23 05:43:20.383
  E0510 05:43:20.442698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:21.443709      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:22.444088      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 05/10/23 05:43:23.394
  E0510 05:43:23.445128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:24.446128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 05/10/23 05:43:25.402
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 05/10/23 05:43:25.406
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 05/10/23 05:43:25.406
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 05/10/23 05:43:25.427
  E0510 05:43:25.446199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 05/10/23 05:43:26.432
  E0510 05:43:26.446513      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:27.447312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:28.447649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:29.448155      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:30.448208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 05/10/23 05:43:30.448
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 05/10/23 05:43:30.452
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 05/10/23 05:43:30.452
  May 10 05:43:30.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6243" for this suite. @ 05/10/23 05:43:30.483
• [26.207 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 05/10/23 05:43:30.487
  May 10 05:43:30.487: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename namespaces @ 05/10/23 05:43:30.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:43:30.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:43:30.499
  STEP: Updating Namespace "namespaces-3996" @ 05/10/23 05:43:30.501
  May 10 05:43:30.511: INFO: Namespace "namespaces-3996" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"f0d8353b-b148-477c-8f9b-38112bf35b15", "kubernetes.io/metadata.name":"namespaces-3996", "namespaces-3996":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  May 10 05:43:30.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3996" for this suite. @ 05/10/23 05:43:30.514
• [0.031 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 05/10/23 05:43:30.519
  May 10 05:43:30.519: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename svcaccounts @ 05/10/23 05:43:30.519
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:43:30.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:43:30.536
  STEP: Creating ServiceAccount "e2e-sa-k796c"  @ 05/10/23 05:43:30.538
  May 10 05:43:30.543: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-k796c"  @ 05/10/23 05:43:30.543
  May 10 05:43:30.548: INFO: AutomountServiceAccountToken: true
  May 10 05:43:30.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8088" for this suite. @ 05/10/23 05:43:30.55
• [0.036 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 05/10/23 05:43:30.555
  May 10 05:43:30.555: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename cronjob @ 05/10/23 05:43:30.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:43:30.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:43:30.573
  STEP: Creating a ForbidConcurrent cronjob @ 05/10/23 05:43:30.576
  STEP: Ensuring a job is scheduled @ 05/10/23 05:43:30.581
  E0510 05:43:31.448532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:32.449017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:33.449126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:34.449386      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:35.449485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:36.449681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:37.449832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:38.450065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:39.450192      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:40.450456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:41.450583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:42.450987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:43.451154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:44.451415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:45.451535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:46.451828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:47.451975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:48.452261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:49.452437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:50.452676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:51.453118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:52.453423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:53.453584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:54.453814      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:55.454076      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:56.454171      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:57.454645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:58.454896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:43:59.455276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:00.455466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/10/23 05:44:00.583
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/10/23 05:44:00.584
  STEP: Ensuring no more jobs are scheduled @ 05/10/23 05:44:00.586
  E0510 05:44:01.456241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:02.456471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:03.457145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:04.457423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:05.458459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:06.458675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:07.459441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:08.459665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:09.460319      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:10.460530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:11.461190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:12.461766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:13.462121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:14.462351      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:15.463050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:16.463274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:17.463482      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:18.463724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:19.464230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:20.464456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:21.464977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:22.465371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:23.465498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:24.465722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:25.466391      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:26.466631      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:27.467029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:28.467247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:29.467337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:30.467627      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:31.468367      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:32.468855      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:33.468966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:34.469201      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:35.469936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:36.470162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:37.470536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:38.470745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:39.470892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:40.471152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:41.471285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:42.471703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:43.471882      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:44.472097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:45.472261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:46.472503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:47.473490      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:48.473717      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:49.473877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:50.474256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:51.474361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:52.474787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:53.474901      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:54.475119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:55.475326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:56.475553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:57.475711      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:58.475944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:44:59.476039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:00.476289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:01.476423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:02.476979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:03.477241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:04.477447      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:05.478210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:06.478391      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:07.478590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:08.478869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:09.479958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:10.480152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:11.480295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:12.480780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:13.481489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:14.481600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:15.482360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:16.482629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:17.483052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:18.483163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:19.484092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:20.484382      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:21.485383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:22.485527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:23.486276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:24.486519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:25.487281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:26.487353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:27.487811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:28.488073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:29.489011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:30.489236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:31.489692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:32.490083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:33.490815      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:34.491016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:35.491880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:36.492139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:37.492267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:38.492390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:39.493036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:40.493193      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:41.493316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:42.493736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:43.494666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:44.494945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:45.495406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:46.495624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:47.496524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:48.496746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:49.497400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:50.497539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:51.498327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:52.498750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:53.499589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:54.499708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:55.500166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:56.500418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:57.500444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:58.500651      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:45:59.501507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:00.501721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:01.501851      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:02.501983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:03.502398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:04.502522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:05.502640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:06.502767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:07.502928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:08.503153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:09.504002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:10.504191      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:11.504304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:12.504699      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:13.505637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:14.505762      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:15.506745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:16.506967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:17.507083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:18.507297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:19.508032      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:20.508259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:21.508420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:22.508622      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:23.509330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:24.509511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:25.510504      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:26.510811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:27.511598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:28.511823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:29.512691      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:30.512893      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:31.513781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:32.513914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:33.514266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:34.514484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:35.515120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:36.515262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:37.515736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:38.515949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:39.516124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:40.516314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:41.516862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:42.517218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:43.518116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:44.518327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:45.518831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:46.518959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:47.519626      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:48.519833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:49.520761      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:50.520891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:51.521571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:52.522104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:53.522226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:54.522389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:55.523013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:56.523241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:57.523401      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:58.523542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:46:59.524329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:00.525330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:01.525706      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:02.525843      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:03.525972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:04.526106      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:05.526762      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:06.527023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:07.528016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:08.528381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:09.528865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:10.529082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:11.529215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:12.529627      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:13.530413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:14.530537      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:15.531460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:16.531653      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:17.532043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:18.532335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:19.533106      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:20.533386      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:21.533553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:22.534077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:23.534234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:24.534473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:25.535048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:26.535335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:27.535997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:28.536085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:29.537125      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:30.537425      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:31.537503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:32.538033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:33.538103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:34.538362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:35.539316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:36.539529      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:37.540604      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:38.540705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:39.540763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:40.541026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:41.541813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:42.542157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:43.542598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:44.542800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:45.543460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:46.543765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:47.543918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:48.544229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:49.545069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:50.545299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:51.545608      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:52.546041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:53.546616      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:54.546801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:55.547687      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:56.547971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:57.548366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:58.548567      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:47:59.548886      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:00.549098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:01.549933      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:02.550254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:03.550959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:04.551421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:05.552047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:06.552296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:07.553296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:08.553516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:09.553621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:10.553865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:11.554840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:12.555170      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:13.555886      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:14.556151      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:15.556855      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:16.557157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:17.557755      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:18.558012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:19.558773      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:20.559410      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:21.559947      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:22.560220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:23.561119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:24.561333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:25.561910      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:26.562312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:27.562529      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:28.562711      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:29.563287      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:30.563606      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:31.564094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:32.564415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:33.564581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:34.564876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:35.565078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:36.565334      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:37.565412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:38.565633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:39.566637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:40.566867      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:41.567439      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:42.567885      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:43.568404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:44.568635      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:45.569252      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:46.569472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:47.569928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:48.570160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:49.570732      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:50.570970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:51.571480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:52.571894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:53.572600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:54.572833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:55.573181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:56.573422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:57.574307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:58.574503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:48:59.575046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:00.575254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/10/23 05:49:00.59
  May 10 05:49:00.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5004" for this suite. @ 05/10/23 05:49:00.597
• [330.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 05/10/23 05:49:00.607
  May 10 05:49:00.607: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:49:00.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:49:00.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:49:00.624
  STEP: Setting up server cert @ 05/10/23 05:49:00.646
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:49:00.974
  STEP: Deploying the webhook pod @ 05/10/23 05:49:00.98
  STEP: Wait for the deployment to be ready @ 05/10/23 05:49:00.993
  May 10 05:49:00.997: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0510 05:49:01.575843      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:02.576320      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:49:03.004
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:49:03.011
  E0510 05:49:03.576643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:49:04.012: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 10 05:49:04.014: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3353-crds.webhook.example.com via the AdmissionRegistration API @ 05/10/23 05:49:04.529
  STEP: Creating a custom resource while v1 is storage version @ 05/10/23 05:49:04.548
  E0510 05:49:04.577206      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:05.577576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:06.577807      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 05/10/23 05:49:06.579
  STEP: Patching the custom resource while v2 is storage version @ 05/10/23 05:49:06.587
  May 10 05:49:06.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5243" for this suite. @ 05/10/23 05:49:07.185
  STEP: Destroying namespace "webhook-markers-7984" for this suite. @ 05/10/23 05:49:07.189
• [6.589 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 05/10/23 05:49:07.197
  May 10 05:49:07.197: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename replicaset @ 05/10/23 05:49:07.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:49:07.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:49:07.217
  STEP: Create a ReplicaSet @ 05/10/23 05:49:07.219
  STEP: Verify that the required pods have come up @ 05/10/23 05:49:07.225
  May 10 05:49:07.227: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0510 05:49:07.577842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:08.578310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:09.578394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:10.578837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:11.579022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:49:12.230: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 05/10/23 05:49:12.23
  May 10 05:49:12.231: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 05/10/23 05:49:12.232
  STEP: DeleteCollection of the ReplicaSets @ 05/10/23 05:49:12.233
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 05/10/23 05:49:12.239
  May 10 05:49:12.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5791" for this suite. @ 05/10/23 05:49:12.243
• [5.066 seconds]
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 05/10/23 05:49:12.263
  May 10 05:49:12.263: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename svc-latency @ 05/10/23 05:49:12.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:49:12.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:49:12.28
  May 10 05:49:12.282: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-27 @ 05/10/23 05:49:12.282
  I0510 05:49:12.297601      22 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-27, replica count: 1
  E0510 05:49:12.580001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 05:49:13.348509      22 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0510 05:49:13.580979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 05:49:14.348703      22 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0510 05:49:14.581093      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 05:49:15.349659      22 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 10 05:49:15.463: INFO: Created: latency-svc-tkgpf
  May 10 05:49:15.476: INFO: Got endpoints: latency-svc-tkgpf [26.285867ms]
  May 10 05:49:15.487: INFO: Created: latency-svc-rgkc7
  May 10 05:49:15.491: INFO: Got endpoints: latency-svc-rgkc7 [14.99681ms]
  May 10 05:49:15.493: INFO: Created: latency-svc-tfv8j
  May 10 05:49:15.502: INFO: Got endpoints: latency-svc-tfv8j [26.053657ms]
  May 10 05:49:15.502: INFO: Created: latency-svc-rjdrv
  May 10 05:49:15.509: INFO: Got endpoints: latency-svc-rjdrv [32.937655ms]
  May 10 05:49:15.510: INFO: Created: latency-svc-bj7ds
  May 10 05:49:15.514: INFO: Created: latency-svc-bm9hq
  May 10 05:49:15.520: INFO: Got endpoints: latency-svc-bj7ds [43.612691ms]
  May 10 05:49:15.522: INFO: Got endpoints: latency-svc-bm9hq [46.137702ms]
  May 10 05:49:15.524: INFO: Created: latency-svc-8bt6r
  May 10 05:49:15.532: INFO: Got endpoints: latency-svc-8bt6r [55.741075ms]
  May 10 05:49:15.534: INFO: Created: latency-svc-q9mql
  May 10 05:49:15.541: INFO: Got endpoints: latency-svc-q9mql [64.646301ms]
  May 10 05:49:15.542: INFO: Created: latency-svc-q98s7
  May 10 05:49:15.547: INFO: Got endpoints: latency-svc-q98s7 [71.570391ms]
  May 10 05:49:15.552: INFO: Created: latency-svc-hsd8q
  May 10 05:49:15.555: INFO: Created: latency-svc-74k2l
  May 10 05:49:15.560: INFO: Got endpoints: latency-svc-hsd8q [83.84857ms]
  May 10 05:49:15.561: INFO: Got endpoints: latency-svc-74k2l [85.586887ms]
  May 10 05:49:15.563: INFO: Created: latency-svc-qbs72
  May 10 05:49:15.568: INFO: Got endpoints: latency-svc-qbs72 [92.738612ms]
  May 10 05:49:15.570: INFO: Created: latency-svc-ksz84
  May 10 05:49:15.579: INFO: Got endpoints: latency-svc-ksz84 [102.520883ms]
  May 10 05:49:15.580: INFO: Created: latency-svc-7vgzv
  E0510 05:49:15.581824      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:49:15.583: INFO: Got endpoints: latency-svc-7vgzv [107.204119ms]
  May 10 05:49:15.591: INFO: Created: latency-svc-w8n9n
  May 10 05:49:15.592: INFO: Created: latency-svc-p5tqv
  May 10 05:49:15.597: INFO: Got endpoints: latency-svc-w8n9n [120.438335ms]
  May 10 05:49:15.598: INFO: Got endpoints: latency-svc-p5tqv [122.02982ms]
  May 10 05:49:15.598: INFO: Created: latency-svc-qcwh5
  May 10 05:49:15.604: INFO: Got endpoints: latency-svc-qcwh5 [112.922015ms]
  May 10 05:49:15.606: INFO: Created: latency-svc-fmbm5
  May 10 05:49:15.612: INFO: Got endpoints: latency-svc-fmbm5 [110.231884ms]
  May 10 05:49:15.613: INFO: Created: latency-svc-d922p
  May 10 05:49:15.617: INFO: Got endpoints: latency-svc-d922p [107.931037ms]
  May 10 05:49:15.623: INFO: Created: latency-svc-8bwv7
  May 10 05:49:15.626: INFO: Got endpoints: latency-svc-8bwv7 [105.827686ms]
  May 10 05:49:15.632: INFO: Created: latency-svc-7282k
  May 10 05:49:15.635: INFO: Created: latency-svc-lsdzq
  May 10 05:49:15.639: INFO: Got endpoints: latency-svc-7282k [116.741168ms]
  May 10 05:49:15.682: INFO: Got endpoints: latency-svc-lsdzq [149.969286ms]
  May 10 05:49:15.682: INFO: Created: latency-svc-dmjqb
  May 10 05:49:15.690: INFO: Got endpoints: latency-svc-dmjqb [148.994845ms]
  May 10 05:49:15.706: INFO: Created: latency-svc-4fsbc
  May 10 05:49:15.710: INFO: Got endpoints: latency-svc-4fsbc [162.290352ms]
  May 10 05:49:15.722: INFO: Created: latency-svc-wtkkr
  May 10 05:49:15.744: INFO: Got endpoints: latency-svc-wtkkr [184.620222ms]
  May 10 05:49:15.746: INFO: Created: latency-svc-p79gn
  May 10 05:49:15.756: INFO: Created: latency-svc-h698v
  May 10 05:49:15.760: INFO: Got endpoints: latency-svc-p79gn [198.483119ms]
  May 10 05:49:15.761: INFO: Got endpoints: latency-svc-h698v [192.994675ms]
  May 10 05:49:15.764: INFO: Created: latency-svc-xxb2x
  May 10 05:49:15.772: INFO: Created: latency-svc-72cfx
  May 10 05:49:15.773: INFO: Got endpoints: latency-svc-xxb2x [194.108199ms]
  May 10 05:49:15.780: INFO: Got endpoints: latency-svc-72cfx [196.977107ms]
  May 10 05:49:15.789: INFO: Created: latency-svc-lstzf
  May 10 05:49:15.791: INFO: Created: latency-svc-ktht6
  May 10 05:49:15.797: INFO: Got endpoints: latency-svc-lstzf [200.011472ms]
  May 10 05:49:15.801: INFO: Got endpoints: latency-svc-ktht6 [203.658995ms]
  May 10 05:49:15.802: INFO: Created: latency-svc-sm94b
  May 10 05:49:15.804: INFO: Got endpoints: latency-svc-sm94b [200.587159ms]
  May 10 05:49:15.806: INFO: Created: latency-svc-5hrnl
  May 10 05:49:15.824: INFO: Got endpoints: latency-svc-5hrnl [212.004072ms]
  May 10 05:49:15.826: INFO: Created: latency-svc-gp9fj
  May 10 05:49:15.835: INFO: Got endpoints: latency-svc-gp9fj [217.760484ms]
  May 10 05:49:15.835: INFO: Created: latency-svc-6g4gt
  May 10 05:49:15.838: INFO: Got endpoints: latency-svc-6g4gt [212.857586ms]
  May 10 05:49:15.846: INFO: Created: latency-svc-p9j7t
  May 10 05:49:15.853: INFO: Created: latency-svc-tk68q
  May 10 05:49:15.858: INFO: Got endpoints: latency-svc-p9j7t [218.532194ms]
  May 10 05:49:15.859: INFO: Got endpoints: latency-svc-tk68q [177.567167ms]
  May 10 05:49:15.860: INFO: Created: latency-svc-qmfcm
  May 10 05:49:15.866: INFO: Created: latency-svc-xn7s7
  May 10 05:49:15.867: INFO: Got endpoints: latency-svc-qmfcm [177.565828ms]
  May 10 05:49:15.869: INFO: Created: latency-svc-k4j2d
  May 10 05:49:15.881: INFO: Created: latency-svc-d552b
  May 10 05:49:15.884: INFO: Created: latency-svc-5cqzd
  May 10 05:49:15.887: INFO: Created: latency-svc-pwsqh
  May 10 05:49:15.896: INFO: Created: latency-svc-cnwh6
  May 10 05:49:15.901: INFO: Created: latency-svc-8bnhv
  May 10 05:49:15.904: INFO: Created: latency-svc-bmn7q
  May 10 05:49:15.914: INFO: Created: latency-svc-bh6cd
  May 10 05:49:15.917: INFO: Got endpoints: latency-svc-xn7s7 [207.235478ms]
  May 10 05:49:15.923: INFO: Created: latency-svc-dxtqm
  May 10 05:49:15.936: INFO: Created: latency-svc-6vqsx
  May 10 05:49:15.944: INFO: Created: latency-svc-blm24
  May 10 05:49:15.947: INFO: Created: latency-svc-vq7j7
  May 10 05:49:15.959: INFO: Created: latency-svc-pqwpm
  May 10 05:49:15.963: INFO: Created: latency-svc-crm8p
  May 10 05:49:15.966: INFO: Created: latency-svc-wxzqp
  May 10 05:49:15.967: INFO: Got endpoints: latency-svc-k4j2d [222.254325ms]
  May 10 05:49:15.980: INFO: Created: latency-svc-pjs5b
  May 10 05:49:16.019: INFO: Got endpoints: latency-svc-d552b [259.395905ms]
  May 10 05:49:16.038: INFO: Created: latency-svc-p558k
  May 10 05:49:16.069: INFO: Got endpoints: latency-svc-5cqzd [307.769201ms]
  May 10 05:49:16.086: INFO: Created: latency-svc-w8dnz
  May 10 05:49:16.132: INFO: Got endpoints: latency-svc-pwsqh [358.792064ms]
  May 10 05:49:16.144: INFO: Created: latency-svc-d4sdd
  May 10 05:49:16.171: INFO: Got endpoints: latency-svc-cnwh6 [390.481309ms]
  May 10 05:49:16.180: INFO: Created: latency-svc-7nrlr
  May 10 05:49:16.218: INFO: Got endpoints: latency-svc-8bnhv [421.453277ms]
  May 10 05:49:16.231: INFO: Created: latency-svc-cvm7t
  May 10 05:49:16.268: INFO: Got endpoints: latency-svc-bmn7q [466.338677ms]
  May 10 05:49:16.288: INFO: Created: latency-svc-jmnmc
  May 10 05:49:16.323: INFO: Got endpoints: latency-svc-bh6cd [518.120877ms]
  May 10 05:49:16.342: INFO: Created: latency-svc-5szzt
  May 10 05:49:16.370: INFO: Got endpoints: latency-svc-dxtqm [545.803933ms]
  May 10 05:49:16.377: INFO: Created: latency-svc-jdzzp
  May 10 05:49:16.421: INFO: Got endpoints: latency-svc-6vqsx [586.944355ms]
  May 10 05:49:16.432: INFO: Created: latency-svc-9wcm7
  May 10 05:49:16.472: INFO: Got endpoints: latency-svc-blm24 [633.513374ms]
  May 10 05:49:16.484: INFO: Created: latency-svc-rvckw
  May 10 05:49:16.518: INFO: Got endpoints: latency-svc-vq7j7 [660.194547ms]
  May 10 05:49:16.539: INFO: Created: latency-svc-wz28f
  May 10 05:49:16.568: INFO: Got endpoints: latency-svc-pqwpm [708.570796ms]
  May 10 05:49:16.579: INFO: Created: latency-svc-752lb
  E0510 05:49:16.582651      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:49:16.621: INFO: Got endpoints: latency-svc-crm8p [754.14819ms]
  May 10 05:49:16.635: INFO: Created: latency-svc-2slq9
  May 10 05:49:16.668: INFO: Got endpoints: latency-svc-wxzqp [750.74856ms]
  May 10 05:49:16.683: INFO: Created: latency-svc-p5n69
  May 10 05:49:16.718: INFO: Got endpoints: latency-svc-pjs5b [751.583222ms]
  May 10 05:49:16.730: INFO: Created: latency-svc-czsj2
  May 10 05:49:16.948: INFO: Got endpoints: latency-svc-p558k [928.555536ms]
  May 10 05:49:16.948: INFO: Got endpoints: latency-svc-d4sdd [816.55406ms]
  May 10 05:49:16.948: INFO: Got endpoints: latency-svc-w8dnz [878.964422ms]
  May 10 05:49:16.948: INFO: Got endpoints: latency-svc-7nrlr [777.538181ms]
  May 10 05:49:16.961: INFO: Created: latency-svc-4kdfr
  May 10 05:49:16.971: INFO: Created: latency-svc-6pblx
  May 10 05:49:16.975: INFO: Got endpoints: latency-svc-cvm7t [757.059745ms]
  May 10 05:49:16.989: INFO: Created: latency-svc-9fcdv
  May 10 05:49:16.996: INFO: Created: latency-svc-4zrmk
  May 10 05:49:17.001: INFO: Created: latency-svc-k74v5
  May 10 05:49:17.027: INFO: Got endpoints: latency-svc-jmnmc [758.737954ms]
  May 10 05:49:17.035: INFO: Created: latency-svc-c4x5j
  May 10 05:49:17.068: INFO: Got endpoints: latency-svc-5szzt [745.297099ms]
  May 10 05:49:17.083: INFO: Created: latency-svc-cwgjl
  May 10 05:49:17.118: INFO: Got endpoints: latency-svc-jdzzp [748.415902ms]
  May 10 05:49:17.130: INFO: Created: latency-svc-s2fnd
  May 10 05:49:17.173: INFO: Got endpoints: latency-svc-9wcm7 [751.195355ms]
  May 10 05:49:17.180: INFO: Created: latency-svc-fh9jk
  May 10 05:49:17.223: INFO: Got endpoints: latency-svc-rvckw [750.539176ms]
  May 10 05:49:17.231: INFO: Created: latency-svc-qgvxv
  May 10 05:49:17.284: INFO: Got endpoints: latency-svc-wz28f [766.500674ms]
  May 10 05:49:17.293: INFO: Created: latency-svc-kzqbl
  May 10 05:49:17.317: INFO: Got endpoints: latency-svc-752lb [749.449116ms]
  May 10 05:49:17.329: INFO: Created: latency-svc-68ctf
  May 10 05:49:17.372: INFO: Got endpoints: latency-svc-2slq9 [750.374075ms]
  May 10 05:49:17.381: INFO: Created: latency-svc-bfgr5
  May 10 05:49:17.418: INFO: Got endpoints: latency-svc-p5n69 [749.842876ms]
  May 10 05:49:17.428: INFO: Created: latency-svc-rb8nl
  May 10 05:49:17.467: INFO: Got endpoints: latency-svc-czsj2 [748.203055ms]
  May 10 05:49:17.476: INFO: Created: latency-svc-xsndv
  May 10 05:49:17.518: INFO: Got endpoints: latency-svc-4kdfr [569.967967ms]
  May 10 05:49:17.530: INFO: Created: latency-svc-87qvf
  May 10 05:49:17.572: INFO: Got endpoints: latency-svc-6pblx [624.153006ms]
  May 10 05:49:17.580: INFO: Created: latency-svc-ln8th
  E0510 05:49:17.583162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:49:17.627: INFO: Got endpoints: latency-svc-9fcdv [678.369935ms]
  May 10 05:49:17.634: INFO: Created: latency-svc-k7tpv
  May 10 05:49:17.672: INFO: Got endpoints: latency-svc-4zrmk [723.786987ms]
  May 10 05:49:17.681: INFO: Created: latency-svc-ppg84
  May 10 05:49:17.718: INFO: Got endpoints: latency-svc-k74v5 [742.98765ms]
  May 10 05:49:17.729: INFO: Created: latency-svc-zs5t4
  May 10 05:49:17.768: INFO: Got endpoints: latency-svc-c4x5j [741.076552ms]
  May 10 05:49:17.780: INFO: Created: latency-svc-ljxn5
  May 10 05:49:17.818: INFO: Got endpoints: latency-svc-cwgjl [750.424684ms]
  May 10 05:49:17.826: INFO: Created: latency-svc-f8vqf
  May 10 05:49:17.871: INFO: Got endpoints: latency-svc-s2fnd [752.627847ms]
  May 10 05:49:17.878: INFO: Created: latency-svc-6krm6
  May 10 05:49:17.918: INFO: Got endpoints: latency-svc-fh9jk [745.390094ms]
  May 10 05:49:17.929: INFO: Created: latency-svc-x4qdd
  May 10 05:49:17.968: INFO: Got endpoints: latency-svc-qgvxv [745.511425ms]
  May 10 05:49:17.979: INFO: Created: latency-svc-28b6p
  May 10 05:49:18.021: INFO: Got endpoints: latency-svc-kzqbl [736.191447ms]
  May 10 05:49:18.031: INFO: Created: latency-svc-lz9vt
  May 10 05:49:18.069: INFO: Got endpoints: latency-svc-68ctf [751.041565ms]
  May 10 05:49:18.075: INFO: Created: latency-svc-pmsrh
  May 10 05:49:18.125: INFO: Got endpoints: latency-svc-bfgr5 [753.652645ms]
  May 10 05:49:18.133: INFO: Created: latency-svc-n5vtd
  May 10 05:49:18.168: INFO: Got endpoints: latency-svc-rb8nl [750.209185ms]
  May 10 05:49:18.179: INFO: Created: latency-svc-5ldk8
  May 10 05:49:18.221: INFO: Got endpoints: latency-svc-xsndv [753.995358ms]
  May 10 05:49:18.233: INFO: Created: latency-svc-nr85s
  May 10 05:49:18.268: INFO: Got endpoints: latency-svc-87qvf [749.92383ms]
  May 10 05:49:18.279: INFO: Created: latency-svc-2lg86
  May 10 05:49:18.319: INFO: Got endpoints: latency-svc-ln8th [746.422286ms]
  May 10 05:49:18.327: INFO: Created: latency-svc-x2ntp
  May 10 05:49:18.372: INFO: Got endpoints: latency-svc-k7tpv [745.447524ms]
  May 10 05:49:18.381: INFO: Created: latency-svc-hwdpf
  May 10 05:49:18.419: INFO: Got endpoints: latency-svc-ppg84 [746.714434ms]
  May 10 05:49:18.428: INFO: Created: latency-svc-rvxcz
  May 10 05:49:18.484: INFO: Got endpoints: latency-svc-zs5t4 [766.130484ms]
  May 10 05:49:18.495: INFO: Created: latency-svc-r8x6p
  May 10 05:49:18.519: INFO: Got endpoints: latency-svc-ljxn5 [751.285041ms]
  May 10 05:49:18.530: INFO: Created: latency-svc-pmtn9
  May 10 05:49:18.568: INFO: Got endpoints: latency-svc-f8vqf [749.718521ms]
  May 10 05:49:18.578: INFO: Created: latency-svc-fv2tf
  E0510 05:49:18.583378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:49:18.617: INFO: Got endpoints: latency-svc-6krm6 [746.138337ms]
  May 10 05:49:18.628: INFO: Created: latency-svc-s9qvf
  May 10 05:49:18.794: INFO: Got endpoints: latency-svc-x4qdd [876.114021ms]
  May 10 05:49:18.797: INFO: Got endpoints: latency-svc-28b6p [828.56036ms]
  May 10 05:49:18.797: INFO: Got endpoints: latency-svc-lz9vt [776.257771ms]
  May 10 05:49:18.803: INFO: Created: latency-svc-vvm26
  May 10 05:49:18.810: INFO: Created: latency-svc-9jrgm
  May 10 05:49:18.814: INFO: Created: latency-svc-kffdd
  May 10 05:49:18.819: INFO: Got endpoints: latency-svc-pmsrh [750.538519ms]
  May 10 05:49:18.827: INFO: Created: latency-svc-hlq8v
  May 10 05:49:18.874: INFO: Got endpoints: latency-svc-n5vtd [748.000308ms]
  May 10 05:49:18.881: INFO: Created: latency-svc-7flhs
  May 10 05:49:18.922: INFO: Got endpoints: latency-svc-5ldk8 [753.863563ms]
  May 10 05:49:18.929: INFO: Created: latency-svc-hdx8m
  May 10 05:49:18.975: INFO: Got endpoints: latency-svc-nr85s [753.990061ms]
  May 10 05:49:18.981: INFO: Created: latency-svc-dj5qn
  May 10 05:49:19.018: INFO: Got endpoints: latency-svc-2lg86 [749.395308ms]
  May 10 05:49:19.032: INFO: Created: latency-svc-56wz7
  May 10 05:49:19.096: INFO: Got endpoints: latency-svc-x2ntp [777.449618ms]
  May 10 05:49:19.106: INFO: Created: latency-svc-w92hj
  May 10 05:49:19.117: INFO: Got endpoints: latency-svc-hwdpf [745.100621ms]
  May 10 05:49:19.130: INFO: Created: latency-svc-9kmtl
  May 10 05:49:19.167: INFO: Got endpoints: latency-svc-rvxcz [748.695596ms]
  May 10 05:49:19.181: INFO: Created: latency-svc-c98ld
  May 10 05:49:19.219: INFO: Got endpoints: latency-svc-r8x6p [734.030033ms]
  May 10 05:49:19.228: INFO: Created: latency-svc-xsccg
  May 10 05:49:19.268: INFO: Got endpoints: latency-svc-pmtn9 [749.000198ms]
  May 10 05:49:19.277: INFO: Created: latency-svc-mtt8h
  May 10 05:49:19.324: INFO: Got endpoints: latency-svc-fv2tf [755.541833ms]
  May 10 05:49:19.331: INFO: Created: latency-svc-xhd9g
  May 10 05:49:19.372: INFO: Got endpoints: latency-svc-s9qvf [755.000858ms]
  May 10 05:49:19.379: INFO: Created: latency-svc-ssdls
  May 10 05:49:19.417: INFO: Got endpoints: latency-svc-vvm26 [623.007102ms]
  May 10 05:49:19.437: INFO: Created: latency-svc-8dwlx
  May 10 05:49:19.468: INFO: Got endpoints: latency-svc-9jrgm [670.908513ms]
  May 10 05:49:19.482: INFO: Created: latency-svc-2286n
  May 10 05:49:19.518: INFO: Got endpoints: latency-svc-kffdd [720.762066ms]
  May 10 05:49:19.525: INFO: Created: latency-svc-mdct8
  May 10 05:49:19.573: INFO: Got endpoints: latency-svc-hlq8v [753.42419ms]
  May 10 05:49:19.579: INFO: Created: latency-svc-h4rwk
  E0510 05:49:19.583578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:49:19.618: INFO: Got endpoints: latency-svc-7flhs [744.450699ms]
  May 10 05:49:19.628: INFO: Created: latency-svc-8smlv
  May 10 05:49:19.674: INFO: Got endpoints: latency-svc-hdx8m [752.603059ms]
  May 10 05:49:19.681: INFO: Created: latency-svc-6s7bk
  May 10 05:49:19.719: INFO: Got endpoints: latency-svc-dj5qn [744.049625ms]
  May 10 05:49:19.736: INFO: Created: latency-svc-xr8cb
  May 10 05:49:19.768: INFO: Got endpoints: latency-svc-56wz7 [750.524897ms]
  May 10 05:49:19.776: INFO: Created: latency-svc-vjwb6
  May 10 05:49:19.822: INFO: Got endpoints: latency-svc-w92hj [725.53438ms]
  May 10 05:49:19.828: INFO: Created: latency-svc-2ngdw
  May 10 05:49:19.868: INFO: Got endpoints: latency-svc-9kmtl [750.310478ms]
  May 10 05:49:19.875: INFO: Created: latency-svc-tdgpt
  May 10 05:49:19.918: INFO: Got endpoints: latency-svc-c98ld [750.496187ms]
  May 10 05:49:19.932: INFO: Created: latency-svc-r25j9
  May 10 05:49:19.967: INFO: Got endpoints: latency-svc-xsccg [748.670777ms]
  May 10 05:49:19.978: INFO: Created: latency-svc-9fcrv
  May 10 05:49:20.017: INFO: Got endpoints: latency-svc-mtt8h [748.756791ms]
  May 10 05:49:20.028: INFO: Created: latency-svc-q6dfh
  May 10 05:49:20.112: INFO: Got endpoints: latency-svc-xhd9g [787.814818ms]
  May 10 05:49:20.121: INFO: Got endpoints: latency-svc-ssdls [748.663579ms]
  May 10 05:49:20.123: INFO: Created: latency-svc-lvhrn
  May 10 05:49:20.128: INFO: Created: latency-svc-wcgx2
  May 10 05:49:20.171: INFO: Got endpoints: latency-svc-8dwlx [753.918187ms]
  May 10 05:49:20.178: INFO: Created: latency-svc-8bhlq
  May 10 05:49:20.217: INFO: Got endpoints: latency-svc-2286n [749.531555ms]
  May 10 05:49:20.225: INFO: Created: latency-svc-9wxrp
  May 10 05:49:20.268: INFO: Got endpoints: latency-svc-mdct8 [750.506815ms]
  May 10 05:49:20.276: INFO: Created: latency-svc-xt42j
  May 10 05:49:20.317: INFO: Got endpoints: latency-svc-h4rwk [744.580653ms]
  May 10 05:49:20.328: INFO: Created: latency-svc-d6lqc
  May 10 05:49:20.372: INFO: Got endpoints: latency-svc-8smlv [754.114943ms]
  May 10 05:49:20.379: INFO: Created: latency-svc-nqf2w
  May 10 05:49:20.421: INFO: Got endpoints: latency-svc-6s7bk [746.706614ms]
  May 10 05:49:20.438: INFO: Created: latency-svc-l8bcz
  May 10 05:49:20.468: INFO: Got endpoints: latency-svc-xr8cb [749.139194ms]
  May 10 05:49:20.474: INFO: Created: latency-svc-9sjbv
  May 10 05:49:20.521: INFO: Got endpoints: latency-svc-vjwb6 [753.180944ms]
  May 10 05:49:20.534: INFO: Created: latency-svc-hxqb6
  E0510 05:49:20.584356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:49:20.700: INFO: Got endpoints: latency-svc-2ngdw [878.2288ms]
  May 10 05:49:20.706: INFO: Got endpoints: latency-svc-tdgpt [837.906689ms]
  May 10 05:49:20.706: INFO: Got endpoints: latency-svc-r25j9 [788.254111ms]
  May 10 05:49:20.711: INFO: Created: latency-svc-wvrzh
  May 10 05:49:20.714: INFO: Created: latency-svc-c6t85
  May 10 05:49:20.724: INFO: Got endpoints: latency-svc-9fcrv [757.101596ms]
  May 10 05:49:20.726: INFO: Created: latency-svc-46tqw
  May 10 05:49:20.738: INFO: Created: latency-svc-hc2s8
  May 10 05:49:20.771: INFO: Got endpoints: latency-svc-q6dfh [754.533714ms]
  May 10 05:49:20.778: INFO: Created: latency-svc-s742k
  May 10 05:49:20.820: INFO: Got endpoints: latency-svc-lvhrn [708.676516ms]
  May 10 05:49:20.836: INFO: Created: latency-svc-kfcws
  May 10 05:49:20.871: INFO: Got endpoints: latency-svc-wcgx2 [749.979953ms]
  May 10 05:49:20.882: INFO: Created: latency-svc-6tx5m
  May 10 05:49:20.919: INFO: Got endpoints: latency-svc-8bhlq [747.233553ms]
  May 10 05:49:20.927: INFO: Created: latency-svc-l84z2
  May 10 05:49:20.968: INFO: Got endpoints: latency-svc-9wxrp [750.597735ms]
  May 10 05:49:20.989: INFO: Created: latency-svc-zcd2x
  May 10 05:49:21.026: INFO: Got endpoints: latency-svc-xt42j [757.45315ms]
  May 10 05:49:21.035: INFO: Created: latency-svc-2fc2x
  May 10 05:49:21.068: INFO: Got endpoints: latency-svc-d6lqc [751.2315ms]
  May 10 05:49:21.080: INFO: Created: latency-svc-p2rs4
  May 10 05:49:21.272: INFO: Got endpoints: latency-svc-nqf2w [899.670126ms]
  May 10 05:49:21.276: INFO: Got endpoints: latency-svc-l8bcz [854.152121ms]
  May 10 05:49:21.276: INFO: Got endpoints: latency-svc-9sjbv [807.959187ms]
  May 10 05:49:21.277: INFO: Got endpoints: latency-svc-hxqb6 [754.936839ms]
  May 10 05:49:21.283: INFO: Created: latency-svc-qq6vk
  May 10 05:49:21.291: INFO: Created: latency-svc-v9pmb
  May 10 05:49:21.298: INFO: Created: latency-svc-6bz74
  May 10 05:49:21.301: INFO: Created: latency-svc-6fl9f
  May 10 05:49:21.325: INFO: Got endpoints: latency-svc-wvrzh [624.920314ms]
  May 10 05:49:21.336: INFO: Created: latency-svc-wxv4s
  May 10 05:49:21.371: INFO: Got endpoints: latency-svc-c6t85 [664.59344ms]
  May 10 05:49:21.390: INFO: Created: latency-svc-96gtc
  May 10 05:49:21.421: INFO: Got endpoints: latency-svc-46tqw [714.505522ms]
  May 10 05:49:21.452: INFO: Created: latency-svc-pxclm
  May 10 05:49:21.471: INFO: Got endpoints: latency-svc-hc2s8 [746.07898ms]
  May 10 05:49:21.487: INFO: Created: latency-svc-qxkfh
  May 10 05:49:21.521: INFO: Got endpoints: latency-svc-s742k [749.986292ms]
  May 10 05:49:21.529: INFO: Created: latency-svc-lkhjr
  May 10 05:49:21.568: INFO: Got endpoints: latency-svc-kfcws [747.796733ms]
  E0510 05:49:21.584665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:49:21.589: INFO: Created: latency-svc-mvnrg
  May 10 05:49:21.617: INFO: Got endpoints: latency-svc-6tx5m [746.365584ms]
  May 10 05:49:21.795: INFO: Created: latency-svc-gl47c
  May 10 05:49:21.799: INFO: Got endpoints: latency-svc-zcd2x [830.731148ms]
  May 10 05:49:21.799: INFO: Got endpoints: latency-svc-l84z2 [879.824739ms]
  May 10 05:49:21.799: INFO: Got endpoints: latency-svc-2fc2x [773.088011ms]
  May 10 05:49:21.820: INFO: Created: latency-svc-ljwkz
  May 10 05:49:21.823: INFO: Got endpoints: latency-svc-p2rs4 [754.798873ms]
  May 10 05:49:21.828: INFO: Created: latency-svc-xxbcz
  May 10 05:49:21.841: INFO: Created: latency-svc-hrtjw
  May 10 05:49:21.844: INFO: Created: latency-svc-kn4c4
  May 10 05:49:21.888: INFO: Got endpoints: latency-svc-qq6vk [615.973961ms]
  May 10 05:49:21.900: INFO: Created: latency-svc-vsqvh
  May 10 05:49:21.919: INFO: Got endpoints: latency-svc-v9pmb [643.051152ms]
  May 10 05:49:21.929: INFO: Created: latency-svc-zwqnt
  May 10 05:49:21.970: INFO: Got endpoints: latency-svc-6fl9f [693.335865ms]
  May 10 05:49:21.979: INFO: Created: latency-svc-llvzk
  May 10 05:49:22.023: INFO: Got endpoints: latency-svc-6bz74 [746.503123ms]
  May 10 05:49:22.030: INFO: Created: latency-svc-p9jjq
  May 10 05:49:22.072: INFO: Got endpoints: latency-svc-wxv4s [746.934726ms]
  May 10 05:49:22.080: INFO: Created: latency-svc-888rg
  May 10 05:49:22.118: INFO: Got endpoints: latency-svc-96gtc [747.458159ms]
  May 10 05:49:22.127: INFO: Created: latency-svc-89j7x
  May 10 05:49:22.167: INFO: Got endpoints: latency-svc-pxclm [746.44449ms]
  May 10 05:49:22.182: INFO: Created: latency-svc-9bqxd
  May 10 05:49:22.217: INFO: Got endpoints: latency-svc-qxkfh [746.610594ms]
  May 10 05:49:22.228: INFO: Created: latency-svc-5wd9p
  May 10 05:49:22.268: INFO: Got endpoints: latency-svc-lkhjr [746.103687ms]
  May 10 05:49:22.275: INFO: Created: latency-svc-h9qn9
  May 10 05:49:22.324: INFO: Got endpoints: latency-svc-mvnrg [756.10644ms]
  May 10 05:49:22.332: INFO: Created: latency-svc-2jkpq
  May 10 05:49:22.370: INFO: Got endpoints: latency-svc-gl47c [752.383564ms]
  May 10 05:49:22.378: INFO: Created: latency-svc-lwtdk
  May 10 05:49:22.420: INFO: Got endpoints: latency-svc-ljwkz [621.147901ms]
  May 10 05:49:22.427: INFO: Created: latency-svc-mzgrc
  May 10 05:49:22.469: INFO: Got endpoints: latency-svc-xxbcz [670.724872ms]
  May 10 05:49:22.479: INFO: Created: latency-svc-nxbmq
  May 10 05:49:22.517: INFO: Got endpoints: latency-svc-hrtjw [718.495552ms]
  May 10 05:49:22.527: INFO: Created: latency-svc-tv6w7
  May 10 05:49:22.571: INFO: Got endpoints: latency-svc-kn4c4 [747.322726ms]
  May 10 05:49:22.578: INFO: Created: latency-svc-rpswr
  E0510 05:49:22.585273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:49:22.618: INFO: Got endpoints: latency-svc-vsqvh [729.253315ms]
  May 10 05:49:22.631: INFO: Created: latency-svc-nbrlj
  May 10 05:49:22.668: INFO: Got endpoints: latency-svc-zwqnt [748.75141ms]
  May 10 05:49:22.677: INFO: Created: latency-svc-nvgk6
  May 10 05:49:22.719: INFO: Got endpoints: latency-svc-llvzk [749.077059ms]
  May 10 05:49:22.734: INFO: Created: latency-svc-l4zpc
  May 10 05:49:22.769: INFO: Got endpoints: latency-svc-p9jjq [745.706765ms]
  May 10 05:49:22.780: INFO: Created: latency-svc-jfbdb
  May 10 05:49:22.818: INFO: Got endpoints: latency-svc-888rg [745.622668ms]
  May 10 05:49:22.826: INFO: Created: latency-svc-xq44n
  May 10 05:49:22.882: INFO: Got endpoints: latency-svc-89j7x [763.296604ms]
  May 10 05:49:22.891: INFO: Created: latency-svc-ggfzc
  May 10 05:49:22.917: INFO: Got endpoints: latency-svc-9bqxd [749.58987ms]
  May 10 05:49:22.927: INFO: Created: latency-svc-h2twb
  May 10 05:49:22.968: INFO: Got endpoints: latency-svc-5wd9p [750.548539ms]
  May 10 05:49:22.981: INFO: Created: latency-svc-zk6lx
  May 10 05:49:23.018: INFO: Got endpoints: latency-svc-h9qn9 [750.29497ms]
  May 10 05:49:23.030: INFO: Created: latency-svc-85rjj
  May 10 05:49:23.071: INFO: Got endpoints: latency-svc-2jkpq [746.813608ms]
  May 10 05:49:23.078: INFO: Created: latency-svc-bftxn
  May 10 05:49:23.120: INFO: Got endpoints: latency-svc-lwtdk [750.209163ms]
  May 10 05:49:23.130: INFO: Created: latency-svc-cm27l
  May 10 05:49:23.173: INFO: Got endpoints: latency-svc-mzgrc [753.440087ms]
  May 10 05:49:23.191: INFO: Created: latency-svc-4lrks
  May 10 05:49:23.219: INFO: Got endpoints: latency-svc-nxbmq [749.555216ms]
  May 10 05:49:23.226: INFO: Created: latency-svc-6882b
  May 10 05:49:23.278: INFO: Got endpoints: latency-svc-tv6w7 [759.92448ms]
  May 10 05:49:23.288: INFO: Created: latency-svc-xq5k9
  May 10 05:49:23.331: INFO: Got endpoints: latency-svc-rpswr [760.600816ms]
  May 10 05:49:23.367: INFO: Got endpoints: latency-svc-nbrlj [749.493091ms]
  May 10 05:49:23.422: INFO: Got endpoints: latency-svc-nvgk6 [754.58802ms]
  May 10 05:49:23.467: INFO: Got endpoints: latency-svc-l4zpc [747.362791ms]
  May 10 05:49:23.520: INFO: Got endpoints: latency-svc-jfbdb [751.290147ms]
  May 10 05:49:23.569: INFO: Got endpoints: latency-svc-xq44n [750.885388ms]
  E0510 05:49:23.585372      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:49:23.622: INFO: Got endpoints: latency-svc-ggfzc [740.115162ms]
  May 10 05:49:23.668: INFO: Got endpoints: latency-svc-h2twb [751.258408ms]
  May 10 05:49:23.721: INFO: Got endpoints: latency-svc-zk6lx [752.801381ms]
  May 10 05:49:23.781: INFO: Got endpoints: latency-svc-85rjj [763.302154ms]
  May 10 05:49:23.820: INFO: Got endpoints: latency-svc-bftxn [749.053481ms]
  May 10 05:49:23.869: INFO: Got endpoints: latency-svc-cm27l [748.72495ms]
  May 10 05:49:23.917: INFO: Got endpoints: latency-svc-4lrks [743.26428ms]
  May 10 05:49:23.971: INFO: Got endpoints: latency-svc-6882b [751.315545ms]
  May 10 05:49:24.017: INFO: Got endpoints: latency-svc-xq5k9 [739.164553ms]
  May 10 05:49:24.017: INFO: Latencies: [14.99681ms 26.053657ms 32.937655ms 43.612691ms 46.137702ms 55.741075ms 64.646301ms 71.570391ms 83.84857ms 85.586887ms 92.738612ms 102.520883ms 105.827686ms 107.204119ms 107.931037ms 110.231884ms 112.922015ms 116.741168ms 120.438335ms 122.02982ms 148.994845ms 149.969286ms 162.290352ms 177.565828ms 177.567167ms 184.620222ms 192.994675ms 194.108199ms 196.977107ms 198.483119ms 200.011472ms 200.587159ms 203.658995ms 207.235478ms 212.004072ms 212.857586ms 217.760484ms 218.532194ms 222.254325ms 259.395905ms 307.769201ms 358.792064ms 390.481309ms 421.453277ms 466.338677ms 518.120877ms 545.803933ms 569.967967ms 586.944355ms 615.973961ms 621.147901ms 623.007102ms 624.153006ms 624.920314ms 633.513374ms 643.051152ms 660.194547ms 664.59344ms 670.724872ms 670.908513ms 678.369935ms 693.335865ms 708.570796ms 708.676516ms 714.505522ms 718.495552ms 720.762066ms 723.786987ms 725.53438ms 729.253315ms 734.030033ms 736.191447ms 739.164553ms 740.115162ms 741.076552ms 742.98765ms 743.26428ms 744.049625ms 744.450699ms 744.580653ms 745.100621ms 745.297099ms 745.390094ms 745.447524ms 745.511425ms 745.622668ms 745.706765ms 746.07898ms 746.103687ms 746.138337ms 746.365584ms 746.422286ms 746.44449ms 746.503123ms 746.610594ms 746.706614ms 746.714434ms 746.813608ms 746.934726ms 747.233553ms 747.322726ms 747.362791ms 747.458159ms 747.796733ms 748.000308ms 748.203055ms 748.415902ms 748.663579ms 748.670777ms 748.695596ms 748.72495ms 748.75141ms 748.756791ms 749.000198ms 749.053481ms 749.077059ms 749.139194ms 749.395308ms 749.449116ms 749.493091ms 749.531555ms 749.555216ms 749.58987ms 749.718521ms 749.842876ms 749.92383ms 749.979953ms 749.986292ms 750.209163ms 750.209185ms 750.29497ms 750.310478ms 750.374075ms 750.424684ms 750.496187ms 750.506815ms 750.524897ms 750.538519ms 750.539176ms 750.548539ms 750.597735ms 750.74856ms 750.885388ms 751.041565ms 751.195355ms 751.2315ms 751.258408ms 751.285041ms 751.290147ms 751.315545ms 751.583222ms 752.383564ms 752.603059ms 752.627847ms 752.801381ms 753.180944ms 753.42419ms 753.440087ms 753.652645ms 753.863563ms 753.918187ms 753.990061ms 753.995358ms 754.114943ms 754.14819ms 754.533714ms 754.58802ms 754.798873ms 754.936839ms 755.000858ms 755.541833ms 756.10644ms 757.059745ms 757.101596ms 757.45315ms 758.737954ms 759.92448ms 760.600816ms 763.296604ms 763.302154ms 766.130484ms 766.500674ms 773.088011ms 776.257771ms 777.449618ms 777.538181ms 787.814818ms 788.254111ms 807.959187ms 816.55406ms 828.56036ms 830.731148ms 837.906689ms 854.152121ms 876.114021ms 878.2288ms 878.964422ms 879.824739ms 899.670126ms 928.555536ms]
  May 10 05:49:24.017: INFO: 50 %ile: 747.322726ms
  May 10 05:49:24.017: INFO: 90 %ile: 766.130484ms
  May 10 05:49:24.017: INFO: 99 %ile: 899.670126ms
  May 10 05:49:24.017: INFO: Total sample count: 200
  May 10 05:49:24.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-27" for this suite. @ 05/10/23 05:49:24.022
• [11.764 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 05/10/23 05:49:24.028
  May 10 05:49:24.028: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pods @ 05/10/23 05:49:24.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:49:24.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:49:24.042
  STEP: Create a pod @ 05/10/23 05:49:24.044
  E0510 05:49:24.585783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:25.586251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/10/23 05:49:26.059
  May 10 05:49:26.070: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  May 10 05:49:26.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5617" for this suite. @ 05/10/23 05:49:26.073
• [2.049 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 05/10/23 05:49:26.078
  May 10 05:49:26.078: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename resourcequota @ 05/10/23 05:49:26.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:49:26.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:49:26.1
  STEP: Counting existing ResourceQuota @ 05/10/23 05:49:26.103
  E0510 05:49:26.587122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:27.587922      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:28.588764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:29.589136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:30.589249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/10/23 05:49:31.105
  STEP: Ensuring resource quota status is calculated @ 05/10/23 05:49:31.113
  E0510 05:49:31.589340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:32.589515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 05/10/23 05:49:33.116
  STEP: Ensuring ResourceQuota status captures the pod usage @ 05/10/23 05:49:33.132
  E0510 05:49:33.590416      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:34.590831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 05/10/23 05:49:35.136
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 05/10/23 05:49:35.138
  STEP: Ensuring a pod cannot update its resource requirements @ 05/10/23 05:49:35.139
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 05/10/23 05:49:35.142
  E0510 05:49:35.591179      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:36.591436      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/10/23 05:49:37.146
  STEP: Ensuring resource quota status released the pod usage @ 05/10/23 05:49:37.161
  E0510 05:49:37.591698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:38.591892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:49:39.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6616" for this suite. @ 05/10/23 05:49:39.166
• [13.094 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 05/10/23 05:49:39.172
  May 10 05:49:39.172: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename job @ 05/10/23 05:49:39.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:49:39.192
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:49:39.194
  STEP: Creating Indexed job @ 05/10/23 05:49:39.197
  STEP: Ensuring job reaches completions @ 05/10/23 05:49:39.204
  E0510 05:49:39.592707      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:40.593246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:41.593455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:42.594145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:43.594912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:44.595091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:45.596204      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:46.596413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:47.597366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:48.597557      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:49.598368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:50.598573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 05/10/23 05:49:51.207
  May 10 05:49:51.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-822" for this suite. @ 05/10/23 05:49:51.212
• [12.044 seconds]
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 05/10/23 05:49:51.216
  May 10 05:49:51.216: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:49:51.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:49:51.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:49:51.235
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 05:49:51.237
  E0510 05:49:51.599197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:52.599812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:53.600581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:54.600822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:55.601895      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:56.602094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:49:57.26
  May 10 05:49:57.262: INFO: Trying to get logs from node node-02 pod downwardapi-volume-e51e1c4c-5c9a-4f72-b908-a147889bb17b container client-container: <nil>
  STEP: delete the pod @ 05/10/23 05:49:57.274
  May 10 05:49:57.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5162" for this suite. @ 05/10/23 05:49:57.29
• [6.079 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 05/10/23 05:49:57.296
  May 10 05:49:57.296: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:49:57.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:49:57.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:49:57.309
  STEP: Setting up server cert @ 05/10/23 05:49:57.328
  E0510 05:49:57.603176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:49:57.854
  STEP: Deploying the webhook pod @ 05/10/23 05:49:57.86
  STEP: Wait for the deployment to be ready @ 05/10/23 05:49:57.873
  May 10 05:49:57.883: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 05:49:58.604300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:49:59.604577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:49:59.914
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:49:59.924
  E0510 05:50:00.605215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:50:00.925: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 10 05:50:00.928: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-376-crds.webhook.example.com via the AdmissionRegistration API @ 05/10/23 05:50:01.442
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/10/23 05:50:01.457
  E0510 05:50:01.605703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:02.606069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:50:03.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0510 05:50:03.607201      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-7115" for this suite. @ 05/10/23 05:50:04.044
  STEP: Destroying namespace "webhook-markers-3400" for this suite. @ 05/10/23 05:50:04.155
• [6.864 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 05/10/23 05:50:04.161
  May 10 05:50:04.161: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename svcaccounts @ 05/10/23 05:50:04.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:50:04.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:50:04.179
  May 10 05:50:04.193: INFO: created pod pod-service-account-defaultsa
  May 10 05:50:04.194: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  May 10 05:50:04.200: INFO: created pod pod-service-account-mountsa
  May 10 05:50:04.200: INFO: pod pod-service-account-mountsa service account token volume mount: true
  May 10 05:50:04.209: INFO: created pod pod-service-account-nomountsa
  May 10 05:50:04.209: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  May 10 05:50:04.220: INFO: created pod pod-service-account-defaultsa-mountspec
  May 10 05:50:04.220: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  May 10 05:50:04.228: INFO: created pod pod-service-account-mountsa-mountspec
  May 10 05:50:04.228: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  May 10 05:50:04.239: INFO: created pod pod-service-account-nomountsa-mountspec
  May 10 05:50:04.239: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  May 10 05:50:04.254: INFO: created pod pod-service-account-defaultsa-nomountspec
  May 10 05:50:04.254: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  May 10 05:50:04.264: INFO: created pod pod-service-account-mountsa-nomountspec
  May 10 05:50:04.264: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  May 10 05:50:04.274: INFO: created pod pod-service-account-nomountsa-nomountspec
  May 10 05:50:04.274: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  May 10 05:50:04.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2120" for this suite. @ 05/10/23 05:50:04.279
• [0.129 seconds]
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 05/10/23 05:50:04.291
  May 10 05:50:04.291: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/10/23 05:50:04.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:50:04.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:50:04.387
  STEP: create the container to handle the HTTPGet hook request. @ 05/10/23 05:50:04.391
  E0510 05:50:04.607412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:05.607614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:06.607972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:07.608466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/10/23 05:50:08.424
  E0510 05:50:08.608645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:09.608747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:10.608886      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:11.609148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 05/10/23 05:50:12.441
  STEP: delete the pod with lifecycle hook @ 05/10/23 05:50:12.453
  E0510 05:50:12.609688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:13.609833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:50:14.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4873" for this suite. @ 05/10/23 05:50:14.465
• [10.179 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 05/10/23 05:50:14.47
  May 10 05:50:14.470: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:50:14.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:50:14.479
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:50:14.483
  STEP: Creating configMap with name configmap-projected-all-test-volume-758370d8-eb25-496b-96db-24ff0b81b74a @ 05/10/23 05:50:14.485
  STEP: Creating secret with name secret-projected-all-test-volume-8693d989-94ba-465d-8bfc-8bd305090b7a @ 05/10/23 05:50:14.496
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 05/10/23 05:50:14.499
  E0510 05:50:14.610508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:15.611145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:16.611230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:17.611754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:50:18.567
  May 10 05:50:18.568: INFO: Trying to get logs from node node-01 pod projected-volume-93b1026c-a9bd-4bff-8284-319978531760 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 05:50:18.585
  May 10 05:50:18.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0510 05:50:18.612339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "projected-7836" for this suite. @ 05/10/23 05:50:18.613
• [4.150 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 05/10/23 05:50:18.62
  May 10 05:50:18.620: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubelet-test @ 05/10/23 05:50:18.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:50:18.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:50:18.637
  STEP: Waiting for pod completion @ 05/10/23 05:50:18.645
  E0510 05:50:19.613459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:20.614487      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:21.615313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:22.615708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:23.615768      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:24.615933      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:50:24.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1186" for this suite. @ 05/10/23 05:50:24.663
• [6.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 05/10/23 05:50:24.67
  May 10 05:50:24.670: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename gc @ 05/10/23 05:50:24.671
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:50:24.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:50:24.684
  STEP: create the deployment @ 05/10/23 05:50:24.685
  W0510 05:50:24.693493      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/10/23 05:50:24.693
  STEP: delete the deployment @ 05/10/23 05:50:25.205
  STEP: wait for all rs to be garbage collected @ 05/10/23 05:50:25.21
  STEP: expected 0 rs, got 1 rs @ 05/10/23 05:50:25.214
  STEP: expected 0 pods, got 2 pods @ 05/10/23 05:50:25.216
  E0510 05:50:25.616937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/10/23 05:50:25.724
  May 10 05:50:25.805: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 10 05:50:25.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1361" for this suite. @ 05/10/23 05:50:25.808
• [1.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 05/10/23 05:50:25.813
  May 10 05:50:25.813: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-probe @ 05/10/23 05:50:25.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:50:25.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:50:25.831
  STEP: Creating pod liveness-2823d111-be8d-4931-a46d-efabd21f32ea in namespace container-probe-1064 @ 05/10/23 05:50:25.834
  E0510 05:50:26.617266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:27.617427      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:50:27.853: INFO: Started pod liveness-2823d111-be8d-4931-a46d-efabd21f32ea in namespace container-probe-1064
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/10/23 05:50:27.853
  May 10 05:50:27.856: INFO: Initial restart count of pod liveness-2823d111-be8d-4931-a46d-efabd21f32ea is 0
  E0510 05:50:28.617897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:29.618090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:30.618461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:31.618864      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:32.618958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:33.619078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:34.619776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:35.619894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:36.620907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:37.621627      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:38.621715      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:39.621836      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:40.622402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:41.622789      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:42.623780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:43.624713      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:44.625340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:45.625607      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:46.625698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:47.625846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:48.625940      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:49.626075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:50.626227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:51.626331      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:52.626480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:53.626560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:54.626653      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:55.626777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:56.626918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:57.627044      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:58.627914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:50:59.628015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:00.628157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:01.628770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:02.628949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:03.629251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:04.630179      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:05.630443      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:06.630629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:07.631176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:08.631613      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:09.631575      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:10.631820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:11.632109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:12.633145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:13.633345      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:14.634031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:15.634203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:16.634685      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:17.635219      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:18.635852      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:19.636048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:20.636350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:21.636809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:22.637810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:23.638005      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:24.638643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:25.638841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:26.639031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:27.639450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:28.640542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:29.641444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:30.642532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:31.642941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:32.643994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:33.644228      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:34.644380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:35.644608      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:36.645557      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:37.646184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:38.646766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:39.646984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:40.647577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:41.647809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:42.648619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:43.648843      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:44.649936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:45.650147      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:46.650577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:47.651116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:48.651308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:49.651557      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:50.652583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:51.652719      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:52.653333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:53.653635      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:54.654062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:55.654279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:56.655343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:57.655850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:58.656830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:51:59.657074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:00.657384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:01.657653      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:02.658395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:03.658692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:04.659575      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:05.659862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:06.660681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:07.661286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:08.662181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:09.662362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:10.662506      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:11.662908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:12.663663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:13.664007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:14.665055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:15.665288      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:16.666038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:17.666621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:18.667576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:19.667849      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:20.668145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:21.668497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:22.668612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:23.668859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:24.669698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:25.669955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:26.670862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:27.671436      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:28.671467      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:29.671703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:30.672159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:31.672467      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:32.672673      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:33.672776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:34.672957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:35.673418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:36.673937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:37.674492      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:38.675051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:39.675270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:40.676258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:41.676488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:42.677506      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:43.677720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:44.678525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:45.678743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:46.679713      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:47.680239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:48.681026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:49.681256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:50.682062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:51.682283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:52.683021      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:53.683227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:54.683570      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:55.684520      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:56.685424      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:57.686225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:58.686326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:52:59.686577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:00.686899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:01.687099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:02.687320      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:03.687534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:04.688037      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:05.688273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:06.688562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:07.689108      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:08.689572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:09.689899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:10.690598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:11.690795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:12.691663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:13.691880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:14.692778      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:15.692980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:16.693029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:17.693493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:18.694562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:19.694799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:20.695173      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:21.695470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:22.696365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:23.696569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:24.697555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:25.697757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:26.698779      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:27.699252      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:28.700104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:29.700324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:30.700362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:31.700572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:32.700592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:33.700806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:34.701642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:35.701835      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:36.702360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:37.702871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:38.703095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:39.703326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:40.703384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:41.704316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:42.704830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:43.705055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:44.705262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:45.705525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:46.706241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:47.706764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:48.707050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:49.707239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:50.707299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:51.707515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:52.707664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:53.707856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:54.708610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:55.708799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:56.709591      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:57.710084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:58.711141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:53:59.711372      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:00.711655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:01.711877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:02.712058      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:03.712278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:04.713029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:05.713274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:06.713791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:07.714267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:08.714805      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:09.715043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:10.715164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:11.715449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:12.716286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:13.716595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:14.716822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:15.717030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:16.717759      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:17.718383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:18.719016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:19.719259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:20.720306      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:21.720513      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:22.721338      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:23.721540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:24.722333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:25.722522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:26.723461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:27.723955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:54:28.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 05:54:28.248
  STEP: Destroying namespace "container-probe-1064" for this suite. @ 05/10/23 05:54:28.262
• [242.457 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 05/10/23 05:54:28.271
  May 10 05:54:28.271: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 05:54:28.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:54:28.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:54:28.289
  STEP: creating a replication controller @ 05/10/23 05:54:28.291
  May 10 05:54:28.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 create -f -'
  E0510 05:54:28.724778      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:54:29.379: INFO: stderr: ""
  May 10 05:54:29.379: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/10/23 05:54:29.379
  May 10 05:54:29.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 10 05:54:29.447: INFO: stderr: ""
  May 10 05:54:29.447: INFO: stdout: "update-demo-nautilus-hsmvc update-demo-nautilus-mt72h "
  May 10 05:54:29.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods update-demo-nautilus-hsmvc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 10 05:54:29.504: INFO: stderr: ""
  May 10 05:54:29.504: INFO: stdout: ""
  May 10 05:54:29.504: INFO: update-demo-nautilus-hsmvc is created but not running
  E0510 05:54:29.725761      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:30.725961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:31.726193      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:32.726603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:33.726789      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:54:34.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 10 05:54:34.564: INFO: stderr: ""
  May 10 05:54:34.564: INFO: stdout: "update-demo-nautilus-hsmvc update-demo-nautilus-mt72h "
  May 10 05:54:34.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods update-demo-nautilus-hsmvc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 10 05:54:34.621: INFO: stderr: ""
  May 10 05:54:34.621: INFO: stdout: "true"
  May 10 05:54:34.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods update-demo-nautilus-hsmvc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 10 05:54:34.678: INFO: stderr: ""
  May 10 05:54:34.678: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 10 05:54:34.678: INFO: validating pod update-demo-nautilus-hsmvc
  May 10 05:54:34.681: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 10 05:54:34.681: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 10 05:54:34.681: INFO: update-demo-nautilus-hsmvc is verified up and running
  May 10 05:54:34.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods update-demo-nautilus-mt72h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0510 05:54:34.727168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:54:34.742: INFO: stderr: ""
  May 10 05:54:34.742: INFO: stdout: "true"
  May 10 05:54:34.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods update-demo-nautilus-mt72h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 10 05:54:34.799: INFO: stderr: ""
  May 10 05:54:34.799: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 10 05:54:34.799: INFO: validating pod update-demo-nautilus-mt72h
  May 10 05:54:34.802: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 10 05:54:34.802: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 10 05:54:34.802: INFO: update-demo-nautilus-mt72h is verified up and running
  STEP: scaling down the replication controller @ 05/10/23 05:54:34.802
  May 10 05:54:34.803: INFO: scanned /root for discovery docs: <nil>
  May 10 05:54:34.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0510 05:54:35.728062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:54:35.879: INFO: stderr: ""
  May 10 05:54:35.879: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/10/23 05:54:35.879
  May 10 05:54:35.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 10 05:54:35.937: INFO: stderr: ""
  May 10 05:54:35.937: INFO: stdout: "update-demo-nautilus-hsmvc update-demo-nautilus-mt72h "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 05/10/23 05:54:35.937
  E0510 05:54:36.728250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:37.728476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:38.729431      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:39.729663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:40.729881      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:54:40.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 10 05:54:41.000: INFO: stderr: ""
  May 10 05:54:41.000: INFO: stdout: "update-demo-nautilus-mt72h "
  May 10 05:54:41.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods update-demo-nautilus-mt72h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 10 05:54:41.062: INFO: stderr: ""
  May 10 05:54:41.062: INFO: stdout: "true"
  May 10 05:54:41.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods update-demo-nautilus-mt72h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 10 05:54:41.125: INFO: stderr: ""
  May 10 05:54:41.125: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 10 05:54:41.125: INFO: validating pod update-demo-nautilus-mt72h
  May 10 05:54:41.128: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 10 05:54:41.128: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 10 05:54:41.128: INFO: update-demo-nautilus-mt72h is verified up and running
  STEP: scaling up the replication controller @ 05/10/23 05:54:41.128
  May 10 05:54:41.129: INFO: scanned /root for discovery docs: <nil>
  May 10 05:54:41.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0510 05:54:41.730472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:54:42.211: INFO: stderr: ""
  May 10 05:54:42.211: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/10/23 05:54:42.211
  May 10 05:54:42.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 10 05:54:42.276: INFO: stderr: ""
  May 10 05:54:42.276: INFO: stdout: "update-demo-nautilus-drggq update-demo-nautilus-mt72h "
  May 10 05:54:42.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods update-demo-nautilus-drggq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 10 05:54:42.337: INFO: stderr: ""
  May 10 05:54:42.337: INFO: stdout: ""
  May 10 05:54:42.337: INFO: update-demo-nautilus-drggq is created but not running
  E0510 05:54:42.731228      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:43.731447      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:44.731692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:45.731897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:46.732143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:54:47.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 10 05:54:47.410: INFO: stderr: ""
  May 10 05:54:47.410: INFO: stdout: "update-demo-nautilus-drggq update-demo-nautilus-mt72h "
  May 10 05:54:47.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods update-demo-nautilus-drggq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 10 05:54:47.466: INFO: stderr: ""
  May 10 05:54:47.466: INFO: stdout: "true"
  May 10 05:54:47.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods update-demo-nautilus-drggq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 10 05:54:47.528: INFO: stderr: ""
  May 10 05:54:47.528: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 10 05:54:47.528: INFO: validating pod update-demo-nautilus-drggq
  May 10 05:54:47.532: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 10 05:54:47.532: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 10 05:54:47.532: INFO: update-demo-nautilus-drggq is verified up and running
  May 10 05:54:47.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods update-demo-nautilus-mt72h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 10 05:54:47.592: INFO: stderr: ""
  May 10 05:54:47.592: INFO: stdout: "true"
  May 10 05:54:47.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods update-demo-nautilus-mt72h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 10 05:54:47.657: INFO: stderr: ""
  May 10 05:54:47.657: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 10 05:54:47.657: INFO: validating pod update-demo-nautilus-mt72h
  May 10 05:54:47.660: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 10 05:54:47.660: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 10 05:54:47.660: INFO: update-demo-nautilus-mt72h is verified up and running
  STEP: using delete to clean up resources @ 05/10/23 05:54:47.66
  May 10 05:54:47.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 delete --grace-period=0 --force -f -'
  May 10 05:54:47.726: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 10 05:54:47.726: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 10 05:54:47.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get rc,svc -l name=update-demo --no-headers'
  E0510 05:54:47.732917      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:54:47.798: INFO: stderr: "No resources found in kubectl-2542 namespace.\n"
  May 10 05:54:47.798: INFO: stdout: ""
  May 10 05:54:47.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2542 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 10 05:54:47.871: INFO: stderr: ""
  May 10 05:54:47.871: INFO: stdout: ""
  May 10 05:54:47.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2542" for this suite. @ 05/10/23 05:54:47.874
• [19.607 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 05/10/23 05:54:47.878
  May 10 05:54:47.878: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename var-expansion @ 05/10/23 05:54:47.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:54:47.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:54:47.894
  E0510 05:54:48.733379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:49.733854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:54:49.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 10 05:54:49.909: INFO: Deleting pod "var-expansion-1bcfd6c8-f3f7-457b-a1ab-ca56a5e8b60d" in namespace "var-expansion-5326"
  May 10 05:54:49.919: INFO: Wait up to 5m0s for pod "var-expansion-1bcfd6c8-f3f7-457b-a1ab-ca56a5e8b60d" to be fully deleted
  E0510 05:54:50.734136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:51.734420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:52.734828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:53.735049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-5326" for this suite. @ 05/10/23 05:54:53.927
• [6.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 05/10/23 05:54:53.937
  May 10 05:54:53.937: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 05:54:53.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:54:53.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:54:53.957
  May 10 05:54:53.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-371 create -f -'
  May 10 05:54:54.298: INFO: stderr: ""
  May 10 05:54:54.298: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  May 10 05:54:54.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-371 create -f -'
  May 10 05:54:54.616: INFO: stderr: ""
  May 10 05:54:54.616: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/10/23 05:54:54.616
  E0510 05:54:54.735079      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:54:55.619: INFO: Selector matched 1 pods for map[app:agnhost]
  May 10 05:54:55.619: INFO: Found 1 / 1
  May 10 05:54:55.619: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 10 05:54:55.621: INFO: Selector matched 1 pods for map[app:agnhost]
  May 10 05:54:55.621: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 10 05:54:55.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-371 describe pod agnhost-primary-v2qk4'
  May 10 05:54:55.689: INFO: stderr: ""
  May 10 05:54:55.689: INFO: stdout: "Name:             agnhost-primary-v2qk4\nNamespace:        kubectl-371\nPriority:         0\nService Account:  default\nNode:             node-02/192.168.60.6\nStart Time:       Wed, 10 May 2023 05:54:54 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: a499203efc986f82d4116eee7a9d2a0ba9d6d58f9812ff9d859db373d814e3d8\n                  cni.projectcalico.org/podIP: 100.114.252.139/32\n                  cni.projectcalico.org/podIPs: 100.114.252.139/32\nStatus:           Running\nIP:               100.114.252.139\nIPs:\n  IP:           100.114.252.139\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://22e1766b95ff234c989cb18a418e40bc4a74bb385db06435fe7a4dabd9e7589c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 10 May 2023 05:54:55 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ljvjp (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-ljvjp:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-371/agnhost-primary-v2qk4 to node-02\n  Normal  Pulled     0s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
  May 10 05:54:55.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-371 describe rc agnhost-primary'
  E0510 05:54:55.735109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:54:55.770: INFO: stderr: ""
  May 10 05:54:55.770: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-371\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-v2qk4\n"
  May 10 05:54:55.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-371 describe service agnhost-primary'
  May 10 05:54:55.837: INFO: stderr: ""
  May 10 05:54:55.837: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-371\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.0.196\nIPs:               10.96.0.196\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.114.252.139:6379\nSession Affinity:  None\nEvents:            <none>\n"
  May 10 05:54:55.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-371 describe node node-01'
  May 10 05:54:55.919: INFO: stderr: ""
  May 10 05:54:55.919: INFO: stdout: "Name:               node-01\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=node-01\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/node=\n                    role=node\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi.tigera.io\":\"node-01\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.60.83/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.67.79.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 08 May 2023 03:40:48 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  node-01\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 10 May 2023 05:54:53 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 08 May 2023 03:41:10 +0000   Mon, 08 May 2023 03:41:10 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 10 May 2023 05:54:47 +0000   Mon, 08 May 2023 03:40:47 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 10 May 2023 05:54:47 +0000   Mon, 08 May 2023 03:40:47 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 10 May 2023 05:54:47 +0000   Mon, 08 May 2023 03:40:47 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 10 May 2023 05:54:47 +0000   Mon, 08 May 2023 03:41:03 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.60.83\n  Hostname:    node-01\nCapacity:\n  cpu:                    8\n  ephemeral-storage:      60795672Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16397168Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    8\n  ephemeral-storage:      56029291223\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16294768Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 d46f1aa3eeac4fc9bcd36b1394b00064\n  System UUID:                96cc01cf-538e-4491-a713-42a803ee6bc6\n  Boot ID:                    2a1220ff-0956-4ad7-8ddb-81bb200fe777\n  Kernel Version:             5.4.0-40-generic\n  OS Image:                   Ubuntu 20.04 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.20\n  Kubelet Version:            v1.27.1\n  Kube-Proxy Version:         v1.27.1\nPodCIDR:                      100.64.1.0/24\nPodCIDRs:                     100.64.1.0/24\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system               calico-node-hrprp                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d2h\n  calico-system               csi-node-driver-wrcs7                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d2h\n  kube-system                 kube-proxy-jtvxj                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d2h\n  kube-system                 kube-sealos-lvscare-node-01                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d2h\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\n  sonobuoy                    sonobuoy-e2e-job-9c188259183c425e                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-xwr24    0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests  Limits\n  --------               --------  ------\n  cpu                    0 (0%)    0 (0%)\n  memory                 0 (0%)    0 (0%)\n  ephemeral-storage      0 (0%)    0 (0%)\n  hugepages-1Gi          0 (0%)    0 (0%)\n  hugepages-2Mi          0 (0%)    0 (0%)\n  scheduling.k8s.io/foo  0         0\nEvents:                  <none>\n"
  May 10 05:54:55.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-371 describe namespace kubectl-371'
  May 10 05:54:55.983: INFO: stderr: ""
  May 10 05:54:55.983: INFO: stdout: "Name:         kubectl-371\nLabels:       e2e-framework=kubectl\n              e2e-run=f0d8353b-b148-477c-8f9b-38112bf35b15\n              kubernetes.io/metadata.name=kubectl-371\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  May 10 05:54:55.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-371" for this suite. @ 05/10/23 05:54:55.985
• [2.053 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 05/10/23 05:54:55.99
  May 10 05:54:55.990: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 05:54:55.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:54:56.001
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:54:56.006
  STEP: creating a collection of services @ 05/10/23 05:54:56.008
  May 10 05:54:56.008: INFO: Creating e2e-svc-a-9rxtg
  May 10 05:54:56.023: INFO: Creating e2e-svc-b-czh42
  May 10 05:54:56.035: INFO: Creating e2e-svc-c-n6gpc
  STEP: deleting service collection @ 05/10/23 05:54:56.045
  May 10 05:54:56.064: INFO: Collection of services has been deleted
  May 10 05:54:56.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5033" for this suite. @ 05/10/23 05:54:56.066
• [0.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 05/10/23 05:54:56.075
  May 10 05:54:56.075: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename watch @ 05/10/23 05:54:56.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:54:56.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:54:56.095
  STEP: creating a watch on configmaps with label A @ 05/10/23 05:54:56.097
  STEP: creating a watch on configmaps with label B @ 05/10/23 05:54:56.098
  STEP: creating a watch on configmaps with label A or B @ 05/10/23 05:54:56.099
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 05/10/23 05:54:56.1
  May 10 05:54:56.104: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2903  82701da4-41df-42f1-bfaa-c56ecec0e494 1075885 0 2023-05-10 05:54:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-10 05:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:54:56.105: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2903  82701da4-41df-42f1-bfaa-c56ecec0e494 1075885 0 2023-05-10 05:54:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-10 05:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 05/10/23 05:54:56.105
  May 10 05:54:56.112: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2903  82701da4-41df-42f1-bfaa-c56ecec0e494 1075886 0 2023-05-10 05:54:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-10 05:54:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:54:56.112: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2903  82701da4-41df-42f1-bfaa-c56ecec0e494 1075886 0 2023-05-10 05:54:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-10 05:54:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 05/10/23 05:54:56.112
  May 10 05:54:56.125: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2903  82701da4-41df-42f1-bfaa-c56ecec0e494 1075887 0 2023-05-10 05:54:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-10 05:54:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:54:56.125: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2903  82701da4-41df-42f1-bfaa-c56ecec0e494 1075887 0 2023-05-10 05:54:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-10 05:54:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 05/10/23 05:54:56.125
  May 10 05:54:56.135: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2903  82701da4-41df-42f1-bfaa-c56ecec0e494 1075888 0 2023-05-10 05:54:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-10 05:54:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:54:56.135: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2903  82701da4-41df-42f1-bfaa-c56ecec0e494 1075888 0 2023-05-10 05:54:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-10 05:54:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 05/10/23 05:54:56.135
  May 10 05:54:56.140: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2903  f03e8338-6671-4050-8163-800a4ab7363e 1075889 0 2023-05-10 05:54:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-10 05:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:54:56.140: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2903  f03e8338-6671-4050-8163-800a4ab7363e 1075889 0 2023-05-10 05:54:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-10 05:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0510 05:54:56.735215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:57.735960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:58.736078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:54:59.736389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:00.736690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:01.736813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:02.737272      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:03.737620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:04.737750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:05.737951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 05/10/23 05:55:06.141
  May 10 05:55:06.151: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2903  f03e8338-6671-4050-8163-800a4ab7363e 1075953 0 2023-05-10 05:54:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-10 05:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 05:55:06.151: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2903  f03e8338-6671-4050-8163-800a4ab7363e 1075953 0 2023-05-10 05:54:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-10 05:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0510 05:55:06.738869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:07.739366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:08.739751      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:09.739681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:10.740040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:11.740247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:12.740671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:13.740877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:14.741110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:15.741448      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:55:16.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2903" for this suite. @ 05/10/23 05:55:16.155
• [20.085 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 05/10/23 05:55:16.16
  May 10 05:55:16.160: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 05:55:16.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:55:16.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:55:16.177
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7839 @ 05/10/23 05:55:16.18
  STEP: changing the ExternalName service to type=NodePort @ 05/10/23 05:55:16.189
  STEP: creating replication controller externalname-service in namespace services-7839 @ 05/10/23 05:55:16.2
  I0510 05:55:16.208977      22 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7839, replica count: 2
  E0510 05:55:16.741664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:17.741716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:18.742000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 05:55:19.261783      22 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 10 05:55:19.261: INFO: Creating new exec pod
  E0510 05:55:19.742696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:20.742792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:21.743132      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:55:22.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7839 exec execpodzj59q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 10 05:55:22.400: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 10 05:55:22.401: INFO: stdout: "externalname-service-qnwjx"
  May 10 05:55:22.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7839 exec execpodzj59q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.255 80'
  May 10 05:55:22.530: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.2.255 80\nConnection to 10.96.2.255 80 port [tcp/http] succeeded!\n"
  May 10 05:55:22.530: INFO: stdout: "externalname-service-5q9qb"
  May 10 05:55:22.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7839 exec execpodzj59q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.60.6 32642'
  E0510 05:55:22.743343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:23.744254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:55:23.765: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.60.6 32642\nConnection to 192.168.60.6 32642 port [tcp/*] succeeded!\n"
  May 10 05:55:23.765: INFO: stdout: "externalname-service-qnwjx"
  May 10 05:55:23.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7839 exec execpodzj59q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.60.83 32642'
  May 10 05:55:23.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.60.83 32642\nConnection to 192.168.60.83 32642 port [tcp/*] succeeded!\n"
  May 10 05:55:23.886: INFO: stdout: "externalname-service-qnwjx"
  May 10 05:55:23.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 10 05:55:23.889: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-7839" for this suite. @ 05/10/23 05:55:23.903
• [7.747 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 05/10/23 05:55:23.908
  May 10 05:55:23.908: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename subpath @ 05/10/23 05:55:23.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:55:23.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:55:23.92
  STEP: Setting up data @ 05/10/23 05:55:23.923
  STEP: Creating pod pod-subpath-test-configmap-r7zm @ 05/10/23 05:55:23.932
  STEP: Creating a pod to test atomic-volume-subpath @ 05/10/23 05:55:23.932
  E0510 05:55:24.745224      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:25.745450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:26.745627      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:27.746227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:28.747260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:29.747538      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:30.748518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:31.748736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:32.749691      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:33.749915      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:34.750445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:35.750528      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:36.750644      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:37.751251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:38.751350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:39.751494      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:40.752516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:41.752740      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:42.753681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:43.754015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:44.754113      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:45.754328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:46.754445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:47.755019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:55:47.983
  May 10 05:55:47.984: INFO: Trying to get logs from node node-02 pod pod-subpath-test-configmap-r7zm container test-container-subpath-configmap-r7zm: <nil>
  STEP: delete the pod @ 05/10/23 05:55:47.998
  STEP: Deleting pod pod-subpath-test-configmap-r7zm @ 05/10/23 05:55:48.011
  May 10 05:55:48.011: INFO: Deleting pod "pod-subpath-test-configmap-r7zm" in namespace "subpath-603"
  May 10 05:55:48.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-603" for this suite. @ 05/10/23 05:55:48.015
• [24.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 05/10/23 05:55:48.023
  May 10 05:55:48.023: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/10/23 05:55:48.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:55:48.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:55:48.038
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 05/10/23 05:55:48.04
  May 10 05:55:48.040: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 05:55:48.755979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:49.756003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:50.756774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:51.757148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:52.758135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:53.758284      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:54.759282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 05/10/23 05:55:54.828
  May 10 05:55:54.829: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 05:55:55.759375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:55:56.264: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 05:55:56.759997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:57.760620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:58.761292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:55:59.761888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:00.762298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:01.762409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:56:02.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0510 05:56:02.762402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-publish-openapi-2011" for this suite. @ 05/10/23 05:56:02.771
• [14.755 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 05/10/23 05:56:02.78
  May 10 05:56:02.780: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename sched-pred @ 05/10/23 05:56:02.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:56:02.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:56:02.792
  May 10 05:56:02.794: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 10 05:56:02.798: INFO: Waiting for terminating namespaces to be deleted...
  May 10 05:56:02.800: INFO: 
  Logging pods the apiserver thinks is on node node-01 before test
  May 10 05:56:02.804: INFO: calico-node-hrprp from calico-system started at 2023-05-08 03:40:50 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.804: INFO: 	Container calico-node ready: true, restart count 0
  May 10 05:56:02.804: INFO: csi-node-driver-wrcs7 from calico-system started at 2023-05-08 03:41:03 +0000 UTC (2 container statuses recorded)
  May 10 05:56:02.804: INFO: 	Container calico-csi ready: true, restart count 0
  May 10 05:56:02.804: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 10 05:56:02.804: INFO: kube-proxy-jtvxj from kube-system started at 2023-05-08 03:40:50 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.804: INFO: 	Container kube-proxy ready: true, restart count 0
  May 10 05:56:02.804: INFO: kube-sealos-lvscare-node-01 from kube-system started at 2023-05-08 03:41:06 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.804: INFO: 	Container kube-sealos-lvscare ready: true, restart count 0
  May 10 05:56:02.804: INFO: sonobuoy from sonobuoy started at 2023-05-10 04:50:14 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.804: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 10 05:56:02.804: INFO: sonobuoy-e2e-job-9c188259183c425e from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 05:56:02.804: INFO: 	Container e2e ready: true, restart count 0
  May 10 05:56:02.804: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 05:56:02.804: INFO: sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-xwr24 from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 05:56:02.804: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 05:56:02.804: INFO: 	Container systemd-logs ready: true, restart count 0
  May 10 05:56:02.804: INFO: 
  Logging pods the apiserver thinks is on node node-02 before test
  May 10 05:56:02.808: INFO: calico-node-96glz from calico-system started at 2023-05-08 05:47:14 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.808: INFO: 	Container calico-node ready: true, restart count 0
  May 10 05:56:02.808: INFO: calico-typha-5777d458f7-g7bn9 from calico-system started at 2023-05-08 05:47:15 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.808: INFO: 	Container calico-typha ready: true, restart count 0
  May 10 05:56:02.808: INFO: csi-node-driver-rh58m from calico-system started at 2023-05-10 05:39:46 +0000 UTC (2 container statuses recorded)
  May 10 05:56:02.808: INFO: 	Container calico-csi ready: true, restart count 0
  May 10 05:56:02.808: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 10 05:56:02.808: INFO: kube-proxy-96kwf from kube-system started at 2023-05-08 05:47:14 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.808: INFO: 	Container kube-proxy ready: true, restart count 0
  May 10 05:56:02.808: INFO: kube-sealos-lvscare-node-02 from kube-system started at 2023-05-08 05:47:31 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.808: INFO: 	Container kube-sealos-lvscare ready: true, restart count 0
  May 10 05:56:02.808: INFO: sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-254cd from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 05:56:02.808: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 05:56:02.808: INFO: 	Container systemd-logs ready: true, restart count 0
  May 10 05:56:02.808: INFO: 
  Logging pods the apiserver thinks is on node ub-test before test
  May 10 05:56:02.812: INFO: calico-apiserver-7f745d8f87-78trq from calico-apiserver started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.813: INFO: 	Container calico-apiserver ready: true, restart count 0
  May 10 05:56:02.813: INFO: calico-apiserver-7f745d8f87-gk9nt from calico-apiserver started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.813: INFO: 	Container calico-apiserver ready: true, restart count 0
  May 10 05:56:02.813: INFO: calico-kube-controllers-6dfbf88686-x7jtc from calico-system started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.813: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 10 05:56:02.813: INFO: calico-node-rwtc6 from calico-system started at 2023-05-06 03:21:16 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.813: INFO: 	Container calico-node ready: true, restart count 2
  May 10 05:56:02.813: INFO: calico-typha-5777d458f7-jvrx5 from calico-system started at 2023-05-06 03:21:16 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.813: INFO: 	Container calico-typha ready: true, restart count 2
  May 10 05:56:02.813: INFO: csi-node-driver-fgjb5 from calico-system started at 2023-05-06 09:37:06 +0000 UTC (2 container statuses recorded)
  May 10 05:56:02.813: INFO: 	Container calico-csi ready: true, restart count 0
  May 10 05:56:02.813: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 10 05:56:02.813: INFO: coredns-5d78c9869d-d9nxz from kube-system started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.813: INFO: 	Container coredns ready: true, restart count 0
  May 10 05:56:02.813: INFO: coredns-5d78c9869d-w9ngw from kube-system started at 2023-05-06 09:37:01 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.813: INFO: 	Container coredns ready: true, restart count 0
  May 10 05:56:02.813: INFO: etcd-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.813: INFO: 	Container etcd ready: true, restart count 2
  May 10 05:56:02.813: INFO: kube-apiserver-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.813: INFO: 	Container kube-apiserver ready: true, restart count 2
  May 10 05:56:02.814: INFO: kube-controller-manager-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.814: INFO: 	Container kube-controller-manager ready: true, restart count 6
  May 10 05:56:02.814: INFO: kube-proxy-hqbzr from kube-system started at 2023-05-06 03:18:37 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.814: INFO: 	Container kube-proxy ready: true, restart count 2
  May 10 05:56:02.814: INFO: kube-scheduler-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.814: INFO: 	Container kube-scheduler ready: true, restart count 6
  May 10 05:56:02.814: INFO: sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-44wmx from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 05:56:02.814: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 05:56:02.814: INFO: 	Container systemd-logs ready: true, restart count 0
  May 10 05:56:02.814: INFO: tigera-operator-7bf7458-fcvpq from tigera-operator started at 2023-05-06 03:21:10 +0000 UTC (1 container statuses recorded)
  May 10 05:56:02.814: INFO: 	Container tigera-operator ready: true, restart count 6
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 05/10/23 05:56:02.814
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.175db28ef81bb542], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 05/10/23 05:56:02.84
  E0510 05:56:03.762902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:56:03.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5065" for this suite. @ 05/10/23 05:56:03.835
• [1.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 05/10/23 05:56:03.841
  May 10 05:56:03.841: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 05:56:03.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:56:03.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:56:03.918
  STEP: creating a replication controller @ 05/10/23 05:56:03.925
  May 10 05:56:03.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5215 create -f -'
  May 10 05:56:04.219: INFO: stderr: ""
  May 10 05:56:04.219: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/10/23 05:56:04.219
  May 10 05:56:04.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5215 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 10 05:56:04.281: INFO: stderr: ""
  May 10 05:56:04.281: INFO: stdout: "update-demo-nautilus-tph6g update-demo-nautilus-vz7fh "
  May 10 05:56:04.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5215 get pods update-demo-nautilus-tph6g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 10 05:56:04.339: INFO: stderr: ""
  May 10 05:56:04.339: INFO: stdout: ""
  May 10 05:56:04.339: INFO: update-demo-nautilus-tph6g is created but not running
  E0510 05:56:04.763839      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:05.764022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:06.764233      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:07.764731      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:08.764908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:56:09.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5215 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 10 05:56:09.404: INFO: stderr: ""
  May 10 05:56:09.404: INFO: stdout: "update-demo-nautilus-tph6g update-demo-nautilus-vz7fh "
  May 10 05:56:09.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5215 get pods update-demo-nautilus-tph6g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 10 05:56:09.463: INFO: stderr: ""
  May 10 05:56:09.463: INFO: stdout: "true"
  May 10 05:56:09.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5215 get pods update-demo-nautilus-tph6g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 10 05:56:09.522: INFO: stderr: ""
  May 10 05:56:09.522: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 10 05:56:09.522: INFO: validating pod update-demo-nautilus-tph6g
  May 10 05:56:09.525: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 10 05:56:09.525: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 10 05:56:09.525: INFO: update-demo-nautilus-tph6g is verified up and running
  May 10 05:56:09.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5215 get pods update-demo-nautilus-vz7fh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 10 05:56:09.582: INFO: stderr: ""
  May 10 05:56:09.582: INFO: stdout: "true"
  May 10 05:56:09.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5215 get pods update-demo-nautilus-vz7fh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 10 05:56:09.639: INFO: stderr: ""
  May 10 05:56:09.639: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 10 05:56:09.639: INFO: validating pod update-demo-nautilus-vz7fh
  May 10 05:56:09.643: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 10 05:56:09.643: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 10 05:56:09.643: INFO: update-demo-nautilus-vz7fh is verified up and running
  STEP: using delete to clean up resources @ 05/10/23 05:56:09.643
  May 10 05:56:09.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5215 delete --grace-period=0 --force -f -'
  May 10 05:56:09.703: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 10 05:56:09.703: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 10 05:56:09.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5215 get rc,svc -l name=update-demo --no-headers'
  E0510 05:56:09.764988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:56:09.771: INFO: stderr: "No resources found in kubectl-5215 namespace.\n"
  May 10 05:56:09.771: INFO: stdout: ""
  May 10 05:56:09.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-5215 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 10 05:56:09.833: INFO: stderr: ""
  May 10 05:56:09.833: INFO: stdout: ""
  May 10 05:56:09.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5215" for this suite. @ 05/10/23 05:56:09.836
• [6.000 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 05/10/23 05:56:09.841
  May 10 05:56:09.841: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:56:09.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:56:09.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:56:09.856
  STEP: Setting up server cert @ 05/10/23 05:56:09.875
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:56:10.574
  STEP: Deploying the webhook pod @ 05/10/23 05:56:10.584
  STEP: Wait for the deployment to be ready @ 05/10/23 05:56:10.598
  May 10 05:56:10.607: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 05:56:10.765064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:11.765264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:56:12.614
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:56:12.62
  E0510 05:56:12.766394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:56:13.621: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 05/10/23 05:56:13.624
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/10/23 05:56:13.639
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 05/10/23 05:56:13.647
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/10/23 05:56:13.656
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 05/10/23 05:56:13.662
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/10/23 05:56:13.67
  May 10 05:56:13.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2219" for this suite. @ 05/10/23 05:56:13.703
  STEP: Destroying namespace "webhook-markers-238" for this suite. @ 05/10/23 05:56:13.711
• [3.876 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 05/10/23 05:56:13.718
  May 10 05:56:13.718: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename statefulset @ 05/10/23 05:56:13.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:56:13.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:56:13.733
  STEP: Creating service test in namespace statefulset-900 @ 05/10/23 05:56:13.735
  STEP: Creating a new StatefulSet @ 05/10/23 05:56:13.739
  May 10 05:56:13.749: INFO: Found 0 stateful pods, waiting for 3
  E0510 05:56:13.766545      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:14.766735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:15.766844      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:16.766899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:17.767038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:18.767184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:19.767325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:20.767455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:21.767572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:22.767679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:56:23.752: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 10 05:56:23.752: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 10 05:56:23.752: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  May 10 05:56:23.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-900 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0510 05:56:23.768743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:56:23.877: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 10 05:56:23.877: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 10 05:56:23.877: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0510 05:56:24.769495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:25.769735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:26.769934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:27.770358      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:28.770589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:29.770948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:30.771165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:31.771408      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:32.771817      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:33.772026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/10/23 05:56:33.887
  May 10 05:56:33.908: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/10/23 05:56:33.908
  E0510 05:56:34.772340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:35.772551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:36.772776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:37.773348      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:38.773565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:39.773770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:40.773981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:41.774208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:42.774673      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:43.774884      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 05/10/23 05:56:43.918
  May 10 05:56:43.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-900 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 10 05:56:44.033: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 10 05:56:44.033: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 10 05:56:44.033: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0510 05:56:44.775324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:45.775532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:46.775853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:47.776426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:48.777430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:49.777640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:50.777818      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:51.778070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:52.778361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:53.778559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 05/10/23 05:56:54.046
  May 10 05:56:54.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-900 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 10 05:56:54.395: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 10 05:56:54.396: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 10 05:56:54.396: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0510 05:56:54.779169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:55.779405      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:56.779728      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:57.780258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:58.780455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:56:59.780702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:00.780990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:01.781196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:02.781562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:03.781785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:57:04.427: INFO: Updating stateful set ss2
  E0510 05:57:04.782029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:05.782315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:06.782896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:07.783468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:08.783774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:09.784021      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:10.784264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:11.784369      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:12.784506      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:13.784703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 05/10/23 05:57:14.439
  May 10 05:57:14.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-900 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 10 05:57:14.556: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 10 05:57:14.556: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 10 05:57:14.556: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0510 05:57:14.784890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:15.785024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:16.785429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:17.785891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:18.786029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:19.786158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:20.786292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:21.786450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:22.786589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:23.786735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:57:24.569: INFO: Deleting all statefulset in ns statefulset-900
  May 10 05:57:24.571: INFO: Scaling statefulset ss2 to 0
  E0510 05:57:24.787223      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:25.787489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:26.788391      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:27.788952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:28.789200      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:29.789391      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:30.789582      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:31.789788      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:32.790197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:33.790405      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:57:34.588: INFO: Waiting for statefulset status.replicas updated to 0
  May 10 05:57:34.590: INFO: Deleting statefulset ss2
  May 10 05:57:34.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-900" for this suite. @ 05/10/23 05:57:34.606
• [80.897 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 05/10/23 05:57:34.615
  May 10 05:57:34.615: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename lease-test @ 05/10/23 05:57:34.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:57:34.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:57:34.634
  May 10 05:57:34.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-3308" for this suite. @ 05/10/23 05:57:34.676
• [0.065 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 05/10/23 05:57:34.68
  May 10 05:57:34.680: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:57:34.681
  E0510 05:57:34.790656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:57:34.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:57:34.827
  STEP: Setting up server cert @ 05/10/23 05:57:34.853
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:57:35.525
  STEP: Deploying the webhook pod @ 05/10/23 05:57:35.538
  STEP: Wait for the deployment to be ready @ 05/10/23 05:57:35.559
  May 10 05:57:35.567: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 05:57:35.791429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:36.791982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:57:37.59
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:57:37.606
  E0510 05:57:37.792013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:57:38.607: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/10/23 05:57:38.609
  STEP: create a pod that should be denied by the webhook @ 05/10/23 05:57:38.661
  STEP: create a pod that causes the webhook to hang @ 05/10/23 05:57:38.67
  E0510 05:57:38.792643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:39.792993      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:40.793467      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:41.793701      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:42.794184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:43.794411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:44.795090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:45.795328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:46.796185      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:47.796642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 05/10/23 05:57:48.675
  STEP: create a configmap that should be admitted by the webhook @ 05/10/23 05:57:48.682
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/10/23 05:57:48.69
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/10/23 05:57:48.695
  STEP: create a namespace that bypass the webhook @ 05/10/23 05:57:48.699
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 05/10/23 05:57:48.71
  May 10 05:57:48.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0510 05:57:48.797420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-548" for this suite. @ 05/10/23 05:57:48.889
  STEP: Destroying namespace "webhook-markers-511" for this suite. @ 05/10/23 05:57:48.899
  STEP: Destroying namespace "exempted-namespace-3382" for this suite. @ 05/10/23 05:57:48.903
• [14.226 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 05/10/23 05:57:48.907
  May 10 05:57:48.907: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename init-container @ 05/10/23 05:57:48.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:57:48.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:57:48.927
  STEP: creating the pod @ 05/10/23 05:57:48.929
  May 10 05:57:48.929: INFO: PodSpec: initContainers in spec.initContainers
  E0510 05:57:49.797716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:50.798260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:51.798256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:52.799033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:53.799442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:54.799589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:57:55.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7709" for this suite. @ 05/10/23 05:57:55.284
• [6.387 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 05/10/23 05:57:55.295
  May 10 05:57:55.295: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:57:55.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:57:55.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:57:55.314
  STEP: Creating projection with secret that has name projected-secret-test-f8431d37-6f15-45b6-95d9-859ded3049b9 @ 05/10/23 05:57:55.316
  STEP: Creating a pod to test consume secrets @ 05/10/23 05:57:55.324
  E0510 05:57:55.799930      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:56.800165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:57.800320      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:58.800562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:57:59.800649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:00.800863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:58:01.348
  May 10 05:58:01.350: INFO: Trying to get logs from node node-02 pod pod-projected-secrets-56e169d8-e58f-45bf-bdec-f5149fe3426c container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 05:58:01.362
  May 10 05:58:01.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5178" for this suite. @ 05/10/23 05:58:01.389
• [6.105 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 05/10/23 05:58:01.4
  May 10 05:58:01.400: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 05/10/23 05:58:01.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:58:01.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:58:01.417
  STEP: Setting up the test @ 05/10/23 05:58:01.419
  STEP: Creating hostNetwork=false pod @ 05/10/23 05:58:01.419
  E0510 05:58:01.801399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:02.801761      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:03.801787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:04.802266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 05/10/23 05:58:05.442
  E0510 05:58:05.802372      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:06.802620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:07.803553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:08.803803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 05/10/23 05:58:09.458
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 05/10/23 05:58:09.458
  May 10 05:58:09.458: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2931 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:58:09.458: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:58:09.458: INFO: ExecWithOptions: Clientset creation
  May 10 05:58:09.458: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2931/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 10 05:58:09.525: INFO: Exec stderr: ""
  May 10 05:58:09.525: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2931 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:58:09.525: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:58:09.526: INFO: ExecWithOptions: Clientset creation
  May 10 05:58:09.526: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2931/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 10 05:58:09.581: INFO: Exec stderr: ""
  May 10 05:58:09.582: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2931 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:58:09.582: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:58:09.582: INFO: ExecWithOptions: Clientset creation
  May 10 05:58:09.582: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2931/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 10 05:58:09.620: INFO: Exec stderr: ""
  May 10 05:58:09.621: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2931 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:58:09.621: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:58:09.621: INFO: ExecWithOptions: Clientset creation
  May 10 05:58:09.621: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2931/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 10 05:58:09.680: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 05/10/23 05:58:09.681
  May 10 05:58:09.681: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2931 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:58:09.681: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:58:09.681: INFO: ExecWithOptions: Clientset creation
  May 10 05:58:09.681: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2931/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 10 05:58:09.739: INFO: Exec stderr: ""
  May 10 05:58:09.739: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2931 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:58:09.740: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:58:09.740: INFO: ExecWithOptions: Clientset creation
  May 10 05:58:09.740: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2931/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 10 05:58:09.787: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 05/10/23 05:58:09.787
  May 10 05:58:09.787: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2931 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:58:09.787: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:58:09.788: INFO: ExecWithOptions: Clientset creation
  May 10 05:58:09.788: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2931/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  E0510 05:58:09.804700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:58:09.844: INFO: Exec stderr: ""
  May 10 05:58:09.844: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2931 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:58:09.844: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:58:09.845: INFO: ExecWithOptions: Clientset creation
  May 10 05:58:09.845: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2931/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 10 05:58:09.917: INFO: Exec stderr: ""
  May 10 05:58:09.917: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2931 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:58:09.917: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:58:09.917: INFO: ExecWithOptions: Clientset creation
  May 10 05:58:09.917: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2931/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 10 05:58:09.955: INFO: Exec stderr: ""
  May 10 05:58:09.955: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2931 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 05:58:09.955: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:58:09.956: INFO: ExecWithOptions: Clientset creation
  May 10 05:58:09.956: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2931/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 10 05:58:10.011: INFO: Exec stderr: ""
  May 10 05:58:10.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-2931" for this suite. @ 05/10/23 05:58:10.014
• [8.619 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 05/10/23 05:58:10.019
  May 10 05:58:10.019: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename replicaset @ 05/10/23 05:58:10.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:58:10.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:58:10.04
  May 10 05:58:10.043: INFO: Creating ReplicaSet my-hostname-basic-9b2fbdf0-004d-4ed5-b8e8-7f28f0ed6ef7
  May 10 05:58:10.057: INFO: Pod name my-hostname-basic-9b2fbdf0-004d-4ed5-b8e8-7f28f0ed6ef7: Found 0 pods out of 1
  E0510 05:58:10.805396      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:11.805736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:12.806169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:13.806394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:14.806598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:58:15.062: INFO: Pod name my-hostname-basic-9b2fbdf0-004d-4ed5-b8e8-7f28f0ed6ef7: Found 1 pods out of 1
  May 10 05:58:15.062: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9b2fbdf0-004d-4ed5-b8e8-7f28f0ed6ef7" is running
  May 10 05:58:15.065: INFO: Pod "my-hostname-basic-9b2fbdf0-004d-4ed5-b8e8-7f28f0ed6ef7-pf745" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-10 05:58:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-10 05:58:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-10 05:58:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-10 05:58:10 +0000 UTC Reason: Message:}])
  May 10 05:58:15.065: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/10/23 05:58:15.065
  May 10 05:58:15.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-110" for this suite. @ 05/10/23 05:58:15.075
• [5.060 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 05/10/23 05:58:15.08
  May 10 05:58:15.080: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename containers @ 05/10/23 05:58:15.08
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:58:15.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:58:15.097
  STEP: Creating a pod to test override command @ 05/10/23 05:58:15.099
  E0510 05:58:15.807295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:16.807413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:17.807721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:18.807958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:58:19.12
  May 10 05:58:19.121: INFO: Trying to get logs from node ub-test pod client-containers-e6ca5be0-fa2e-4047-b92b-1dccaa7068c4 container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 05:58:19.133
  May 10 05:58:19.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3390" for this suite. @ 05/10/23 05:58:19.158
• [4.082 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 05/10/23 05:58:19.162
  May 10 05:58:19.162: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 05:58:19.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:58:19.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:58:19.174
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 05/10/23 05:58:19.176
  E0510 05:58:19.809016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:20.809241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:21.810291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:22.810768      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:58:23.198
  May 10 05:58:23.200: INFO: Trying to get logs from node node-02 pod pod-e301fef1-4d62-4f14-9ac1-ea5c5f8874c4 container test-container: <nil>
  STEP: delete the pod @ 05/10/23 05:58:23.204
  May 10 05:58:23.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3546" for this suite. @ 05/10/23 05:58:23.222
• [4.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 05/10/23 05:58:23.229
  May 10 05:58:23.229: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename events @ 05/10/23 05:58:23.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:58:23.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:58:23.25
  STEP: Create set of events @ 05/10/23 05:58:23.252
  May 10 05:58:23.256: INFO: created test-event-1
  May 10 05:58:23.259: INFO: created test-event-2
  May 10 05:58:23.266: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 05/10/23 05:58:23.266
  STEP: delete collection of events @ 05/10/23 05:58:23.268
  May 10 05:58:23.268: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/10/23 05:58:23.283
  May 10 05:58:23.283: INFO: requesting list of events to confirm quantity
  May 10 05:58:23.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9281" for this suite. @ 05/10/23 05:58:23.287
• [0.061 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 05/10/23 05:58:23.29
  May 10 05:58:23.290: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:58:23.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:58:23.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:58:23.304
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 05:58:23.306
  E0510 05:58:23.811292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:24.811796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:25.811932      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:26.812195      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:27.812274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:28.812527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:58:29.324
  May 10 05:58:29.326: INFO: Trying to get logs from node node-02 pod downwardapi-volume-815089f2-b6f6-4341-bd30-11ce5be14a71 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 05:58:29.33
  May 10 05:58:29.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5746" for this suite. @ 05/10/23 05:58:29.351
• [6.070 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 05/10/23 05:58:29.36
  May 10 05:58:29.360: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename statefulset @ 05/10/23 05:58:29.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:58:29.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:58:29.375
  STEP: Creating service test in namespace statefulset-6529 @ 05/10/23 05:58:29.377
  STEP: Looking for a node to schedule stateful set and pod @ 05/10/23 05:58:29.385
  STEP: Creating pod with conflicting port in namespace statefulset-6529 @ 05/10/23 05:58:29.388
  STEP: Waiting until pod test-pod will start running in namespace statefulset-6529 @ 05/10/23 05:58:29.393
  E0510 05:58:29.813394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:30.814071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-6529 @ 05/10/23 05:58:31.398
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6529 @ 05/10/23 05:58:31.404
  May 10 05:58:31.421: INFO: Observed stateful pod in namespace: statefulset-6529, name: ss-0, uid: 7f504491-2599-4c6f-a37e-9fa75d453593, status phase: Pending. Waiting for statefulset controller to delete.
  May 10 05:58:31.432: INFO: Observed stateful pod in namespace: statefulset-6529, name: ss-0, uid: 7f504491-2599-4c6f-a37e-9fa75d453593, status phase: Failed. Waiting for statefulset controller to delete.
  May 10 05:58:31.443: INFO: Observed stateful pod in namespace: statefulset-6529, name: ss-0, uid: 7f504491-2599-4c6f-a37e-9fa75d453593, status phase: Failed. Waiting for statefulset controller to delete.
  May 10 05:58:31.448: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6529
  STEP: Removing pod with conflicting port in namespace statefulset-6529 @ 05/10/23 05:58:31.448
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6529 and will be in running state @ 05/10/23 05:58:31.473
  E0510 05:58:31.814332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:32.815201      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:58:33.483: INFO: Deleting all statefulset in ns statefulset-6529
  May 10 05:58:33.485: INFO: Scaling statefulset ss to 0
  E0510 05:58:33.815413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:34.815837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:35.816014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:36.816313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:37.816457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:38.816777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:39.817787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:40.817874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:41.817994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:42.818126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:58:43.506: INFO: Waiting for statefulset status.replicas updated to 0
  May 10 05:58:43.508: INFO: Deleting statefulset ss
  May 10 05:58:43.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6529" for this suite. @ 05/10/23 05:58:43.517
• [14.160 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 05/10/23 05:58:43.521
  May 10 05:58:43.521: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-runtime @ 05/10/23 05:58:43.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:58:43.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:58:43.534
  STEP: create the container @ 05/10/23 05:58:43.536
  W0510 05:58:43.655276      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/10/23 05:58:43.655
  E0510 05:58:43.818459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:44.819367      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:45.819434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:46.819534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:47.820287      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/10/23 05:58:48.67
  STEP: the container should be terminated @ 05/10/23 05:58:48.674
  STEP: the termination message should be set @ 05/10/23 05:58:48.674
  May 10 05:58:48.674: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 05/10/23 05:58:48.674
  May 10 05:58:48.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6066" for this suite. @ 05/10/23 05:58:48.697
• [5.184 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 05/10/23 05:58:48.706
  May 10 05:58:48.706: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 05:58:48.706
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:58:48.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:58:48.727
  STEP: Creating configMap with name configmap-test-upd-76ae1f35-754e-468d-bfde-42994e9281fe @ 05/10/23 05:58:48.731
  STEP: Creating the pod @ 05/10/23 05:58:48.735
  E0510 05:58:48.820598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:49.821249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:50.821692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:51.821827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 05/10/23 05:58:52.766
  STEP: Waiting for pod with binary data @ 05/10/23 05:58:52.77
  May 10 05:58:52.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9513" for this suite. @ 05/10/23 05:58:52.776
• [4.075 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 05/10/23 05:58:52.781
  May 10 05:58:52.781: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:58:52.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:58:52.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:58:52.8
  STEP: Setting up server cert @ 05/10/23 05:58:52.818
  E0510 05:58:52.822578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:58:53.105
  STEP: Deploying the webhook pod @ 05/10/23 05:58:53.112
  STEP: Wait for the deployment to be ready @ 05/10/23 05:58:53.124
  May 10 05:58:53.135: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 05:58:53.822724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:54.822973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:58:55.142
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:58:55.148
  E0510 05:58:55.823773      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:58:56.149: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 05/10/23 05:58:56.151
  STEP: Creating a custom resource definition that should be denied by the webhook @ 05/10/23 05:58:56.168
  May 10 05:58:56.168: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 05:58:56.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7304" for this suite. @ 05/10/23 05:58:56.218
  STEP: Destroying namespace "webhook-markers-4080" for this suite. @ 05/10/23 05:58:56.23
• [3.455 seconds]
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 05/10/23 05:58:56.237
  May 10 05:58:56.237: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:58:56.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:58:56.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:58:56.254
  STEP: Creating configMap with name cm-test-opt-del-ef9ebbc3-2925-4886-bcb5-d9e21849e39c @ 05/10/23 05:58:56.259
  STEP: Creating configMap with name cm-test-opt-upd-c7754c39-bde7-4a43-b9ab-c0b1db090b0f @ 05/10/23 05:58:56.262
  STEP: Creating the pod @ 05/10/23 05:58:56.266
  E0510 05:58:56.823849      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:57.824450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:58.825372      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:58:59.825635      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-ef9ebbc3-2925-4886-bcb5-d9e21849e39c @ 05/10/23 05:59:00.318
  STEP: Updating configmap cm-test-opt-upd-c7754c39-bde7-4a43-b9ab-c0b1db090b0f @ 05/10/23 05:59:00.322
  STEP: Creating configMap with name cm-test-opt-create-79166729-98eb-4bbc-ad50-304a978ca722 @ 05/10/23 05:59:00.326
  STEP: waiting to observe update in volume @ 05/10/23 05:59:00.33
  E0510 05:59:00.826621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:01.826832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:02.827001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:03.827369      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:04.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5375" for this suite. @ 05/10/23 05:59:04.388
• [8.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 05/10/23 05:59:04.4
  May 10 05:59:04.400: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename job @ 05/10/23 05:59:04.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:59:04.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:59:04.415
  STEP: Creating a job @ 05/10/23 05:59:04.417
  STEP: Ensuring active pods == parallelism @ 05/10/23 05:59:04.43
  E0510 05:59:04.828295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:05.828559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 05/10/23 05:59:06.433
  E0510 05:59:06.828857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:07.137: INFO: Successfully updated pod "adopt-release-bm8b7"
  STEP: Checking that the Job readopts the Pod @ 05/10/23 05:59:07.137
  E0510 05:59:07.829343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:08.829705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 05/10/23 05:59:09.271
  May 10 05:59:09.787: INFO: Successfully updated pod "adopt-release-bm8b7"
  STEP: Checking that the Job releases the Pod @ 05/10/23 05:59:09.787
  E0510 05:59:09.830158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:10.830535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:11.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-600" for this suite. @ 05/10/23 05:59:11.794
• [7.400 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 05/10/23 05:59:11.802
  May 10 05:59:11.802: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename subjectreview @ 05/10/23 05:59:11.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:59:11.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:59:11.823
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-4628" @ 05/10/23 05:59:11.825
  E0510 05:59:11.831237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:11.833: INFO: saUsername: "system:serviceaccount:subjectreview-4628:e2e"
  May 10 05:59:11.833: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-4628"}
  May 10 05:59:11.833: INFO: saUID: "cdfda212-95d4-4673-ab20-fc965740d40c"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-4628:e2e" @ 05/10/23 05:59:11.833
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-4628:e2e" @ 05/10/23 05:59:11.834
  May 10 05:59:11.836: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-4628:e2e" api 'list' configmaps in "subjectreview-4628" namespace @ 05/10/23 05:59:11.836
  May 10 05:59:11.837: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-4628:e2e" @ 05/10/23 05:59:11.837
  May 10 05:59:11.838: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  May 10 05:59:11.838: INFO: LocalSubjectAccessReview has been verified
  May 10 05:59:11.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-4628" for this suite. @ 05/10/23 05:59:11.841
• [0.044 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 05/10/23 05:59:11.847
  May 10 05:59:11.847: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename limitrange @ 05/10/23 05:59:11.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:59:11.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:59:11.869
  STEP: Creating a LimitRange @ 05/10/23 05:59:11.871
  STEP: Setting up watch @ 05/10/23 05:59:11.871
  STEP: Submitting a LimitRange @ 05/10/23 05:59:11.973
  STEP: Verifying LimitRange creation was observed @ 05/10/23 05:59:11.981
  STEP: Fetching the LimitRange to ensure it has proper values @ 05/10/23 05:59:11.981
  May 10 05:59:11.983: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 10 05:59:11.983: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 05/10/23 05:59:11.983
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 05/10/23 05:59:11.993
  May 10 05:59:11.995: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 10 05:59:11.995: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 05/10/23 05:59:11.995
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 05/10/23 05:59:12.007
  May 10 05:59:12.009: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  May 10 05:59:12.009: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 05/10/23 05:59:12.009
  STEP: Failing to create a Pod with more than max resources @ 05/10/23 05:59:12.011
  STEP: Updating a LimitRange @ 05/10/23 05:59:12.012
  STEP: Verifying LimitRange updating is effective @ 05/10/23 05:59:12.022
  E0510 05:59:12.832295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:13.832398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 05/10/23 05:59:14.026
  STEP: Failing to create a Pod with more than max resources @ 05/10/23 05:59:14.032
  STEP: Deleting a LimitRange @ 05/10/23 05:59:14.034
  STEP: Verifying the LimitRange was deleted @ 05/10/23 05:59:14.041
  E0510 05:59:14.832524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:15.832729      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:16.832955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:17.833465      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:18.833564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:19.044: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 05/10/23 05:59:19.044
  May 10 05:59:19.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-4836" for this suite. @ 05/10/23 05:59:19.053
• [7.217 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 05/10/23 05:59:19.064
  May 10 05:59:19.064: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename init-container @ 05/10/23 05:59:19.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:59:19.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:59:19.08
  STEP: creating the pod @ 05/10/23 05:59:19.082
  May 10 05:59:19.082: INFO: PodSpec: initContainers in spec.initContainers
  E0510 05:59:19.834617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:20.835507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:21.835524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:22.836070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:23.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4506" for this suite. @ 05/10/23 05:59:23.081
• [4.030 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 05/10/23 05:59:23.094
  May 10 05:59:23.094: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 05:59:23.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:59:23.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:59:23.115
  STEP: Setting up server cert @ 05/10/23 05:59:23.137
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 05:59:23.327
  STEP: Deploying the webhook pod @ 05/10/23 05:59:23.335
  STEP: Wait for the deployment to be ready @ 05/10/23 05:59:23.358
  May 10 05:59:23.365: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 05:59:23.836166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:24.836471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 05:59:25.373
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 05:59:25.387
  E0510 05:59:25.836800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:26.387: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 05/10/23 05:59:26.39
  STEP: create a namespace for the webhook @ 05/10/23 05:59:26.413
  STEP: create a configmap should be unconditionally rejected by the webhook @ 05/10/23 05:59:26.558
  May 10 05:59:26.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2862" for this suite. @ 05/10/23 05:59:26.608
  STEP: Destroying namespace "webhook-markers-1290" for this suite. @ 05/10/23 05:59:26.621
  STEP: Destroying namespace "fail-closed-namespace-7430" for this suite. @ 05/10/23 05:59:26.629
• [3.540 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 05/10/23 05:59:26.634
  May 10 05:59:26.634: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 05:59:26.635
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:59:26.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:59:26.658
  STEP: Creating configMap with name projected-configmap-test-volume-map-6d4d2d48-ab77-407c-a921-c4969128fc44 @ 05/10/23 05:59:26.661
  STEP: Creating a pod to test consume configMaps @ 05/10/23 05:59:26.666
  E0510 05:59:26.836968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:27.837527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:28.837881      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:29.838172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 05:59:30.686
  May 10 05:59:30.688: INFO: Trying to get logs from node node-01 pod pod-projected-configmaps-2f54a1f7-794d-4674-98e5-19d788c36404 container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 05:59:30.692
  May 10 05:59:30.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4480" for this suite. @ 05/10/23 05:59:30.715
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 05/10/23 05:59:30.72
  May 10 05:59:30.720: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename security-context-test @ 05/10/23 05:59:30.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:59:30.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:59:30.739
  E0510 05:59:30.838787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:31.839292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:32.840072      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:33.840416      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:34.766: INFO: Got logs for pod "busybox-privileged-false-cced3fce-5d65-40ae-a5f7-8f2cba0e18d8": "ip: RTNETLINK answers: Operation not permitted\n"
  May 10 05:59:34.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-1159" for this suite. @ 05/10/23 05:59:34.772
• [4.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 05/10/23 05:59:34.777
  May 10 05:59:34.777: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename security-context-test @ 05/10/23 05:59:34.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:59:34.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:59:34.801
  E0510 05:59:34.840705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:35.841387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:36.841962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:37.842442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:38.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4323" for this suite. @ 05/10/23 05:59:38.826
• [4.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 05/10/23 05:59:38.831
  May 10 05:59:38.831: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename field-validation @ 05/10/23 05:59:38.832
  E0510 05:59:38.842443      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:59:38.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:59:38.846
  May 10 05:59:38.848: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  W0510 05:59:38.849208      22 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc00039b210 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0510 05:59:39.843471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:40.844019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0510 05:59:41.388239      22 warnings.go:70] unknown field "alpha"
  W0510 05:59:41.388260      22 warnings.go:70] unknown field "beta"
  W0510 05:59:41.388265      22 warnings.go:70] unknown field "delta"
  W0510 05:59:41.388270      22 warnings.go:70] unknown field "epsilon"
  W0510 05:59:41.388275      22 warnings.go:70] unknown field "gamma"
  May 10 05:59:41.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1277" for this suite. @ 05/10/23 05:59:41.411
• [2.583 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 05/10/23 05:59:41.415
  May 10 05:59:41.415: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 05:59:41.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:59:41.423
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:59:41.424
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/10/23 05:59:41.426
  May 10 05:59:41.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-889 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 10 05:59:41.492: INFO: stderr: ""
  May 10 05:59:41.492: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 05/10/23 05:59:41.492
  E0510 05:59:41.844102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:42.845092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:43.845300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:44.845503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:45.845702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/10/23 05:59:46.543
  May 10 05:59:46.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-889 get pod e2e-test-httpd-pod -o json'
  May 10 05:59:46.600: INFO: stderr: ""
  May 10 05:59:46.600: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"525f1d9af15fb05c3edcbae8ea4f0a66b58bdadc775182bf389cc0649d49b6f4\",\n            \"cni.projectcalico.org/podIP\": \"100.67.79.162/32\",\n            \"cni.projectcalico.org/podIPs\": \"100.67.79.162/32\"\n        },\n        \"creationTimestamp\": \"2023-05-10T05:59:41Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-889\",\n        \"resourceVersion\": \"1078537\",\n        \"uid\": \"537ac8f4-d1b9-43f3-affa-e77b15b4e0a9\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-6l4hn\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node-01\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-6l4hn\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-10T05:59:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-10T05:59:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-10T05:59:42Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-10T05:59:41Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://07a2e96af2598c4bd1018e90757c88e7d1d77e3aa2ead21b5dcd8b758b074868\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-10T05:59:42Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.60.83\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.67.79.162\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.67.79.162\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-10T05:59:41Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 05/10/23 05:59:46.6
  May 10 05:59:46.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-889 replace -f -'
  E0510 05:59:46.846123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:47.708: INFO: stderr: ""
  May 10 05:59:47.708: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 05/10/23 05:59:47.708
  May 10 05:59:47.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-889 delete pods e2e-test-httpd-pod'
  E0510 05:59:47.846656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:48.846912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:49.847171      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:50.794: INFO: stderr: ""
  May 10 05:59:50.794: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 10 05:59:50.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-889" for this suite. @ 05/10/23 05:59:50.798
• [9.391 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 05/10/23 05:59:50.806
  May 10 05:59:50.806: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename gc @ 05/10/23 05:59:50.806
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 05:59:50.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 05:59:50.82
  STEP: create the rc @ 05/10/23 05:59:50.824
  W0510 05:59:50.829898      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0510 05:59:50.848091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:51.848346      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:52.849427      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:53.849807      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:54.850040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:55.851127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 05:59:56.851612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/10/23 05:59:56.894
  STEP: wait for the rc to be deleted @ 05/10/23 05:59:56.915
  E0510 05:59:57.852676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:58.520: INFO: 81 pods remaining
  May 10 05:59:58.520: INFO: 80 pods has nil DeletionTimestamp
  May 10 05:59:58.520: INFO: 
  E0510 05:59:58.852820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 05:59:59.472: INFO: 76 pods remaining
  May 10 05:59:59.472: INFO: 64 pods has nil DeletionTimestamp
  May 10 05:59:59.472: INFO: 
  E0510 05:59:59.852903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:00:00.512: INFO: 59 pods remaining
  May 10 06:00:00.512: INFO: 56 pods has nil DeletionTimestamp
  May 10 06:00:00.512: INFO: 
  E0510 06:00:00.853415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:00:01.429: INFO: 46 pods remaining
  May 10 06:00:01.429: INFO: 40 pods has nil DeletionTimestamp
  May 10 06:00:01.429: INFO: 
  E0510 06:00:01.853976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:00:02.508: INFO: 26 pods remaining
  May 10 06:00:02.508: INFO: 26 pods has nil DeletionTimestamp
  May 10 06:00:02.508: INFO: 
  E0510 06:00:02.854489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:00:03.442: INFO: 22 pods remaining
  May 10 06:00:03.442: INFO: 13 pods has nil DeletionTimestamp
  May 10 06:00:03.442: INFO: 
  E0510 06:00:03.855049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:00:04.071: INFO: 5 pods remaining
  May 10 06:00:04.071: INFO: 3 pods has nil DeletionTimestamp
  May 10 06:00:04.071: INFO: 
  E0510 06:00:04.856027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/10/23 06:00:05.446
  E0510 06:00:05.856716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:06.857601      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:00:06.906: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 10 06:00:06.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1938" for this suite. @ 05/10/23 06:00:06.916
• [16.130 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 05/10/23 06:00:06.937
  May 10 06:00:06.937: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 06:00:06.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:00:06.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:00:06.953
  STEP: Creating configMap with name configmap-test-volume-f5d3a23e-ca55-46ba-b4cb-9d68727b37d5 @ 05/10/23 06:00:06.955
  STEP: Creating a pod to test consume configMaps @ 05/10/23 06:00:06.969
  E0510 06:00:07.857908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:08.858510      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:09.858781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:10.859050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:11.860126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:12.860480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:13.861414      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:14.861608      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:15.861737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:16.862405      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:17.862585      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:18.862708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:00:19.009
  May 10 06:00:19.011: INFO: Trying to get logs from node node-02 pod pod-configmaps-726f4b4f-9ea6-4a8a-b569-1215019e7294 container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 06:00:19.015
  May 10 06:00:19.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4509" for this suite. @ 05/10/23 06:00:19.036
• [12.103 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 05/10/23 06:00:19.04
  May 10 06:00:19.040: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 06:00:19.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:00:19.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:00:19.051
  STEP: creating an Endpoint @ 05/10/23 06:00:19.054
  STEP: waiting for available Endpoint @ 05/10/23 06:00:19.061
  STEP: listing all Endpoints @ 05/10/23 06:00:19.062
  STEP: updating the Endpoint @ 05/10/23 06:00:19.063
  STEP: fetching the Endpoint @ 05/10/23 06:00:19.068
  STEP: patching the Endpoint @ 05/10/23 06:00:19.07
  STEP: fetching the Endpoint @ 05/10/23 06:00:19.074
  STEP: deleting the Endpoint by Collection @ 05/10/23 06:00:19.075
  STEP: waiting for Endpoint deletion @ 05/10/23 06:00:19.079
  STEP: fetching the Endpoint @ 05/10/23 06:00:19.08
  May 10 06:00:19.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3407" for this suite. @ 05/10/23 06:00:19.084
• [0.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 05/10/23 06:00:19.091
  May 10 06:00:19.091: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename svcaccounts @ 05/10/23 06:00:19.092
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:00:19.104
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:00:19.106
  E0510 06:00:19.862880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:20.863344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 05/10/23 06:00:21.123
  May 10 06:00:21.123: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6700 pod-service-account-e22abb43-8d13-43ad-b0cf-c329be7ce3d7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 05/10/23 06:00:21.266
  May 10 06:00:21.266: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6700 pod-service-account-e22abb43-8d13-43ad-b0cf-c329be7ce3d7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 05/10/23 06:00:21.389
  May 10 06:00:21.389: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6700 pod-service-account-e22abb43-8d13-43ad-b0cf-c329be7ce3d7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  May 10 06:00:21.521: INFO: Got root ca configmap in namespace "svcaccounts-6700"
  May 10 06:00:21.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6700" for this suite. @ 05/10/23 06:00:21.526
• [2.439 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 05/10/23 06:00:21.531
  May 10 06:00:21.531: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename runtimeclass @ 05/10/23 06:00:21.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:00:21.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:00:21.547
  E0510 06:00:21.863619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:22.864290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:00:23.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4664" for this suite. @ 05/10/23 06:00:23.577
• [2.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 05/10/23 06:00:23.583
  May 10 06:00:23.583: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename proxy @ 05/10/23 06:00:23.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:00:23.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:00:23.596
  May 10 06:00:23.599: INFO: Creating pod...
  E0510 06:00:23.864830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:24.865603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:00:25.612: INFO: Creating service...
  May 10 06:00:25.620: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/pods/agnhost/proxy?method=DELETE
  May 10 06:00:25.623: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 10 06:00:25.623: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/pods/agnhost/proxy?method=OPTIONS
  May 10 06:00:25.631: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 10 06:00:25.631: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/pods/agnhost/proxy?method=PATCH
  May 10 06:00:25.633: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 10 06:00:25.633: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/pods/agnhost/proxy?method=POST
  May 10 06:00:25.636: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 10 06:00:25.636: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/pods/agnhost/proxy?method=PUT
  May 10 06:00:25.638: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 10 06:00:25.638: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/services/e2e-proxy-test-service/proxy?method=DELETE
  May 10 06:00:25.641: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 10 06:00:25.641: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/services/e2e-proxy-test-service/proxy?method=OPTIONS
  May 10 06:00:25.644: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 10 06:00:25.644: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/services/e2e-proxy-test-service/proxy?method=PATCH
  May 10 06:00:25.648: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 10 06:00:25.648: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/services/e2e-proxy-test-service/proxy?method=POST
  May 10 06:00:25.651: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 10 06:00:25.651: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/services/e2e-proxy-test-service/proxy?method=PUT
  May 10 06:00:25.654: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 10 06:00:25.654: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/pods/agnhost/proxy?method=GET
  May 10 06:00:25.656: INFO: http.Client request:GET StatusCode:301
  May 10 06:00:25.656: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/services/e2e-proxy-test-service/proxy?method=GET
  May 10 06:00:25.659: INFO: http.Client request:GET StatusCode:301
  May 10 06:00:25.659: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/pods/agnhost/proxy?method=HEAD
  May 10 06:00:25.660: INFO: http.Client request:HEAD StatusCode:301
  May 10 06:00:25.660: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-896/services/e2e-proxy-test-service/proxy?method=HEAD
  May 10 06:00:25.662: INFO: http.Client request:HEAD StatusCode:301
  May 10 06:00:25.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-896" for this suite. @ 05/10/23 06:00:25.665
• [2.086 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 05/10/23 06:00:25.67
  May 10 06:00:25.670: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename sched-preemption @ 05/10/23 06:00:25.671
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:00:25.687
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:00:25.689
  May 10 06:00:25.705: INFO: Waiting up to 1m0s for all nodes to be ready
  E0510 06:00:25.866496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:26.867026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:27.867531      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:28.867662      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:29.868067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:30.868262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:31.868564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:32.868869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:33.868954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:34.869404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:35.869586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:36.870533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:37.871111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:38.871314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:39.872044      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:40.872324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:41.872466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:42.872807      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:43.873475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:44.873688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:45.874732      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:46.875553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:47.876019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:48.876207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:49.877103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:50.877265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:51.877562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:52.877844      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:53.878356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:54.878565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:55.878695      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:56.879019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:57.879722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:58.879995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:00:59.880890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:00.881119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:01.881393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:02.881629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:03.881853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:04.882093      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:05.882496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:06.882989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:07.883115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:08.883301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:09.884371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:10.884604      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:11.885615      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:12.885873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:13.886993      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:14.887246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:15.887783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:16.888790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:17.889474      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:18.889711      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:19.890115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:20.890340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:21.890587      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:22.890866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:23.891009      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:24.891235      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:01:25.727: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/10/23 06:01:25.729
  May 10 06:01:25.729: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/10/23 06:01:25.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:01:25.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:01:25.746
  May 10 06:01:25.758: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  May 10 06:01:25.760: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  May 10 06:01:25.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 10 06:01:25.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-1118" for this suite. @ 05/10/23 06:01:25.84
  STEP: Destroying namespace "sched-preemption-3952" for this suite. @ 05/10/23 06:01:25.845
• [60.184 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 05/10/23 06:01:25.854
  May 10 06:01:25.854: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename job @ 05/10/23 06:01:25.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:01:25.87
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:01:25.872
  STEP: Creating a job @ 05/10/23 06:01:25.875
  STEP: Ensure pods equal to parallelism count is attached to the job @ 05/10/23 06:01:25.883
  E0510 06:01:25.891820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:26.892838      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/10/23 06:01:27.887
  E0510 06:01:27.893161      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating /status @ 05/10/23 06:01:27.897
  STEP: get /status @ 05/10/23 06:01:27.921
  May 10 06:01:27.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5581" for this suite. @ 05/10/23 06:01:27.925
• [2.078 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 05/10/23 06:01:27.932
  May 10 06:01:27.932: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 06:01:27.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:01:27.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:01:27.945
  STEP: Creating configMap with name cm-test-opt-del-48f9b5bf-3de6-458a-b1cb-c82df9d2e051 @ 05/10/23 06:01:27.949
  STEP: Creating configMap with name cm-test-opt-upd-070a2a47-dc1c-426f-88ae-53fe0fd88c33 @ 05/10/23 06:01:27.953
  STEP: Creating the pod @ 05/10/23 06:01:27.959
  E0510 06:01:28.893356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:29.893482      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:30.894285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:31.894616      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-48f9b5bf-3de6-458a-b1cb-c82df9d2e051 @ 05/10/23 06:01:31.987
  STEP: Updating configmap cm-test-opt-upd-070a2a47-dc1c-426f-88ae-53fe0fd88c33 @ 05/10/23 06:01:31.991
  STEP: Creating configMap with name cm-test-opt-create-0995ff34-3de3-4b8e-82fb-04b9c6c32a45 @ 05/10/23 06:01:32.038
  STEP: waiting to observe update in volume @ 05/10/23 06:01:32.042
  E0510 06:01:32.894763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:33.894927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:34.895514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:35.895667      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:36.896368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:37.896421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:38.897357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:39.898241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:40.898404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:41.898915      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:42.899547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:43.899719      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:44.899851      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:45.900069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:46.900787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:47.900986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:48.901166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:49.901428      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:50.901751      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:51.902707      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:52.903281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:53.903491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:54.903625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:55.903850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:56.903929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:57.904077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:58.905113      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:01:59.905334      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:00.906294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:01.906614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:02.906758      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:03.906984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:04.907126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:05.907393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:06.907829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:07.908049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:08.908024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:09.908273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:10.908482      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:11.908971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:12.909072      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:13.909314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:14.910339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:15.910562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:16.910672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:17.910894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:18.911686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:19.911946      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:20.912203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:21.912399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:22.912665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:23.912813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:24.912964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:25.913164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:26.913911      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:27.914777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:28.914919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:29.915145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:30.915699      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:31.916790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:32.917622      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:33.917837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:34.918424      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:35.918692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:36.919519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:37.919573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:38.920592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:39.920801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:40.921766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:41.922131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:42.922734      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:43.922932      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:44.923080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:45.923690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:46.923973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:47.924034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:02:48.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-590" for this suite. @ 05/10/23 06:02:48.294
• [80.371 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 05/10/23 06:02:48.304
  May 10 06:02:48.304: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename resourcequota @ 05/10/23 06:02:48.305
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:02:48.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:02:48.316
  STEP: Counting existing ResourceQuota @ 05/10/23 06:02:48.318
  E0510 06:02:48.924512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:49.924669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:50.924843      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:51.925630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:52.926546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/10/23 06:02:53.324
  STEP: Ensuring resource quota status is calculated @ 05/10/23 06:02:53.33
  E0510 06:02:53.926882      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:54.927084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:02:55.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3597" for this suite. @ 05/10/23 06:02:55.336
• [7.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 05/10/23 06:02:55.342
  May 10 06:02:55.342: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/10/23 06:02:55.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:02:55.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:02:55.366
  May 10 06:02:55.368: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 06:02:55.927226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:56.927335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:57.928376      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:58.929269      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:02:59.929550      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:00.929702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:03:01.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2448" for this suite. @ 05/10/23 06:03:01.663
• [6.390 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 05/10/23 06:03:01.733
  May 10 06:03:01.733: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 06:03:01.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:03:01.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:03:01.765
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 06:03:01.767
  E0510 06:03:01.930004      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:02.930326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:03.930925      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:04.931310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:05.932116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:06.932958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:03:07.791
  May 10 06:03:07.793: INFO: Trying to get logs from node node-02 pod downwardapi-volume-7431618b-d625-427d-885d-31fa0c8d2827 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 06:03:07.807
  E0510 06:03:07.933902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:03:07.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1060" for this suite. @ 05/10/23 06:03:07.944
• [6.216 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 05/10/23 06:03:07.95
  May 10 06:03:07.950: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:03:07.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:03:07.963
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:03:07.965
  STEP: Creating projection with secret that has name projected-secret-test-map-702b5c5b-f7b5-4baa-a2cb-d6d1e732ae72 @ 05/10/23 06:03:07.967
  STEP: Creating a pod to test consume secrets @ 05/10/23 06:03:07.971
  E0510 06:03:08.934031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:09.934942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:10.935118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:11.935540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:03:11.988
  May 10 06:03:11.990: INFO: Trying to get logs from node node-02 pod pod-projected-secrets-d898b2a6-90d8-4189-a1cd-8e78edde4b06 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 06:03:11.995
  May 10 06:03:12.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6131" for this suite. @ 05/10/23 06:03:12.013
• [4.069 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 05/10/23 06:03:12.021
  May 10 06:03:12.021: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename endpointslice @ 05/10/23 06:03:12.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:03:12.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:03:12.034
  STEP: getting /apis @ 05/10/23 06:03:12.036
  STEP: getting /apis/discovery.k8s.io @ 05/10/23 06:03:12.04
  STEP: getting /apis/discovery.k8s.iov1 @ 05/10/23 06:03:12.04
  STEP: creating @ 05/10/23 06:03:12.041
  STEP: getting @ 05/10/23 06:03:12.062
  STEP: listing @ 05/10/23 06:03:12.064
  STEP: watching @ 05/10/23 06:03:12.065
  May 10 06:03:12.065: INFO: starting watch
  STEP: cluster-wide listing @ 05/10/23 06:03:12.066
  STEP: cluster-wide watching @ 05/10/23 06:03:12.068
  May 10 06:03:12.068: INFO: starting watch
  STEP: patching @ 05/10/23 06:03:12.069
  STEP: updating @ 05/10/23 06:03:12.079
  May 10 06:03:12.088: INFO: waiting for watch events with expected annotations
  May 10 06:03:12.088: INFO: saw patched and updated annotations
  STEP: deleting @ 05/10/23 06:03:12.088
  STEP: deleting a collection @ 05/10/23 06:03:12.095
  May 10 06:03:12.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4451" for this suite. @ 05/10/23 06:03:12.109
• [0.093 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 05/10/23 06:03:12.114
  May 10 06:03:12.114: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 06:03:12.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:03:12.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:03:12.13
  STEP: Creating the pod @ 05/10/23 06:03:12.132
  E0510 06:03:12.935559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:13.936073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:03:14.675: INFO: Successfully updated pod "labelsupdate873f3205-2cd2-4c6c-afbf-c1530cf87d22"
  E0510 06:03:14.936585      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:15.937300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:16.938273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:17.938540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:03:18.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3549" for this suite. @ 05/10/23 06:03:18.7
• [6.591 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 05/10/23 06:03:18.705
  May 10 06:03:18.705: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 06:03:18.706
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:03:18.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:03:18.724
  STEP: Setting up server cert @ 05/10/23 06:03:18.744
  E0510 06:03:18.939628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 06:03:19.479
  STEP: Deploying the webhook pod @ 05/10/23 06:03:19.486
  STEP: Wait for the deployment to be ready @ 05/10/23 06:03:19.506
  May 10 06:03:19.511: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 06:03:19.940540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:20.940788      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 06:03:21.524
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 06:03:21.562
  E0510 06:03:21.940906      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:03:22.562: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 05/10/23 06:03:22.565
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 05/10/23 06:03:22.594
  STEP: Creating a configMap that should not be mutated @ 05/10/23 06:03:22.604
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 05/10/23 06:03:22.615
  STEP: Creating a configMap that should be mutated @ 05/10/23 06:03:22.624
  May 10 06:03:22.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-206" for this suite. @ 05/10/23 06:03:22.688
  STEP: Destroying namespace "webhook-markers-3595" for this suite. @ 05/10/23 06:03:22.698
• [4.004 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 05/10/23 06:03:22.711
  May 10 06:03:22.711: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename security-context-test @ 05/10/23 06:03:22.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:03:22.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:03:22.73
  E0510 06:03:22.941430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:23.941842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:24.942274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:25.942456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:26.943344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:27.943510      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:03:28.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6072" for this suite. @ 05/10/23 06:03:28.752
• [6.047 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 05/10/23 06:03:28.758
  May 10 06:03:28.758: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/10/23 06:03:28.759
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:03:28.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:03:28.774
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 05/10/23 06:03:28.776
  May 10 06:03:28.776: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 06:03:28.944434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:29.945279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:03:30.263: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 06:03:30.946261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:31.946465      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:32.947451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:33.947697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:34.948380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:35.949377      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:03:36.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6046" for this suite. @ 05/10/23 06:03:36.297
• [7.545 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 05/10/23 06:03:36.305
  May 10 06:03:36.305: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename ingress @ 05/10/23 06:03:36.305
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:03:36.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:03:36.326
  STEP: getting /apis @ 05/10/23 06:03:36.328
  STEP: getting /apis/networking.k8s.io @ 05/10/23 06:03:36.332
  STEP: getting /apis/networking.k8s.iov1 @ 05/10/23 06:03:36.332
  STEP: creating @ 05/10/23 06:03:36.333
  STEP: getting @ 05/10/23 06:03:36.352
  STEP: listing @ 05/10/23 06:03:36.353
  STEP: watching @ 05/10/23 06:03:36.355
  May 10 06:03:36.355: INFO: starting watch
  STEP: cluster-wide listing @ 05/10/23 06:03:36.356
  STEP: cluster-wide watching @ 05/10/23 06:03:36.357
  May 10 06:03:36.357: INFO: starting watch
  STEP: patching @ 05/10/23 06:03:36.358
  STEP: updating @ 05/10/23 06:03:36.362
  May 10 06:03:36.381: INFO: waiting for watch events with expected annotations
  May 10 06:03:36.381: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/10/23 06:03:36.381
  STEP: updating /status @ 05/10/23 06:03:36.386
  STEP: get /status @ 05/10/23 06:03:36.392
  STEP: deleting @ 05/10/23 06:03:36.394
  STEP: deleting a collection @ 05/10/23 06:03:36.402
  May 10 06:03:36.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-8595" for this suite. @ 05/10/23 06:03:36.422
• [0.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 05/10/23 06:03:36.429
  May 10 06:03:36.429: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename namespaces @ 05/10/23 06:03:36.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:03:36.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:03:36.451
  STEP: Creating a test namespace @ 05/10/23 06:03:36.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:03:36.464
  STEP: Creating a service in the namespace @ 05/10/23 06:03:36.466
  STEP: Deleting the namespace @ 05/10/23 06:03:36.474
  STEP: Waiting for the namespace to be removed. @ 05/10/23 06:03:36.492
  E0510 06:03:36.949476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:37.950124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:38.950244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:39.950928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:40.951133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:41.952145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 05/10/23 06:03:42.494
  STEP: Verifying there is no service in the namespace @ 05/10/23 06:03:42.502
  May 10 06:03:42.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6996" for this suite. @ 05/10/23 06:03:42.511
  STEP: Destroying namespace "nsdeletetest-515" for this suite. @ 05/10/23 06:03:42.515
  May 10 06:03:42.516: INFO: Namespace nsdeletetest-515 was already deleted
  STEP: Destroying namespace "nsdeletetest-5163" for this suite. @ 05/10/23 06:03:42.516
• [6.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 05/10/23 06:03:42.52
  May 10 06:03:42.520: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:03:42.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:03:42.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:03:42.532
  STEP: Creating the pod @ 05/10/23 06:03:42.537
  E0510 06:03:42.952764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:43.953046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:44.953702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:45.953934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:46.954491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:03:47.076: INFO: Successfully updated pod "annotationupdate762c3556-aa51-4c30-8f1d-239cb874d82c"
  E0510 06:03:47.954606      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:48.954953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:03:49.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7164" for this suite. @ 05/10/23 06:03:49.089
• [6.573 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 05/10/23 06:03:49.094
  May 10 06:03:49.094: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename statefulset @ 05/10/23 06:03:49.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:03:49.108
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:03:49.11
  STEP: Creating service test in namespace statefulset-9624 @ 05/10/23 06:03:49.112
  STEP: Creating a new StatefulSet @ 05/10/23 06:03:49.116
  May 10 06:03:49.126: INFO: Found 0 stateful pods, waiting for 3
  E0510 06:03:49.955322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:50.955660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:51.955756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:52.956070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:53.956218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:54.956340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:55.956518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:56.957178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:57.957406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:03:58.957604      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:03:59.129: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 10 06:03:59.129: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 10 06:03:59.129: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/10/23 06:03:59.134
  May 10 06:03:59.151: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/10/23 06:03:59.151
  E0510 06:03:59.958370      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:00.958600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:01.959023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:02.959253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:03.959476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:04.959687      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:05.959959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:06.960744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:07.960962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:08.961165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 05/10/23 06:04:09.168
  STEP: Performing a canary update @ 05/10/23 06:04:09.168
  May 10 06:04:09.186: INFO: Updating stateful set ss2
  May 10 06:04:09.194: INFO: Waiting for Pod statefulset-9624/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0510 06:04:09.961698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:10.961930      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:11.962822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:12.963021      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:13.963240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:14.963373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:15.963397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:16.964124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:17.964283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:18.964763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 05/10/23 06:04:19.199
  May 10 06:04:19.243: INFO: Found 2 stateful pods, waiting for 3
  E0510 06:04:19.964900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:20.966020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:21.966828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:22.967143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:23.967430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:24.967631      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:25.967975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:26.968763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:27.968892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:28.969087      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:04:29.248: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 10 06:04:29.248: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 10 06:04:29.248: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 05/10/23 06:04:29.252
  May 10 06:04:29.274: INFO: Updating stateful set ss2
  May 10 06:04:29.278: INFO: Waiting for Pod statefulset-9624/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0510 06:04:29.970137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:30.970761      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:31.971652      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:32.971808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:33.971908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:34.972270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:35.972434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:36.972890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:37.973833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:38.973950      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:04:39.301: INFO: Updating stateful set ss2
  May 10 06:04:39.305: INFO: Waiting for StatefulSet statefulset-9624/ss2 to complete update
  May 10 06:04:39.305: INFO: Waiting for Pod statefulset-9624/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0510 06:04:39.974848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:40.975046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:41.975445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:42.975586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:43.975723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:44.975918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:45.976107      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:46.976964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:47.977186      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:48.977426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:04:49.311: INFO: Deleting all statefulset in ns statefulset-9624
  May 10 06:04:49.312: INFO: Scaling statefulset ss2 to 0
  E0510 06:04:49.978456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:50.978745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:51.979234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:52.979408      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:53.979619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:54.979872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:55.979988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:56.980540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:57.980776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:04:58.980884      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:04:59.329: INFO: Waiting for statefulset status.replicas updated to 0
  May 10 06:04:59.331: INFO: Deleting statefulset ss2
  May 10 06:04:59.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9624" for this suite. @ 05/10/23 06:04:59.341
• [70.254 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 05/10/23 06:04:59.349
  May 10 06:04:59.349: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/10/23 06:04:59.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:04:59.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:04:59.378
  May 10 06:04:59.381: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 06:04:59.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6519" for this suite. @ 05/10/23 06:04:59.928
• [0.584 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 05/10/23 06:04:59.933
  May 10 06:04:59.933: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename deployment @ 05/10/23 06:04:59.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:04:59.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:04:59.949
  May 10 06:04:59.951: INFO: Creating deployment "webserver-deployment"
  May 10 06:04:59.955: INFO: Waiting for observed generation 1
  E0510 06:04:59.981346      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:00.981671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:01.960: INFO: Waiting for all required pods to come up
  May 10 06:05:01.963: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 05/10/23 06:05:01.964
  E0510 06:05:01.982657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:02.982822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:03.969: INFO: Waiting for deployment "webserver-deployment" to complete
  May 10 06:05:03.972: INFO: Updating deployment "webserver-deployment" with a non-existent image
  May 10 06:05:03.981: INFO: Updating deployment webserver-deployment
  May 10 06:05:03.981: INFO: Waiting for observed generation 2
  E0510 06:05:03.982874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:04.983583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:05.983931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:05.986: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  May 10 06:05:05.988: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  May 10 06:05:05.989: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 10 06:05:05.995: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  May 10 06:05:05.995: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  May 10 06:05:05.997: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 10 06:05:06.000: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  May 10 06:05:06.000: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  May 10 06:05:06.010: INFO: Updating deployment webserver-deployment
  May 10 06:05:06.010: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  May 10 06:05:06.013: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  May 10 06:05:06.015: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  E0510 06:05:06.983928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:07.984169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:08.117: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-1188  f3bb482c-75e5-4d99-9477-8e01ae0037f5 1082472 3 2023-05-10 06:04:59 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-10 06:05:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005040668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-10 06:05:06 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-05-10 06:05:06 +0000 UTC,LastTransitionTime:2023-05-10 06:04:59 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  May 10 06:05:08.120: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-1188  441011e8-4b4d-45a3-bd25-6150039f74be 1082467 3 2023-05-10 06:05:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment f3bb482c-75e5-4d99-9477-8e01ae0037f5 0xc004818c07 0xc004818c08}] [] [{kube-controller-manager Update apps/v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3bb482c-75e5-4d99-9477-8e01ae0037f5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004818ca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 10 06:05:08.120: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  May 10 06:05:08.121: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-1188  5f0bfad7-7c45-4e9a-a2d9-72488e107a82 1082463 3 2023-05-10 06:04:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment f3bb482c-75e5-4d99-9477-8e01ae0037f5 0xc004818af7 0xc004818af8}] [] [{kube-controller-manager Update apps/v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3bb482c-75e5-4d99-9477-8e01ae0037f5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004818b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  May 10 06:05:08.125: INFO: Pod "webserver-deployment-67bd4bf6dc-2c9rt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2c9rt webserver-deployment-67bd4bf6dc- deployment-1188  6652ace1-7c1f-45e9-8a83-fe5d4e74ac2e 1082544 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:57188108fb6cbc50a2cdfa80e0463a3a2cbd9c6a9aa21f3440c26512d9a5a977 cni.projectcalico.org/podIP:100.114.252.174/32 cni.projectcalico.org/podIPs:100.114.252.174/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0040c88d7 0xc0040c88d8}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4lk9v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4lk9v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.126: INFO: Pod "webserver-deployment-67bd4bf6dc-665g4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-665g4 webserver-deployment-67bd4bf6dc- deployment-1188  73003445-90ba-465f-bfd4-24c9baf0da70 1082482 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0040c8ac7 0xc0040c8ac8}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zhqm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zhqm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.126: INFO: Pod "webserver-deployment-67bd4bf6dc-6k2m8" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6k2m8 webserver-deployment-67bd4bf6dc- deployment-1188  d5345c9c-060d-4fc5-9364-cc5d3ee707b9 1082234 0 2023-05-10 06:04:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:354bb41ed6555f6326684a19e7542353b9cef7ae73946e468034acbb89d9885f cni.projectcalico.org/podIP:100.114.252.178/32 cni.projectcalico.org/podIPs:100.114.252.178/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0040c8ca7 0xc0040c8ca8}] [] [{kube-controller-manager Update v1 2023-05-10 06:04:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 06:05:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 06:05:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.114.252.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9srg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9srg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:100.114.252.178,StartTime:2023-05-10 06:05:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 06:05:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://798eaa405bc95db26a5bacb69fad25e8c076c028811883d87f4c2cc959ca22aa,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.114.252.178,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.126: INFO: Pod "webserver-deployment-67bd4bf6dc-6kzjm" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6kzjm webserver-deployment-67bd4bf6dc- deployment-1188  9bcb2780-3421-421e-994e-cfe4e3f2830b 1082239 0 2023-05-10 06:04:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:2eb6251101ada107ac1d0bcd82f9d2c8c2324a92755948418831749bb42e87e5 cni.projectcalico.org/podIP:100.67.79.131/32 cni.projectcalico.org/podIPs:100.67.79.131/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0040c8ed7 0xc0040c8ed8}] [] [{kube-controller-manager Update v1 2023-05-10 06:04:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 06:05:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 06:05:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.67.79.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54jx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54jx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:04:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.83,PodIP:100.67.79.131,StartTime:2023-05-10 06:05:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 06:05:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://ffaed44b94df58b17742ab44e3f157e4bbf69a1c128fe160eb06e81236997ee2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.67.79.131,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.126: INFO: Pod "webserver-deployment-67bd4bf6dc-84mqq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-84mqq webserver-deployment-67bd4bf6dc- deployment-1188  6add7a73-6dd5-4bce-967d-bb90ccf38e9a 1082559 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:8f89f3e42c430f6c62db8bfdc76753c105c5605c14a7fa6a5a102e0df9195f7b cni.projectcalico.org/podIP:100.114.252.130/32 cni.projectcalico.org/podIPs:100.114.252.130/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0040c90e7 0xc0040c90e8}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2sn4c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2sn4c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.126: INFO: Pod "webserver-deployment-67bd4bf6dc-9twzf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9twzf webserver-deployment-67bd4bf6dc- deployment-1188  eeecfed1-900b-453d-b025-f73b3777fc02 1082579 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:dd16fcdb7c266adffc3b8529bc4503655b24bdb997b7741c59ac6d98337ca1ff cni.projectcalico.org/podIP:100.67.79.140/32 cni.projectcalico.org/podIPs:100.67.79.140/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0040c9297 0xc0040c9298}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b8t4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b8t4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.83,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.126: INFO: Pod "webserver-deployment-67bd4bf6dc-cw5qr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-cw5qr webserver-deployment-67bd4bf6dc- deployment-1188  741cb36e-ceb9-4874-9eb9-fbc9d74cc74a 1082518 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:aa96acc218420a46cc2f5876a9390e71e4addbf2233ad8118b9fb04e63400fc0 cni.projectcalico.org/podIP:100.74.79.61/32 cni.projectcalico.org/podIPs:100.74.79.61/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0040c94c7 0xc0040c94c8}] [] [{calico Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6v5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6v5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.152,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.126: INFO: Pod "webserver-deployment-67bd4bf6dc-czbdz" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-czbdz webserver-deployment-67bd4bf6dc- deployment-1188  94973911-e283-4d67-a5a0-b6ca60a42b33 1082242 0 2023-05-10 06:05:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:27661bf4c63748683c4ad3122fa5006c0df83ce53ccb16618be640c3ea653f36 cni.projectcalico.org/podIP:100.67.79.184/32 cni.projectcalico.org/podIPs:100.67.79.184/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0040c96c7 0xc0040c96c8}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 06:05:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 06:05:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.67.79.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8d8f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8d8f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.83,PodIP:100.67.79.184,StartTime:2023-05-10 06:05:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 06:05:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://35299361463af2ac665e53bb9df25d32577c556c0d6690fe1d197418414d0947,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.67.79.184,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.127: INFO: Pod "webserver-deployment-67bd4bf6dc-dvjs7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dvjs7 webserver-deployment-67bd4bf6dc- deployment-1188  c2457ba9-241b-4fd5-88b9-da3c661c384b 1082540 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:5d6e198d42721b414b7e4574b0e77ac23a17d8db410a6f1d77594c58a2744e2e cni.projectcalico.org/podIP:100.67.79.161/32 cni.projectcalico.org/podIPs:100.67.79.161/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0040c98f7 0xc0040c98f8}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wtwjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wtwjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.83,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.127: INFO: Pod "webserver-deployment-67bd4bf6dc-gk7xh" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gk7xh webserver-deployment-67bd4bf6dc- deployment-1188  ee709ff9-eda0-4e46-a500-4949c10b34e6 1082237 0 2023-05-10 06:04:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:7660802f22a0b3c7797f0f5d90d3b09daa081b580e615ab733d2b7f3e90b68de cni.projectcalico.org/podIP:100.67.79.147/32 cni.projectcalico.org/podIPs:100.67.79.147/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0040c9b07 0xc0040c9b08}] [] [{kube-controller-manager Update v1 2023-05-10 06:04:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 06:05:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 06:05:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.67.79.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-97xnm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-97xnm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.83,PodIP:100.67.79.147,StartTime:2023-05-10 06:05:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 06:05:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://fb7a6d8a6a432f1932ece091b15e79757cb733579d6013d3d00bc073d4138d4d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.67.79.147,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.127: INFO: Pod "webserver-deployment-67bd4bf6dc-gz2sd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gz2sd webserver-deployment-67bd4bf6dc- deployment-1188  52cee664-1df9-4b03-8e33-2f28604dd924 1082543 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:f0e7061234673b38d7d25b246216290bf1bb5cf42258b4ec4c7e781ef1b2e323 cni.projectcalico.org/podIP:100.74.79.46/32 cni.projectcalico.org/podIPs:100.74.79.46/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0040c9d37 0xc0040c9d38}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tgjkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tgjkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.152,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.127: INFO: Pod "webserver-deployment-67bd4bf6dc-m56xn" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-m56xn webserver-deployment-67bd4bf6dc- deployment-1188  37f9287a-8ab1-42ab-b613-7a9050a110e1 1082249 0 2023-05-10 06:05:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:a6c7f2b6fc49d2486c949382437f56b071a399f45e8781eae283f57c37b4485f cni.projectcalico.org/podIP:100.74.79.26/32 cni.projectcalico.org/podIPs:100.74.79.26/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0040c9f47 0xc0040c9f48}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 06:05:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 06:05:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.74.79.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4wfhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4wfhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.152,PodIP:100.74.79.26,StartTime:2023-05-10 06:05:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 06:05:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://f91b1c101c0d1679b27a093126da2915fde63490d39e6b99d7d154888bc1524c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.74.79.26,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.127: INFO: Pod "webserver-deployment-67bd4bf6dc-mlsw4" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mlsw4 webserver-deployment-67bd4bf6dc- deployment-1188  3f0fd8cb-6d76-40b5-8e32-83652eef56bb 1082246 0 2023-05-10 06:04:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:a37f128afd91d52a16eea429b8dd9ccabb8dd8bdc861dae64c4b50f197fd2797 cni.projectcalico.org/podIP:100.114.252.173/32 cni.projectcalico.org/podIPs:100.114.252.173/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0048f6157 0xc0048f6158}] [] [{kube-controller-manager Update v1 2023-05-10 06:04:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 06:05:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 06:05:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.114.252.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgfpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgfpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:04:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:04:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:100.114.252.173,StartTime:2023-05-10 06:04:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 06:05:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://bf80cfd0458077950d780b02cdea2fc32c94a70c0d67062f1c054fa20f328682,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.114.252.173,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.127: INFO: Pod "webserver-deployment-67bd4bf6dc-pxbfs" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-pxbfs webserver-deployment-67bd4bf6dc- deployment-1188  4f8b3871-a2c7-47c8-afdc-a50f7deecd53 1082221 0 2023-05-10 06:04:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:a08cd9645d376aaeefae1b5cdc256686e1d51da70924182aea448e9c031bdc49 cni.projectcalico.org/podIP:100.74.79.37/32 cni.projectcalico.org/podIPs:100.74.79.37/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0048f6387 0xc0048f6388}] [] [{kube-controller-manager Update v1 2023-05-10 06:04:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 06:05:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 06:05:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.74.79.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lmsln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lmsln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.152,PodIP:100.74.79.37,StartTime:2023-05-10 06:05:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 06:05:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://e4e6abe0a6cbbdb9d32774d75924af20371ba84d49cb1043136b7e5b64913ef9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.74.79.37,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.127: INFO: Pod "webserver-deployment-67bd4bf6dc-r4x6f" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-r4x6f webserver-deployment-67bd4bf6dc- deployment-1188  003e6f65-c4c0-4c19-9b7f-d7ffde1536b7 1082572 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0048f6597 0xc0048f6598}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4czcn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4czcn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.83,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.127: INFO: Pod "webserver-deployment-67bd4bf6dc-tm9t8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tm9t8 webserver-deployment-67bd4bf6dc- deployment-1188  d30e7e6d-2b05-4c59-834c-f5d792d19b9b 1082480 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0048f6767 0xc0048f6768}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47krd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47krd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.152,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.128: INFO: Pod "webserver-deployment-67bd4bf6dc-wgrnr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wgrnr webserver-deployment-67bd4bf6dc- deployment-1188  e7b72e83-b8d7-49cc-bcdb-4c612eed4438 1082537 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:4c64bd944b99c157989439c1715b1efc871276adf2da645889f7d3763d6d57f8 cni.projectcalico.org/podIP:100.114.252.158/32 cni.projectcalico.org/podIPs:100.114.252.158/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0048f6937 0xc0048f6938}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkxfs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkxfs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.128: INFO: Pod "webserver-deployment-67bd4bf6dc-wwkwl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wwkwl webserver-deployment-67bd4bf6dc- deployment-1188  f4bbffe9-bd82-4069-934e-e36ac0e6f566 1082443 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0048f7367 0xc0048f7368}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7n6sx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7n6sx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.128: INFO: Pod "webserver-deployment-67bd4bf6dc-wwpp2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wwpp2 webserver-deployment-67bd4bf6dc- deployment-1188  69695cee-bfa6-4461-bf0e-85facf5e2d0e 1082411 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0048f74d7 0xc0048f74d8}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xwn4s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xwn4s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.128: INFO: Pod "webserver-deployment-67bd4bf6dc-xtvlx" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xtvlx webserver-deployment-67bd4bf6dc- deployment-1188  8bc5b4b5-1682-413c-9b74-c79d73382d38 1082223 0 2023-05-10 06:04:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:9b60bb6018cdf75d99181be619cf4f6d2912bc5359c98821bc3ab10d4c1bf6ad cni.projectcalico.org/podIP:100.74.79.6/32 cni.projectcalico.org/podIPs:100.74.79.6/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5f0bfad7-7c45-4e9a-a2d9-72488e107a82 0xc0048f76d7 0xc0048f76d8}] [] [{kube-controller-manager Update v1 2023-05-10 06:04:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0bfad7-7c45-4e9a-a2d9-72488e107a82\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 06:05:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 06:05:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.74.79.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-44694,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-44694,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:04:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.152,PodIP:100.74.79.6,StartTime:2023-05-10 06:05:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 06:05:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://fd67b65db812da6aaa975f20c59a500b16ea5933c69508479eb0300b6280f16c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.74.79.6,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.128: INFO: Pod "webserver-deployment-7b75d79cf5-2695b" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-2695b webserver-deployment-7b75d79cf5- deployment-1188  074329a4-1e0a-4cf4-aa46-5771df17238b 1082497 0 2023-05-10 06:05:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:54b0b497270c8b9b02a7285aff6d5eb8d5d4a0cbead5ecd88ca4411d5608e2be cni.projectcalico.org/podIP:100.114.252.164/32 cni.projectcalico.org/podIPs:100.114.252.164/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc0048f7c37 0xc0048f7c38}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k2lbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k2lbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:,StartTime:2023-05-10 06:05:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.128: INFO: Pod "webserver-deployment-7b75d79cf5-92th8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-92th8 webserver-deployment-7b75d79cf5- deployment-1188  56903dc8-32b1-45ec-8181-705086e0be3c 1082354 0 2023-05-10 06:05:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:318d249e5d9e78dda0741a5f22b81d2a9b54b65760f4f87dd3aca9a56a5af91a cni.projectcalico.org/podIP:100.74.79.34/32 cni.projectcalico.org/podIPs:100.74.79.34/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc005260037 0xc005260038}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 06:05:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 06:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bj2tn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bj2tn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.152,PodIP:,StartTime:2023-05-10 06:05:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.128: INFO: Pod "webserver-deployment-7b75d79cf5-crjt5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-crjt5 webserver-deployment-7b75d79cf5- deployment-1188  6f7a79ee-185a-4a71-97bd-579140f086eb 1082567 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:705768096371562c19b3f4816e192f2bac8c27b451cf7680d55dfed37f1a83b0 cni.projectcalico.org/podIP:100.74.79.21/32 cni.projectcalico.org/podIPs:100.74.79.21/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc005260277 0xc005260278}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9m65k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9m65k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.152,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.129: INFO: Pod "webserver-deployment-7b75d79cf5-drc4z" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-drc4z webserver-deployment-7b75d79cf5- deployment-1188  5140bdd8-7aff-4dae-8093-46e7b7c47a4a 1082499 0 2023-05-10 06:05:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:eaf03b63e408be1ef731a829f705a0e3ea7d5bca7a1752c6b72b326845790307 cni.projectcalico.org/podIP:100.114.252.183/32 cni.projectcalico.org/podIPs:100.114.252.183/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc0052604a7 0xc0052604a8}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxp9m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxp9m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:,StartTime:2023-05-10 06:05:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.129: INFO: Pod "webserver-deployment-7b75d79cf5-h5llf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-h5llf webserver-deployment-7b75d79cf5- deployment-1188  479b8e41-63f8-4d62-8373-f83741551d09 1082550 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:9c0530cad82c3bc07b29d6bd4c9d3de41b10f1d1bf1716eeef82acea2d1ca4d3 cni.projectcalico.org/podIP:100.67.79.142/32 cni.projectcalico.org/podIPs:100.67.79.142/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc0052606e7 0xc0052606e8}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t9xm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t9xm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.83,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.129: INFO: Pod "webserver-deployment-7b75d79cf5-jnxtf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jnxtf webserver-deployment-7b75d79cf5- deployment-1188  43f9aa93-7bb6-4061-89ac-e19622590fa5 1082371 0 2023-05-10 06:05:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:3a940e3198652419d7feb3ce9b6ac25b504983e08149941fcc2354ac62191ffd cni.projectcalico.org/podIP:100.74.79.12/32 cni.projectcalico.org/podIPs:100.74.79.12/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc005260937 0xc005260938}] [] [{calico Update v1 2023-05-10 06:05:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-10 06:05:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rkmbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rkmbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.152,PodIP:,StartTime:2023-05-10 06:05:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.129: INFO: Pod "webserver-deployment-7b75d79cf5-m2xxk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-m2xxk webserver-deployment-7b75d79cf5- deployment-1188  617c300e-172d-42ee-a0b0-22586625f983 1082584 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:e982350af66637f15010078ebdbc9962135e65878b1d709b33e6a6ddafb9cc3c cni.projectcalico.org/podIP:100.74.79.31/32 cni.projectcalico.org/podIPs:100.74.79.31/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc005260b57 0xc005260b58}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s7f74,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s7f74,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.152,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.129: INFO: Pod "webserver-deployment-7b75d79cf5-p72pk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-p72pk webserver-deployment-7b75d79cf5- deployment-1188  2a20c584-4087-4381-ab7c-99d1e0c9ec47 1082519 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:262a0c162b421f2fd88a9d7f850efe20b459b334a64396af959ef7bfb97c97f8 cni.projectcalico.org/podIP:100.67.79.130/32 cni.projectcalico.org/podIPs:100.67.79.130/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc005260d87 0xc005260d88}] [] [{calico Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dqccs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqccs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.83,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.129: INFO: Pod "webserver-deployment-7b75d79cf5-pmznd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-pmznd webserver-deployment-7b75d79cf5- deployment-1188  43ef5572-7921-4964-a462-9b5b74e9c316 1082353 0 2023-05-10 06:05:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:2778c6d926d34bad8c2413a8d133b7bd829b9a84fbbe79fa5dd9098b3b018f88 cni.projectcalico.org/podIP:100.67.79.160/32 cni.projectcalico.org/podIPs:100.67.79.160/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc005260fa7 0xc005260fa8}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 06:05:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 06:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vgnb2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vgnb2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.83,PodIP:,StartTime:2023-05-10 06:05:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.129: INFO: Pod "webserver-deployment-7b75d79cf5-tt42k" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-tt42k webserver-deployment-7b75d79cf5- deployment-1188  a03d2f5c-bef8-4a13-b7e3-05ca9335804d 1082564 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:70ee471c034809c39edb5a53ce0aa559191aacb99285d533281e37a4c22efcf1 cni.projectcalico.org/podIP:100.67.79.163/32 cni.projectcalico.org/podIPs:100.67.79.163/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc0052611d7 0xc0052611d8}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c48wl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c48wl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.83,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.129: INFO: Pod "webserver-deployment-7b75d79cf5-w2c7l" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-w2c7l webserver-deployment-7b75d79cf5- deployment-1188  3435623c-086f-460c-b0b1-e9113758f0f5 1082591 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:bdfcc6a8c4a0b954cff3dcfb6054e71288d8fd5a6ffa95609ddb08f29fd84622 cni.projectcalico.org/podIP:100.114.252.175/32 cni.projectcalico.org/podIPs:100.114.252.175/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc0052613e7 0xc0052613e8}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lk5q8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lk5q8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.130: INFO: Pod "webserver-deployment-7b75d79cf5-wbsdq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wbsdq webserver-deployment-7b75d79cf5- deployment-1188  5ab76e69-90f8-4f7f-beeb-9c7853cbd815 1082565 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:3346f174a731c5080a8c31e5394580ef2678bd89911175fa0890a4ff86614561 cni.projectcalico.org/podIP:100.74.79.8/32 cni.projectcalico.org/podIPs:100.74.79.8/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc005261627 0xc005261628}] [] [{kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-10 06:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8jhwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8jhwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ub-test,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.152,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.130: INFO: Pod "webserver-deployment-7b75d79cf5-xqxh6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-xqxh6 webserver-deployment-7b75d79cf5- deployment-1188  abd73a1b-e7e5-43ae-9e2b-b6830171f2a4 1082516 0 2023-05-10 06:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:80ae12d81f29d3796e50e6b59cf4c480bf7dff31b93a15efd7d3f785f7e6bd17 cni.projectcalico.org/podIP:100.114.252.147/32 cni.projectcalico.org/podIPs:100.114.252.147/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 441011e8-4b4d-45a3-bd25-6150039f74be 0xc005261847 0xc005261848}] [] [{calico Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"441011e8-4b4d-45a3-bd25-6150039f74be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x72gq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x72gq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:,StartTime:2023-05-10 06:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:05:08.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1188" for this suite. @ 05/10/23 06:05:08.132
• [8.207 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 05/10/23 06:05:08.14
  May 10 06:05:08.140: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename statefulset @ 05/10/23 06:05:08.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:05:08.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:05:08.161
  STEP: Creating service test in namespace statefulset-7443 @ 05/10/23 06:05:08.163
  STEP: Creating stateful set ss in namespace statefulset-7443 @ 05/10/23 06:05:08.217
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7443 @ 05/10/23 06:05:08.228
  May 10 06:05:08.230: INFO: Found 0 stateful pods, waiting for 1
  E0510 06:05:08.984743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:09.984986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:10.985213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:11.985954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:12.986214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:13.986457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:14.986668      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:15.986734      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:16.986785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:17.986989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:18.322: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 05/10/23 06:05:18.323
  May 10 06:05:18.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-7443 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 10 06:05:18.709: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 10 06:05:18.709: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 10 06:05:18.709: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 10 06:05:18.712: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0510 06:05:18.987460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:19.987863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:20.988121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:21.988672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:22.988971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:23.989335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:24.989553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:25.989802      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:26.989871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:27.990101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:28.715: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 10 06:05:28.715: INFO: Waiting for statefulset status.replicas updated to 0
  May 10 06:05:28.726: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
  May 10 06:05:28.726: INFO: ss-0  ub-test  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:08 +0000 UTC  }]
  May 10 06:05:28.726: INFO: 
  May 10 06:05:28.726: INFO: StatefulSet ss has not reached scale 3, at 1
  E0510 06:05:28.990398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:29.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997883906s
  E0510 06:05:29.990878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:30.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991377357s
  E0510 06:05:30.991650      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:31.739: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987584515s
  E0510 06:05:31.992409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:32.743: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983665874s
  E0510 06:05:32.993480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:33.746: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980774113s
  E0510 06:05:33.994492      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:34.749: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977198755s
  E0510 06:05:34.995229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:35.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974006282s
  E0510 06:05:35.995453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:36.764: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.962448534s
  E0510 06:05:36.996496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:37.768: INFO: Verifying statefulset ss doesn't scale past 3 for another 958.72979ms
  E0510 06:05:37.997143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7443 @ 05/10/23 06:05:38.769
  May 10 06:05:38.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-7443 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 10 06:05:38.905: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 10 06:05:38.905: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 10 06:05:38.905: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 10 06:05:38.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-7443 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0510 06:05:38.997968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:39.032: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 10 06:05:39.032: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 10 06:05:39.032: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 10 06:05:39.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-7443 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 10 06:05:39.182: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 10 06:05:39.182: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 10 06:05:39.182: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 10 06:05:39.185: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0510 06:05:39.998608      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:40.998854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:41.999721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:42.999950      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:44.000200      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:45.000409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:46.000622      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:47.001483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:48.001792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:49.001997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:49.189: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 10 06:05:49.189: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 10 06:05:49.189: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 05/10/23 06:05:49.189
  May 10 06:05:49.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-7443 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 10 06:05:49.306: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 10 06:05:49.307: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 10 06:05:49.307: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 10 06:05:49.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-7443 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 10 06:05:49.433: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 10 06:05:49.433: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 10 06:05:49.433: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 10 06:05:49.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-7443 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 10 06:05:49.557: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 10 06:05:49.557: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 10 06:05:49.557: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 10 06:05:49.557: INFO: Waiting for statefulset status.replicas updated to 0
  May 10 06:05:49.559: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0510 06:05:50.002163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:51.002445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:52.002939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:53.003233      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:54.003479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:55.003781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:56.004107      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:57.004637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:58.004913      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:05:59.005240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:05:59.566: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 10 06:05:59.566: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 10 06:05:59.566: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 10 06:05:59.578: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
  May 10 06:05:59.578: INFO: ss-0  ub-test  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:08 +0000 UTC  }]
  May 10 06:05:59.578: INFO: ss-1  node-02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:28 +0000 UTC  }]
  May 10 06:05:59.578: INFO: ss-2  node-01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:28 +0000 UTC  }]
  May 10 06:05:59.578: INFO: 
  May 10 06:05:59.578: INFO: StatefulSet ss has not reached scale 0, at 3
  E0510 06:06:00.005543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:06:00.581: INFO: POD   NODE     PHASE      GRACE  CONDITIONS
  May 10 06:06:00.581: INFO: ss-0  ub-test  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:08 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:49 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:49 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:08 +0000 UTC  }]
  May 10 06:06:00.581: INFO: ss-1  node-02  Running    30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:28 +0000 UTC  }]
  May 10 06:06:00.581: INFO: ss-2  node-01  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:28 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:50 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:50 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-10 06:05:28 +0000 UTC  }]
  May 10 06:06:00.582: INFO: 
  May 10 06:06:00.582: INFO: StatefulSet ss has not reached scale 0, at 3
  E0510 06:06:01.006388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:06:01.584: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.9941921s
  E0510 06:06:02.006956      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:06:02.587: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.991612379s
  E0510 06:06:03.007906      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:06:03.589: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.988712118s
  E0510 06:06:04.008273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:06:04.592: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.986333386s
  E0510 06:06:05.009035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:06:05.594: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.983526993s
  E0510 06:06:06.009109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:06:06.597: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.981495017s
  E0510 06:06:07.009550      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:06:07.600: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.978066415s
  E0510 06:06:08.010618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:06:08.603: INFO: Verifying statefulset ss doesn't scale past 0 for another 974.965605ms
  E0510 06:06:09.011152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7443 @ 05/10/23 06:06:09.603
  May 10 06:06:09.606: INFO: Scaling statefulset ss to 0
  May 10 06:06:09.612: INFO: Waiting for statefulset status.replicas updated to 0
  May 10 06:06:09.613: INFO: Deleting all statefulset in ns statefulset-7443
  May 10 06:06:09.615: INFO: Scaling statefulset ss to 0
  May 10 06:06:09.626: INFO: Waiting for statefulset status.replicas updated to 0
  May 10 06:06:09.627: INFO: Deleting statefulset ss
  May 10 06:06:09.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7443" for this suite. @ 05/10/23 06:06:09.637
• [61.501 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 05/10/23 06:06:09.641
  May 10 06:06:09.641: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:06:09.642
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:06:09.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:06:09.657
  STEP: Creating projection with secret that has name projected-secret-test-6d9ad006-67e8-410f-91cc-42fb8d715871 @ 05/10/23 06:06:09.659
  STEP: Creating a pod to test consume secrets @ 05/10/23 06:06:09.689
  E0510 06:06:10.011381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:11.011648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:12.012145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:13.012480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:06:13.705
  May 10 06:06:13.707: INFO: Trying to get logs from node node-02 pod pod-projected-secrets-8ed7c353-2dcc-4061-b401-e05c25e52a03 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 06:06:13.721
  May 10 06:06:13.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4980" for this suite. @ 05/10/23 06:06:13.739
• [4.102 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 05/10/23 06:06:13.743
  May 10 06:06:13.743: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 06:06:13.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:06:13.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:06:13.758
  STEP: creating service in namespace services-7407 @ 05/10/23 06:06:13.76
  STEP: creating service affinity-nodeport in namespace services-7407 @ 05/10/23 06:06:13.76
  STEP: creating replication controller affinity-nodeport in namespace services-7407 @ 05/10/23 06:06:13.781
  I0510 06:06:13.787143      22 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-7407, replica count: 3
  E0510 06:06:14.013375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:15.014406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:16.014496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 06:06:16.839551      22 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 10 06:06:16.846: INFO: Creating new exec pod
  E0510 06:06:17.014884      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:18.015153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:19.015864      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:06:19.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7407 exec execpod-affinityzb2f9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  May 10 06:06:20.015: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  May 10 06:06:20.015: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 06:06:20.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7407 exec execpod-affinityzb2f9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.59 80'
  E0510 06:06:20.016558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:06:20.144: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.2.59 80\nConnection to 10.96.2.59 80 port [tcp/http] succeeded!\n"
  May 10 06:06:20.144: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 06:06:20.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7407 exec execpod-affinityzb2f9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.60.6 30531'
  May 10 06:06:20.272: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.60.6 30531\nConnection to 192.168.60.6 30531 port [tcp/*] succeeded!\n"
  May 10 06:06:20.272: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 06:06:20.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7407 exec execpod-affinityzb2f9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.60.83 30531'
  May 10 06:06:20.396: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.60.83 30531\nConnection to 192.168.60.83 30531 port [tcp/*] succeeded!\n"
  May 10 06:06:20.396: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 06:06:20.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7407 exec execpod-affinityzb2f9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.60.83:30531/ ; done'
  May 10 06:06:20.580: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.60.83:30531/\n"
  May 10 06:06:20.580: INFO: stdout: "\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq\naffinity-nodeport-dkbdq"
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Received response from host: affinity-nodeport-dkbdq
  May 10 06:06:20.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 10 06:06:20.584: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-7407, will wait for the garbage collector to delete the pods @ 05/10/23 06:06:20.599
  May 10 06:06:20.657: INFO: Deleting ReplicationController affinity-nodeport took: 5.469619ms
  May 10 06:06:20.758: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.464911ms
  E0510 06:06:21.017321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:22.018081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:23.018995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-7407" for this suite. @ 05/10/23 06:06:23.372
• [9.636 seconds]
------------------------------
S
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 05/10/23 06:06:23.379
  May 10 06:06:23.379: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename proxy @ 05/10/23 06:06:23.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:06:23.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:06:23.394
  May 10 06:06:23.396: INFO: Creating pod...
  E0510 06:06:24.019604      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:25.019701      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:06:25.406: INFO: Creating service...
  May 10 06:06:25.415: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/pods/agnhost/proxy/some/path/with/DELETE
  May 10 06:06:25.418: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 10 06:06:25.418: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/pods/agnhost/proxy/some/path/with/GET
  May 10 06:06:25.424: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 10 06:06:25.424: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/pods/agnhost/proxy/some/path/with/HEAD
  May 10 06:06:25.426: INFO: http.Client request:HEAD | StatusCode:200
  May 10 06:06:25.426: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/pods/agnhost/proxy/some/path/with/OPTIONS
  May 10 06:06:25.428: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 10 06:06:25.428: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/pods/agnhost/proxy/some/path/with/PATCH
  May 10 06:06:25.430: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 10 06:06:25.430: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/pods/agnhost/proxy/some/path/with/POST
  May 10 06:06:25.431: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 10 06:06:25.431: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/pods/agnhost/proxy/some/path/with/PUT
  May 10 06:06:25.433: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 10 06:06:25.433: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/services/test-service/proxy/some/path/with/DELETE
  May 10 06:06:25.436: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 10 06:06:25.436: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/services/test-service/proxy/some/path/with/GET
  May 10 06:06:25.438: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 10 06:06:25.438: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/services/test-service/proxy/some/path/with/HEAD
  May 10 06:06:25.440: INFO: http.Client request:HEAD | StatusCode:200
  May 10 06:06:25.440: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/services/test-service/proxy/some/path/with/OPTIONS
  May 10 06:06:25.443: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 10 06:06:25.443: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/services/test-service/proxy/some/path/with/PATCH
  May 10 06:06:25.445: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 10 06:06:25.445: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/services/test-service/proxy/some/path/with/POST
  May 10 06:06:25.447: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 10 06:06:25.447: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6283/services/test-service/proxy/some/path/with/PUT
  May 10 06:06:25.449: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 10 06:06:25.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-6283" for this suite. @ 05/10/23 06:06:25.451
• [2.078 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 05/10/23 06:06:25.458
  May 10 06:06:25.458: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename namespaces @ 05/10/23 06:06:25.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:06:25.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:06:25.475
  STEP: Creating namespace "e2e-ns-9kf4j" @ 05/10/23 06:06:25.477
  May 10 06:06:25.491: INFO: Namespace "e2e-ns-9kf4j-2684" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-9kf4j-2684" @ 05/10/23 06:06:25.491
  May 10 06:06:25.497: INFO: Namespace "e2e-ns-9kf4j-2684" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-9kf4j-2684" @ 05/10/23 06:06:25.497
  May 10 06:06:25.507: INFO: Namespace "e2e-ns-9kf4j-2684" has []v1.FinalizerName{"kubernetes"}
  May 10 06:06:25.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-583" for this suite. @ 05/10/23 06:06:25.509
  STEP: Destroying namespace "e2e-ns-9kf4j-2684" for this suite. @ 05/10/23 06:06:25.514
• [0.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 05/10/23 06:06:25.52
  May 10 06:06:25.520: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename sched-pred @ 05/10/23 06:06:25.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:06:25.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:06:25.54
  May 10 06:06:25.542: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 10 06:06:25.546: INFO: Waiting for terminating namespaces to be deleted...
  May 10 06:06:25.548: INFO: 
  Logging pods the apiserver thinks is on node node-01 before test
  May 10 06:06:25.553: INFO: calico-node-hrprp from calico-system started at 2023-05-08 03:40:50 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.553: INFO: 	Container calico-node ready: true, restart count 0
  May 10 06:06:25.553: INFO: csi-node-driver-wrcs7 from calico-system started at 2023-05-08 03:41:03 +0000 UTC (2 container statuses recorded)
  May 10 06:06:25.553: INFO: 	Container calico-csi ready: true, restart count 0
  May 10 06:06:25.553: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 10 06:06:25.553: INFO: kube-proxy-jtvxj from kube-system started at 2023-05-08 03:40:50 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.553: INFO: 	Container kube-proxy ready: true, restart count 0
  May 10 06:06:25.553: INFO: kube-sealos-lvscare-node-01 from kube-system started at 2023-05-08 03:41:06 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.553: INFO: 	Container kube-sealos-lvscare ready: true, restart count 0
  May 10 06:06:25.553: INFO: sonobuoy from sonobuoy started at 2023-05-10 04:50:14 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.553: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 10 06:06:25.553: INFO: sonobuoy-e2e-job-9c188259183c425e from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 06:06:25.553: INFO: 	Container e2e ready: true, restart count 0
  May 10 06:06:25.553: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 06:06:25.553: INFO: sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-xwr24 from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 06:06:25.553: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 06:06:25.553: INFO: 	Container systemd-logs ready: true, restart count 0
  May 10 06:06:25.553: INFO: 
  Logging pods the apiserver thinks is on node node-02 before test
  May 10 06:06:25.558: INFO: calico-node-96glz from calico-system started at 2023-05-08 05:47:14 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.558: INFO: 	Container calico-node ready: true, restart count 0
  May 10 06:06:25.558: INFO: calico-typha-5777d458f7-g7bn9 from calico-system started at 2023-05-08 05:47:15 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.558: INFO: 	Container calico-typha ready: true, restart count 0
  May 10 06:06:25.558: INFO: csi-node-driver-rh58m from calico-system started at 2023-05-10 05:39:46 +0000 UTC (2 container statuses recorded)
  May 10 06:06:25.558: INFO: 	Container calico-csi ready: true, restart count 0
  May 10 06:06:25.558: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 10 06:06:25.558: INFO: kube-proxy-96kwf from kube-system started at 2023-05-08 05:47:14 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.558: INFO: 	Container kube-proxy ready: true, restart count 0
  May 10 06:06:25.558: INFO: kube-sealos-lvscare-node-02 from kube-system started at 2023-05-08 05:47:31 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.558: INFO: 	Container kube-sealos-lvscare ready: true, restart count 0
  May 10 06:06:25.558: INFO: agnhost from proxy-6283 started at 2023-05-10 06:06:23 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.558: INFO: 	Container agnhost ready: true, restart count 0
  May 10 06:06:25.558: INFO: sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-254cd from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 06:06:25.558: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 06:06:25.558: INFO: 	Container systemd-logs ready: true, restart count 0
  May 10 06:06:25.558: INFO: 
  Logging pods the apiserver thinks is on node ub-test before test
  May 10 06:06:25.564: INFO: calico-apiserver-7f745d8f87-78trq from calico-apiserver started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container calico-apiserver ready: true, restart count 0
  May 10 06:06:25.564: INFO: calico-apiserver-7f745d8f87-gk9nt from calico-apiserver started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container calico-apiserver ready: true, restart count 0
  May 10 06:06:25.564: INFO: calico-kube-controllers-6dfbf88686-x7jtc from calico-system started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 10 06:06:25.564: INFO: calico-node-rwtc6 from calico-system started at 2023-05-06 03:21:16 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container calico-node ready: true, restart count 2
  May 10 06:06:25.564: INFO: calico-typha-5777d458f7-jvrx5 from calico-system started at 2023-05-06 03:21:16 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container calico-typha ready: true, restart count 2
  May 10 06:06:25.564: INFO: csi-node-driver-fgjb5 from calico-system started at 2023-05-06 09:37:06 +0000 UTC (2 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container calico-csi ready: true, restart count 0
  May 10 06:06:25.564: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 10 06:06:25.564: INFO: coredns-5d78c9869d-d9nxz from kube-system started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container coredns ready: true, restart count 0
  May 10 06:06:25.564: INFO: coredns-5d78c9869d-w9ngw from kube-system started at 2023-05-06 09:37:01 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container coredns ready: true, restart count 0
  May 10 06:06:25.564: INFO: etcd-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container etcd ready: true, restart count 2
  May 10 06:06:25.564: INFO: kube-apiserver-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container kube-apiserver ready: true, restart count 2
  May 10 06:06:25.564: INFO: kube-controller-manager-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container kube-controller-manager ready: true, restart count 6
  May 10 06:06:25.564: INFO: kube-proxy-hqbzr from kube-system started at 2023-05-06 03:18:37 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container kube-proxy ready: true, restart count 2
  May 10 06:06:25.564: INFO: kube-scheduler-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container kube-scheduler ready: true, restart count 6
  May 10 06:06:25.564: INFO: sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-44wmx from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 06:06:25.564: INFO: 	Container systemd-logs ready: true, restart count 0
  May 10 06:06:25.564: INFO: tigera-operator-7bf7458-fcvpq from tigera-operator started at 2023-05-06 03:21:10 +0000 UTC (1 container statuses recorded)
  May 10 06:06:25.564: INFO: 	Container tigera-operator ready: true, restart count 6
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/10/23 06:06:25.564
  E0510 06:06:26.020647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:27.021189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/10/23 06:06:27.582
  STEP: Trying to apply a random label on the found node. @ 05/10/23 06:06:27.595
  STEP: verifying the node has the label kubernetes.io/e2e-b22786ac-ce7f-4536-835d-876795199627 95 @ 05/10/23 06:06:27.606
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 05/10/23 06:06:27.608
  E0510 06:06:28.021610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:29.021776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.60.6 on the node which pod4 resides and expect not scheduled @ 05/10/23 06:06:29.62
  E0510 06:06:30.021905      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:31.022269      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:32.022848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:33.023111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:34.024069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:35.024522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:36.025392      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:37.025910      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:38.026594      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:39.026890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:40.027618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:41.027774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:42.027838      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:43.028284      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:44.029257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:45.029501      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:46.029463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:47.029999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:48.030735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:49.031223      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:50.032013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:51.032310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:52.033207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:53.033388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:54.034156      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:55.034307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:56.034829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:57.035374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:58.036322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:06:59.036463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:00.037308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:01.037503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:02.037678      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:03.037894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:04.038578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:05.038766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:06.039469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:07.040047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:08.040352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:09.040573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:10.040732      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:11.041036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:12.041063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:13.041302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:14.041936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:15.042191      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:16.042357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:17.042962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:18.043582      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:19.043684      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:20.043766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:21.044009      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:22.044502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:23.044707      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:24.045672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:25.045827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:26.045900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:27.046390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:28.046611      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:29.046833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:30.047642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:31.047858      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:32.048059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:33.048312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:34.048380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:35.049455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:36.050487      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:37.051272      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:38.052226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:39.052628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:40.052699      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:41.053002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:42.053784      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:43.053991      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:44.054785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:45.055308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:46.056101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:47.056636      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:48.057168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:49.057426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:50.058300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:51.058703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:52.059203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:53.059593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:54.059695      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:55.059989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:56.060710      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:57.060897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:58.061530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:07:59.061839      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:00.062370      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:01.062682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:02.063542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:03.063755      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:04.064350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:05.064555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:06.065279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:07.065922      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:08.066674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:09.066880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:10.067719      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:11.067975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:12.068475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:13.068849      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:14.069627      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:15.069840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:16.070309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:17.070983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:18.072034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:19.072264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:20.072934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:21.074014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:22.074722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:23.074976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:24.075962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:25.076198      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:26.076289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:27.077088      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:28.078047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:29.078245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:30.078976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:31.079156      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:32.079877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:33.080068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:34.081001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:35.081217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:36.081794      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:37.082424      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:38.083391      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:39.083688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:40.084379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:41.084589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:42.085380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:43.085721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:44.086278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:45.086473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:46.086551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:47.087080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:48.087102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:49.087283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:50.087828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:51.088067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:52.088301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:53.088491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:54.088979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:55.089224      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:56.090200      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:57.090770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:58.091431      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:08:59.091641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:00.092355      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:01.092593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:02.093434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:03.093640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:04.094018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:05.094242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:06.094288      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:07.094953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:08.095907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:09.096111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:10.096912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:11.097122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:12.098015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:13.098214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:14.099030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:15.099253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:16.099300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:17.099908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:18.100669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:19.100939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:20.101746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:21.101971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:22.102701      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:23.102942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:24.103091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:25.103328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:26.103661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:27.104222      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:28.105126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:29.105350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:30.106406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:31.106618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:32.107467      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:33.107764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:34.108266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:35.108502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:36.108935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:37.109576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:38.110523      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:39.110769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:40.111704      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:41.111810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:42.112574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:43.112818      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:44.113499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:45.113714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:46.114509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:47.115213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:48.115956      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:49.116069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:50.116972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:51.117247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:52.117752      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:53.117926      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:54.118001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:55.118276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:56.118590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:57.119103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:58.119560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:09:59.119793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:00.120633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:01.120735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:02.121286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:03.121493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:04.121813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:05.121972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:06.122732      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:07.123199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:08.123490      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:09.123735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:10.124237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:11.124437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:12.125393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:13.125599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:14.126658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:15.126866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:16.127486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:17.127972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:18.128561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:19.128755      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:20.129617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:21.129821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:22.130544      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:23.130837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:24.131543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:25.131813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:26.132600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:27.133016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:28.133238      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:29.133452      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:30.134427      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:31.134634      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:32.134835      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:33.135134      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:34.135535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:35.135758      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:36.136506      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:37.137024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:38.137533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:39.137735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:40.138491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:41.138739      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:42.139135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:43.139346      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:44.139399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:45.139661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:46.140450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:47.141153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:48.142110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:49.142387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:50.143136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:51.143379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:52.143547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:53.143771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:54.144229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:55.144514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:56.145070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:57.145623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:58.146485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:10:59.146786      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:00.147385      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:01.147616      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:02.147873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:03.148100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:04.148535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:05.148744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:06.149046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:07.150092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:08.150203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:09.150623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:10.150983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:11.151149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:12.151633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:13.151780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:14.152009      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:15.152138      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:16.152372      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:17.152853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:18.152983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:19.153883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:20.154073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:21.154163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:22.154442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:23.154569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:24.154928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:25.155036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:26.155267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:27.155815      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:28.156225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:29.157180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-b22786ac-ce7f-4536-835d-876795199627 off the node node-02 @ 05/10/23 06:11:29.626
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-b22786ac-ce7f-4536-835d-876795199627 @ 05/10/23 06:11:29.641
  May 10 06:11:29.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9606" for this suite. @ 05/10/23 06:11:29.645
• [304.129 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 05/10/23 06:11:29.652
  May 10 06:11:29.652: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 06:11:29.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:11:29.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:11:29.663
  STEP: Creating configMap with name configmap-test-volume-490449cc-3135-4576-b0a2-0cd52367b626 @ 05/10/23 06:11:29.667
  STEP: Creating a pod to test consume configMaps @ 05/10/23 06:11:29.673
  E0510 06:11:30.157666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:31.158066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:32.158563      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:33.158780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:11:33.688
  May 10 06:11:33.690: INFO: Trying to get logs from node node-02 pod pod-configmaps-815aa04e-fd40-4987-9e77-39c0b8d96a62 container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 06:11:33.704
  May 10 06:11:33.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6528" for this suite. @ 05/10/23 06:11:33.719
• [4.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 05/10/23 06:11:33.724
  May 10 06:11:33.724: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 06:11:33.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:11:33.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:11:33.735
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/10/23 06:11:33.737
  E0510 06:11:34.159571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:35.159862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:36.160602      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:37.161005      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:11:37.752
  May 10 06:11:37.754: INFO: Trying to get logs from node node-02 pod pod-35ae1658-1006-4ec8-8827-e2b32259a4f7 container test-container: <nil>
  STEP: delete the pod @ 05/10/23 06:11:37.757
  May 10 06:11:37.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5269" for this suite. @ 05/10/23 06:11:37.772
• [4.054 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 05/10/23 06:11:37.779
  May 10 06:11:37.779: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 06:11:37.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:11:37.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:11:37.791
  STEP: Setting up server cert @ 05/10/23 06:11:37.809
  E0510 06:11:38.161702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 06:11:38.234
  STEP: Deploying the webhook pod @ 05/10/23 06:11:38.24
  STEP: Wait for the deployment to be ready @ 05/10/23 06:11:38.25
  May 10 06:11:38.253: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0510 06:11:39.162728      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:40.162972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 06:11:40.26
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 06:11:40.266
  E0510 06:11:41.164064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:11:41.267: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/10/23 06:11:41.269
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/10/23 06:11:41.284
  STEP: Creating a dummy validating-webhook-configuration object @ 05/10/23 06:11:41.297
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 05/10/23 06:11:41.303
  STEP: Creating a dummy mutating-webhook-configuration object @ 05/10/23 06:11:41.307
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 05/10/23 06:11:41.315
  May 10 06:11:41.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7369" for this suite. @ 05/10/23 06:11:41.358
  STEP: Destroying namespace "webhook-markers-6342" for this suite. @ 05/10/23 06:11:41.362
• [3.589 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 05/10/23 06:11:41.369
  May 10 06:11:41.369: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pod-network-test @ 05/10/23 06:11:41.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:11:41.381
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:11:41.384
  STEP: Performing setup for networking test in namespace pod-network-test-1167 @ 05/10/23 06:11:41.386
  STEP: creating a selector @ 05/10/23 06:11:41.386
  STEP: Creating the service pods in kubernetes @ 05/10/23 06:11:41.386
  May 10 06:11:41.386: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0510 06:11:42.164614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:43.164876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:44.165008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:45.165214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:46.165513      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:47.166171      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:48.167116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:49.167456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:50.168359      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:51.168596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:52.168621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:53.168922      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:54.168953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:55.169165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:56.169300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:57.169851      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:58.169986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:11:59.170242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:00.170327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:01.170614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:02.171412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:03.171534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/10/23 06:12:03.459
  E0510 06:12:04.171631      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:05.171790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:12:05.473: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 10 06:12:05.473: INFO: Breadth first check of 100.67.79.182 on host 192.168.60.83...
  May 10 06:12:05.475: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.114.252.129:9080/dial?request=hostname&protocol=http&host=100.67.79.182&port=8083&tries=1'] Namespace:pod-network-test-1167 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 06:12:05.475: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 06:12:05.475: INFO: ExecWithOptions: Clientset creation
  May 10 06:12:05.475: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1167/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.114.252.129%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.67.79.182%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 10 06:12:05.540: INFO: Waiting for responses: map[]
  May 10 06:12:05.540: INFO: reached 100.67.79.182 after 0/1 tries
  May 10 06:12:05.540: INFO: Breadth first check of 100.114.252.155 on host 192.168.60.6...
  May 10 06:12:05.542: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.114.252.129:9080/dial?request=hostname&protocol=http&host=100.114.252.155&port=8083&tries=1'] Namespace:pod-network-test-1167 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 06:12:05.542: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 06:12:05.543: INFO: ExecWithOptions: Clientset creation
  May 10 06:12:05.543: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1167/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.114.252.129%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.114.252.155%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 10 06:12:05.587: INFO: Waiting for responses: map[]
  May 10 06:12:05.587: INFO: reached 100.114.252.155 after 0/1 tries
  May 10 06:12:05.587: INFO: Breadth first check of 100.74.79.58 on host 192.168.60.152...
  May 10 06:12:05.590: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.114.252.129:9080/dial?request=hostname&protocol=http&host=100.74.79.58&port=8083&tries=1'] Namespace:pod-network-test-1167 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 06:12:05.590: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 06:12:05.591: INFO: ExecWithOptions: Clientset creation
  May 10 06:12:05.591: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1167/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.114.252.129%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.74.79.58%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 10 06:12:05.659: INFO: Waiting for responses: map[]
  May 10 06:12:05.659: INFO: reached 100.74.79.58 after 0/1 tries
  May 10 06:12:05.659: INFO: Going to retry 0 out of 3 pods....
  May 10 06:12:05.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1167" for this suite. @ 05/10/23 06:12:05.662
• [24.299 seconds]
------------------------------
S
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 05/10/23 06:12:05.667
  May 10 06:12:05.667: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename cronjob @ 05/10/23 06:12:05.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:05.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:05.682
  STEP: Creating a cronjob @ 05/10/23 06:12:05.684
  STEP: creating @ 05/10/23 06:12:05.684
  STEP: getting @ 05/10/23 06:12:05.688
  STEP: listing @ 05/10/23 06:12:05.689
  STEP: watching @ 05/10/23 06:12:05.691
  May 10 06:12:05.691: INFO: starting watch
  STEP: cluster-wide listing @ 05/10/23 06:12:05.692
  STEP: cluster-wide watching @ 05/10/23 06:12:05.694
  May 10 06:12:05.694: INFO: starting watch
  STEP: patching @ 05/10/23 06:12:05.695
  STEP: updating @ 05/10/23 06:12:05.703
  May 10 06:12:05.710: INFO: waiting for watch events with expected annotations
  May 10 06:12:05.710: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/10/23 06:12:05.71
  STEP: updating /status @ 05/10/23 06:12:05.715
  STEP: get /status @ 05/10/23 06:12:05.72
  STEP: deleting @ 05/10/23 06:12:05.722
  STEP: deleting a collection @ 05/10/23 06:12:05.733
  May 10 06:12:05.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4059" for this suite. @ 05/10/23 06:12:05.74
• [0.077 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 05/10/23 06:12:05.745
  May 10 06:12:05.745: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename resourcequota @ 05/10/23 06:12:05.746
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:05.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:05.762
  STEP: Counting existing ResourceQuota @ 05/10/23 06:12:05.764
  E0510 06:12:06.172111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:07.173113      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:08.173810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:09.174687      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:10.175164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/10/23 06:12:10.766
  STEP: Ensuring resource quota status is calculated @ 05/10/23 06:12:10.777
  E0510 06:12:11.176154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:12.176606      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 05/10/23 06:12:12.781
  STEP: Creating a NodePort Service @ 05/10/23 06:12:12.802
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 05/10/23 06:12:12.819
  STEP: Ensuring resource quota status captures service creation @ 05/10/23 06:12:12.831
  E0510 06:12:13.177534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:14.177763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 05/10/23 06:12:14.835
  STEP: Ensuring resource quota status released usage @ 05/10/23 06:12:14.89
  E0510 06:12:15.177791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:16.178140      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:12:16.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9383" for this suite. @ 05/10/23 06:12:16.895
• [11.154 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 05/10/23 06:12:16.9
  May 10 06:12:16.900: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 06:12:16.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:16.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:16.914
  STEP: validating cluster-info @ 05/10/23 06:12:16.917
  May 10 06:12:16.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-1244 cluster-info'
  May 10 06:12:16.974: INFO: stderr: ""
  May 10 06:12:16.974: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  May 10 06:12:16.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1244" for this suite. @ 05/10/23 06:12:16.976
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 05/10/23 06:12:16.985
  May 10 06:12:16.985: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename svcaccounts @ 05/10/23 06:12:16.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:16.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:16.998
  May 10 06:12:17.001: INFO: Got root ca configmap in namespace "svcaccounts-7887"
  May 10 06:12:17.005: INFO: Deleted root ca configmap in namespace "svcaccounts-7887"
  E0510 06:12:17.178690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for a new root ca configmap created @ 05/10/23 06:12:17.507
  May 10 06:12:17.516: INFO: Recreated root ca configmap in namespace "svcaccounts-7887"
  May 10 06:12:17.525: INFO: Updated root ca configmap in namespace "svcaccounts-7887"
  STEP: waiting for the root ca configmap reconciled @ 05/10/23 06:12:18.026
  May 10 06:12:18.028: INFO: Reconciled root ca configmap in namespace "svcaccounts-7887"
  May 10 06:12:18.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7887" for this suite. @ 05/10/23 06:12:18.031
• [1.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 05/10/23 06:12:18.036
  May 10 06:12:18.036: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename svcaccounts @ 05/10/23 06:12:18.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:18.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:18.059
  STEP: Creating a pod to test service account token:  @ 05/10/23 06:12:18.074
  E0510 06:12:18.179498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:19.179828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:20.180203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:21.180441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:22.180967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:23.181121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:12:24.089
  May 10 06:12:24.090: INFO: Trying to get logs from node node-02 pod test-pod-047399a7-7157-45ff-a538-1d7e5540d27a container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 06:12:24.095
  May 10 06:12:24.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6025" for this suite. @ 05/10/23 06:12:24.112
• [6.080 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 05/10/23 06:12:24.117
  May 10 06:12:24.117: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename podtemplate @ 05/10/23 06:12:24.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:24.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:24.13
  May 10 06:12:24.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8312" for this suite. @ 05/10/23 06:12:24.159
• [0.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 05/10/23 06:12:24.164
  May 10 06:12:24.164: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 06:12:24.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:24.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:24.178
  STEP: starting the proxy server @ 05/10/23 06:12:24.18
  May 10 06:12:24.180: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2847 proxy -p 0 --disable-filter'
  E0510 06:12:24.181297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: curling proxy /api/ output @ 05/10/23 06:12:24.225
  May 10 06:12:24.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2847" for this suite. @ 05/10/23 06:12:24.234
• [0.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 05/10/23 06:12:24.24
  May 10 06:12:24.240: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename job @ 05/10/23 06:12:24.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:24.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:24.255
  STEP: Creating a suspended job @ 05/10/23 06:12:24.26
  STEP: Patching the Job @ 05/10/23 06:12:24.265
  STEP: Watching for Job to be patched @ 05/10/23 06:12:24.274
  May 10 06:12:24.275: INFO: Event ADDED observed for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52] and annotations: map[batch.kubernetes.io/job-tracking:]
  May 10 06:12:24.275: INFO: Event MODIFIED found for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 05/10/23 06:12:24.275
  STEP: Watching for Job to be updated @ 05/10/23 06:12:24.305
  May 10 06:12:24.307: INFO: Event MODIFIED found for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 10 06:12:24.307: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 05/10/23 06:12:24.307
  May 10 06:12:24.308: INFO: Job: e2e-rpr52 as labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched]
  STEP: Waiting for job to complete @ 05/10/23 06:12:24.308
  E0510 06:12:25.181890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:26.182244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:27.183100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:28.183998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:29.185095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:30.185392      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:31.185846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:32.186118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:33.186235      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:34.186458      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 05/10/23 06:12:34.312
  STEP: Watching for Job to be deleted @ 05/10/23 06:12:34.317
  May 10 06:12:34.318: INFO: Event MODIFIED observed for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 10 06:12:34.318: INFO: Event MODIFIED observed for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 10 06:12:34.318: INFO: Event MODIFIED observed for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 10 06:12:34.318: INFO: Event MODIFIED observed for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 10 06:12:34.318: INFO: Event MODIFIED observed for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 10 06:12:34.319: INFO: Event MODIFIED observed for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 10 06:12:34.319: INFO: Event MODIFIED observed for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 10 06:12:34.319: INFO: Event MODIFIED observed for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 10 06:12:34.319: INFO: Event MODIFIED observed for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 10 06:12:34.319: INFO: Event MODIFIED observed for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 10 06:12:34.319: INFO: Event MODIFIED observed for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 10 06:12:34.319: INFO: Event DELETED found for Job e2e-rpr52 in namespace job-6445 with labels: map[e2e-job-label:e2e-rpr52 e2e-rpr52:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 05/10/23 06:12:34.319
  May 10 06:12:34.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6445" for this suite. @ 05/10/23 06:12:34.324
• [10.088 seconds]
------------------------------
S
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 05/10/23 06:12:34.328
  May 10 06:12:34.328: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 05/10/23 06:12:34.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:34.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:34.348
  STEP: creating a target pod @ 05/10/23 06:12:34.35
  E0510 06:12:35.187108      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:36.187365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 05/10/23 06:12:36.362
  E0510 06:12:37.187756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:38.187804      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:39.188478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:40.188735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 05/10/23 06:12:40.38
  May 10 06:12:40.380: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6559 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 06:12:40.380: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 06:12:40.381: INFO: ExecWithOptions: Clientset creation
  May 10 06:12:40.381: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-6559/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  May 10 06:12:40.439: INFO: Exec stderr: ""
  May 10 06:12:40.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-6559" for this suite. @ 05/10/23 06:12:40.446
• [6.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 05/10/23 06:12:40.45
  May 10 06:12:40.450: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 06:12:40.451
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:40.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:40.46
  STEP: Creating secret with name secret-test-map-fba0009b-d7ac-410d-852a-25e07875cd41 @ 05/10/23 06:12:40.462
  STEP: Creating a pod to test consume secrets @ 05/10/23 06:12:40.468
  E0510 06:12:41.188802      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:42.189239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:43.189559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:44.189782      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:12:44.48
  May 10 06:12:44.482: INFO: Trying to get logs from node node-02 pod pod-secrets-c005f4ae-098c-4a87-a415-005b484e0d2c container secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 06:12:44.489
  May 10 06:12:44.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6674" for this suite. @ 05/10/23 06:12:44.505
• [4.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 05/10/23 06:12:44.511
  May 10 06:12:44.511: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename security-context-test @ 05/10/23 06:12:44.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:44.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:44.526
  E0510 06:12:45.190180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:46.190479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:47.190677      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:48.190932      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:12:48.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-2655" for this suite. @ 05/10/23 06:12:48.547
• [4.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 05/10/23 06:12:48.558
  May 10 06:12:48.558: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 06:12:48.559
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:48.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:48.57
  STEP: Creating secret with name secret-test-2e7bebf5-3b21-4539-b914-08daff97d8bb @ 05/10/23 06:12:48.571
  STEP: Creating a pod to test consume secrets @ 05/10/23 06:12:48.576
  E0510 06:12:49.191445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:50.191769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:51.192400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:52.192975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:12:52.591
  May 10 06:12:52.593: INFO: Trying to get logs from node node-02 pod pod-secrets-e59794cd-50a6-4533-93bf-4b020e0c1918 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 06:12:52.601
  May 10 06:12:52.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1101" for this suite. @ 05/10/23 06:12:52.685
• [4.156 seconds]
------------------------------
S
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 05/10/23 06:12:52.714
  May 10 06:12:52.714: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename proxy @ 05/10/23 06:12:52.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:12:52.775
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:12:52.779
  STEP: starting an echo server on multiple ports @ 05/10/23 06:12:52.787
  STEP: creating replication controller proxy-service-k7l2j in namespace proxy-7938 @ 05/10/23 06:12:52.787
  I0510 06:12:52.795856      22 runners.go:194] Created replication controller with name: proxy-service-k7l2j, namespace: proxy-7938, replica count: 1
  E0510 06:12:53.193958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 06:12:53.847350      22 runners.go:194] proxy-service-k7l2j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0510 06:12:54.194109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 06:12:54.847514      22 runners.go:194] proxy-service-k7l2j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0510 06:12:55.195031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 06:12:55.847786      22 runners.go:194] proxy-service-k7l2j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0510 06:12:56.196081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 06:12:56.848050      22 runners.go:194] proxy-service-k7l2j Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 10 06:12:56.850: INFO: setup took 4.069470965s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 05/10/23 06:12:56.85
  May 10 06:12:56.855: INFO: (0) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 4.191ms)
  May 10 06:12:56.855: INFO: (0) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 4.855729ms)
  May 10 06:12:56.855: INFO: (0) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 4.945007ms)
  May 10 06:12:56.856: INFO: (0) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 5.150692ms)
  May 10 06:12:56.856: INFO: (0) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 5.242792ms)
  May 10 06:12:56.856: INFO: (0) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 5.115787ms)
  May 10 06:12:56.856: INFO: (0) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 5.322763ms)
  May 10 06:12:56.856: INFO: (0) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 5.18451ms)
  May 10 06:12:56.856: INFO: (0) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 5.260104ms)
  May 10 06:12:56.856: INFO: (0) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 5.23347ms)
  May 10 06:12:56.856: INFO: (0) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 5.364396ms)
  May 10 06:12:56.860: INFO: (0) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 9.825069ms)
  May 10 06:12:56.860: INFO: (0) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 9.948863ms)
  May 10 06:12:56.861: INFO: (0) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 10.071607ms)
  May 10 06:12:56.861: INFO: (0) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 10.070492ms)
  May 10 06:12:56.861: INFO: (0) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 10.624462ms)
  May 10 06:12:56.864: INFO: (1) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 2.981564ms)
  May 10 06:12:56.865: INFO: (1) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 3.333862ms)
  May 10 06:12:56.865: INFO: (1) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.666741ms)
  May 10 06:12:56.865: INFO: (1) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 3.70996ms)
  May 10 06:12:56.865: INFO: (1) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 3.6786ms)
  May 10 06:12:56.865: INFO: (1) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 3.808417ms)
  May 10 06:12:56.865: INFO: (1) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.785119ms)
  May 10 06:12:56.865: INFO: (1) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 3.821111ms)
  May 10 06:12:56.865: INFO: (1) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 3.838512ms)
  May 10 06:12:56.866: INFO: (1) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 4.601254ms)
  May 10 06:12:56.866: INFO: (1) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 4.651722ms)
  May 10 06:12:56.866: INFO: (1) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 4.703398ms)
  May 10 06:12:56.866: INFO: (1) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 4.740944ms)
  May 10 06:12:56.866: INFO: (1) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 4.667179ms)
  May 10 06:12:56.866: INFO: (1) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 4.681915ms)
  May 10 06:12:56.866: INFO: (1) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 4.860099ms)
  May 10 06:12:56.868: INFO: (2) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 1.804249ms)
  May 10 06:12:56.869: INFO: (2) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 3.135057ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 3.344947ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 3.43294ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.518825ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 3.549564ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 3.715679ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 3.943965ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 3.975395ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 3.963079ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 4.067927ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 4.064253ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 4.140885ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 4.019154ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 4.128153ms)
  May 10 06:12:56.870: INFO: (2) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 4.128253ms)
  May 10 06:12:56.873: INFO: (3) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 2.599696ms)
  May 10 06:12:56.873: INFO: (3) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 2.775082ms)
  May 10 06:12:56.873: INFO: (3) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 2.527384ms)
  May 10 06:12:56.873: INFO: (3) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 2.741537ms)
  May 10 06:12:56.874: INFO: (3) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.932756ms)
  May 10 06:12:56.874: INFO: (3) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 3.423153ms)
  May 10 06:12:56.874: INFO: (3) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 3.183467ms)
  May 10 06:12:56.874: INFO: (3) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 3.072722ms)
  May 10 06:12:56.874: INFO: (3) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.165585ms)
  May 10 06:12:56.874: INFO: (3) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 3.235756ms)
  May 10 06:12:56.874: INFO: (3) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.518868ms)
  May 10 06:12:56.874: INFO: (3) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 3.788249ms)
  May 10 06:12:56.875: INFO: (3) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 4.466679ms)
  May 10 06:12:56.875: INFO: (3) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 4.315095ms)
  May 10 06:12:56.875: INFO: (3) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 4.547916ms)
  May 10 06:12:56.875: INFO: (3) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 4.604356ms)
  May 10 06:12:56.878: INFO: (4) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.843512ms)
  May 10 06:12:56.878: INFO: (4) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 2.889026ms)
  May 10 06:12:56.878: INFO: (4) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.888957ms)
  May 10 06:12:56.878: INFO: (4) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 3.050386ms)
  May 10 06:12:56.879: INFO: (4) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 2.782983ms)
  May 10 06:12:56.879: INFO: (4) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 2.774956ms)
  May 10 06:12:56.879: INFO: (4) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.005022ms)
  May 10 06:12:56.879: INFO: (4) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 2.837301ms)
  May 10 06:12:56.879: INFO: (4) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.946613ms)
  May 10 06:12:56.879: INFO: (4) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 2.940102ms)
  May 10 06:12:56.879: INFO: (4) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 3.667813ms)
  May 10 06:12:56.880: INFO: (4) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 4.004151ms)
  May 10 06:12:56.879: INFO: (4) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 3.680034ms)
  May 10 06:12:56.879: INFO: (4) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 3.684062ms)
  May 10 06:12:56.880: INFO: (4) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 3.802056ms)
  May 10 06:12:56.880: INFO: (4) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 3.790736ms)
  May 10 06:12:56.882: INFO: (5) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 2.359117ms)
  May 10 06:12:56.882: INFO: (5) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.367135ms)
  May 10 06:12:56.882: INFO: (5) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 2.659967ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.730222ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 2.977348ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 3.100164ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 3.118555ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.295989ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 3.325399ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.238995ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 3.338617ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 3.433693ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 3.438612ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 3.483203ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 3.626148ms)
  May 10 06:12:56.883: INFO: (5) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 3.683411ms)
  May 10 06:12:56.886: INFO: (6) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 1.989807ms)
  May 10 06:12:56.886: INFO: (6) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 2.042177ms)
  May 10 06:12:56.886: INFO: (6) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.014697ms)
  May 10 06:12:56.886: INFO: (6) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.165996ms)
  May 10 06:12:56.886: INFO: (6) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 2.26463ms)
  May 10 06:12:56.886: INFO: (6) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.432991ms)
  May 10 06:12:56.886: INFO: (6) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 2.601249ms)
  May 10 06:12:56.886: INFO: (6) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 2.668938ms)
  May 10 06:12:56.886: INFO: (6) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 2.727214ms)
  May 10 06:12:56.886: INFO: (6) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 2.731889ms)
  May 10 06:12:56.887: INFO: (6) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 2.970318ms)
  May 10 06:12:56.887: INFO: (6) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 3.030149ms)
  May 10 06:12:56.887: INFO: (6) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 3.185342ms)
  May 10 06:12:56.887: INFO: (6) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 3.170018ms)
  May 10 06:12:56.887: INFO: (6) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 3.138371ms)
  May 10 06:12:56.887: INFO: (6) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 3.15802ms)
  May 10 06:12:56.889: INFO: (7) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 2.118075ms)
  May 10 06:12:56.889: INFO: (7) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 2.132333ms)
  May 10 06:12:56.889: INFO: (7) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.135121ms)
  May 10 06:12:56.889: INFO: (7) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 2.565297ms)
  May 10 06:12:56.889: INFO: (7) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 2.612573ms)
  May 10 06:12:56.890: INFO: (7) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.628903ms)
  May 10 06:12:56.890: INFO: (7) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 2.606985ms)
  May 10 06:12:56.890: INFO: (7) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 2.836045ms)
  May 10 06:12:56.890: INFO: (7) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 2.857201ms)
  May 10 06:12:56.890: INFO: (7) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 2.829137ms)
  May 10 06:12:56.890: INFO: (7) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 3.062804ms)
  May 10 06:12:56.890: INFO: (7) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 3.042153ms)
  May 10 06:12:56.890: INFO: (7) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 3.030984ms)
  May 10 06:12:56.890: INFO: (7) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 3.056124ms)
  May 10 06:12:56.890: INFO: (7) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 3.34269ms)
  May 10 06:12:56.890: INFO: (7) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 3.315786ms)
  May 10 06:12:56.894: INFO: (8) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 4.02922ms)
  May 10 06:12:56.895: INFO: (8) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 4.088932ms)
  May 10 06:12:56.895: INFO: (8) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 4.255337ms)
  May 10 06:12:56.895: INFO: (8) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 4.389262ms)
  May 10 06:12:56.895: INFO: (8) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 4.333215ms)
  May 10 06:12:56.895: INFO: (8) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 4.409737ms)
  May 10 06:12:56.895: INFO: (8) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 4.364472ms)
  May 10 06:12:56.895: INFO: (8) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 4.311666ms)
  May 10 06:12:56.895: INFO: (8) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 4.361837ms)
  May 10 06:12:56.895: INFO: (8) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 4.540809ms)
  May 10 06:12:56.895: INFO: (8) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 5.033484ms)
  May 10 06:12:56.896: INFO: (8) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 5.303589ms)
  May 10 06:12:56.896: INFO: (8) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 5.300512ms)
  May 10 06:12:56.896: INFO: (8) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 5.410335ms)
  May 10 06:12:56.896: INFO: (8) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 5.350744ms)
  May 10 06:12:56.896: INFO: (8) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 5.999066ms)
  May 10 06:12:56.901: INFO: (9) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 4.742376ms)
  May 10 06:12:56.901: INFO: (9) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 4.831798ms)
  May 10 06:12:56.901: INFO: (9) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 4.947862ms)
  May 10 06:12:56.901: INFO: (9) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 4.968291ms)
  May 10 06:12:56.902: INFO: (9) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 4.992937ms)
  May 10 06:12:56.902: INFO: (9) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 5.120454ms)
  May 10 06:12:56.902: INFO: (9) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 5.197566ms)
  May 10 06:12:56.902: INFO: (9) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 5.207003ms)
  May 10 06:12:56.902: INFO: (9) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 5.180295ms)
  May 10 06:12:56.902: INFO: (9) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 5.23403ms)
  May 10 06:12:56.903: INFO: (9) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 6.189694ms)
  May 10 06:12:56.903: INFO: (9) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 5.901333ms)
  May 10 06:12:56.903: INFO: (9) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 6.386769ms)
  May 10 06:12:56.903: INFO: (9) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 6.595768ms)
  May 10 06:12:56.903: INFO: (9) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 6.65978ms)
  May 10 06:12:56.903: INFO: (9) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 6.815184ms)
  May 10 06:12:56.906: INFO: (10) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.34232ms)
  May 10 06:12:56.906: INFO: (10) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 2.30758ms)
  May 10 06:12:56.906: INFO: (10) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 2.395883ms)
  May 10 06:12:56.906: INFO: (10) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.360818ms)
  May 10 06:12:56.906: INFO: (10) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.472944ms)
  May 10 06:12:56.906: INFO: (10) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 2.453786ms)
  May 10 06:12:56.906: INFO: (10) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 2.575673ms)
  May 10 06:12:56.906: INFO: (10) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.572029ms)
  May 10 06:12:56.906: INFO: (10) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 2.616653ms)
  May 10 06:12:56.906: INFO: (10) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 2.779572ms)
  May 10 06:12:56.906: INFO: (10) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 2.9086ms)
  May 10 06:12:56.907: INFO: (10) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 3.227799ms)
  May 10 06:12:56.907: INFO: (10) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 3.218504ms)
  May 10 06:12:56.907: INFO: (10) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 3.32279ms)
  May 10 06:12:56.907: INFO: (10) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 3.389815ms)
  May 10 06:12:56.907: INFO: (10) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 3.32711ms)
  May 10 06:12:56.909: INFO: (11) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 2.33467ms)
  May 10 06:12:56.909: INFO: (11) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 2.560016ms)
  May 10 06:12:56.910: INFO: (11) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 2.808916ms)
  May 10 06:12:56.910: INFO: (11) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 3.021719ms)
  May 10 06:12:56.910: INFO: (11) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 3.018454ms)
  May 10 06:12:56.910: INFO: (11) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.997443ms)
  May 10 06:12:56.910: INFO: (11) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 3.087646ms)
  May 10 06:12:56.910: INFO: (11) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 3.234853ms)
  May 10 06:12:56.910: INFO: (11) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 3.195791ms)
  May 10 06:12:56.910: INFO: (11) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.211983ms)
  May 10 06:12:56.911: INFO: (11) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 3.758996ms)
  May 10 06:12:56.911: INFO: (11) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 3.814719ms)
  May 10 06:12:56.911: INFO: (11) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 4.026484ms)
  May 10 06:12:56.911: INFO: (11) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 4.02342ms)
  May 10 06:12:56.911: INFO: (11) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 4.030419ms)
  May 10 06:12:56.911: INFO: (11) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 4.086752ms)
  May 10 06:12:56.913: INFO: (12) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 1.806689ms)
  May 10 06:12:56.913: INFO: (12) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 1.865501ms)
  May 10 06:12:56.914: INFO: (12) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 2.391378ms)
  May 10 06:12:56.914: INFO: (12) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 2.43161ms)
  May 10 06:12:56.914: INFO: (12) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 2.440319ms)
  May 10 06:12:56.914: INFO: (12) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.467751ms)
  May 10 06:12:56.914: INFO: (12) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.185623ms)
  May 10 06:12:56.914: INFO: (12) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 3.232944ms)
  May 10 06:12:56.914: INFO: (12) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 3.291841ms)
  May 10 06:12:56.914: INFO: (12) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 3.334512ms)
  May 10 06:12:56.914: INFO: (12) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 3.367552ms)
  May 10 06:12:56.915: INFO: (12) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 3.431743ms)
  May 10 06:12:56.915: INFO: (12) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 3.454826ms)
  May 10 06:12:56.915: INFO: (12) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 3.587809ms)
  May 10 06:12:56.915: INFO: (12) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 3.658324ms)
  May 10 06:12:56.915: INFO: (12) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 3.71643ms)
  May 10 06:12:56.917: INFO: (13) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 1.734482ms)
  May 10 06:12:56.917: INFO: (13) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.102798ms)
  May 10 06:12:56.917: INFO: (13) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.074752ms)
  May 10 06:12:56.917: INFO: (13) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 2.148813ms)
  May 10 06:12:56.917: INFO: (13) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 2.14402ms)
  May 10 06:12:56.917: INFO: (13) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.275505ms)
  May 10 06:12:56.917: INFO: (13) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 2.452844ms)
  May 10 06:12:56.917: INFO: (13) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 2.553771ms)
  May 10 06:12:56.917: INFO: (13) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.542943ms)
  May 10 06:12:56.917: INFO: (13) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 2.612786ms)
  May 10 06:12:56.918: INFO: (13) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 2.947505ms)
  May 10 06:12:56.918: INFO: (13) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 2.985402ms)
  May 10 06:12:56.918: INFO: (13) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 2.997135ms)
  May 10 06:12:56.918: INFO: (13) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 3.023423ms)
  May 10 06:12:56.918: INFO: (13) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 2.997761ms)
  May 10 06:12:56.918: INFO: (13) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 2.998688ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 2.705003ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 2.700723ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.690128ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.782948ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 2.767302ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.78697ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.758313ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 2.7771ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 2.835272ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 2.881107ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 3.091122ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 3.094114ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 3.088602ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 3.088984ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 3.069895ms)
  May 10 06:12:56.921: INFO: (14) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 3.202876ms)
  May 10 06:12:56.924: INFO: (15) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 3.011579ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 3.204471ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 3.209801ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 3.302773ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 3.297892ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.255286ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 3.280422ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 3.351238ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 3.28311ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 3.312011ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.722871ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 3.735761ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 3.805625ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 3.917386ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 4.129854ms)
  May 10 06:12:56.925: INFO: (15) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 4.132423ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 3.072919ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.813334ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 2.90938ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 2.89281ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 3.227752ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 3.356199ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.563029ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 3.163516ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 3.440356ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 3.070155ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 3.32848ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 3.155782ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 3.200726ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 3.719001ms)
  May 10 06:12:56.929: INFO: (16) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 3.863527ms)
  May 10 06:12:56.930: INFO: (16) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 3.64527ms)
  May 10 06:12:56.932: INFO: (17) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 1.979658ms)
  May 10 06:12:56.932: INFO: (17) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.233776ms)
  May 10 06:12:56.932: INFO: (17) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 1.914882ms)
  May 10 06:12:56.932: INFO: (17) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 1.750602ms)
  May 10 06:12:56.932: INFO: (17) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.403315ms)
  May 10 06:12:56.932: INFO: (17) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 2.497356ms)
  May 10 06:12:56.932: INFO: (17) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 2.582719ms)
  May 10 06:12:56.932: INFO: (17) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 2.83836ms)
  May 10 06:12:56.932: INFO: (17) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.802111ms)
  May 10 06:12:56.933: INFO: (17) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 2.70595ms)
  May 10 06:12:56.933: INFO: (17) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 2.43074ms)
  May 10 06:12:56.933: INFO: (17) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 2.480233ms)
  May 10 06:12:56.933: INFO: (17) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 2.773543ms)
  May 10 06:12:56.933: INFO: (17) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 3.022603ms)
  May 10 06:12:56.933: INFO: (17) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.777101ms)
  May 10 06:12:56.933: INFO: (17) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 2.734076ms)
  May 10 06:12:56.935: INFO: (18) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.112834ms)
  May 10 06:12:56.935: INFO: (18) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 2.186853ms)
  May 10 06:12:56.935: INFO: (18) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 2.240711ms)
  May 10 06:12:56.935: INFO: (18) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.171319ms)
  May 10 06:12:56.935: INFO: (18) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.289643ms)
  May 10 06:12:56.935: INFO: (18) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 2.42964ms)
  May 10 06:12:56.935: INFO: (18) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 2.47506ms)
  May 10 06:12:56.936: INFO: (18) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 2.68031ms)
  May 10 06:12:56.936: INFO: (18) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 2.663131ms)
  May 10 06:12:56.936: INFO: (18) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 2.648419ms)
  May 10 06:12:56.936: INFO: (18) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 3.120543ms)
  May 10 06:12:56.936: INFO: (18) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 3.139015ms)
  May 10 06:12:56.936: INFO: (18) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 3.133505ms)
  May 10 06:12:56.936: INFO: (18) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 3.16429ms)
  May 10 06:12:56.936: INFO: (18) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 3.150227ms)
  May 10 06:12:56.936: INFO: (18) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 3.184697ms)
  May 10 06:12:56.938: INFO: (19) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls/proxy/rewriteme">test</a> (200; 1.539939ms)
  May 10 06:12:56.938: INFO: (19) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">test<... (200; 1.696514ms)
  May 10 06:12:56.938: INFO: (19) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 1.646036ms)
  May 10 06:12:56.938: INFO: (19) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 1.732673ms)
  May 10 06:12:56.938: INFO: (19) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:162/proxy/: bar (200; 1.889594ms)
  May 10 06:12:56.939: INFO: (19) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname2/proxy/: bar (200; 2.076721ms)
  May 10 06:12:56.939: INFO: (19) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname1/proxy/: foo (200; 2.548987ms)
  May 10 06:12:56.939: INFO: (19) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:443/proxy/tlsrewritem... (200; 2.477962ms)
  May 10 06:12:56.939: INFO: (19) /api/v1/namespaces/proxy-7938/services/http:proxy-service-k7l2j:portname1/proxy/: foo (200; 2.465455ms)
  May 10 06:12:56.939: INFO: (19) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname1/proxy/: tls baz (200; 3.005908ms)
  May 10 06:12:56.939: INFO: (19) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:462/proxy/: tls qux (200; 2.544573ms)
  May 10 06:12:56.939: INFO: (19) /api/v1/namespaces/proxy-7938/services/https:proxy-service-k7l2j:tlsportname2/proxy/: tls qux (200; 2.805679ms)
  May 10 06:12:56.939: INFO: (19) /api/v1/namespaces/proxy-7938/services/proxy-service-k7l2j:portname2/proxy/: bar (200; 2.868202ms)
  May 10 06:12:56.939: INFO: (19) /api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/: <a href="/api/v1/namespaces/proxy-7938/pods/http:proxy-service-k7l2j-5cjls:1080/proxy/rewriteme">... (200; 2.550701ms)
  May 10 06:12:56.939: INFO: (19) /api/v1/namespaces/proxy-7938/pods/https:proxy-service-k7l2j-5cjls:460/proxy/: tls baz (200; 2.506215ms)
  May 10 06:12:56.939: INFO: (19) /api/v1/namespaces/proxy-7938/pods/proxy-service-k7l2j-5cjls:160/proxy/: foo (200; 2.803418ms)
  May 10 06:12:56.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-k7l2j in namespace proxy-7938, will wait for the garbage collector to delete the pods @ 05/10/23 06:12:56.941
  May 10 06:12:56.998: INFO: Deleting ReplicationController proxy-service-k7l2j took: 4.609523ms
  May 10 06:12:57.099: INFO: Terminating ReplicationController proxy-service-k7l2j pods took: 100.856089ms
  E0510 06:12:57.197157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:58.197939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:12:59.198449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-7938" for this suite. @ 05/10/23 06:13:00
• [7.290 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 05/10/23 06:13:00.004
  May 10 06:13:00.004: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/10/23 06:13:00.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:13:00.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:13:00.017
  E0510 06:13:00.199340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:01.199577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:13:02.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 05/10/23 06:13:02.051
  STEP: Cleaning up the configmap @ 05/10/23 06:13:02.055
  STEP: Cleaning up the pod @ 05/10/23 06:13:02.059
  STEP: Destroying namespace "emptydir-wrapper-2767" for this suite. @ 05/10/23 06:13:02.072
• [2.071 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 05/10/23 06:13:02.076
  May 10 06:13:02.076: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 06:13:02.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:13:02.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:13:02.091
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 06:13:02.093
  E0510 06:13:02.200342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:03.200682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:04.201644      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:05.201793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:06.202754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:07.203540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:08.203887      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:13:08.208
  May 10 06:13:08.210: INFO: Trying to get logs from node node-02 pod downwardapi-volume-4c33d696-5733-40f7-b1fd-a793038bfaa4 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 06:13:08.214
  May 10 06:13:08.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3498" for this suite. @ 05/10/23 06:13:08.24
• [6.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 05/10/23 06:13:08.246
  May 10 06:13:08.246: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename job @ 05/10/23 06:13:08.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:13:08.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:13:08.257
  STEP: Creating a job @ 05/10/23 06:13:08.26
  STEP: Ensuring active pods == parallelism @ 05/10/23 06:13:08.264
  E0510 06:13:09.204145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:10.205015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 05/10/23 06:13:10.271
  STEP: deleting Job.batch foo in namespace job-3138, will wait for the garbage collector to delete the pods @ 05/10/23 06:13:10.271
  May 10 06:13:10.342: INFO: Deleting Job.batch foo took: 18.036191ms
  May 10 06:13:10.443: INFO: Terminating Job.batch foo pods took: 101.217702ms
  E0510 06:13:11.205004      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:12.205305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:13.206295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:14.207282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:15.207406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:16.208276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:17.209273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:18.209986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:19.210014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:20.210071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:21.211041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:22.211273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:23.212339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:24.213324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:25.214290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:26.214359      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:27.214917      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:28.215193      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:29.215239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:30.215613      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:31.215910      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:32.216326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:33.216608      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:34.217081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:35.217575      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:36.217928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:37.218420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:38.218654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:39.218961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:40.219334      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:41.219613      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:42.219863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 05/10/23 06:13:43.143
  May 10 06:13:43.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3138" for this suite. @ 05/10/23 06:13:43.148
• [34.913 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 05/10/23 06:13:43.159
  May 10 06:13:43.160: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename controllerrevisions @ 05/10/23 06:13:43.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:13:43.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:13:43.171
  STEP: Creating DaemonSet "e2e-zrfxt-daemon-set" @ 05/10/23 06:13:43.186
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/10/23 06:13:43.193
  May 10 06:13:43.198: INFO: Number of nodes with available pods controlled by daemonset e2e-zrfxt-daemon-set: 0
  May 10 06:13:43.198: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 06:13:43.220254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:13:44.206: INFO: Number of nodes with available pods controlled by daemonset e2e-zrfxt-daemon-set: 0
  May 10 06:13:44.206: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 06:13:44.220715      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:13:45.204: INFO: Number of nodes with available pods controlled by daemonset e2e-zrfxt-daemon-set: 3
  May 10 06:13:45.204: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-zrfxt-daemon-set
  STEP: Confirm DaemonSet "e2e-zrfxt-daemon-set" successfully created with "daemonset-name=e2e-zrfxt-daemon-set" label @ 05/10/23 06:13:45.205
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-zrfxt-daemon-set" @ 05/10/23 06:13:45.21
  May 10 06:13:45.212: INFO: Located ControllerRevision: "e2e-zrfxt-daemon-set-bc8d768bc"
  STEP: Patching ControllerRevision "e2e-zrfxt-daemon-set-bc8d768bc" @ 05/10/23 06:13:45.213
  May 10 06:13:45.218: INFO: e2e-zrfxt-daemon-set-bc8d768bc has been patched
  STEP: Create a new ControllerRevision @ 05/10/23 06:13:45.218
  E0510 06:13:45.221565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:13:45.225: INFO: Created ControllerRevision: e2e-zrfxt-daemon-set-f4d897bbc
  STEP: Confirm that there are two ControllerRevisions @ 05/10/23 06:13:45.225
  May 10 06:13:45.225: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 10 06:13:45.227: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-zrfxt-daemon-set-bc8d768bc" @ 05/10/23 06:13:45.227
  STEP: Confirm that there is only one ControllerRevision @ 05/10/23 06:13:45.231
  May 10 06:13:45.231: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 10 06:13:45.233: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-zrfxt-daemon-set-f4d897bbc" @ 05/10/23 06:13:45.234
  May 10 06:13:45.240: INFO: e2e-zrfxt-daemon-set-f4d897bbc has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 05/10/23 06:13:45.24
  W0510 06:13:45.249561      22 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 05/10/23 06:13:45.249
  May 10 06:13:45.249: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0510 06:13:46.222588      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:13:46.251: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 10 06:13:46.254: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-zrfxt-daemon-set-f4d897bbc=updated" @ 05/10/23 06:13:46.254
  STEP: Confirm that there is only one ControllerRevision @ 05/10/23 06:13:46.272
  May 10 06:13:46.272: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 10 06:13:46.274: INFO: Found 1 ControllerRevisions
  May 10 06:13:46.275: INFO: ControllerRevision "e2e-zrfxt-daemon-set-85db9fcc74" has revision 3
  STEP: Deleting DaemonSet "e2e-zrfxt-daemon-set" @ 05/10/23 06:13:46.277
  STEP: deleting DaemonSet.extensions e2e-zrfxt-daemon-set in namespace controllerrevisions-1933, will wait for the garbage collector to delete the pods @ 05/10/23 06:13:46.277
  May 10 06:13:46.334: INFO: Deleting DaemonSet.extensions e2e-zrfxt-daemon-set took: 4.750059ms
  May 10 06:13:46.434: INFO: Terminating DaemonSet.extensions e2e-zrfxt-daemon-set pods took: 100.420021ms
  E0510 06:13:47.223189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:48.223667      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:13:49.138: INFO: Number of nodes with available pods controlled by daemonset e2e-zrfxt-daemon-set: 0
  May 10 06:13:49.138: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-zrfxt-daemon-set
  May 10 06:13:49.140: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1085836"},"items":null}

  May 10 06:13:49.142: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1085836"},"items":null}

  May 10 06:13:49.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-1933" for this suite. @ 05/10/23 06:13:49.151
• [5.996 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 05/10/23 06:13:49.156
  May 10 06:13:49.156: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 06:13:49.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:13:49.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:13:49.181
  STEP: creating Agnhost RC @ 05/10/23 06:13:49.183
  May 10 06:13:49.183: INFO: namespace kubectl-2394
  May 10 06:13:49.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2394 create -f -'
  E0510 06:13:49.223994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:13:49.636: INFO: stderr: ""
  May 10 06:13:49.636: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/10/23 06:13:49.636
  E0510 06:13:50.225099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:13:50.639: INFO: Selector matched 1 pods for map[app:agnhost]
  May 10 06:13:50.639: INFO: Found 0 / 1
  E0510 06:13:51.225454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:13:51.640: INFO: Selector matched 1 pods for map[app:agnhost]
  May 10 06:13:51.640: INFO: Found 1 / 1
  May 10 06:13:51.640: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 10 06:13:51.641: INFO: Selector matched 1 pods for map[app:agnhost]
  May 10 06:13:51.641: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 10 06:13:51.641: INFO: wait on agnhost-primary startup in kubectl-2394 
  May 10 06:13:51.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2394 logs agnhost-primary-8ck76 agnhost-primary'
  May 10 06:13:51.708: INFO: stderr: ""
  May 10 06:13:51.708: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 05/10/23 06:13:51.708
  May 10 06:13:51.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2394 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  May 10 06:13:51.785: INFO: stderr: ""
  May 10 06:13:51.785: INFO: stdout: "service/rm2 exposed\n"
  May 10 06:13:51.795: INFO: Service rm2 in namespace kubectl-2394 found.
  E0510 06:13:52.226160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:53.226502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 05/10/23 06:13:53.8
  May 10 06:13:53.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-2394 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  May 10 06:13:53.878: INFO: stderr: ""
  May 10 06:13:53.878: INFO: stdout: "service/rm3 exposed\n"
  May 10 06:13:53.893: INFO: Service rm3 in namespace kubectl-2394 found.
  E0510 06:13:54.227130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:55.227230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:13:55.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2394" for this suite. @ 05/10/23 06:13:55.899
• [6.748 seconds]
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 05/10/23 06:13:55.905
  May 10 06:13:55.905: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename disruption @ 05/10/23 06:13:55.906
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:13:55.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:13:55.924
  STEP: Creating a pdb that targets all three pods in a test replica set @ 05/10/23 06:13:55.926
  STEP: Waiting for the pdb to be processed @ 05/10/23 06:13:55.932
  E0510 06:13:56.228092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:57.228681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 05/10/23 06:13:58.026
  STEP: Waiting for all pods to be running @ 05/10/23 06:13:58.026
  May 10 06:13:58.028: INFO: pods: 0 < 3
  E0510 06:13:58.229301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:13:59.229500      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:14:00.032: INFO: running pods: 1 < 3
  E0510 06:14:00.230163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:01.230468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/10/23 06:14:02.032
  STEP: Updating the pdb to allow a pod to be evicted @ 05/10/23 06:14:02.039
  STEP: Waiting for the pdb to be processed @ 05/10/23 06:14:02.051
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/10/23 06:14:02.057
  STEP: Waiting for all pods to be running @ 05/10/23 06:14:02.057
  STEP: Waiting for the pdb to observed all healthy pods @ 05/10/23 06:14:02.059
  STEP: Patching the pdb to disallow a pod to be evicted @ 05/10/23 06:14:02.09
  E0510 06:14:02.231426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 05/10/23 06:14:02.651
  STEP: Waiting for all pods to be running @ 05/10/23 06:14:02.705
  May 10 06:14:02.707: INFO: running pods: 2 < 3
  E0510 06:14:03.232386      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:04.232509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:14:04.710: INFO: running pods: 2 < 3
  E0510 06:14:05.232535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:06.232750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/10/23 06:14:06.71
  STEP: Deleting the pdb to allow a pod to be evicted @ 05/10/23 06:14:06.715
  STEP: Waiting for the pdb to be deleted @ 05/10/23 06:14:06.719
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/10/23 06:14:06.72
  STEP: Waiting for all pods to be running @ 05/10/23 06:14:06.72
  May 10 06:14:06.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2142" for this suite. @ 05/10/23 06:14:06.741
• [10.853 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 05/10/23 06:14:06.758
  May 10 06:14:06.758: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 06:14:06.759
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:14:06.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:14:06.776
  STEP: creating a Service @ 05/10/23 06:14:06.78
  STEP: watching for the Service to be added @ 05/10/23 06:14:06.791
  May 10 06:14:06.792: INFO: Found Service test-service-xsdw9 in namespace services-3736 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  May 10 06:14:06.792: INFO: Service test-service-xsdw9 created
  STEP: Getting /status @ 05/10/23 06:14:06.792
  May 10 06:14:06.794: INFO: Service test-service-xsdw9 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 05/10/23 06:14:06.794
  STEP: watching for the Service to be patched @ 05/10/23 06:14:06.798
  May 10 06:14:06.799: INFO: observed Service test-service-xsdw9 in namespace services-3736 with annotations: map[] & LoadBalancer: {[]}
  May 10 06:14:06.799: INFO: Found Service test-service-xsdw9 in namespace services-3736 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  May 10 06:14:06.799: INFO: Service test-service-xsdw9 has service status patched
  STEP: updating the ServiceStatus @ 05/10/23 06:14:06.799
  May 10 06:14:06.804: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 05/10/23 06:14:06.804
  May 10 06:14:06.805: INFO: Observed Service test-service-xsdw9 in namespace services-3736 with annotations: map[] & Conditions: {[]}
  May 10 06:14:06.805: INFO: Observed event: &Service{ObjectMeta:{test-service-xsdw9  services-3736  93d1709f-59e3-4b94-8594-389b313421b0 1086073 0 2023-05-10 06:14:06 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-10 06:14:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-10 06:14:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.0.77,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.0.77],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  May 10 06:14:06.805: INFO: Found Service test-service-xsdw9 in namespace services-3736 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 10 06:14:06.805: INFO: Service test-service-xsdw9 has service status updated
  STEP: patching the service @ 05/10/23 06:14:06.805
  STEP: watching for the Service to be patched @ 05/10/23 06:14:06.814
  May 10 06:14:06.815: INFO: observed Service test-service-xsdw9 in namespace services-3736 with labels: map[test-service-static:true]
  May 10 06:14:06.815: INFO: observed Service test-service-xsdw9 in namespace services-3736 with labels: map[test-service-static:true]
  May 10 06:14:06.815: INFO: observed Service test-service-xsdw9 in namespace services-3736 with labels: map[test-service-static:true]
  May 10 06:14:06.815: INFO: Found Service test-service-xsdw9 in namespace services-3736 with labels: map[test-service:patched test-service-static:true]
  May 10 06:14:06.815: INFO: Service test-service-xsdw9 patched
  STEP: deleting the service @ 05/10/23 06:14:06.815
  STEP: watching for the Service to be deleted @ 05/10/23 06:14:06.823
  May 10 06:14:06.823: INFO: Observed event: ADDED
  May 10 06:14:06.823: INFO: Observed event: MODIFIED
  May 10 06:14:06.823: INFO: Observed event: MODIFIED
  May 10 06:14:06.824: INFO: Observed event: MODIFIED
  May 10 06:14:06.824: INFO: Found Service test-service-xsdw9 in namespace services-3736 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  May 10 06:14:06.824: INFO: Service test-service-xsdw9 deleted
  May 10 06:14:06.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3736" for this suite. @ 05/10/23 06:14:06.826
• [0.072 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 05/10/23 06:14:06.83
  May 10 06:14:06.830: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename watch @ 05/10/23 06:14:06.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:14:06.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:14:06.842
  STEP: creating a new configmap @ 05/10/23 06:14:06.846
  STEP: modifying the configmap once @ 05/10/23 06:14:06.85
  STEP: modifying the configmap a second time @ 05/10/23 06:14:06.856
  STEP: deleting the configmap @ 05/10/23 06:14:06.871
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 05/10/23 06:14:06.875
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 05/10/23 06:14:06.876
  May 10 06:14:06.876: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5973  52704de4-6383-4a20-9be2-35004eb133ab 1086084 0 2023-05-10 06:14:06 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-10 06:14:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 06:14:06.876: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5973  52704de4-6383-4a20-9be2-35004eb133ab 1086085 0 2023-05-10 06:14:06 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-10 06:14:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 10 06:14:06.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5973" for this suite. @ 05/10/23 06:14:06.879
• [0.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 05/10/23 06:14:06.885
  May 10 06:14:06.885: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename dns @ 05/10/23 06:14:06.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:14:06.897
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:14:06.9
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/10/23 06:14:06.902
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/10/23 06:14:06.902
  STEP: creating a pod to probe DNS @ 05/10/23 06:14:06.902
  STEP: submitting the pod to kubernetes @ 05/10/23 06:14:06.902
  E0510 06:14:07.233409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:08.233827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:09.234799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:10.234901      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/10/23 06:14:10.921
  STEP: looking for the results for each expected name from probers @ 05/10/23 06:14:10.923
  May 10 06:14:10.932: INFO: DNS probes using dns-939/dns-test-7ab69d8b-e7da-4a05-9335-5148838a88e5 succeeded

  May 10 06:14:10.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 06:14:10.934
  STEP: Destroying namespace "dns-939" for this suite. @ 05/10/23 06:14:10.949
• [4.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 05/10/23 06:14:10.954
  May 10 06:14:10.954: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-probe @ 05/10/23 06:14:10.955
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:14:10.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:14:10.975
  STEP: Creating pod test-grpc-40691a5a-a9af-4cf7-a30a-c17c04fa24ca in namespace container-probe-1397 @ 05/10/23 06:14:10.977
  E0510 06:14:11.235495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:12.236086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:13.236720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:14.236977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:14:14.991: INFO: Started pod test-grpc-40691a5a-a9af-4cf7-a30a-c17c04fa24ca in namespace container-probe-1397
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/10/23 06:14:14.991
  May 10 06:14:14.993: INFO: Initial restart count of pod test-grpc-40691a5a-a9af-4cf7-a30a-c17c04fa24ca is 0
  E0510 06:14:15.237707      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:16.237935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:17.238665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:18.238883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:19.239473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:20.239696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:21.240777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:22.240913      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:23.241834      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:24.242073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:25.242124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:26.242380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:27.242508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:28.242811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:29.242835      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:30.242967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:31.243638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:32.243952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:33.244874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:34.245038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:35.246024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:36.246218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:37.246891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:38.247190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:39.247927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:40.248155      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:41.248654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:42.248837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:43.249328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:44.249537      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:45.249746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:46.249990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:47.250593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:48.250735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:49.250938      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:50.251161      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:51.252147      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:52.252310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:53.253239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:54.253387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:55.254331      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:56.254542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:57.254665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:58.254855      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:14:59.255251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:00.256145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:01.256800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:02.257252      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:03.258004      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:04.258233      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:05.258678      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:06.258906      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:07.259298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:08.259532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:09.260377      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:10.260599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:11.261378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:12.261799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:13.262242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:14.262441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:15.262873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:16.263145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:17.264032      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:18.264264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:19.265020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:20.265240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:21.265443      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:22.265868      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:23.266680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:24.267184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:25.268012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:26.268246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:27.269253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:28.269364      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:15:29.117: INFO: Restart count of pod container-probe-1397/test-grpc-40691a5a-a9af-4cf7-a30a-c17c04fa24ca is now 1 (1m14.123689551s elapsed)
  May 10 06:15:29.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 06:15:29.12
  STEP: Destroying namespace "container-probe-1397" for this suite. @ 05/10/23 06:15:29.133
• [78.191 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 05/10/23 06:15:29.146
  May 10 06:15:29.146: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubelet-test @ 05/10/23 06:15:29.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:15:29.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:15:29.16
  E0510 06:15:29.269605      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:30.269962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:31.270525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:32.271067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:15:33.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6215" for this suite. @ 05/10/23 06:15:33.199
• [4.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 05/10/23 06:15:33.205
  May 10 06:15:33.205: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 06:15:33.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:15:33.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:15:33.222
  STEP: validating api versions @ 05/10/23 06:15:33.224
  May 10 06:15:33.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-8750 api-versions'
  E0510 06:15:33.271970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:15:33.286: INFO: stderr: ""
  May 10 06:15:33.286: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperator.tigera.io/v1\npolicy/v1\nprojectcalico.org/v3\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  May 10 06:15:33.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8750" for this suite. @ 05/10/23 06:15:33.289
• [0.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 05/10/23 06:15:33.297
  May 10 06:15:33.297: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:15:33.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:15:33.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:15:33.31
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 06:15:33.313
  E0510 06:15:34.272497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:35.272831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:36.273615      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:37.274088      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:38.274069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:39.274062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:15:39.337
  May 10 06:15:39.339: INFO: Trying to get logs from node node-02 pod downwardapi-volume-b0a7ab7b-ee3b-4d3f-b934-87c8a53a2dc1 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 06:15:39.342
  May 10 06:15:39.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2215" for this suite. @ 05/10/23 06:15:39.358
• [6.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 05/10/23 06:15:39.366
  May 10 06:15:39.366: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 06:15:39.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:15:39.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:15:39.378
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/10/23 06:15:39.38
  May 10 06:15:39.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-854 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  May 10 06:15:39.464: INFO: stderr: ""
  May 10 06:15:39.464: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/10/23 06:15:39.464
  May 10 06:15:39.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-854 delete pods e2e-test-httpd-pod'
  E0510 06:15:40.274428      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:41.274927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:42.275238      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:43.275451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:15:43.355: INFO: stderr: ""
  May 10 06:15:43.355: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 10 06:15:43.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-854" for this suite. @ 05/10/23 06:15:43.357
• [3.995 seconds]
------------------------------
S
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 05/10/23 06:15:43.361
  May 10 06:15:43.361: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename prestop @ 05/10/23 06:15:43.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:15:43.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:15:43.376
  STEP: Creating server pod server in namespace prestop-4246 @ 05/10/23 06:15:43.378
  STEP: Waiting for pods to come up. @ 05/10/23 06:15:43.386
  E0510 06:15:44.275792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:45.276297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-4246 @ 05/10/23 06:15:45.392
  E0510 06:15:46.276311      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:47.276932      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 05/10/23 06:15:47.407
  E0510 06:15:48.277019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:49.277374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:50.277570      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:51.277786      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:52.278184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:15:52.417: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  May 10 06:15:52.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 05/10/23 06:15:52.42
  STEP: Destroying namespace "prestop-4246" for this suite. @ 05/10/23 06:15:52.434
• [9.082 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 05/10/23 06:15:52.444
  May 10 06:15:52.444: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:15:52.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:15:52.454
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:15:52.456
  STEP: Creating the pod @ 05/10/23 06:15:52.458
  E0510 06:15:53.278645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:54.278926      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:15:55.003: INFO: Successfully updated pod "labelsupdate0579a521-69eb-4a37-9c40-00c6288fd608"
  E0510 06:15:55.279183      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:56.279393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:15:57.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1451" for this suite. @ 05/10/23 06:15:57.017
• [4.578 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 05/10/23 06:15:57.022
  May 10 06:15:57.022: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pods @ 05/10/23 06:15:57.022
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:15:57.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:15:57.033
  STEP: creating the pod @ 05/10/23 06:15:57.035
  STEP: submitting the pod to kubernetes @ 05/10/23 06:15:57.035
  STEP: verifying QOS class is set on the pod @ 05/10/23 06:15:57.041
  May 10 06:15:57.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7376" for this suite. @ 05/10/23 06:15:57.047
• [0.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 05/10/23 06:15:57.056
  May 10 06:15:57.056: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 06:15:57.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:15:57.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:15:57.072
  STEP: creating secret secrets-7961/secret-test-31660324-6743-4deb-b2b0-aba27acf29f2 @ 05/10/23 06:15:57.074
  STEP: Creating a pod to test consume secrets @ 05/10/23 06:15:57.078
  E0510 06:15:57.280351      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:58.280774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:15:59.281672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:00.281955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:16:01.096
  May 10 06:16:01.098: INFO: Trying to get logs from node node-02 pod pod-configmaps-07e246a1-e405-4bad-9bb1-24ea45e9b338 container env-test: <nil>
  STEP: delete the pod @ 05/10/23 06:16:01.102
  May 10 06:16:01.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7961" for this suite. @ 05/10/23 06:16:01.117
• [4.068 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 05/10/23 06:16:01.124
  May 10 06:16:01.124: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/10/23 06:16:01.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:16:01.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:16:01.136
  May 10 06:16:01.139: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 06:16:01.282119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:02.282285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 05/10/23 06:16:02.609
  May 10 06:16:02.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 --namespace=crd-publish-openapi-4431 create -f -'
  E0510 06:16:03.283308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:03.594: INFO: stderr: ""
  May 10 06:16:03.594: INFO: stdout: "e2e-test-crd-publish-openapi-3702-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 10 06:16:03.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 --namespace=crd-publish-openapi-4431 delete e2e-test-crd-publish-openapi-3702-crds test-foo'
  May 10 06:16:03.655: INFO: stderr: ""
  May 10 06:16:03.655: INFO: stdout: "e2e-test-crd-publish-openapi-3702-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  May 10 06:16:03.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 --namespace=crd-publish-openapi-4431 apply -f -'
  May 10 06:16:03.961: INFO: stderr: ""
  May 10 06:16:03.961: INFO: stdout: "e2e-test-crd-publish-openapi-3702-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 10 06:16:03.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 --namespace=crd-publish-openapi-4431 delete e2e-test-crd-publish-openapi-3702-crds test-foo'
  May 10 06:16:04.022: INFO: stderr: ""
  May 10 06:16:04.022: INFO: stdout: "e2e-test-crd-publish-openapi-3702-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 05/10/23 06:16:04.022
  May 10 06:16:04.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 --namespace=crd-publish-openapi-4431 create -f -'
  May 10 06:16:04.279: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 05/10/23 06:16:04.28
  May 10 06:16:04.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 --namespace=crd-publish-openapi-4431 create -f -'
  E0510 06:16:04.283618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:04.544: INFO: rc: 1
  May 10 06:16:04.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 --namespace=crd-publish-openapi-4431 apply -f -'
  May 10 06:16:04.832: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 05/10/23 06:16:04.832
  May 10 06:16:04.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 --namespace=crd-publish-openapi-4431 create -f -'
  May 10 06:16:05.113: INFO: rc: 1
  May 10 06:16:05.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 --namespace=crd-publish-openapi-4431 apply -f -'
  E0510 06:16:05.283759      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:05.379: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 05/10/23 06:16:05.379
  May 10 06:16:05.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 explain e2e-test-crd-publish-openapi-3702-crds'
  May 10 06:16:05.627: INFO: stderr: ""
  May 10 06:16:05.627: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3702-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 05/10/23 06:16:05.628
  May 10 06:16:05.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 explain e2e-test-crd-publish-openapi-3702-crds.metadata'
  May 10 06:16:05.898: INFO: stderr: ""
  May 10 06:16:05.898: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3702-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  May 10 06:16:05.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 explain e2e-test-crd-publish-openapi-3702-crds.spec'
  May 10 06:16:06.153: INFO: stderr: ""
  May 10 06:16:06.153: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3702-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  May 10 06:16:06.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 explain e2e-test-crd-publish-openapi-3702-crds.spec.bars'
  E0510 06:16:06.284659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:06.410: INFO: stderr: ""
  May 10 06:16:06.410: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3702-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 05/10/23 06:16:06.41
  May 10 06:16:06.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-4431 explain e2e-test-crd-publish-openapi-3702-crds.spec.bars2'
  May 10 06:16:06.670: INFO: rc: 1
  E0510 06:16:07.285592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:08.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4431" for this suite. @ 05/10/23 06:16:08.111
• [6.992 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 05/10/23 06:16:08.116
  May 10 06:16:08.116: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-probe @ 05/10/23 06:16:08.117
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:16:08.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:16:08.134
  STEP: Creating pod liveness-b61d4530-e24a-40b9-a851-ece056737a9a in namespace container-probe-4182 @ 05/10/23 06:16:08.136
  E0510 06:16:08.286623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:09.286889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:10.286918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:11.287130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:12.152: INFO: Started pod liveness-b61d4530-e24a-40b9-a851-ece056737a9a in namespace container-probe-4182
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/10/23 06:16:12.152
  May 10 06:16:12.154: INFO: Initial restart count of pod liveness-b61d4530-e24a-40b9-a851-ece056737a9a is 0
  E0510 06:16:12.287577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:13.287903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:14.288103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:15.288303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:16.288862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:17.289040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:18.289597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:19.290593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:20.290949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:21.291174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:22.292294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:23.292536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:24.292906      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:25.293196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:26.293979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:27.294315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:28.295154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:29.295448      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:30.184: INFO: Restart count of pod container-probe-4182/liveness-b61d4530-e24a-40b9-a851-ece056737a9a is now 1 (18.029779502s elapsed)
  May 10 06:16:30.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 06:16:30.186
  STEP: Destroying namespace "container-probe-4182" for this suite. @ 05/10/23 06:16:30.196
• [22.083 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 05/10/23 06:16:30.2
  May 10 06:16:30.200: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 06:16:30.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:16:30.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:16:30.215
  STEP: Starting the proxy @ 05/10/23 06:16:30.217
  May 10 06:16:30.218: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-3246 proxy --unix-socket=/tmp/kubectl-proxy-unix2962806995/test'
  STEP: retrieving proxy /api/ output @ 05/10/23 06:16:30.269
  May 10 06:16:30.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3246" for this suite. @ 05/10/23 06:16:30.273
• [0.079 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 05/10/23 06:16:30.279
  May 10 06:16:30.279: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename deployment @ 05/10/23 06:16:30.28
  E0510 06:16:30.295392      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:16:30.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:16:30.299
  May 10 06:16:30.307: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0510 06:16:31.295732      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:32.296322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:33.296656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:34.297065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:35.297473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:35.354: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/10/23 06:16:35.354
  May 10 06:16:35.354: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0510 06:16:36.298251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:37.298685      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:37.357: INFO: Creating deployment "test-rollover-deployment"
  May 10 06:16:37.364: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0510 06:16:38.298794      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:39.298934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:39.370: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  May 10 06:16:39.373: INFO: Ensure that both replica sets have 1 created replica
  May 10 06:16:39.377: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  May 10 06:16:39.385: INFO: Updating deployment test-rollover-deployment
  May 10 06:16:39.385: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0510 06:16:40.299071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:41.299268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:41.390: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  May 10 06:16:41.393: INFO: Make sure deployment "test-rollover-deployment" is complete
  May 10 06:16:41.396: INFO: all replica sets need to contain the pod-template-hash label
  May 10 06:16:41.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 6, 16, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 06:16:42.300327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:43.300583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:43.402: INFO: all replica sets need to contain the pod-template-hash label
  May 10 06:16:43.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 6, 16, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 06:16:44.300598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:45.300754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:45.401: INFO: all replica sets need to contain the pod-template-hash label
  May 10 06:16:45.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 6, 16, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 06:16:46.301304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:47.302019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:47.400: INFO: all replica sets need to contain the pod-template-hash label
  May 10 06:16:47.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 6, 16, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 06:16:48.302421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:49.302640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:49.726: INFO: all replica sets need to contain the pod-template-hash label
  May 10 06:16:49.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 6, 16, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 6, 16, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0510 06:16:50.303223      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:51.303329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:16:51.401: INFO: 
  May 10 06:16:51.401: INFO: Ensure that both old replica sets have no replicas
  May 10 06:16:51.405: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-5601  1fbbfe4b-5b39-4dea-9f6e-7cf2772dca7a 1087072 2 2023-05-10 06:16:37 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-10 06:16:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:16:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dadab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-10 06:16:37 +0000 UTC,LastTransitionTime:2023-05-10 06:16:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-05-10 06:16:50 +0000 UTC,LastTransitionTime:2023-05-10 06:16:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 10 06:16:51.407: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-5601  5749efdf-c8f5-4501-97be-929a13971bf1 1087062 2 2023-05-10 06:16:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1fbbfe4b-5b39-4dea-9f6e-7cf2772dca7a 0xc0046c2257 0xc0046c2258}] [] [{kube-controller-manager Update apps/v1 2023-05-10 06:16:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1fbbfe4b-5b39-4dea-9f6e-7cf2772dca7a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:16:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046c2318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 10 06:16:51.407: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  May 10 06:16:51.407: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5601  25c4892f-e909-4e93-a500-a893282bb3c8 1087071 2 2023-05-10 06:16:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1fbbfe4b-5b39-4dea-9f6e-7cf2772dca7a 0xc001df3fd7 0xc001df3fd8}] [] [{e2e.test Update apps/v1 2023-05-10 06:16:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:16:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1fbbfe4b-5b39-4dea-9f6e-7cf2772dca7a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:16:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046c21b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 10 06:16:51.407: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-5601  f9b1b0f3-9583-46e3-afdf-5107cbd35142 1087013 2 2023-05-10 06:16:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1fbbfe4b-5b39-4dea-9f6e-7cf2772dca7a 0xc0046c2387 0xc0046c2388}] [] [{kube-controller-manager Update apps/v1 2023-05-10 06:16:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1fbbfe4b-5b39-4dea-9f6e-7cf2772dca7a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:16:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046c2448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 10 06:16:51.409: INFO: Pod "test-rollover-deployment-57777854c9-gqp5l" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-gqp5l test-rollover-deployment-57777854c9- deployment-5601  9cdaecc5-da4a-4ba7-a081-77170397a363 1087031 0 2023-05-10 06:16:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[cni.projectcalico.org/containerID:1dbfc1b0bf15f55cbf8c66d3a67b59dacfd22c91d5c724d76ba620ac3ca36a2d cni.projectcalico.org/podIP:100.114.252.154/32 cni.projectcalico.org/podIPs:100.114.252.154/32] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 5749efdf-c8f5-4501-97be-929a13971bf1 0xc003e70227 0xc003e70228}] [] [{calico Update v1 2023-05-10 06:16:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-10 06:16:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5749efdf-c8f5-4501-97be-929a13971bf1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:16:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.114.252.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pmp65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pmp65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:16:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:16:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:16:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:100.114.252.154,StartTime:2023-05-10 06:16:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 06:16:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:containerd://738b951f271438efe56bd2f8bed4c40e80a7f997a828d2412b8824594710572c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.114.252.154,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:16:51.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5601" for this suite. @ 05/10/23 06:16:51.411
• [21.136 seconds]
------------------------------
SS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 05/10/23 06:16:51.415
  May 10 06:16:51.415: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename disruption @ 05/10/23 06:16:51.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:16:51.424
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:16:51.427
  STEP: Creating a kubernetes client @ 05/10/23 06:16:51.429
  May 10 06:16:51.429: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename disruption-2 @ 05/10/23 06:16:51.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:16:51.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:16:51.443
  STEP: Waiting for the pdb to be processed @ 05/10/23 06:16:51.448
  E0510 06:16:52.303472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:53.303728      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 05/10/23 06:16:53.458
  E0510 06:16:54.303855      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:55.304083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 05/10/23 06:16:55.468
  E0510 06:16:56.304374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:16:57.305059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 05/10/23 06:16:57.474
  STEP: listing a collection of PDBs in namespace disruption-700 @ 05/10/23 06:16:57.476
  STEP: deleting a collection of PDBs @ 05/10/23 06:16:57.478
  STEP: Waiting for the PDB collection to be deleted @ 05/10/23 06:16:57.486
  May 10 06:16:57.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 10 06:16:57.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-4597" for this suite. @ 05/10/23 06:16:57.492
  STEP: Destroying namespace "disruption-700" for this suite. @ 05/10/23 06:16:57.496
• [6.089 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 05/10/23 06:16:57.504
  May 10 06:16:57.504: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename webhook @ 05/10/23 06:16:57.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:16:57.515
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:16:57.518
  STEP: Setting up server cert @ 05/10/23 06:16:57.535
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/10/23 06:16:58.302
  E0510 06:16:58.306006      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook pod @ 05/10/23 06:16:58.308
  STEP: Wait for the deployment to be ready @ 05/10/23 06:16:58.319
  May 10 06:16:58.326: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0510 06:16:59.306178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:00.306415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 06:17:00.332
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 06:17:00.339
  E0510 06:17:01.307100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:17:01.339: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 10 06:17:01.342: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9343-crds.webhook.example.com via the AdmissionRegistration API @ 05/10/23 06:17:01.853
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/10/23 06:17:01.869
  E0510 06:17:02.307239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:03.307429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:17:03.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0510 06:17:04.308487      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-694" for this suite. @ 05/10/23 06:17:04.432
  STEP: Destroying namespace "webhook-markers-7777" for this suite. @ 05/10/23 06:17:04.436
• [6.935 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 05/10/23 06:17:04.44
  May 10 06:17:04.440: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename tables @ 05/10/23 06:17:04.441
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:17:04.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:17:04.455
  May 10 06:17:04.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-7443" for this suite. @ 05/10/23 06:17:04.46
• [0.023 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 05/10/23 06:17:04.464
  May 10 06:17:04.464: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pods @ 05/10/23 06:17:04.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:17:04.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:17:04.475
  STEP: creating the pod @ 05/10/23 06:17:04.479
  STEP: submitting the pod to kubernetes @ 05/10/23 06:17:04.48
  W0510 06:17:04.484711      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0510 06:17:05.309572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:06.310159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:07.310585      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:08.311480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 05/10/23 06:17:08.495
  STEP: updating the pod @ 05/10/23 06:17:08.496
  May 10 06:17:09.193: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e564122f-24c3-4868-a3b9-5ce121a06c24"
  E0510 06:17:09.312304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:10.312554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:17:11.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3127" for this suite. @ 05/10/23 06:17:11.2
• [6.740 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 05/10/23 06:17:11.204
  May 10 06:17:11.204: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename dns @ 05/10/23 06:17:11.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:17:11.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:17:11.22
  STEP: Creating a test externalName service @ 05/10/23 06:17:11.222
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1670.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local; sleep 1; done
   @ 05/10/23 06:17:11.231
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1670.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local; sleep 1; done
   @ 05/10/23 06:17:11.231
  STEP: creating a pod to probe DNS @ 05/10/23 06:17:11.231
  STEP: submitting the pod to kubernetes @ 05/10/23 06:17:11.231
  E0510 06:17:11.313129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:12.313618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:13.314580      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:14.314809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/10/23 06:17:15.251
  STEP: looking for the results for each expected name from probers @ 05/10/23 06:17:15.253
  May 10 06:17:15.258: INFO: DNS probes using dns-test-33561a57-89c9-4c61-8b0e-d4fc369a0950 succeeded

  STEP: changing the externalName to bar.example.com @ 05/10/23 06:17:15.258
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1670.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local; sleep 1; done
   @ 05/10/23 06:17:15.264
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1670.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local; sleep 1; done
   @ 05/10/23 06:17:15.264
  STEP: creating a second pod to probe DNS @ 05/10/23 06:17:15.264
  STEP: submitting the pod to kubernetes @ 05/10/23 06:17:15.264
  E0510 06:17:15.315845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:16.316031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:17.316920      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:18.317325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/10/23 06:17:19.281
  STEP: looking for the results for each expected name from probers @ 05/10/23 06:17:19.283
  May 10 06:17:19.285: INFO: File wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local from pod  dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 10 06:17:19.287: INFO: File jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local from pod  dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 10 06:17:19.287: INFO: Lookups using dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe failed for: [wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local]

  E0510 06:17:19.317482      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:20.317716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:21.317818      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:22.318146      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:23.318357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:17:24.289: INFO: File wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local from pod  dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 10 06:17:24.291: INFO: File jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local from pod  dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 10 06:17:24.291: INFO: Lookups using dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe failed for: [wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local]

  E0510 06:17:24.319059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:25.319204      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:26.319382      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:27.319937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:28.320062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:17:29.295: INFO: File wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local from pod  dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 10 06:17:29.297: INFO: File jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local from pod  dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 10 06:17:29.297: INFO: Lookups using dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe failed for: [wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local]

  E0510 06:17:29.320729      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:30.320972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:31.321225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:32.321707      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:33.321952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:17:34.291: INFO: File wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local from pod  dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 10 06:17:34.293: INFO: File jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local from pod  dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 10 06:17:34.293: INFO: Lookups using dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe failed for: [wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local]

  E0510 06:17:34.322551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:35.322884      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:36.323007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:37.323793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:38.324007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:17:39.290: INFO: File wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local from pod  dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 10 06:17:39.293: INFO: File jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local from pod  dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 10 06:17:39.293: INFO: Lookups using dns-1670/dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe failed for: [wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local]

  E0510 06:17:39.324481      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:40.324703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:41.325083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:42.325361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:43.325616      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:17:44.292: INFO: DNS probes using dns-test-99ab4502-e4a7-472e-bfea-ccb40b99f5fe succeeded

  STEP: changing the service to type=ClusterIP @ 05/10/23 06:17:44.292
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1670.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1670.svc.cluster.local; sleep 1; done
   @ 05/10/23 06:17:44.301
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1670.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1670.svc.cluster.local; sleep 1; done
   @ 05/10/23 06:17:44.301
  STEP: creating a third pod to probe DNS @ 05/10/23 06:17:44.301
  STEP: submitting the pod to kubernetes @ 05/10/23 06:17:44.303
  E0510 06:17:44.327252      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:45.326654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:46.327547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:47.328278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/10/23 06:17:48.319
  STEP: looking for the results for each expected name from probers @ 05/10/23 06:17:48.321
  May 10 06:17:48.325: INFO: DNS probes using dns-test-5cd1b08c-a742-4bd3-8eaa-0496bfeb1ce7 succeeded

  May 10 06:17:48.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 06:17:48.328
  E0510 06:17:48.328264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod @ 05/10/23 06:17:48.339
  STEP: deleting the pod @ 05/10/23 06:17:48.348
  STEP: deleting the test externalName service @ 05/10/23 06:17:48.361
  STEP: Destroying namespace "dns-1670" for this suite. @ 05/10/23 06:17:48.374
• [37.172 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 05/10/23 06:17:48.377
  May 10 06:17:48.377: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename var-expansion @ 05/10/23 06:17:48.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:17:48.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:17:48.394
  STEP: Creating a pod to test substitution in volume subpath @ 05/10/23 06:17:48.395
  E0510 06:17:49.328647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:50.328734      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:51.329447      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:52.329649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:53.329726      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:54.329857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:17:54.415
  May 10 06:17:54.417: INFO: Trying to get logs from node node-02 pod var-expansion-01a9fc9e-4df7-4236-8479-faa1be0da3c3 container dapi-container: <nil>
  STEP: delete the pod @ 05/10/23 06:17:54.431
  May 10 06:17:54.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6252" for this suite. @ 05/10/23 06:17:54.444
• [6.070 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 05/10/23 06:17:54.448
  May 10 06:17:54.448: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 06:17:54.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:17:54.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:17:54.465
  STEP: Creating a pod to test downward api env vars @ 05/10/23 06:17:54.467
  E0510 06:17:55.330812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:56.331006      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:57.331169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:17:58.331411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:17:58.484
  May 10 06:17:58.486: INFO: Trying to get logs from node node-02 pod downward-api-79cbdab8-cdca-440e-b2ee-a0c0488b0dcf container dapi-container: <nil>
  STEP: delete the pod @ 05/10/23 06:17:58.491
  May 10 06:17:58.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2310" for this suite. @ 05/10/23 06:17:58.508
• [4.067 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 05/10/23 06:17:58.515
  May 10 06:17:58.515: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename endpointslice @ 05/10/23 06:17:58.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:17:58.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:17:58.527
  E0510 06:17:59.331451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:00.332257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:18:00.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2984" for this suite. @ 05/10/23 06:18:00.568
• [2.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 05/10/23 06:18:00.573
  May 10 06:18:00.573: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 06:18:00.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:18:00.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:18:00.584
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/10/23 06:18:00.587
  E0510 06:18:01.333288      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:02.333976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:03.334442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:04.334643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:18:04.599
  May 10 06:18:04.601: INFO: Trying to get logs from node node-02 pod pod-982ee162-8623-4431-a752-3f2074e194da container test-container: <nil>
  STEP: delete the pod @ 05/10/23 06:18:04.605
  May 10 06:18:04.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3033" for this suite. @ 05/10/23 06:18:04.625
• [4.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 05/10/23 06:18:04.63
  May 10 06:18:04.630: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 06:18:04.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:18:04.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:18:04.641
  STEP: create deployment with httpd image @ 05/10/23 06:18:04.643
  May 10 06:18:04.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-1363 create -f -'
  E0510 06:18:05.334857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:18:05.763: INFO: stderr: ""
  May 10 06:18:05.763: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 05/10/23 06:18:05.763
  May 10 06:18:05.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-1363 diff -f -'
  May 10 06:18:06.016: INFO: rc: 1
  May 10 06:18:06.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-1363 delete -f -'
  May 10 06:18:06.073: INFO: stderr: ""
  May 10 06:18:06.073: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  May 10 06:18:06.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1363" for this suite. @ 05/10/23 06:18:06.076
• [1.449 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 05/10/23 06:18:06.081
  May 10 06:18:06.081: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 06:18:06.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:18:06.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:18:06.099
  May 10 06:18:06.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1924" for this suite. @ 05/10/23 06:18:06.129
• [0.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 05/10/23 06:18:06.134
  May 10 06:18:06.134: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename job @ 05/10/23 06:18:06.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:18:06.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:18:06.145
  STEP: Creating a job @ 05/10/23 06:18:06.148
  STEP: Ensuring job reaches completions @ 05/10/23 06:18:06.154
  E0510 06:18:06.335649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:07.336644      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:08.337654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:09.337709      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:10.338157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:11.338360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:12.338488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:13.338637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:14.339680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:15.339805      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:16.339953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:17.340079      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:18.340562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:19.340694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:18:20.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5519" for this suite. @ 05/10/23 06:18:20.158
• [14.028 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 05/10/23 06:18:20.163
  May 10 06:18:20.163: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 06:18:20.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:18:20.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:18:20.18
  STEP: Creating a pod to test downward api env vars @ 05/10/23 06:18:20.182
  E0510 06:18:20.341053      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:21.341384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:22.341672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:23.341812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:18:24.194
  May 10 06:18:24.196: INFO: Trying to get logs from node node-02 pod downward-api-a2023914-1d5b-4026-ab9a-78b6a552dc02 container dapi-container: <nil>
  STEP: delete the pod @ 05/10/23 06:18:24.201
  May 10 06:18:24.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9445" for this suite. @ 05/10/23 06:18:24.218
• [4.059 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 05/10/23 06:18:24.222
  May 10 06:18:24.222: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:18:24.223
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:18:24.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:18:24.239
  STEP: Creating configMap with name projected-configmap-test-volume-0fa8a99d-0f76-47e6-b06a-951bbac4ae03 @ 05/10/23 06:18:24.241
  STEP: Creating a pod to test consume configMaps @ 05/10/23 06:18:24.244
  E0510 06:18:24.342485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:25.342695      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:26.342876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:27.343351      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:18:28.262
  May 10 06:18:28.264: INFO: Trying to get logs from node node-02 pod pod-projected-configmaps-d34ca23c-8442-445f-8df4-9e195ffe96cd container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 06:18:28.269
  May 10 06:18:28.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6997" for this suite. @ 05/10/23 06:18:28.286
• [4.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 05/10/23 06:18:28.291
  May 10 06:18:28.291: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 06:18:28.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:18:28.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:18:28.304
  STEP: Creating configMap with name configmap-test-volume-c902fe42-f259-4e3f-a17a-ce8170a9b2b5 @ 05/10/23 06:18:28.306
  STEP: Creating a pod to test consume configMaps @ 05/10/23 06:18:28.311
  E0510 06:18:28.344286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:29.344626      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:30.345328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:31.345507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:18:32.326
  May 10 06:18:32.328: INFO: Trying to get logs from node node-02 pod pod-configmaps-a7c78736-f495-4ee0-b92c-92078f2eb970 container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 06:18:32.332
  E0510 06:18:32.345991      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:18:32.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9521" for this suite. @ 05/10/23 06:18:32.348
• [4.061 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 05/10/23 06:18:32.353
  May 10 06:18:32.353: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename runtimeclass @ 05/10/23 06:18:32.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:18:32.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:18:32.366
  STEP: Deleting RuntimeClass runtimeclass-9148-delete-me @ 05/10/23 06:18:32.371
  STEP: Waiting for the RuntimeClass to disappear @ 05/10/23 06:18:32.374
  May 10 06:18:32.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9148" for this suite. @ 05/10/23 06:18:32.381
• [0.032 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 05/10/23 06:18:32.385
  May 10 06:18:32.385: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename endpointslice @ 05/10/23 06:18:32.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:18:32.398
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:18:32.4
  May 10 06:18:32.410: INFO: Endpoints addresses: [192.168.60.152] , ports: [6443]
  May 10 06:18:32.410: INFO: EndpointSlices addresses: [192.168.60.152] , ports: [6443]
  May 10 06:18:32.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7455" for this suite. @ 05/10/23 06:18:32.412
• [0.030 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 05/10/23 06:18:32.421
  May 10 06:18:32.421: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/10/23 06:18:32.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:18:32.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:18:32.434
  STEP: create the container to handle the HTTPGet hook request. @ 05/10/23 06:18:32.439
  E0510 06:18:33.346679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:34.346878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:35.347084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:36.347312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/10/23 06:18:36.458
  E0510 06:18:37.347973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:38.348192      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 05/10/23 06:18:38.469
  E0510 06:18:39.348354      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:40.348727      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:41.348821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:42.349149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 05/10/23 06:18:42.488
  May 10 06:18:42.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9329" for this suite. @ 05/10/23 06:18:42.494
• [10.077 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 05/10/23 06:18:42.498
  May 10 06:18:42.498: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename sched-pred @ 05/10/23 06:18:42.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:18:42.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:18:42.511
  May 10 06:18:42.514: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 10 06:18:42.518: INFO: Waiting for terminating namespaces to be deleted...
  May 10 06:18:42.519: INFO: 
  Logging pods the apiserver thinks is on node node-01 before test
  May 10 06:18:42.523: INFO: calico-node-hrprp from calico-system started at 2023-05-08 03:40:50 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.523: INFO: 	Container calico-node ready: true, restart count 0
  May 10 06:18:42.523: INFO: csi-node-driver-wrcs7 from calico-system started at 2023-05-08 03:41:03 +0000 UTC (2 container statuses recorded)
  May 10 06:18:42.523: INFO: 	Container calico-csi ready: true, restart count 0
  May 10 06:18:42.523: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 10 06:18:42.523: INFO: pod-handle-http-request from container-lifecycle-hook-9329 started at 2023-05-10 06:18:32 +0000 UTC (2 container statuses recorded)
  May 10 06:18:42.523: INFO: 	Container container-handle-http-request ready: true, restart count 0
  May 10 06:18:42.523: INFO: 	Container container-handle-https-request ready: true, restart count 0
  May 10 06:18:42.523: INFO: kube-proxy-jtvxj from kube-system started at 2023-05-08 03:40:50 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.523: INFO: 	Container kube-proxy ready: true, restart count 0
  May 10 06:18:42.523: INFO: kube-sealos-lvscare-node-01 from kube-system started at 2023-05-08 03:41:06 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.523: INFO: 	Container kube-sealos-lvscare ready: true, restart count 0
  May 10 06:18:42.523: INFO: sonobuoy from sonobuoy started at 2023-05-10 04:50:14 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.523: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 10 06:18:42.523: INFO: sonobuoy-e2e-job-9c188259183c425e from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 06:18:42.523: INFO: 	Container e2e ready: true, restart count 0
  May 10 06:18:42.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 06:18:42.523: INFO: sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-xwr24 from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 06:18:42.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 06:18:42.523: INFO: 	Container systemd-logs ready: true, restart count 0
  May 10 06:18:42.523: INFO: 
  Logging pods the apiserver thinks is on node node-02 before test
  May 10 06:18:42.527: INFO: calico-node-96glz from calico-system started at 2023-05-08 05:47:14 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.527: INFO: 	Container calico-node ready: true, restart count 0
  May 10 06:18:42.527: INFO: calico-typha-5777d458f7-g7bn9 from calico-system started at 2023-05-08 05:47:15 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.527: INFO: 	Container calico-typha ready: true, restart count 0
  May 10 06:18:42.527: INFO: csi-node-driver-rh58m from calico-system started at 2023-05-10 05:39:46 +0000 UTC (2 container statuses recorded)
  May 10 06:18:42.527: INFO: 	Container calico-csi ready: true, restart count 0
  May 10 06:18:42.527: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 10 06:18:42.527: INFO: kube-proxy-96kwf from kube-system started at 2023-05-08 05:47:14 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.527: INFO: 	Container kube-proxy ready: true, restart count 0
  May 10 06:18:42.527: INFO: kube-sealos-lvscare-node-02 from kube-system started at 2023-05-08 05:47:31 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.527: INFO: 	Container kube-sealos-lvscare ready: true, restart count 0
  May 10 06:18:42.527: INFO: sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-254cd from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 06:18:42.527: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 06:18:42.527: INFO: 	Container systemd-logs ready: true, restart count 0
  May 10 06:18:42.527: INFO: 
  Logging pods the apiserver thinks is on node ub-test before test
  May 10 06:18:42.531: INFO: calico-apiserver-7f745d8f87-78trq from calico-apiserver started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container calico-apiserver ready: true, restart count 0
  May 10 06:18:42.531: INFO: calico-apiserver-7f745d8f87-gk9nt from calico-apiserver started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container calico-apiserver ready: true, restart count 0
  May 10 06:18:42.531: INFO: calico-kube-controllers-6dfbf88686-x7jtc from calico-system started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 10 06:18:42.531: INFO: calico-node-rwtc6 from calico-system started at 2023-05-06 03:21:16 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container calico-node ready: true, restart count 2
  May 10 06:18:42.531: INFO: calico-typha-5777d458f7-jvrx5 from calico-system started at 2023-05-06 03:21:16 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container calico-typha ready: true, restart count 2
  May 10 06:18:42.531: INFO: csi-node-driver-fgjb5 from calico-system started at 2023-05-06 09:37:06 +0000 UTC (2 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container calico-csi ready: true, restart count 0
  May 10 06:18:42.531: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 10 06:18:42.531: INFO: coredns-5d78c9869d-d9nxz from kube-system started at 2023-05-06 09:37:00 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container coredns ready: true, restart count 0
  May 10 06:18:42.531: INFO: coredns-5d78c9869d-w9ngw from kube-system started at 2023-05-06 09:37:01 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container coredns ready: true, restart count 0
  May 10 06:18:42.531: INFO: etcd-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container etcd ready: true, restart count 2
  May 10 06:18:42.531: INFO: kube-apiserver-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container kube-apiserver ready: true, restart count 2
  May 10 06:18:42.531: INFO: kube-controller-manager-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container kube-controller-manager ready: true, restart count 6
  May 10 06:18:42.531: INFO: kube-proxy-hqbzr from kube-system started at 2023-05-06 03:18:37 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container kube-proxy ready: true, restart count 2
  May 10 06:18:42.531: INFO: kube-scheduler-ub-test from kube-system started at 2023-05-06 08:00:25 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container kube-scheduler ready: true, restart count 6
  May 10 06:18:42.531: INFO: sonobuoy-systemd-logs-daemon-set-808cc5eb4fd4408f-44wmx from sonobuoy started at 2023-05-10 04:50:15 +0000 UTC (2 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 10 06:18:42.531: INFO: 	Container systemd-logs ready: true, restart count 0
  May 10 06:18:42.531: INFO: tigera-operator-7bf7458-fcvpq from tigera-operator started at 2023-05-06 03:21:10 +0000 UTC (1 container statuses recorded)
  May 10 06:18:42.531: INFO: 	Container tigera-operator ready: true, restart count 6
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/10/23 06:18:42.531
  E0510 06:18:43.349212      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:44.349572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/10/23 06:18:44.542
  STEP: Trying to apply a random label on the found node. @ 05/10/23 06:18:44.552
  STEP: verifying the node has the label kubernetes.io/e2e-800269ce-baff-40e9-8f1c-ab9636bb5879 42 @ 05/10/23 06:18:44.564
  STEP: Trying to relaunch the pod, now with labels. @ 05/10/23 06:18:44.566
  E0510 06:18:45.350015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:46.350283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-800269ce-baff-40e9-8f1c-ab9636bb5879 off the node node-02 @ 05/10/23 06:18:46.579
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-800269ce-baff-40e9-8f1c-ab9636bb5879 @ 05/10/23 06:18:46.592
  May 10 06:18:46.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1452" for this suite. @ 05/10/23 06:18:46.596
• [4.102 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 05/10/23 06:18:46.6
  May 10 06:18:46.600: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename var-expansion @ 05/10/23 06:18:46.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:18:46.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:18:46.616
  STEP: creating the pod @ 05/10/23 06:18:46.618
  STEP: waiting for pod running @ 05/10/23 06:18:46.628
  E0510 06:18:47.351364      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:48.351775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 05/10/23 06:18:48.633
  May 10 06:18:48.635: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4672 PodName:var-expansion-7759a3c1-08db-43c9-9505-7d465b068077 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 06:18:48.635: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 06:18:48.636: INFO: ExecWithOptions: Clientset creation
  May 10 06:18:48.636: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4672/pods/var-expansion-7759a3c1-08db-43c9-9505-7d465b068077/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 05/10/23 06:18:48.694
  May 10 06:18:48.696: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4672 PodName:var-expansion-7759a3c1-08db-43c9-9505-7d465b068077 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 06:18:48.696: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 06:18:48.697: INFO: ExecWithOptions: Clientset creation
  May 10 06:18:48.697: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4672/pods/var-expansion-7759a3c1-08db-43c9-9505-7d465b068077/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 05/10/23 06:18:48.731
  May 10 06:18:49.245: INFO: Successfully updated pod "var-expansion-7759a3c1-08db-43c9-9505-7d465b068077"
  STEP: waiting for annotated pod running @ 05/10/23 06:18:49.245
  STEP: deleting the pod gracefully @ 05/10/23 06:18:49.247
  May 10 06:18:49.247: INFO: Deleting pod "var-expansion-7759a3c1-08db-43c9-9505-7d465b068077" in namespace "var-expansion-4672"
  May 10 06:18:49.255: INFO: Wait up to 5m0s for pod "var-expansion-7759a3c1-08db-43c9-9505-7d465b068077" to be fully deleted
  E0510 06:18:49.352357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:50.352492      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:51.352681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:52.353053      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:53.353393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:54.353629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:55.354675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:56.354913      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:57.355345      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:58.355571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:18:59.356549      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:00.356787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:01.357512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:02.357941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:03.358749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:04.359045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:05.359883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:06.360099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:07.360624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:08.360807      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:09.360888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:10.361083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:11.361850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:12.362078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:13.363001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:14.363163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:15.363848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:16.364601      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:17.365027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:18.365174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:19.365876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:20.366140      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:21.366672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:22.367098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:19:23.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4672" for this suite. @ 05/10/23 06:19:23.315
• [36.741 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 05/10/23 06:19:23.341
  May 10 06:19:23.341: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 06:19:23.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:19:23.358
  E0510 06:19:23.367361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:19:23.367
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 06:19:23.371
  E0510 06:19:24.368249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:25.368637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:26.368723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:27.369444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:19:27.392
  May 10 06:19:27.393: INFO: Trying to get logs from node node-02 pod downwardapi-volume-e913d3fe-08eb-459b-a02b-a6030c05fa4b container client-container: <nil>
  STEP: delete the pod @ 05/10/23 06:19:27.398
  May 10 06:19:27.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4187" for this suite. @ 05/10/23 06:19:27.429
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 05/10/23 06:19:27.439
  May 10 06:19:27.439: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-runtime @ 05/10/23 06:19:27.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:19:27.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:19:27.455
  STEP: create the container @ 05/10/23 06:19:27.464
  W0510 06:19:27.470783      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/10/23 06:19:27.471
  E0510 06:19:28.370306      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:29.370798      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:30.371596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/10/23 06:19:30.496
  STEP: the container should be terminated @ 05/10/23 06:19:30.498
  STEP: the termination message should be set @ 05/10/23 06:19:30.498
  May 10 06:19:30.498: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 05/10/23 06:19:30.498
  May 10 06:19:30.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6255" for this suite. @ 05/10/23 06:19:30.519
• [3.084 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 05/10/23 06:19:30.523
  May 10 06:19:30.523: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:19:30.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:19:30.54
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:19:30.542
  STEP: Creating configMap with name projected-configmap-test-volume-49ccc251-45e3-4a45-a01d-23e87e1c0d5f @ 05/10/23 06:19:30.544
  STEP: Creating a pod to test consume configMaps @ 05/10/23 06:19:30.547
  E0510 06:19:31.372489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:32.372914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:33.373440      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:34.373582      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:19:34.566
  May 10 06:19:34.568: INFO: Trying to get logs from node node-02 pod pod-projected-configmaps-b159ebda-452e-4dc0-9ad5-3991b42fa28e container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 06:19:34.572
  May 10 06:19:34.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3734" for this suite. @ 05/10/23 06:19:34.585
• [4.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 05/10/23 06:19:34.595
  May 10 06:19:34.595: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:19:34.595
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:19:34.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:19:34.606
  STEP: Creating configMap with name projected-configmap-test-volume-map-f8131273-17ab-4cc6-90f5-ddb70110207d @ 05/10/23 06:19:34.608
  STEP: Creating a pod to test consume configMaps @ 05/10/23 06:19:34.612
  E0510 06:19:35.374053      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:36.374319      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:37.374415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:38.374606      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:19:38.631
  May 10 06:19:38.633: INFO: Trying to get logs from node node-02 pod pod-projected-configmaps-55b9c50c-7dd4-4884-9c13-34d252133238 container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 06:19:38.637
  May 10 06:19:38.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8784" for this suite. @ 05/10/23 06:19:38.655
• [4.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 05/10/23 06:19:38.66
  May 10 06:19:38.660: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubelet-test @ 05/10/23 06:19:38.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:19:38.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:19:38.676
  E0510 06:19:39.375230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:40.375612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:19:40.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4714" for this suite. @ 05/10/23 06:19:40.695
• [2.039 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 05/10/23 06:19:40.7
  May 10 06:19:40.700: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename replication-controller @ 05/10/23 06:19:40.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:19:40.71
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:19:40.712
  May 10 06:19:40.714: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0510 06:19:41.375741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 05/10/23 06:19:41.728
  STEP: Checking rc "condition-test" has the desired failure condition set @ 05/10/23 06:19:41.736
  E0510 06:19:42.376507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 05/10/23 06:19:42.744
  May 10 06:19:42.759: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 05/10/23 06:19:42.759
  E0510 06:19:43.376862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:19:43.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1900" for this suite. @ 05/10/23 06:19:43.765
• [3.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 05/10/23 06:19:43.772
  May 10 06:19:43.772: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:19:43.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:19:43.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:19:43.785
  STEP: Creating secret with name s-test-opt-del-1325e084-0487-4685-870c-04ae9fcf3b6e @ 05/10/23 06:19:43.794
  STEP: Creating secret with name s-test-opt-upd-fc704285-d340-4f24-9f15-e3fc22091305 @ 05/10/23 06:19:43.799
  STEP: Creating the pod @ 05/10/23 06:19:43.803
  E0510 06:19:44.377830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:45.378126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:46.378113      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:47.378584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-1325e084-0487-4685-870c-04ae9fcf3b6e @ 05/10/23 06:19:47.837
  STEP: Updating secret s-test-opt-upd-fc704285-d340-4f24-9f15-e3fc22091305 @ 05/10/23 06:19:47.844
  STEP: Creating secret with name s-test-opt-create-059c4c4d-e3e0-4fcc-b2d4-934dae518aee @ 05/10/23 06:19:47.848
  STEP: waiting to observe update in volume @ 05/10/23 06:19:47.857
  E0510 06:19:48.379664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:49.379880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:50.380132      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:51.380374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:52.381381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:53.381591      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:54.382263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:55.382543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:56.382603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:57.383271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:58.384305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:19:59.384534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:00.385333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:01.385544      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:02.385686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:03.385881      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:04.386744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:05.386952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:06.387914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:07.388601      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:08.389031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:09.389801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:10.389891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:11.390100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:12.390826      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:13.391128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:14.392061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:15.392308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:16.392728      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:17.393248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:18.393860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:19.394083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:20.394335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:21.395046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:22.395245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:23.395449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:24.396192      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:25.396406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:26.397375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:27.397846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:28.398661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:29.399042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:30.400051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:31.400293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:32.400612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:33.400981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:34.401354      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:35.401555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:36.402522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:37.403017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:38.403208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:39.403474      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:40.404293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:41.404585      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:42.405089      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:43.405612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:44.406673      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:45.407081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:46.407369      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:47.408298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:48.408506      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:49.408873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:50.409043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:51.409264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:52.409871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:53.410180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:54.410775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:55.411129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:56.411617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:57.412384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:58.413254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:20:59.413628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:00.414362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:01.415322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:02.416150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:03.416483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:04.417470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:05.417901      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:06.417981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:07.418177      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:08.418932      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:09.419215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:21:10.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1655" for this suite. @ 05/10/23 06:21:10.156
• [86.390 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 05/10/23 06:21:10.162
  May 10 06:21:10.162: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename certificates @ 05/10/23 06:21:10.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:21:10.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:21:10.176
  E0510 06:21:10.419745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 05/10/23 06:21:10.476
  STEP: getting /apis/certificates.k8s.io @ 05/10/23 06:21:10.481
  STEP: getting /apis/certificates.k8s.io/v1 @ 05/10/23 06:21:10.482
  STEP: creating @ 05/10/23 06:21:10.483
  STEP: getting @ 05/10/23 06:21:10.516
  STEP: listing @ 05/10/23 06:21:10.517
  STEP: watching @ 05/10/23 06:21:10.519
  May 10 06:21:10.519: INFO: starting watch
  STEP: patching @ 05/10/23 06:21:10.52
  STEP: updating @ 05/10/23 06:21:10.526
  May 10 06:21:10.535: INFO: waiting for watch events with expected annotations
  May 10 06:21:10.535: INFO: saw patched and updated annotations
  STEP: getting /approval @ 05/10/23 06:21:10.535
  STEP: patching /approval @ 05/10/23 06:21:10.537
  STEP: updating /approval @ 05/10/23 06:21:10.542
  STEP: getting /status @ 05/10/23 06:21:10.55
  STEP: patching /status @ 05/10/23 06:21:10.552
  STEP: updating /status @ 05/10/23 06:21:10.56
  STEP: deleting @ 05/10/23 06:21:10.568
  STEP: deleting a collection @ 05/10/23 06:21:10.579
  May 10 06:21:10.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-7817" for this suite. @ 05/10/23 06:21:10.592
• [0.434 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 05/10/23 06:21:10.596
  May 10 06:21:10.596: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename var-expansion @ 05/10/23 06:21:10.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:21:10.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:21:10.612
  STEP: Creating a pod to test env composition @ 05/10/23 06:21:10.617
  E0510 06:21:11.420159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:12.420485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:13.420977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:14.421220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:21:14.632
  May 10 06:21:14.634: INFO: Trying to get logs from node node-01 pod var-expansion-39a4d3bf-d994-4812-8c13-7356fb660ce7 container dapi-container: <nil>
  STEP: delete the pod @ 05/10/23 06:21:14.646
  May 10 06:21:14.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7116" for this suite. @ 05/10/23 06:21:14.667
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 05/10/23 06:21:14.671
  May 10 06:21:14.671: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 06:21:14.671
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:21:14.68
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:21:14.682
  STEP: Creating secret with name secret-test-21bc8186-45b8-4c24-83f5-071890184ce3 @ 05/10/23 06:21:14.683
  STEP: Creating a pod to test consume secrets @ 05/10/23 06:21:14.692
  E0510 06:21:15.422181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:16.422387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:17.423135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:18.423259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:21:18.711
  May 10 06:21:18.713: INFO: Trying to get logs from node node-01 pod pod-secrets-67577ab0-253e-421b-96d1-5d4fd1314c8e container secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 06:21:18.716
  May 10 06:21:18.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-806" for this suite. @ 05/10/23 06:21:18.735
• [4.069 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 05/10/23 06:21:18.74
  May 10 06:21:18.740: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pods @ 05/10/23 06:21:18.741
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:21:18.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:21:18.757
  STEP: creating the pod @ 05/10/23 06:21:18.759
  STEP: setting up watch @ 05/10/23 06:21:18.759
  STEP: submitting the pod to kubernetes @ 05/10/23 06:21:18.862
  STEP: verifying the pod is in kubernetes @ 05/10/23 06:21:18.868
  STEP: verifying pod creation was observed @ 05/10/23 06:21:18.87
  E0510 06:21:19.423439      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:20.423496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 05/10/23 06:21:20.882
  STEP: verifying pod deletion was observed @ 05/10/23 06:21:20.893
  E0510 06:21:21.424320      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:22.424709      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:23.425609      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:21:23.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5241" for this suite. @ 05/10/23 06:21:23.495
• [4.760 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 05/10/23 06:21:23.5
  May 10 06:21:23.500: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename resourcequota @ 05/10/23 06:21:23.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:21:23.515
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:21:23.517
  STEP: Counting existing ResourceQuota @ 05/10/23 06:21:23.519
  E0510 06:21:24.425734      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:25.426034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:26.426170      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:27.427214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:28.428026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/10/23 06:21:28.522
  STEP: Ensuring resource quota status is calculated @ 05/10/23 06:21:28.526
  E0510 06:21:29.428159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:30.428522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 05/10/23 06:21:30.529
  STEP: Ensuring resource quota status captures replication controller creation @ 05/10/23 06:21:30.544
  E0510 06:21:31.428647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:32.429329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 05/10/23 06:21:32.547
  STEP: Ensuring resource quota status released usage @ 05/10/23 06:21:32.552
  E0510 06:21:33.429445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:34.429641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:21:34.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6439" for this suite. @ 05/10/23 06:21:34.559
• [11.064 seconds]
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 05/10/23 06:21:34.564
  May 10 06:21:34.564: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename gc @ 05/10/23 06:21:34.564
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:21:34.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:21:34.578
  STEP: create the rc @ 05/10/23 06:21:34.582
  W0510 06:21:34.587338      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0510 06:21:35.430332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:36.430736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:37.431656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:38.431936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:39.432337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:40.433392      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/10/23 06:21:40.661
  STEP: wait for the rc to be deleted @ 05/10/23 06:21:40.774
  E0510 06:21:41.433481      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:42.433931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:43.434176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:44.434416      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:45.434511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 05/10/23 06:21:45.777
  E0510 06:21:46.434640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:47.435193      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:48.435492      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:49.436145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:50.436384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:51.436656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:52.437020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:53.437344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:54.437712      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:55.438323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:56.438557      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:57.439045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:58.439251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:21:59.439459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:00.439646      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:01.439794      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:02.440094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:03.440305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:04.440503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:05.440917      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:06.441135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:07.441693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:08.441906      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:09.442771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:10.443009      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:11.443400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:12.444448      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:13.444704      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:14.445394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:15.445586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/10/23 06:22:15.788
  May 10 06:22:15.867: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 10 06:22:15.868: INFO: Deleting pod "simpletest.rc-26758" in namespace "gc-3125"
  May 10 06:22:15.881: INFO: Deleting pod "simpletest.rc-2n2p9" in namespace "gc-3125"
  May 10 06:22:15.960: INFO: Deleting pod "simpletest.rc-2thlw" in namespace "gc-3125"
  May 10 06:22:16.096: INFO: Deleting pod "simpletest.rc-2w82p" in namespace "gc-3125"
  May 10 06:22:16.294: INFO: Deleting pod "simpletest.rc-46rz7" in namespace "gc-3125"
  May 10 06:22:16.310: INFO: Deleting pod "simpletest.rc-49ml9" in namespace "gc-3125"
  May 10 06:22:16.326: INFO: Deleting pod "simpletest.rc-4qwg9" in namespace "gc-3125"
  May 10 06:22:16.337: INFO: Deleting pod "simpletest.rc-4vg6n" in namespace "gc-3125"
  May 10 06:22:16.350: INFO: Deleting pod "simpletest.rc-552kb" in namespace "gc-3125"
  May 10 06:22:16.364: INFO: Deleting pod "simpletest.rc-59frp" in namespace "gc-3125"
  E0510 06:22:16.446151      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:16.908: INFO: Deleting pod "simpletest.rc-5ghj4" in namespace "gc-3125"
  May 10 06:22:16.928: INFO: Deleting pod "simpletest.rc-5jntt" in namespace "gc-3125"
  May 10 06:22:16.940: INFO: Deleting pod "simpletest.rc-5mb52" in namespace "gc-3125"
  May 10 06:22:16.951: INFO: Deleting pod "simpletest.rc-5rqxg" in namespace "gc-3125"
  May 10 06:22:16.964: INFO: Deleting pod "simpletest.rc-5znwg" in namespace "gc-3125"
  May 10 06:22:16.976: INFO: Deleting pod "simpletest.rc-6hjwl" in namespace "gc-3125"
  May 10 06:22:16.985: INFO: Deleting pod "simpletest.rc-6rhzc" in namespace "gc-3125"
  E0510 06:22:17.446785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:17.605: INFO: Deleting pod "simpletest.rc-6t8vj" in namespace "gc-3125"
  May 10 06:22:17.633: INFO: Deleting pod "simpletest.rc-7b7kk" in namespace "gc-3125"
  May 10 06:22:17.652: INFO: Deleting pod "simpletest.rc-7gw8g" in namespace "gc-3125"
  May 10 06:22:17.671: INFO: Deleting pod "simpletest.rc-7kcg8" in namespace "gc-3125"
  May 10 06:22:18.238: INFO: Deleting pod "simpletest.rc-7ldsg" in namespace "gc-3125"
  May 10 06:22:18.267: INFO: Deleting pod "simpletest.rc-7p5nh" in namespace "gc-3125"
  May 10 06:22:18.283: INFO: Deleting pod "simpletest.rc-8bljv" in namespace "gc-3125"
  E0510 06:22:18.447799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:18.649: INFO: Deleting pod "simpletest.rc-8cflv" in namespace "gc-3125"
  May 10 06:22:18.822: INFO: Deleting pod "simpletest.rc-94wjt" in namespace "gc-3125"
  May 10 06:22:18.857: INFO: Deleting pod "simpletest.rc-9dxkh" in namespace "gc-3125"
  May 10 06:22:18.869: INFO: Deleting pod "simpletest.rc-9jzxr" in namespace "gc-3125"
  May 10 06:22:18.882: INFO: Deleting pod "simpletest.rc-9vk78" in namespace "gc-3125"
  E0510 06:22:19.448736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:20.041: INFO: Deleting pod "simpletest.rc-bw25f" in namespace "gc-3125"
  May 10 06:22:20.270: INFO: Deleting pod "simpletest.rc-bwxvj" in namespace "gc-3125"
  May 10 06:22:20.294: INFO: Deleting pod "simpletest.rc-cb4lj" in namespace "gc-3125"
  E0510 06:22:20.449583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:20.943: INFO: Deleting pod "simpletest.rc-cc6sz" in namespace "gc-3125"
  May 10 06:22:20.980: INFO: Deleting pod "simpletest.rc-cr669" in namespace "gc-3125"
  May 10 06:22:21.001: INFO: Deleting pod "simpletest.rc-d2h4f" in namespace "gc-3125"
  May 10 06:22:21.022: INFO: Deleting pod "simpletest.rc-d62mt" in namespace "gc-3125"
  May 10 06:22:21.061: INFO: Deleting pod "simpletest.rc-d7v49" in namespace "gc-3125"
  May 10 06:22:21.075: INFO: Deleting pod "simpletest.rc-d8wz6" in namespace "gc-3125"
  May 10 06:22:21.089: INFO: Deleting pod "simpletest.rc-d9bwk" in namespace "gc-3125"
  E0510 06:22:21.450630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:22.054: INFO: Deleting pod "simpletest.rc-dlvv7" in namespace "gc-3125"
  May 10 06:22:22.067: INFO: Deleting pod "simpletest.rc-dpqtj" in namespace "gc-3125"
  May 10 06:22:22.084: INFO: Deleting pod "simpletest.rc-f4w8w" in namespace "gc-3125"
  May 10 06:22:22.132: INFO: Deleting pod "simpletest.rc-fw6r2" in namespace "gc-3125"
  May 10 06:22:22.147: INFO: Deleting pod "simpletest.rc-g58vk" in namespace "gc-3125"
  E0510 06:22:22.450755      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:22.844: INFO: Deleting pod "simpletest.rc-ggz2h" in namespace "gc-3125"
  May 10 06:22:22.867: INFO: Deleting pod "simpletest.rc-gn9lf" in namespace "gc-3125"
  May 10 06:22:22.887: INFO: Deleting pod "simpletest.rc-gwlsp" in namespace "gc-3125"
  May 10 06:22:22.916: INFO: Deleting pod "simpletest.rc-h8vdf" in namespace "gc-3125"
  May 10 06:22:22.937: INFO: Deleting pod "simpletest.rc-hhdj7" in namespace "gc-3125"
  May 10 06:22:22.961: INFO: Deleting pod "simpletest.rc-hwz6p" in namespace "gc-3125"
  E0510 06:22:23.451763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:23.709: INFO: Deleting pod "simpletest.rc-j6fp6" in namespace "gc-3125"
  May 10 06:22:23.762: INFO: Deleting pod "simpletest.rc-j7wd7" in namespace "gc-3125"
  May 10 06:22:23.774: INFO: Deleting pod "simpletest.rc-j9sqd" in namespace "gc-3125"
  May 10 06:22:24.319: INFO: Deleting pod "simpletest.rc-jh2nm" in namespace "gc-3125"
  May 10 06:22:24.341: INFO: Deleting pod "simpletest.rc-jhsfw" in namespace "gc-3125"
  May 10 06:22:24.359: INFO: Deleting pod "simpletest.rc-jvtfw" in namespace "gc-3125"
  May 10 06:22:24.379: INFO: Deleting pod "simpletest.rc-jw56k" in namespace "gc-3125"
  May 10 06:22:24.399: INFO: Deleting pod "simpletest.rc-kf8fp" in namespace "gc-3125"
  E0510 06:22:24.452665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:24.805: INFO: Deleting pod "simpletest.rc-kxqxw" in namespace "gc-3125"
  May 10 06:22:25.446: INFO: Deleting pod "simpletest.rc-kzxdz" in namespace "gc-3125"
  E0510 06:22:25.452738      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:25.483: INFO: Deleting pod "simpletest.rc-lbdbr" in namespace "gc-3125"
  May 10 06:22:25.503: INFO: Deleting pod "simpletest.rc-ljltf" in namespace "gc-3125"
  May 10 06:22:25.513: INFO: Deleting pod "simpletest.rc-lp5f9" in namespace "gc-3125"
  May 10 06:22:26.031: INFO: Deleting pod "simpletest.rc-md4p7" in namespace "gc-3125"
  May 10 06:22:26.056: INFO: Deleting pod "simpletest.rc-mfdhc" in namespace "gc-3125"
  May 10 06:22:26.069: INFO: Deleting pod "simpletest.rc-mnq6v" in namespace "gc-3125"
  May 10 06:22:26.369: INFO: Deleting pod "simpletest.rc-mxj6q" in namespace "gc-3125"
  May 10 06:22:26.388: INFO: Deleting pod "simpletest.rc-nfs6j" in namespace "gc-3125"
  May 10 06:22:26.415: INFO: Deleting pod "simpletest.rc-nhtw8" in namespace "gc-3125"
  May 10 06:22:26.434: INFO: Deleting pod "simpletest.rc-nvjj5" in namespace "gc-3125"
  May 10 06:22:26.451: INFO: Deleting pod "simpletest.rc-nznqb" in namespace "gc-3125"
  E0510 06:22:26.453982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:27.102: INFO: Deleting pod "simpletest.rc-phmdl" in namespace "gc-3125"
  May 10 06:22:27.383: INFO: Deleting pod "simpletest.rc-pvl5c" in namespace "gc-3125"
  E0510 06:22:27.454438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:27.577: INFO: Deleting pod "simpletest.rc-pzbnf" in namespace "gc-3125"
  May 10 06:22:27.590: INFO: Deleting pod "simpletest.rc-pzxk2" in namespace "gc-3125"
  May 10 06:22:27.607: INFO: Deleting pod "simpletest.rc-q8p2z" in namespace "gc-3125"
  May 10 06:22:27.624: INFO: Deleting pod "simpletest.rc-qqlnb" in namespace "gc-3125"
  E0510 06:22:28.454496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:28.489: INFO: Deleting pod "simpletest.rc-qs7sb" in namespace "gc-3125"
  May 10 06:22:28.505: INFO: Deleting pod "simpletest.rc-qtphd" in namespace "gc-3125"
  May 10 06:22:29.240: INFO: Deleting pod "simpletest.rc-qz9j6" in namespace "gc-3125"
  May 10 06:22:29.268: INFO: Deleting pod "simpletest.rc-r26sf" in namespace "gc-3125"
  May 10 06:22:29.312: INFO: Deleting pod "simpletest.rc-r7gqz" in namespace "gc-3125"
  May 10 06:22:29.330: INFO: Deleting pod "simpletest.rc-rbqrs" in namespace "gc-3125"
  May 10 06:22:29.353: INFO: Deleting pod "simpletest.rc-sr5wj" in namespace "gc-3125"
  E0510 06:22:29.455437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:29.856: INFO: Deleting pod "simpletest.rc-t4f4b" in namespace "gc-3125"
  May 10 06:22:30.228: INFO: Deleting pod "simpletest.rc-tjkc6" in namespace "gc-3125"
  May 10 06:22:30.274: INFO: Deleting pod "simpletest.rc-tzkkv" in namespace "gc-3125"
  E0510 06:22:30.455865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:30.467: INFO: Deleting pod "simpletest.rc-v4gmb" in namespace "gc-3125"
  May 10 06:22:30.490: INFO: Deleting pod "simpletest.rc-v5t7l" in namespace "gc-3125"
  May 10 06:22:30.499: INFO: Deleting pod "simpletest.rc-vlptc" in namespace "gc-3125"
  May 10 06:22:30.525: INFO: Deleting pod "simpletest.rc-vn5k9" in namespace "gc-3125"
  May 10 06:22:31.092: INFO: Deleting pod "simpletest.rc-w9h72" in namespace "gc-3125"
  May 10 06:22:31.116: INFO: Deleting pod "simpletest.rc-wlx6s" in namespace "gc-3125"
  May 10 06:22:31.158: INFO: Deleting pod "simpletest.rc-xcmz7" in namespace "gc-3125"
  May 10 06:22:31.174: INFO: Deleting pod "simpletest.rc-xldw6" in namespace "gc-3125"
  E0510 06:22:31.457028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:32.045: INFO: Deleting pod "simpletest.rc-xwf8j" in namespace "gc-3125"
  May 10 06:22:32.067: INFO: Deleting pod "simpletest.rc-z462b" in namespace "gc-3125"
  May 10 06:22:32.087: INFO: Deleting pod "simpletest.rc-zc984" in namespace "gc-3125"
  May 10 06:22:32.097: INFO: Deleting pod "simpletest.rc-zswxw" in namespace "gc-3125"
  May 10 06:22:32.110: INFO: Deleting pod "simpletest.rc-zzmpx" in namespace "gc-3125"
  May 10 06:22:32.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3125" for this suite. @ 05/10/23 06:22:32.129
• [57.578 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 05/10/23 06:22:32.146
  May 10 06:22:32.146: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename replication-controller @ 05/10/23 06:22:32.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:22:32.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:22:32.166
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 05/10/23 06:22:32.168
  E0510 06:22:32.457414      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:33.458079      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:34.458293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:35.458472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:36.458725      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:37.459396      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:38.459733      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 05/10/23 06:22:39.243
  STEP: Then the orphan pod is adopted @ 05/10/23 06:22:39.255
  E0510 06:22:39.459767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:40.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8157" for this suite. @ 05/10/23 06:22:40.264
• [8.124 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 05/10/23 06:22:40.271
  May 10 06:22:40.271: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 06:22:40.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:22:40.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:22:40.286
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 06:22:40.288
  E0510 06:22:40.460365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:41.460571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:42.461146      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:43.461368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:22:44.303
  May 10 06:22:44.305: INFO: Trying to get logs from node node-02 pod downwardapi-volume-da90af3f-ce13-445b-88ed-ec78a2abf988 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 06:22:44.317
  May 10 06:22:44.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9324" for this suite. @ 05/10/23 06:22:44.332
• [4.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 05/10/23 06:22:44.337
  May 10 06:22:44.337: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename gc @ 05/10/23 06:22:44.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:22:44.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:22:44.352
  STEP: create the rc @ 05/10/23 06:22:44.355
  W0510 06:22:44.359303      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0510 06:22:44.461592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:45.461793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:46.462741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:47.463235      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:48.463384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/10/23 06:22:49.361
  STEP: wait for all pods to be garbage collected @ 05/10/23 06:22:49.366
  E0510 06:22:49.463980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:50.464384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:51.465362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:52.466273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:53.466446      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/10/23 06:22:54.371
  May 10 06:22:54.429: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 10 06:22:54.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2207" for this suite. @ 05/10/23 06:22:54.432
• [10.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 05/10/23 06:22:54.437
  May 10 06:22:54.437: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-probe @ 05/10/23 06:22:54.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:22:54.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:22:54.454
  STEP: Creating pod busybox-2fdae0ba-c5e4-4a06-806b-26453fa2658d in namespace container-probe-4550 @ 05/10/23 06:22:54.456
  E0510 06:22:54.467124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:55.467464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:56.466: INFO: Started pod busybox-2fdae0ba-c5e4-4a06-806b-26453fa2658d in namespace container-probe-4550
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/10/23 06:22:56.466
  E0510 06:22:56.468028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:22:56.468: INFO: Initial restart count of pod busybox-2fdae0ba-c5e4-4a06-806b-26453fa2658d is 0
  E0510 06:22:57.468574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:58.468810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:22:59.469660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:00.470050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:01.471124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:02.471519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:03.472433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:04.472658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:05.473357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:06.473473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:07.473770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:08.474051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:09.475063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:10.475263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:11.475819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:12.475986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:13.476090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:14.476240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:15.477213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:16.478269      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:17.478890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:18.479103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:19.479392      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:20.479682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:21.480099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:22.480426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:23.481046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:24.481576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:25.482415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:26.482615      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:27.483062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:28.483395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:29.483866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:30.484102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:31.484361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:32.485036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:33.485590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:34.485690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:35.486658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:36.487147      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:37.488224      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:38.488455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:39.489383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:40.489578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:41.489971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:42.490439      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:43.491013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:44.491027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:45.491740      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:46.491743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:23:46.644: INFO: Restart count of pod container-probe-4550/busybox-2fdae0ba-c5e4-4a06-806b-26453fa2658d is now 1 (50.176774798s elapsed)
  May 10 06:23:46.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 06:23:46.647
  STEP: Destroying namespace "container-probe-4550" for this suite. @ 05/10/23 06:23:46.66
• [52.230 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 05/10/23 06:23:46.667
  May 10 06:23:46.667: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 06:23:46.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:23:46.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:23:46.688
  STEP: creating the pod @ 05/10/23 06:23:46.69
  May 10 06:23:46.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-9886 create -f -'
  May 10 06:23:47.038: INFO: stderr: ""
  May 10 06:23:47.038: INFO: stdout: "pod/pause created\n"
  E0510 06:23:47.492532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:48.493584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:49.494578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:50.495205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 05/10/23 06:23:51.047
  May 10 06:23:51.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-9886 label pods pause testing-label=testing-label-value'
  May 10 06:23:51.137: INFO: stderr: ""
  May 10 06:23:51.137: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 05/10/23 06:23:51.137
  May 10 06:23:51.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-9886 get pod pause -L testing-label'
  May 10 06:23:51.195: INFO: stderr: ""
  May 10 06:23:51.195: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 05/10/23 06:23:51.195
  May 10 06:23:51.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-9886 label pods pause testing-label-'
  May 10 06:23:51.266: INFO: stderr: ""
  May 10 06:23:51.266: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 05/10/23 06:23:51.266
  May 10 06:23:51.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-9886 get pod pause -L testing-label'
  May 10 06:23:51.323: INFO: stderr: ""
  May 10 06:23:51.323: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
  STEP: using delete to clean up resources @ 05/10/23 06:23:51.323
  May 10 06:23:51.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-9886 delete --grace-period=0 --force -f -'
  May 10 06:23:51.388: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 10 06:23:51.389: INFO: stdout: "pod \"pause\" force deleted\n"
  May 10 06:23:51.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-9886 get rc,svc -l name=pause --no-headers'
  May 10 06:23:51.449: INFO: stderr: "No resources found in kubectl-9886 namespace.\n"
  May 10 06:23:51.450: INFO: stdout: ""
  May 10 06:23:51.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-9886 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  E0510 06:23:51.495437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:23:51.504: INFO: stderr: ""
  May 10 06:23:51.504: INFO: stdout: ""
  May 10 06:23:51.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9886" for this suite. @ 05/10/23 06:23:51.507
• [4.845 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 05/10/23 06:23:51.512
  May 10 06:23:51.512: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 06:23:51.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:23:51.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:23:51.527
  STEP: Creating configMap with name configmap-test-upd-d624fd3d-4d3b-4995-b01f-9cee0f94a7a9 @ 05/10/23 06:23:51.531
  STEP: Creating the pod @ 05/10/23 06:23:51.534
  E0510 06:23:52.496486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:53.496720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-d624fd3d-4d3b-4995-b01f-9cee0f94a7a9 @ 05/10/23 06:23:53.555
  STEP: waiting to observe update in volume @ 05/10/23 06:23:53.562
  E0510 06:23:54.497483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:55.497705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:23:55.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7393" for this suite. @ 05/10/23 06:23:55.574
• [4.066 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 05/10/23 06:23:55.579
  May 10 06:23:55.579: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:23:55.579
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:23:55.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:23:55.597
  STEP: Creating configMap with name projected-configmap-test-volume-374b8bca-952f-4d6d-a037-d4fd90eb0420 @ 05/10/23 06:23:55.599
  STEP: Creating a pod to test consume configMaps @ 05/10/23 06:23:55.603
  E0510 06:23:56.498307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:57.499135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:58.499802      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:23:59.500164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:23:59.691
  May 10 06:23:59.693: INFO: Trying to get logs from node node-02 pod pod-projected-configmaps-68a09e9f-6715-431f-885a-8cfcd09e2633 container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 06:23:59.697
  May 10 06:23:59.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4726" for this suite. @ 05/10/23 06:23:59.715
• [4.141 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 05/10/23 06:23:59.721
  May 10 06:23:59.721: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pods @ 05/10/23 06:23:59.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:23:59.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:23:59.737
  STEP: creating the pod @ 05/10/23 06:23:59.739
  STEP: submitting the pod to kubernetes @ 05/10/23 06:23:59.739
  E0510 06:24:00.500401      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:01.500767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:02.501052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:03.501267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 05/10/23 06:24:03.756
  STEP: updating the pod @ 05/10/23 06:24:03.758
  May 10 06:24:04.272: INFO: Successfully updated pod "pod-update-cc804a1f-0308-4e4b-80b8-43519b1dc85d"
  STEP: verifying the updated pod is in kubernetes @ 05/10/23 06:24:04.274
  May 10 06:24:04.277: INFO: Pod update OK
  May 10 06:24:04.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2892" for this suite. @ 05/10/23 06:24:04.279
• [4.563 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 05/10/23 06:24:04.284
  May 10 06:24:04.284: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename statefulset @ 05/10/23 06:24:04.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:24:04.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:24:04.298
  STEP: Creating service test in namespace statefulset-2749 @ 05/10/23 06:24:04.3
  May 10 06:24:04.311: INFO: Found 0 stateful pods, waiting for 1
  E0510 06:24:04.502024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:05.502265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:06.502540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:07.503142      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:08.503756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:09.504171      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:10.504595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:11.504903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:12.505008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:13.505210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:24:14.315: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 05/10/23 06:24:14.318
  W0510 06:24:14.323625      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 10 06:24:14.332: INFO: Found 1 stateful pods, waiting for 2
  E0510 06:24:14.505945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:15.506497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:16.506714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:17.507589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:18.507816      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:19.507953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:20.508051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:21.508369      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:22.508827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:23.509539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:24:24.336: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 10 06:24:24.336: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 05/10/23 06:24:24.34
  STEP: Delete all of the StatefulSets @ 05/10/23 06:24:24.341
  STEP: Verify that StatefulSets have been deleted @ 05/10/23 06:24:24.351
  May 10 06:24:24.354: INFO: Deleting all statefulset in ns statefulset-2749
  May 10 06:24:24.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2749" for this suite. @ 05/10/23 06:24:24.364
• [20.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 05/10/23 06:24:24.37
  May 10 06:24:24.370: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename sysctl @ 05/10/23 06:24:24.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:24:24.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:24:24.388
  STEP: Creating a pod with one valid and two invalid sysctls @ 05/10/23 06:24:24.39
  May 10 06:24:24.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-283" for this suite. @ 05/10/23 06:24:24.399
• [0.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 05/10/23 06:24:24.405
  May 10 06:24:24.405: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pod-network-test @ 05/10/23 06:24:24.405
  E0510 06:24:24.509640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:24:24.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:24:24.705
  STEP: Performing setup for networking test in namespace pod-network-test-5645 @ 05/10/23 06:24:24.708
  STEP: creating a selector @ 05/10/23 06:24:24.708
  STEP: Creating the service pods in kubernetes @ 05/10/23 06:24:24.708
  May 10 06:24:24.708: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0510 06:24:25.510572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:26.510760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:27.511396      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:28.511533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:29.512573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:30.512799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:31.512930      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:32.513306      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:33.513390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:34.513475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:35.514284      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:36.514546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:37.514735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:38.514955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:39.515084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:40.515208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:41.516309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:42.516741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:43.516824      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:44.517330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:45.517657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:46.518207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/10/23 06:24:46.988
  E0510 06:24:47.518941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:48.519229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:24:49.009: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 10 06:24:49.009: INFO: Going to poll 100.67.79.155 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May 10 06:24:49.011: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.67.79.155 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5645 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 06:24:49.011: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 06:24:49.011: INFO: ExecWithOptions: Clientset creation
  May 10 06:24:49.011: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5645/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.67.79.155+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0510 06:24:49.519360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:24:50.085: INFO: Found all 1 expected endpoints: [netserver-0]
  May 10 06:24:50.085: INFO: Going to poll 100.114.252.171 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May 10 06:24:50.086: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.114.252.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5645 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 06:24:50.086: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 06:24:50.087: INFO: ExecWithOptions: Clientset creation
  May 10 06:24:50.087: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5645/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.114.252.171+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0510 06:24:50.520291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:24:51.156: INFO: Found all 1 expected endpoints: [netserver-1]
  May 10 06:24:51.156: INFO: Going to poll 100.74.79.56 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May 10 06:24:51.159: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.74.79.56 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5645 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 10 06:24:51.159: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  May 10 06:24:51.159: INFO: ExecWithOptions: Clientset creation
  May 10 06:24:51.159: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5645/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.74.79.56+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0510 06:24:51.521114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:24:52.217: INFO: Found all 1 expected endpoints: [netserver-2]
  May 10 06:24:52.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5645" for this suite. @ 05/10/23 06:24:52.219
• [27.819 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 05/10/23 06:24:52.225
  May 10 06:24:52.225: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename disruption @ 05/10/23 06:24:52.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:24:52.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:24:52.241
  STEP: Waiting for the pdb to be processed @ 05/10/23 06:24:52.247
  E0510 06:24:52.521397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:53.521652      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 05/10/23 06:24:54.282
  May 10 06:24:54.284: INFO: running pods: 0 < 3
  E0510 06:24:54.521982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:55.522257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:24:56.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5551" for this suite. @ 05/10/23 06:24:56.294
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 05/10/23 06:24:56.3
  May 10 06:24:56.300: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename runtimeclass @ 05/10/23 06:24:56.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:24:56.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:24:56.319
  May 10 06:24:56.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3038" for this suite. @ 05/10/23 06:24:56.326
• [0.031 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 05/10/23 06:24:56.331
  May 10 06:24:56.331: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename namespaces @ 05/10/23 06:24:56.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:24:56.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:24:56.344
  STEP: creating a Namespace @ 05/10/23 06:24:56.346
  STEP: patching the Namespace @ 05/10/23 06:24:56.357
  STEP: get the Namespace and ensuring it has the label @ 05/10/23 06:24:56.366
  May 10 06:24:56.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5310" for this suite. @ 05/10/23 06:24:56.369
  STEP: Destroying namespace "nspatchtest-1b7c6dfb-694c-4d8f-8da2-cbac278f1ccb-827" for this suite. @ 05/10/23 06:24:56.375
• [0.047 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 05/10/23 06:24:56.379
  May 10 06:24:56.379: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename replication-controller @ 05/10/23 06:24:56.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:24:56.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:24:56.395
  STEP: Given a ReplicationController is created @ 05/10/23 06:24:56.397
  STEP: When the matched label of one of its pods change @ 05/10/23 06:24:56.402
  May 10 06:24:56.403: INFO: Pod name pod-release: Found 0 pods out of 1
  E0510 06:24:56.522800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:57.523363      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:58.523565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:24:59.523864      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:00.524298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:01.410: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/10/23 06:25:01.422
  E0510 06:25:01.524737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:02.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5286" for this suite. @ 05/10/23 06:25:02.432
• [6.058 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 05/10/23 06:25:02.437
  May 10 06:25:02.437: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/10/23 06:25:02.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:25:02.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:25:02.454
  STEP: set up a multi version CRD @ 05/10/23 06:25:02.457
  May 10 06:25:02.458: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 06:25:02.525588      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:03.525879      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:04.525973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:05.526652      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:06.527117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 05/10/23 06:25:06.53
  STEP: check the new version name is served @ 05/10/23 06:25:06.546
  E0510 06:25:07.527526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 05/10/23 06:25:08.527
  E0510 06:25:08.528057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 05/10/23 06:25:09.372
  E0510 06:25:09.528645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:10.529298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:11.529332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:12.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7035" for this suite. @ 05/10/23 06:25:12.479
• [10.051 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 05/10/23 06:25:12.489
  May 10 06:25:12.489: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename deployment @ 05/10/23 06:25:12.489
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:25:12.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:25:12.501
  May 10 06:25:12.503: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  May 10 06:25:12.511: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0510 06:25:12.529953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:13.530181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:14.530431      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:15.530638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:16.530869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:17.514: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/10/23 06:25:17.514
  May 10 06:25:17.514: INFO: Creating deployment "test-rolling-update-deployment"
  May 10 06:25:17.518: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  May 10 06:25:17.521: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0510 06:25:17.531571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:18.531793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:19.532457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:19.563: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  May 10 06:25:19.565: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  May 10 06:25:19.570: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-887  8beab175-9f2a-403c-8744-0384862d3944 1092983 1 2023-05-10 06:25:17 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-10 06:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:25:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003603b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-10 06:25:17 +0000 UTC,LastTransitionTime:2023-05-10 06:25:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-05-10 06:25:19 +0000 UTC,LastTransitionTime:2023-05-10 06:25:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 10 06:25:19.572: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-887  5fbadae2-348d-4825-9c40-e4afa8466bb0 1092973 1 2023-05-10 06:25:17 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 8beab175-9f2a-403c-8744-0384862d3944 0xc008382357 0xc008382358}] [] [{kube-controller-manager Update apps/v1 2023-05-10 06:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8beab175-9f2a-403c-8744-0384862d3944\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:25:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008382408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 10 06:25:19.572: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  May 10 06:25:19.572: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-887  f8ef8356-ddb6-4456-b63a-60db9f03086c 1092982 2 2023-05-10 06:25:12 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 8beab175-9f2a-403c-8744-0384862d3944 0xc003603f67 0xc003603f68}] [] [{e2e.test Update apps/v1 2023-05-10 06:25:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:25:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8beab175-9f2a-403c-8744-0384862d3944\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:25:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0083822e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 10 06:25:19.574: INFO: Pod "test-rolling-update-deployment-656d657cd8-chfx5" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-chfx5 test-rolling-update-deployment-656d657cd8- deployment-887  7947f68b-559d-4a95-9ea4-5e1287e4055c 1092972 0 2023-05-10 06:25:17 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[cni.projectcalico.org/containerID:261f38ac36fe66e470888761de997bdb42dedae56eceb9cc7bde0efecb055bef cni.projectcalico.org/podIP:100.114.252.164/32 cni.projectcalico.org/podIPs:100.114.252.164/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 5fbadae2-348d-4825-9c40-e4afa8466bb0 0xc008382877 0xc008382878}] [] [{kube-controller-manager Update v1 2023-05-10 06:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5fbadae2-348d-4825-9c40-e4afa8466bb0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-10 06:25:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-10 06:25:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.114.252.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpdwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpdwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:25:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:25:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:100.114.252.164,StartTime:2023-05-10 06:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 06:25:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:containerd://a1939411cda756e8b6a0fbf41018475af42947add4cdc992f9b2ce1000c3cf2a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.114.252.164,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:25:19.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-887" for this suite. @ 05/10/23 06:25:19.58
• [7.097 seconds]
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 05/10/23 06:25:19.586
  May 10 06:25:19.586: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:25:19.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:25:19.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:25:19.602
  STEP: Creating secret with name projected-secret-test-b95bdf3d-d23d-47ea-a414-3cafef10eca5 @ 05/10/23 06:25:19.605
  STEP: Creating a pod to test consume secrets @ 05/10/23 06:25:19.612
  E0510 06:25:20.533449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:21.533981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:22.535103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:23.535213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:24.535353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:25.535668      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:25:25.631
  May 10 06:25:25.633: INFO: Trying to get logs from node node-02 pod pod-projected-secrets-ba9a96d1-842b-4f7c-ba00-5f1ed5acc47b container secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 06:25:25.637
  May 10 06:25:25.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2909" for this suite. @ 05/10/23 06:25:25.655
• [6.074 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 05/10/23 06:25:25.66
  May 10 06:25:25.660: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 06:25:25.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:25:25.68
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:25:25.683
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 06:25:25.685
  E0510 06:25:26.536374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:27.537025      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:28.539110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:29.539307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:25:29.701
  May 10 06:25:29.703: INFO: Trying to get logs from node node-02 pod downwardapi-volume-86e71a06-b86e-4867-81ba-bb32000142ac container client-container: <nil>
  STEP: delete the pod @ 05/10/23 06:25:29.708
  May 10 06:25:29.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5889" for this suite. @ 05/10/23 06:25:29.724
• [4.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 05/10/23 06:25:29.729
  May 10 06:25:29.729: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 06:25:29.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:25:29.747
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:25:29.749
  STEP: Creating configMap with name configmap-test-volume-map-6462d636-9036-470e-ab4b-c9f1161a7604 @ 05/10/23 06:25:29.751
  STEP: Creating a pod to test consume configMaps @ 05/10/23 06:25:29.755
  E0510 06:25:30.539958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:31.540301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:32.540507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:33.541382      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:25:33.771
  May 10 06:25:33.773: INFO: Trying to get logs from node node-02 pod pod-configmaps-dfa41577-831e-4ea1-8af1-8b7ed7a5183a container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 06:25:33.777
  May 10 06:25:33.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1488" for this suite. @ 05/10/23 06:25:33.792
• [4.272 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 05/10/23 06:25:34.002
  May 10 06:25:34.002: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 06:25:34.003
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:25:34.016
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:25:34.018
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/10/23 06:25:34.02
  E0510 06:25:34.541674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:35.541823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:36.542777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:37.542850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:25:38.04
  May 10 06:25:38.042: INFO: Trying to get logs from node node-02 pod pod-0d48c23d-babf-4313-8e17-6bc4563b47fe container test-container: <nil>
  STEP: delete the pod @ 05/10/23 06:25:38.046
  May 10 06:25:38.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6932" for this suite. @ 05/10/23 06:25:38.061
• [4.067 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 05/10/23 06:25:38.069
  May 10 06:25:38.069: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename daemonsets @ 05/10/23 06:25:38.07
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:25:38.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:25:38.084
  STEP: Creating a simple DaemonSet "daemon-set" @ 05/10/23 06:25:38.098
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/10/23 06:25:38.105
  May 10 06:25:38.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:25:38.109: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 06:25:38.543189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:39.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:25:39.114: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 06:25:39.543239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:40.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 10 06:25:40.115: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 05/10/23 06:25:40.117
  May 10 06:25:40.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 10 06:25:40.138: INFO: Node node-02 is running 0 daemon pod, expected 1
  E0510 06:25:40.544110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:41.151: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 10 06:25:41.152: INFO: Node node-02 is running 0 daemon pod, expected 1
  E0510 06:25:41.544363      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:42.144: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 10 06:25:42.144: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 05/10/23 06:25:42.144
  STEP: Deleting DaemonSet "daemon-set" @ 05/10/23 06:25:42.147
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9824, will wait for the garbage collector to delete the pods @ 05/10/23 06:25:42.147
  May 10 06:25:42.203: INFO: Deleting DaemonSet.extensions daemon-set took: 4.448289ms
  May 10 06:25:42.304: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.753654ms
  E0510 06:25:42.545199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:43.546011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:44.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:25:44.406: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 10 06:25:44.408: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1093312"},"items":null}

  May 10 06:25:44.409: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1093312"},"items":null}

  May 10 06:25:44.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9824" for this suite. @ 05/10/23 06:25:44.416
• [6.351 seconds]
------------------------------
SS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 05/10/23 06:25:44.42
  May 10 06:25:44.420: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename disruption @ 05/10/23 06:25:44.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:25:44.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:25:44.431
  STEP: creating the pdb @ 05/10/23 06:25:44.433
  STEP: Waiting for the pdb to be processed @ 05/10/23 06:25:44.436
  E0510 06:25:44.547041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:45.547244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 05/10/23 06:25:46.44
  STEP: Waiting for the pdb to be processed @ 05/10/23 06:25:46.446
  E0510 06:25:46.547750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:47.548187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 05/10/23 06:25:48.451
  STEP: Waiting for the pdb to be processed @ 05/10/23 06:25:48.457
  E0510 06:25:48.549114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:49.549403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 05/10/23 06:25:50.467
  May 10 06:25:50.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7762" for this suite. @ 05/10/23 06:25:50.471
• [6.054 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 05/10/23 06:25:50.475
  May 10 06:25:50.475: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pods @ 05/10/23 06:25:50.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:25:50.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:25:50.507
  STEP: Create set of pods @ 05/10/23 06:25:50.509
  May 10 06:25:50.514: INFO: created test-pod-1
  May 10 06:25:50.523: INFO: created test-pod-2
  May 10 06:25:50.527: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 05/10/23 06:25:50.527
  E0510 06:25:50.549797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:51.549985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:52.550422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 05/10/23 06:25:52.58
  May 10 06:25:52.583: INFO: Pod quantity 3 is different from expected quantity 0
  E0510 06:25:53.551179      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:53.586: INFO: Pod quantity 3 is different from expected quantity 0
  E0510 06:25:54.552156      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:54.586: INFO: Pod quantity 3 is different from expected quantity 0
  E0510 06:25:55.552798      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:55.588: INFO: Pod quantity 3 is different from expected quantity 0
  E0510 06:25:56.553177      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:56.585: INFO: Pod quantity 1 is different from expected quantity 0
  E0510 06:25:57.554248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:25:57.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7866" for this suite. @ 05/10/23 06:25:57.588
• [7.118 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 05/10/23 06:25:57.593
  May 10 06:25:57.593: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/10/23 06:25:57.594
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:25:57.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:25:57.607
  STEP: set up a multi version CRD @ 05/10/23 06:25:57.609
  May 10 06:25:57.610: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 06:25:58.555145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:25:59.555643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:00.556485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 05/10/23 06:26:01.519
  STEP: check the unserved version gets removed @ 05/10/23 06:26:01.534
  E0510 06:26:01.556832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 05/10/23 06:26:02.493
  E0510 06:26:02.557752      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:03.558004      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:04.558266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:05.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3651" for this suite. @ 05/10/23 06:26:05.54
• [7.952 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 05/10/23 06:26:05.545
  May 10 06:26:05.545: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 06:26:05.546
  E0510 06:26:05.558883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:26:05.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:26:05.562
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/10/23 06:26:05.564
  E0510 06:26:06.559680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:07.559839      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:08.560822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:09.561388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:26:09.579
  May 10 06:26:09.581: INFO: Trying to get logs from node node-02 pod pod-0a82eb1a-d9b7-42eb-942a-2e73a0d8d189 container test-container: <nil>
  STEP: delete the pod @ 05/10/23 06:26:09.585
  May 10 06:26:09.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7468" for this suite. @ 05/10/23 06:26:09.602
• [4.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 05/10/23 06:26:09.606
  May 10 06:26:09.606: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename replicaset @ 05/10/23 06:26:09.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:26:09.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:26:09.619
  May 10 06:26:09.629: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0510 06:26:10.562033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:11.562462      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:12.562936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:13.563130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:14.563324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:14.632: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/10/23 06:26:14.632
  STEP: Scaling up "test-rs" replicaset  @ 05/10/23 06:26:14.632
  May 10 06:26:14.639: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 05/10/23 06:26:14.639
  W0510 06:26:14.647702      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 10 06:26:14.649: INFO: observed ReplicaSet test-rs in namespace replicaset-1672 with ReadyReplicas 1, AvailableReplicas 1
  May 10 06:26:14.659: INFO: observed ReplicaSet test-rs in namespace replicaset-1672 with ReadyReplicas 1, AvailableReplicas 1
  May 10 06:26:14.676: INFO: observed ReplicaSet test-rs in namespace replicaset-1672 with ReadyReplicas 1, AvailableReplicas 1
  May 10 06:26:14.682: INFO: observed ReplicaSet test-rs in namespace replicaset-1672 with ReadyReplicas 1, AvailableReplicas 1
  E0510 06:26:15.564248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:16.564359      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:16.658: INFO: observed ReplicaSet test-rs in namespace replicaset-1672 with ReadyReplicas 2, AvailableReplicas 2
  May 10 06:26:17.101: INFO: observed Replicaset test-rs in namespace replicaset-1672 with ReadyReplicas 3 found true
  May 10 06:26:17.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1672" for this suite. @ 05/10/23 06:26:17.104
• [7.503 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 05/10/23 06:26:17.11
  May 10 06:26:17.110: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename statefulset @ 05/10/23 06:26:17.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:26:17.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:26:17.126
  STEP: Creating service test in namespace statefulset-8586 @ 05/10/23 06:26:17.128
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 05/10/23 06:26:17.132
  STEP: Creating stateful set ss in namespace statefulset-8586 @ 05/10/23 06:26:17.135
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8586 @ 05/10/23 06:26:17.142
  May 10 06:26:17.144: INFO: Found 0 stateful pods, waiting for 1
  E0510 06:26:17.565459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:18.565975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:19.566052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:20.566264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:21.566496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:22.566741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:23.566994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:24.567209      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:25.567419      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:26.567624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:27.147: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 05/10/23 06:26:27.147
  May 10 06:26:27.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-8586 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 10 06:26:27.286: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 10 06:26:27.286: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 10 06:26:27.286: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 10 06:26:27.290: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0510 06:26:27.568395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:28.568610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:29.568765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:30.568914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:31.569043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:32.569383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:33.569618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:34.569792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:35.570017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:36.570232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:37.293: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 10 06:26:37.293: INFO: Waiting for statefulset status.replicas updated to 0
  May 10 06:26:37.303: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999824s
  E0510 06:26:37.570996      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:38.306: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998217732s
  E0510 06:26:38.571702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:39.309: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99541481s
  E0510 06:26:39.571827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:40.312: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.992297002s
  E0510 06:26:40.572608      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:41.314: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989580378s
  E0510 06:26:41.573394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:42.316: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.986923318s
  E0510 06:26:42.574254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:43.320: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.984896917s
  E0510 06:26:43.574501      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:44.323: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.981660728s
  E0510 06:26:44.575076      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:45.326: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.978113665s
  E0510 06:26:45.575172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:46.329: INFO: Verifying statefulset ss doesn't scale past 1 for another 974.99193ms
  E0510 06:26:46.576149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8586 @ 05/10/23 06:26:47.33
  May 10 06:26:47.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-8586 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 10 06:26:47.455: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 10 06:26:47.455: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 10 06:26:47.455: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 10 06:26:47.457: INFO: Found 1 stateful pods, waiting for 3
  E0510 06:26:47.576294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:48.577313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:49.578350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:50.578564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:51.578793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:52.579088      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:53.579401      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:54.579642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:55.579838      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:56.580052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:57.461: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 10 06:26:57.461: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 10 06:26:57.461: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 05/10/23 06:26:57.461
  STEP: Scale down will halt with unhealthy stateful pod @ 05/10/23 06:26:57.461
  May 10 06:26:57.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-8586 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 10 06:26:57.576: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 10 06:26:57.577: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 10 06:26:57.577: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 10 06:26:57.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-8586 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0510 06:26:57.580382      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:26:57.702: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 10 06:26:57.702: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 10 06:26:57.702: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 10 06:26:57.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-8586 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 10 06:26:57.842: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 10 06:26:57.843: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 10 06:26:57.843: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 10 06:26:57.843: INFO: Waiting for statefulset status.replicas updated to 0
  May 10 06:26:57.845: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0510 06:26:58.580514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:26:59.580722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:00.581117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:01.581326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:02.581771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:03.581982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:04.582110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:05.582298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:06.582538      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:07.583312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:07.850: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 10 06:27:07.850: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 10 06:27:07.850: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 10 06:27:07.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999818s
  E0510 06:27:08.584347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:08.862: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998127541s
  E0510 06:27:09.584793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:09.864: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995519772s
  E0510 06:27:10.585676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:10.867: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992641938s
  E0510 06:27:11.586577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:11.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989742807s
  E0510 06:27:12.587324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:12.873: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98697552s
  E0510 06:27:13.588330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:13.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983976556s
  E0510 06:27:14.589365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:14.879: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.980911241s
  E0510 06:27:15.590347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:15.882: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.977753989s
  E0510 06:27:16.591260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:16.885: INFO: Verifying statefulset ss doesn't scale past 3 for another 974.639083ms
  E0510 06:27:17.591446      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8586 @ 05/10/23 06:27:17.885
  May 10 06:27:17.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-8586 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 10 06:27:18.007: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 10 06:27:18.007: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 10 06:27:18.007: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 10 06:27:18.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-8586 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 10 06:27:18.128: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 10 06:27:18.128: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 10 06:27:18.128: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 10 06:27:18.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=statefulset-8586 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 10 06:27:18.256: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 10 06:27:18.256: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 10 06:27:18.256: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 10 06:27:18.256: INFO: Scaling statefulset ss to 0
  E0510 06:27:18.591669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:19.592372      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:20.593268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:21.593775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:22.594190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:23.594596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:24.594809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:25.595016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:26.595223      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:27.595683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 05/10/23 06:27:28.267
  May 10 06:27:28.267: INFO: Deleting all statefulset in ns statefulset-8586
  May 10 06:27:28.269: INFO: Scaling statefulset ss to 0
  May 10 06:27:28.275: INFO: Waiting for statefulset status.replicas updated to 0
  May 10 06:27:28.277: INFO: Deleting statefulset ss
  May 10 06:27:28.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8586" for this suite. @ 05/10/23 06:27:28.286
• [71.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 05/10/23 06:27:28.293
  May 10 06:27:28.293: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/10/23 06:27:28.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:27:28.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:27:28.312
  May 10 06:27:28.314: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 06:27:28.595986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:29.596777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:30.596998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:31.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-337" for this suite. @ 05/10/23 06:27:31.4
• [3.112 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 05/10/23 06:27:31.405
  May 10 06:27:31.405: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename daemonsets @ 05/10/23 06:27:31.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:27:31.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:27:31.418
  May 10 06:27:31.431: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/10/23 06:27:31.438
  May 10 06:27:31.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:27:31.441: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 06:27:31.596986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:32.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:27:32.447: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 06:27:32.597360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:33.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 10 06:27:33.523: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 05/10/23 06:27:33.531
  STEP: Check that daemon pods images are updated. @ 05/10/23 06:27:33.543
  May 10 06:27:33.545: INFO: Wrong image for pod: daemon-set-bxq2f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 10 06:27:33.545: INFO: Wrong image for pod: daemon-set-l5q9t. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 10 06:27:33.545: INFO: Wrong image for pod: daemon-set-zmzvj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0510 06:27:33.597834      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:34.553: INFO: Wrong image for pod: daemon-set-bxq2f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 10 06:27:34.553: INFO: Wrong image for pod: daemon-set-zmzvj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0510 06:27:34.598267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:35.598648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:35.762: INFO: Wrong image for pod: daemon-set-bxq2f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 10 06:27:35.762: INFO: Wrong image for pod: daemon-set-zmzvj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 10 06:27:36.552: INFO: Wrong image for pod: daemon-set-bxq2f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 10 06:27:36.552: INFO: Wrong image for pod: daemon-set-zmzvj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0510 06:27:36.599605      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:37.553: INFO: Wrong image for pod: daemon-set-bxq2f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 10 06:27:37.553: INFO: Pod daemon-set-f7dvv is not available
  May 10 06:27:37.553: INFO: Wrong image for pod: daemon-set-zmzvj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0510 06:27:37.599635      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:38.554: INFO: Wrong image for pod: daemon-set-zmzvj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0510 06:27:38.600634      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:39.553: INFO: Wrong image for pod: daemon-set-zmzvj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0510 06:27:39.601292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:40.601854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:40.694: INFO: Pod daemon-set-l2bxz is not available
  May 10 06:27:40.694: INFO: Wrong image for pod: daemon-set-zmzvj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 10 06:27:41.554: INFO: Pod daemon-set-l2bxz is not available
  May 10 06:27:41.554: INFO: Wrong image for pod: daemon-set-zmzvj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0510 06:27:41.602949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:42.603196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:43.552: INFO: Pod daemon-set-2ckpq is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 05/10/23 06:27:43.554
  May 10 06:27:43.563: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 10 06:27:43.563: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 06:27:43.604069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:44.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 10 06:27:44.569: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/10/23 06:27:44.578
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8008, will wait for the garbage collector to delete the pods @ 05/10/23 06:27:44.578
  E0510 06:27:44.605001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:44.634: INFO: Deleting DaemonSet.extensions daemon-set took: 4.702514ms
  May 10 06:27:44.735: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.296039ms
  E0510 06:27:45.605646      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:46.606681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:27:47.137: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:27:47.137: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 10 06:27:47.139: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1094357"},"items":null}

  May 10 06:27:47.141: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1094357"},"items":null}

  May 10 06:27:47.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8008" for this suite. @ 05/10/23 06:27:47.149
• [15.748 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 05/10/23 06:27:47.153
  May 10 06:27:47.153: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 06:27:47.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:27:47.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:27:47.17
  STEP: Creating configMap with name configmap-test-volume-map-0d45586d-46af-4cfa-9a35-ff0747e14695 @ 05/10/23 06:27:47.172
  STEP: Creating a pod to test consume configMaps @ 05/10/23 06:27:47.178
  E0510 06:27:47.607417      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:48.607767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:49.608416      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:50.608645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:27:51.191
  May 10 06:27:51.193: INFO: Trying to get logs from node node-02 pod pod-configmaps-cc77985f-7870-4532-ba6c-dc72647be6cc container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 06:27:51.203
  May 10 06:27:51.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8765" for this suite. @ 05/10/23 06:27:51.22
• [4.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 05/10/23 06:27:51.225
  May 10 06:27:51.225: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:27:51.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:27:51.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:27:51.24
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 06:27:51.242
  E0510 06:27:51.609236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:52.609747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:53.610212      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:54.610820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:27:55.258
  May 10 06:27:55.260: INFO: Trying to get logs from node node-02 pod downwardapi-volume-6b3d990e-5253-4d5c-b351-880d52ae226c container client-container: <nil>
  STEP: delete the pod @ 05/10/23 06:27:55.264
  May 10 06:27:55.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1043" for this suite. @ 05/10/23 06:27:55.283
• [4.067 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 05/10/23 06:27:55.293
  May 10 06:27:55.293: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename projected @ 05/10/23 06:27:55.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:27:55.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:27:55.305
  STEP: Creating a pod to test downward API volume plugin @ 05/10/23 06:27:55.307
  E0510 06:27:55.611465      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:56.611827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:57.612554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:27:58.612923      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:27:59.321
  May 10 06:27:59.322: INFO: Trying to get logs from node node-02 pod downwardapi-volume-fdb2fb54-f663-4d45-b186-86a56d107bf0 container client-container: <nil>
  STEP: delete the pod @ 05/10/23 06:27:59.328
  May 10 06:27:59.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9764" for this suite. @ 05/10/23 06:27:59.351
• [4.063 seconds]
------------------------------
SS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 05/10/23 06:27:59.355
  May 10 06:27:59.355: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename containers @ 05/10/23 06:27:59.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:27:59.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:27:59.368
  STEP: Creating a pod to test override all @ 05/10/23 06:27:59.37
  E0510 06:27:59.613872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:00.614482      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:01.615418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:02.615887      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:28:03.386
  May 10 06:28:03.388: INFO: Trying to get logs from node node-02 pod client-containers-dc77f1f0-8125-4166-a848-254b39017787 container agnhost-container: <nil>
  STEP: delete the pod @ 05/10/23 06:28:03.393
  May 10 06:28:03.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4414" for this suite. @ 05/10/23 06:28:03.413
• [4.062 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 05/10/23 06:28:03.419
  May 10 06:28:03.419: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 06:28:03.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:28:03.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:28:03.436
  STEP: Creating secret with name secret-test-map-0b81f7d0-c9dc-43fd-9409-2c8b9517a553 @ 05/10/23 06:28:03.438
  STEP: Creating a pod to test consume secrets @ 05/10/23 06:28:03.442
  E0510 06:28:03.616367      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:04.616828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:05.617609      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:06.618036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:28:07.493
  May 10 06:28:07.495: INFO: Trying to get logs from node node-02 pod pod-secrets-b3ee5dfe-62df-4c44-a185-26fb7091493c container secret-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 06:28:07.5
  May 10 06:28:07.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7237" for this suite. @ 05/10/23 06:28:07.515
• [4.100 seconds]
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 05/10/23 06:28:07.519
  May 10 06:28:07.519: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename init-container @ 05/10/23 06:28:07.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:28:07.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:28:07.534
  STEP: creating the pod @ 05/10/23 06:28:07.537
  May 10 06:28:07.537: INFO: PodSpec: initContainers in spec.initContainers
  E0510 06:28:07.618653      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:08.619430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:09.620341      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:10.620743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:11.621369      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:12.621524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:13.621861      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:14.622148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:15.622290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:16.622561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:17.623119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:18.623309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:19.624383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:20.624513      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:21.624769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:22.625207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:23.625332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:24.625524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:25.625813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:26.625917      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:27.626172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:28.626486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:29.626611      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:30.626978      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:31.627242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:32.627393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:33.627600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:34.627916      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:35.628031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:36.628202      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:37.628319      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:38.629184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:39.629301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:40.629444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:41.630398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:42.630516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:43.630651      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:44.630837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:45.630933      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:46.631121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:47.631248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:48.631433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:49.631495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:50.631620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:51.631945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:52.632051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:28:52.802: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-10cc2462-f4ac-44a4-b24f-626316c9ee50", GenerateName:"", Namespace:"init-container-1887", SelfLink:"", UID:"806061db-db21-4ab0-bdda-02f96cd56cf7", ResourceVersion:"1094776", Generation:0, CreationTimestamp:time.Date(2023, time.May, 10, 6, 28, 7, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"537347295"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"c7e99d146f46a20cb2e12d868878e004ee4fc60310abb46c4bca2eace5ce532e", "cni.projectcalico.org/podIP":"100.114.252.159/32", "cni.projectcalico.org/podIPs":"100.114.252.159/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 10, 6, 28, 7, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc007abfa10), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 10, 6, 28, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc007abfa40), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 10, 6, 28, 52, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc007abfa70), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-wvqg9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0049347c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wvqg9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wvqg9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wvqg9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc007b311d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node-02", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000762f50), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007b31250)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007b31270)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc007b31278), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc007b3127c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005ea1a00), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 10, 6, 28, 7, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 10, 6, 28, 7, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 10, 6, 28, 7, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 10, 6, 28, 7, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.60.6", PodIP:"100.114.252.159", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.114.252.159"}}, StartTime:time.Date(2023, time.May, 10, 6, 28, 7, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000763030)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0007630a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"sha256:d59c675982d8692814ec9e1486d4c645cd86ad825ef33975a5db196cf2801592", ContainerID:"containerd://b885940faedec50883fa39088f34db31018121bc980e530ce1f893f37fed21ee", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004934840), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004934820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc007b312ff), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  May 10 06:28:52.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1887" for this suite. @ 05/10/23 06:28:52.805
• [45.289 seconds]
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 05/10/23 06:28:52.809
  May 10 06:28:52.809: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename events @ 05/10/23 06:28:52.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:28:52.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:28:52.82
  STEP: creating a test event @ 05/10/23 06:28:52.829
  STEP: listing events in all namespaces @ 05/10/23 06:28:52.834
  STEP: listing events in test namespace @ 05/10/23 06:28:52.836
  STEP: listing events with field selection filtering on source @ 05/10/23 06:28:52.838
  STEP: listing events with field selection filtering on reportingController @ 05/10/23 06:28:52.839
  STEP: getting the test event @ 05/10/23 06:28:52.841
  STEP: patching the test event @ 05/10/23 06:28:52.842
  STEP: getting the test event @ 05/10/23 06:28:52.847
  STEP: updating the test event @ 05/10/23 06:28:52.848
  STEP: getting the test event @ 05/10/23 06:28:52.852
  STEP: deleting the test event @ 05/10/23 06:28:52.854
  STEP: listing events in all namespaces @ 05/10/23 06:28:52.858
  STEP: listing events in test namespace @ 05/10/23 06:28:52.86
  May 10 06:28:52.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9525" for this suite. @ 05/10/23 06:28:52.863
• [0.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 05/10/23 06:28:52.868
  May 10 06:28:52.868: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pods @ 05/10/23 06:28:52.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:28:52.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:28:52.883
  May 10 06:28:52.885: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: creating the pod @ 05/10/23 06:28:52.885
  STEP: submitting the pod to kubernetes @ 05/10/23 06:28:52.886
  E0510 06:28:53.632101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:54.632556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:28:54.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4293" for this suite. @ 05/10/23 06:28:54.912
• [2.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 05/10/23 06:28:54.917
  May 10 06:28:54.917: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 06:28:54.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:28:54.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:28:54.928
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/10/23 06:28:54.93
  E0510 06:28:55.633465      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:56.633654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:57.634648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:28:58.634743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:28:58.943
  May 10 06:28:58.945: INFO: Trying to get logs from node node-01 pod pod-43fa305e-81e3-4a73-a05f-5587b8571199 container test-container: <nil>
  STEP: delete the pod @ 05/10/23 06:28:58.958
  May 10 06:28:58.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1053" for this suite. @ 05/10/23 06:28:58.976
• [4.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 05/10/23 06:28:58.982
  May 10 06:28:58.982: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename statefulset @ 05/10/23 06:28:58.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:28:59.001
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:28:59.003
  STEP: Creating service test in namespace statefulset-6324 @ 05/10/23 06:28:59.005
  STEP: Creating statefulset ss in namespace statefulset-6324 @ 05/10/23 06:28:59.013
  May 10 06:28:59.022: INFO: Found 0 stateful pods, waiting for 1
  E0510 06:28:59.634959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:00.635136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:01.635432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:02.635830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:03.636007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:04.636498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:05.636729      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:06.636959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:07.637737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:08.637913      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:29:09.025: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 05/10/23 06:29:09.028
  STEP: Getting /status @ 05/10/23 06:29:09.037
  May 10 06:29:09.039: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 05/10/23 06:29:09.039
  May 10 06:29:09.050: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 05/10/23 06:29:09.05
  May 10 06:29:09.052: INFO: Observed &StatefulSet event: ADDED
  May 10 06:29:09.052: INFO: Found Statefulset ss in namespace statefulset-6324 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 10 06:29:09.052: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 05/10/23 06:29:09.052
  May 10 06:29:09.052: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 10 06:29:09.060: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 05/10/23 06:29:09.06
  May 10 06:29:09.061: INFO: Observed &StatefulSet event: ADDED
  May 10 06:29:09.062: INFO: Observed Statefulset ss in namespace statefulset-6324 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 10 06:29:09.062: INFO: Observed &StatefulSet event: MODIFIED
  May 10 06:29:09.062: INFO: Found Statefulset ss in namespace statefulset-6324 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  May 10 06:29:09.062: INFO: Deleting all statefulset in ns statefulset-6324
  May 10 06:29:09.064: INFO: Scaling statefulset ss to 0
  E0510 06:29:09.638428      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:10.638578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:11.638735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:12.638874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:13.639124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:14.639273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:15.639415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:16.639708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:17.639817      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:18.639954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:29:19.077: INFO: Waiting for statefulset status.replicas updated to 0
  May 10 06:29:19.079: INFO: Deleting statefulset ss
  May 10 06:29:19.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6324" for this suite. @ 05/10/23 06:29:19.092
• [20.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 05/10/23 06:29:19.101
  May 10 06:29:19.101: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename deployment @ 05/10/23 06:29:19.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:29:19.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:29:19.113
  STEP: creating a Deployment @ 05/10/23 06:29:19.117
  May 10 06:29:19.117: INFO: Creating simple deployment test-deployment-xvc5h
  May 10 06:29:19.131: INFO: deployment "test-deployment-xvc5h" doesn't have the required revision set
  E0510 06:29:19.640877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:20.641118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 05/10/23 06:29:21.138
  May 10 06:29:21.140: INFO: Deployment test-deployment-xvc5h has Conditions: [{Available True 2023-05-10 06:29:20 +0000 UTC 2023-05-10 06:29:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-10 06:29:20 +0000 UTC 2023-05-10 06:29:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xvc5h-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 05/10/23 06:29:21.14
  May 10 06:29:21.151: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 6, 29, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 6, 29, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 10, 6, 29, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 10, 6, 29, 19, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-xvc5h-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 05/10/23 06:29:21.151
  May 10 06:29:21.153: INFO: Observed &Deployment event: ADDED
  May 10 06:29:21.153: INFO: Observed Deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-10 06:29:19 +0000 UTC 2023-05-10 06:29:19 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xvc5h-5994cf9475"}
  May 10 06:29:21.153: INFO: Observed &Deployment event: MODIFIED
  May 10 06:29:21.153: INFO: Observed Deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-10 06:29:19 +0000 UTC 2023-05-10 06:29:19 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xvc5h-5994cf9475"}
  May 10 06:29:21.153: INFO: Observed Deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-10 06:29:19 +0000 UTC 2023-05-10 06:29:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 10 06:29:21.153: INFO: Observed &Deployment event: MODIFIED
  May 10 06:29:21.153: INFO: Observed Deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-10 06:29:19 +0000 UTC 2023-05-10 06:29:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 10 06:29:21.153: INFO: Observed Deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-10 06:29:19 +0000 UTC 2023-05-10 06:29:19 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-xvc5h-5994cf9475" is progressing.}
  May 10 06:29:21.153: INFO: Observed &Deployment event: MODIFIED
  May 10 06:29:21.153: INFO: Observed Deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-10 06:29:20 +0000 UTC 2023-05-10 06:29:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 10 06:29:21.153: INFO: Observed Deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-10 06:29:20 +0000 UTC 2023-05-10 06:29:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xvc5h-5994cf9475" has successfully progressed.}
  May 10 06:29:21.153: INFO: Observed &Deployment event: MODIFIED
  May 10 06:29:21.153: INFO: Observed Deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-10 06:29:20 +0000 UTC 2023-05-10 06:29:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 10 06:29:21.153: INFO: Observed Deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-10 06:29:20 +0000 UTC 2023-05-10 06:29:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xvc5h-5994cf9475" has successfully progressed.}
  May 10 06:29:21.153: INFO: Found Deployment test-deployment-xvc5h in namespace deployment-1636 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 10 06:29:21.153: INFO: Deployment test-deployment-xvc5h has an updated status
  STEP: patching the Statefulset Status @ 05/10/23 06:29:21.153
  May 10 06:29:21.153: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 10 06:29:21.158: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 05/10/23 06:29:21.158
  May 10 06:29:21.160: INFO: Observed &Deployment event: ADDED
  May 10 06:29:21.160: INFO: Observed deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-10 06:29:19 +0000 UTC 2023-05-10 06:29:19 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xvc5h-5994cf9475"}
  May 10 06:29:21.160: INFO: Observed &Deployment event: MODIFIED
  May 10 06:29:21.160: INFO: Observed deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-10 06:29:19 +0000 UTC 2023-05-10 06:29:19 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xvc5h-5994cf9475"}
  May 10 06:29:21.160: INFO: Observed deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-10 06:29:19 +0000 UTC 2023-05-10 06:29:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 10 06:29:21.160: INFO: Observed &Deployment event: MODIFIED
  May 10 06:29:21.160: INFO: Observed deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-10 06:29:19 +0000 UTC 2023-05-10 06:29:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 10 06:29:21.160: INFO: Observed deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-10 06:29:19 +0000 UTC 2023-05-10 06:29:19 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-xvc5h-5994cf9475" is progressing.}
  May 10 06:29:21.160: INFO: Observed &Deployment event: MODIFIED
  May 10 06:29:21.160: INFO: Observed deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-10 06:29:20 +0000 UTC 2023-05-10 06:29:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 10 06:29:21.160: INFO: Observed deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-10 06:29:20 +0000 UTC 2023-05-10 06:29:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xvc5h-5994cf9475" has successfully progressed.}
  May 10 06:29:21.160: INFO: Observed &Deployment event: MODIFIED
  May 10 06:29:21.160: INFO: Observed deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-10 06:29:20 +0000 UTC 2023-05-10 06:29:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 10 06:29:21.160: INFO: Observed deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-10 06:29:20 +0000 UTC 2023-05-10 06:29:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xvc5h-5994cf9475" has successfully progressed.}
  May 10 06:29:21.160: INFO: Observed deployment test-deployment-xvc5h in namespace deployment-1636 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 10 06:29:21.160: INFO: Observed &Deployment event: MODIFIED
  May 10 06:29:21.160: INFO: Found deployment test-deployment-xvc5h in namespace deployment-1636 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  May 10 06:29:21.160: INFO: Deployment test-deployment-xvc5h has a patched status
  May 10 06:29:21.167: INFO: Deployment "test-deployment-xvc5h":
  &Deployment{ObjectMeta:{test-deployment-xvc5h  deployment-1636  071c4883-0683-4eeb-aba4-4b5b325f64ea 1095027 1 2023-05-10 06:29:19 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-10 06:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-10 06:29:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-10 06:29:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0057add58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-xvc5h-5994cf9475",LastUpdateTime:2023-05-10 06:29:21 +0000 UTC,LastTransitionTime:2023-05-10 06:29:21 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 10 06:29:21.169: INFO: New ReplicaSet "test-deployment-xvc5h-5994cf9475" of Deployment "test-deployment-xvc5h":
  &ReplicaSet{ObjectMeta:{test-deployment-xvc5h-5994cf9475  deployment-1636  52a05095-c14d-4c55-80a5-16c1c21b022d 1095023 1 2023-05-10 06:29:19 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-xvc5h 071c4883-0683-4eeb-aba4-4b5b325f64ea 0xc008383107 0xc008383108}] [] [{kube-controller-manager Update apps/v1 2023-05-10 06:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"071c4883-0683-4eeb-aba4-4b5b325f64ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:29:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083831c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 10 06:29:21.171: INFO: Pod "test-deployment-xvc5h-5994cf9475-9lj5z" is available:
  &Pod{ObjectMeta:{test-deployment-xvc5h-5994cf9475-9lj5z test-deployment-xvc5h-5994cf9475- deployment-1636  71e263ad-5062-4ae1-b6b8-a78157c2ff78 1095022 0 2023-05-10 06:29:19 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[cni.projectcalico.org/containerID:3ad7bb4362cdc26d9b86203d4f7fc2b6a455fc13dc59c5740e57f4f6eafea531 cni.projectcalico.org/podIP:100.114.252.150/32 cni.projectcalico.org/podIPs:100.114.252.150/32] [{apps/v1 ReplicaSet test-deployment-xvc5h-5994cf9475 52a05095-c14d-4c55-80a5-16c1c21b022d 0xc0083835b7 0xc0083835b8}] [] [{calico Update v1 2023-05-10 06:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-10 06:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52a05095-c14d-4c55-80a5-16c1c21b022d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:29:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.114.252.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5r5qp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5r5qp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:29:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:100.114.252.150,StartTime:2023-05-10 06:29:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 06:29:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://4162c654fc0fb2c26569ad44c52b47210e1ece71e2cf6dd2b0aa27e93562ce51,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.114.252.150,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:29:21.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1636" for this suite. @ 05/10/23 06:29:21.174
• [2.082 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 05/10/23 06:29:21.183
  May 10 06:29:21.183: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename pods @ 05/10/23 06:29:21.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:29:21.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:29:21.198
  May 10 06:29:21.200: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: creating the pod @ 05/10/23 06:29:21.202
  STEP: submitting the pod to kubernetes @ 05/10/23 06:29:21.202
  E0510 06:29:21.641418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:22.642033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:29:23.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4209" for this suite. @ 05/10/23 06:29:23.305
• [2.126 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 05/10/23 06:29:23.31
  May 10 06:29:23.310: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 06:29:23.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:29:23.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:29:23.323
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/10/23 06:29:23.326
  E0510 06:29:23.643050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:24.643386      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:25.643906      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:26.644262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:29:27.347
  May 10 06:29:27.350: INFO: Trying to get logs from node node-02 pod pod-dc605470-0022-4dc7-be1d-bcdac049786a container test-container: <nil>
  STEP: delete the pod @ 05/10/23 06:29:27.356
  May 10 06:29:27.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3395" for this suite. @ 05/10/23 06:29:27.374
• [4.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 05/10/23 06:29:27.386
  May 10 06:29:27.386: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename sched-preemption @ 05/10/23 06:29:27.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:29:27.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:29:27.402
  May 10 06:29:27.419: INFO: Waiting up to 1m0s for all nodes to be ready
  E0510 06:29:27.645215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:28.645454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:29.646472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:30.646711      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:31.647313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:32.647863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:33.648372      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:34.648603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:35.649162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:36.649359      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:37.649495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:38.649727      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:39.650772      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:40.650975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:41.651758      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:42.652244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:43.652636      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:44.652862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:45.653289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:46.653517      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:47.653905      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:48.654126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:49.654272      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:50.654540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:51.655704      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:52.656086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:53.656626      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:54.656921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:55.657623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:56.657840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:57.658421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:58.658650      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:29:59.659543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:00.659765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:01.660237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:02.660674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:03.661736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:04.661958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:05.662357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:06.662767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:07.663017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:08.663184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:09.663540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:10.663753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:11.663988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:12.664112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:13.664528      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:14.664747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:15.664804      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:16.665382      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:17.665926      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:18.666049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:19.666885      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:20.667097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:21.668023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:22.668333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:23.669063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:24.669286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:25.670009      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:26.670157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:27.442: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/10/23 06:30:27.445
  May 10 06:30:27.467: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 10 06:30:27.475: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May 10 06:30:27.491: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 10 06:30:27.501: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  May 10 06:30:27.527: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  May 10 06:30:27.540: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/10/23 06:30:27.541
  E0510 06:30:27.670472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:28.670675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:29.671086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:30.671426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 05/10/23 06:30:31.56
  E0510 06:30:31.672518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:32.672920      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:33.672959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:34.673023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:35.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-6413" for this suite. @ 05/10/23 06:30:35.621
• [68.238 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 05/10/23 06:30:35.625
  May 10 06:30:35.625: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename daemonsets @ 05/10/23 06:30:35.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:30:35.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:30:35.636
  May 10 06:30:35.651: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 05/10/23 06:30:35.654
  May 10 06:30:35.656: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:30:35.656: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 05/10/23 06:30:35.656
  E0510 06:30:35.673314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:35.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:30:35.675: INFO: Node ub-test is running 0 daemon pod, expected 1
  E0510 06:30:36.673627      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:36.677: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:30:36.677: INFO: Node ub-test is running 0 daemon pod, expected 1
  E0510 06:30:37.674344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:37.678: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:30:37.678: INFO: Node ub-test is running 0 daemon pod, expected 1
  E0510 06:30:38.674625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:38.678: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 10 06:30:38.678: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 05/10/23 06:30:38.679
  May 10 06:30:38.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 10 06:30:38.694: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0510 06:30:39.675160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:39.696: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:30:39.696: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 05/10/23 06:30:39.696
  May 10 06:30:39.708: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:30:39.708: INFO: Node ub-test is running 0 daemon pod, expected 1
  E0510 06:30:40.675918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:40.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:30:40.711: INFO: Node ub-test is running 0 daemon pod, expected 1
  E0510 06:30:41.676505      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:41.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:30:41.808: INFO: Node ub-test is running 0 daemon pod, expected 1
  E0510 06:30:42.678988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:42.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:30:42.714: INFO: Node ub-test is running 0 daemon pod, expected 1
  E0510 06:30:43.680009      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:43.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 10 06:30:43.711: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/10/23 06:30:43.717
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2760, will wait for the garbage collector to delete the pods @ 05/10/23 06:30:43.717
  May 10 06:30:43.774: INFO: Deleting DaemonSet.extensions daemon-set took: 5.18676ms
  May 10 06:30:43.874: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.76015ms
  E0510 06:30:44.680754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:45.681783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:46.682703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:47.682988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:47.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:30:47.778: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 10 06:30:47.779: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1095641"},"items":null}

  May 10 06:30:47.781: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1095641"},"items":null}

  May 10 06:30:47.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2760" for this suite. @ 05/10/23 06:30:47.805
• [12.185 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 05/10/23 06:30:47.81
  May 10 06:30:47.810: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 06:30:47.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:30:47.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:30:47.825
  STEP: creating service endpoint-test2 in namespace services-7282 @ 05/10/23 06:30:47.827
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7282 to expose endpoints map[] @ 05/10/23 06:30:47.834
  May 10 06:30:47.836: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0510 06:30:48.683298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:48.841: INFO: successfully validated that service endpoint-test2 in namespace services-7282 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-7282 @ 05/10/23 06:30:48.841
  E0510 06:30:49.683894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:50.684420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7282 to expose endpoints map[pod1:[80]] @ 05/10/23 06:30:50.856
  May 10 06:30:50.861: INFO: successfully validated that service endpoint-test2 in namespace services-7282 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 05/10/23 06:30:50.861
  May 10 06:30:50.861: INFO: Creating new exec pod
  E0510 06:30:51.684655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:52.685025      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:53.685267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:53.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7282 exec execpod9m9ct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 10 06:30:54.001: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 10 06:30:54.001: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 06:30:54.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7282 exec execpod9m9ct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.0.155 80'
  May 10 06:30:54.119: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.0.155 80\nConnection to 10.96.0.155 80 port [tcp/http] succeeded!\n"
  May 10 06:30:54.119: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-7282 @ 05/10/23 06:30:54.119
  E0510 06:30:54.685935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:55.686465      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:56.687422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:30:57.687985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7282 to expose endpoints map[pod1:[80] pod2:[80]] @ 05/10/23 06:30:58.134
  May 10 06:30:58.141: INFO: successfully validated that service endpoint-test2 in namespace services-7282 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 05/10/23 06:30:58.141
  E0510 06:30:58.688913      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:30:59.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7282 exec execpod9m9ct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 10 06:30:59.259: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 10 06:30:59.259: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 06:30:59.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7282 exec execpod9m9ct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.0.155 80'
  May 10 06:30:59.391: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.0.155 80\nConnection to 10.96.0.155 80 port [tcp/http] succeeded!\n"
  May 10 06:30:59.391: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-7282 @ 05/10/23 06:30:59.391
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7282 to expose endpoints map[pod2:[80]] @ 05/10/23 06:30:59.405
  May 10 06:30:59.415: INFO: successfully validated that service endpoint-test2 in namespace services-7282 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 05/10/23 06:30:59.415
  E0510 06:30:59.689151      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:31:00.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7282 exec execpod9m9ct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 10 06:31:00.549: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 10 06:31:00.549: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 06:31:00.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-7282 exec execpod9m9ct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.0.155 80'
  May 10 06:31:00.663: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.0.155 80\nConnection to 10.96.0.155 80 port [tcp/http] succeeded!\n"
  May 10 06:31:00.663: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-7282 @ 05/10/23 06:31:00.663
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7282 to expose endpoints map[] @ 05/10/23 06:31:00.681
  E0510 06:31:00.689408      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:31:00.692: INFO: successfully validated that service endpoint-test2 in namespace services-7282 exposes endpoints map[]
  May 10 06:31:00.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7282" for this suite. @ 05/10/23 06:31:00.713
• [12.909 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 05/10/23 06:31:00.719
  May 10 06:31:00.719: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl-logs @ 05/10/23 06:31:00.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:31:00.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:31:00.741
  STEP: creating an pod @ 05/10/23 06:31:00.743
  May 10 06:31:00.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-logs-5997 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  May 10 06:31:00.811: INFO: stderr: ""
  May 10 06:31:00.811: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 05/10/23 06:31:00.811
  May 10 06:31:00.811: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0510 06:31:01.689749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:02.690179      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:31:02.820: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 05/10/23 06:31:02.82
  May 10 06:31:02.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-logs-5997 logs logs-generator logs-generator'
  May 10 06:31:02.891: INFO: stderr: ""
  May 10 06:31:02.891: INFO: stdout: "I0510 06:31:01.886040       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/cmv7 403\nI0510 06:31:02.086426       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/qkf 506\nI0510 06:31:02.286766       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/xvm 205\nI0510 06:31:02.486114       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/gvf6 561\nI0510 06:31:02.686540       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/t8kr 549\nI0510 06:31:02.886868       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/vszm 475\n"
  STEP: limiting log lines @ 05/10/23 06:31:02.891
  May 10 06:31:02.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-logs-5997 logs logs-generator logs-generator --tail=1'
  May 10 06:31:02.952: INFO: stderr: ""
  May 10 06:31:02.952: INFO: stdout: "I0510 06:31:02.886868       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/vszm 475\n"
  May 10 06:31:02.952: INFO: got output "I0510 06:31:02.886868       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/vszm 475\n"
  STEP: limiting log bytes @ 05/10/23 06:31:02.952
  May 10 06:31:02.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-logs-5997 logs logs-generator logs-generator --limit-bytes=1'
  May 10 06:31:03.015: INFO: stderr: ""
  May 10 06:31:03.015: INFO: stdout: "I"
  May 10 06:31:03.015: INFO: got output "I"
  STEP: exposing timestamps @ 05/10/23 06:31:03.016
  May 10 06:31:03.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-logs-5997 logs logs-generator logs-generator --tail=1 --timestamps'
  May 10 06:31:03.077: INFO: stderr: ""
  May 10 06:31:03.077: INFO: stdout: "2023-05-10T06:31:02.887032067Z I0510 06:31:02.886868       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/vszm 475\n"
  May 10 06:31:03.077: INFO: got output "2023-05-10T06:31:02.887032067Z I0510 06:31:02.886868       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/vszm 475\n"
  STEP: restricting to a time range @ 05/10/23 06:31:03.077
  E0510 06:31:03.690890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:04.691126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:31:05.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-logs-5997 logs logs-generator logs-generator --since=1s'
  May 10 06:31:05.639: INFO: stderr: ""
  May 10 06:31:05.639: INFO: stdout: "I0510 06:31:04.686916       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/sdl 325\nI0510 06:31:04.886231       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/jlfv 270\nI0510 06:31:05.086631       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/gqb 516\nI0510 06:31:05.287031       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/5hs 531\nI0510 06:31:05.486337       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/scs 592\n"
  May 10 06:31:05.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-logs-5997 logs logs-generator logs-generator --since=24h'
  E0510 06:31:05.691324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:31:05.699: INFO: stderr: ""
  May 10 06:31:05.699: INFO: stdout: "I0510 06:31:01.886040       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/cmv7 403\nI0510 06:31:02.086426       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/qkf 506\nI0510 06:31:02.286766       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/xvm 205\nI0510 06:31:02.486114       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/gvf6 561\nI0510 06:31:02.686540       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/t8kr 549\nI0510 06:31:02.886868       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/vszm 475\nI0510 06:31:03.086108       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/dbmd 446\nI0510 06:31:03.286486       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/nwf 308\nI0510 06:31:03.486836       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/kvng 341\nI0510 06:31:03.686165       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/b7s5 264\nI0510 06:31:03.886538       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/2jqm 470\nI0510 06:31:04.086946       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/m6d9 268\nI0510 06:31:04.286142       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/dmrz 200\nI0510 06:31:04.486534       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/ds8d 385\nI0510 06:31:04.686916       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/sdl 325\nI0510 06:31:04.886231       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/jlfv 270\nI0510 06:31:05.086631       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/gqb 516\nI0510 06:31:05.287031       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/5hs 531\nI0510 06:31:05.486337       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/scs 592\nI0510 06:31:05.686739       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/kbf9 396\n"
  May 10 06:31:05.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-logs-5997 delete pod logs-generator'
  E0510 06:31:06.692243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:31:07.337: INFO: stderr: ""
  May 10 06:31:07.337: INFO: stdout: "pod \"logs-generator\" deleted\n"
  May 10 06:31:07.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-5997" for this suite. @ 05/10/23 06:31:07.341
• [6.633 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 05/10/23 06:31:07.353
  May 10 06:31:07.353: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename deployment @ 05/10/23 06:31:07.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:31:07.365
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:31:07.367
  May 10 06:31:07.369: INFO: Creating simple deployment test-new-deployment
  May 10 06:31:07.382: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0510 06:31:07.692660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:08.692958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 05/10/23 06:31:09.389
  STEP: updating a scale subresource @ 05/10/23 06:31:09.391
  STEP: verifying the deployment Spec.Replicas was modified @ 05/10/23 06:31:09.395
  STEP: Patch a scale subresource @ 05/10/23 06:31:09.396
  May 10 06:31:09.420: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-4720  8859aafb-939e-4182-87ff-a7e0ac953e5a 1095861 3 2023-05-10 06:31:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-10 06:31:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00784d9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-10 06:31:09 +0000 UTC,LastTransitionTime:2023-05-10 06:31:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-05-10 06:31:09 +0000 UTC,LastTransitionTime:2023-05-10 06:31:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 10 06:31:09.434: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-4720  fd9eed37-197e-494c-9450-125bc964a8c6 1095865 2 2023-05-10 06:31:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 8859aafb-939e-4182-87ff-a7e0ac953e5a 0xc00784de07 0xc00784de08}] [] [{kube-controller-manager Update apps/v1 2023-05-10 06:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8859aafb-939e-4182-87ff-a7e0ac953e5a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-10 06:31:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00784de98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 10 06:31:09.436: INFO: Pod "test-new-deployment-67bd4bf6dc-g5b9d" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-g5b9d test-new-deployment-67bd4bf6dc- deployment-4720  44d95de5-60ea-4533-bc80-39b58ce3a3b3 1095856 0 2023-05-10 06:31:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:141e8b76ac2e65c37699bd787cfe2666e3d28b0533961313bd41a9e3cc92641f cni.projectcalico.org/podIP:100.114.252.140/32 cni.projectcalico.org/podIPs:100.114.252.140/32] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc fd9eed37-197e-494c-9450-125bc964a8c6 0xc004716137 0xc004716138}] [] [{calico Update v1 2023-05-10 06:31:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-10 06:31:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd9eed37-197e-494c-9450-125bc964a8c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-10 06:31:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.114.252.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-224pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-224pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:31:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:31:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.60.6,PodIP:100.114.252.140,StartTime:2023-05-10 06:31:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-10 06:31:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://395a32a9e127d68652ef2753d7160264bc1becff9831a7b26f42abd8c6d9d6c7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.114.252.140,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:31:09.436: INFO: Pod "test-new-deployment-67bd4bf6dc-g6w7w" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-g6w7w test-new-deployment-67bd4bf6dc- deployment-4720  d8fafec9-761d-46d4-8005-50d5817d1807 1095866 0 2023-05-10 06:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc fd9eed37-197e-494c-9450-125bc964a8c6 0xc004716337 0xc004716338}] [] [{kube-controller-manager Update v1 2023-05-10 06:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd9eed37-197e-494c-9450-125bc964a8c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5xwk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5xwk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-10 06:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 10 06:31:09.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4720" for this suite. @ 05/10/23 06:31:09.445
• [2.104 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 05/10/23 06:31:09.457
  May 10 06:31:09.457: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename sysctl @ 05/10/23 06:31:09.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:31:09.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:31:09.482
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 05/10/23 06:31:09.484
  STEP: Watching for error events or started pod @ 05/10/23 06:31:09.493
  E0510 06:31:09.693981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:10.694254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 05/10/23 06:31:11.496
  E0510 06:31:11.694434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:12.694958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 05/10/23 06:31:13.507
  STEP: Getting logs from the pod @ 05/10/23 06:31:13.507
  STEP: Checking that the sysctl is actually updated @ 05/10/23 06:31:13.511
  May 10 06:31:13.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-11" for this suite. @ 05/10/23 06:31:13.513
• [4.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 05/10/23 06:31:13.518
  May 10 06:31:13.518: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 06:31:13.519
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:31:13.536
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:31:13.538
  STEP: Creating a pod to test downward api env vars @ 05/10/23 06:31:13.54
  E0510 06:31:13.695904      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:14.696274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:15.697328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:16.697694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:31:17.554
  May 10 06:31:17.556: INFO: Trying to get logs from node node-02 pod downward-api-5f447798-e0a1-4c58-bed7-46876c0c441e container dapi-container: <nil>
  STEP: delete the pod @ 05/10/23 06:31:17.561
  May 10 06:31:17.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1075" for this suite. @ 05/10/23 06:31:17.579
• [4.065 seconds]
------------------------------
SSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 05/10/23 06:31:17.583
  May 10 06:31:17.583: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename conformance-tests @ 05/10/23 06:31:17.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:31:17.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:31:17.601
  STEP: Getting node addresses @ 05/10/23 06:31:17.603
  May 10 06:31:17.603: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  May 10 06:31:17.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-4257" for this suite. @ 05/10/23 06:31:17.608
• [0.030 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 05/10/23 06:31:17.613
  May 10 06:31:17.613: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename configmap @ 05/10/23 06:31:17.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:31:17.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:31:17.628
  STEP: Creating configMap with name configmap-test-volume-8c402e6e-7b78-4ed6-951b-ade5cc2402a5 @ 05/10/23 06:31:17.63
  STEP: Creating a pod to test consume configMaps @ 05/10/23 06:31:17.633
  E0510 06:31:17.698603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:18.698839      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:19.699787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:20.700122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:31:21.651
  May 10 06:31:21.653: INFO: Trying to get logs from node node-02 pod pod-configmaps-1c8ca4bf-44c0-42ee-a512-a27290233d32 container configmap-volume-test: <nil>
  STEP: delete the pod @ 05/10/23 06:31:21.657
  May 10 06:31:21.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7727" for this suite. @ 05/10/23 06:31:21.679
• [4.072 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 05/10/23 06:31:21.685
  May 10 06:31:21.685: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 06:31:21.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:31:21.694
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:31:21.699
  E0510 06:31:21.700130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/10/23 06:31:21.701
  May 10 06:31:21.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-384 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 10 06:31:21.766: INFO: stderr: ""
  May 10 06:31:21.766: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 05/10/23 06:31:21.766
  May 10 06:31:21.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-384 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  May 10 06:31:21.841: INFO: stderr: ""
  May 10 06:31:21.841: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/10/23 06:31:21.841
  May 10 06:31:21.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-384 delete pods e2e-test-httpd-pod'
  E0510 06:31:22.700925      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:23.701305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:31:24.465: INFO: stderr: ""
  May 10 06:31:24.465: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 10 06:31:24.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-384" for this suite. @ 05/10/23 06:31:24.468
• [2.791 seconds]
------------------------------
SSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 05/10/23 06:31:24.476
  May 10 06:31:24.476: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename downward-api @ 05/10/23 06:31:24.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:31:24.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:31:24.488
  STEP: Creating a pod to test downward api env vars @ 05/10/23 06:31:24.49
  E0510 06:31:24.701969      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:25.702502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:26.702605      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:27.703095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:31:28.509
  May 10 06:31:28.511: INFO: Trying to get logs from node node-02 pod downward-api-b412e0fd-4e21-4ac9-a28f-9811af861230 container dapi-container: <nil>
  STEP: delete the pod @ 05/10/23 06:31:28.516
  May 10 06:31:28.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8745" for this suite. @ 05/10/23 06:31:28.535
• [4.063 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 05/10/23 06:31:28.54
  May 10 06:31:28.540: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename resourcequota @ 05/10/23 06:31:28.54
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:31:28.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:31:28.552
  STEP: Discovering how many secrets are in namespace by default @ 05/10/23 06:31:28.555
  E0510 06:31:28.703565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:29.703662      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:30.704574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:31.704942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:32.705746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 05/10/23 06:31:33.557
  E0510 06:31:33.705827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:34.706625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:35.707657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:36.708671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:37.709260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/10/23 06:31:38.56
  STEP: Ensuring resource quota status is calculated @ 05/10/23 06:31:38.565
  E0510 06:31:38.709853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:39.710091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 05/10/23 06:31:40.568
  STEP: Ensuring resource quota status captures secret creation @ 05/10/23 06:31:40.583
  E0510 06:31:40.710335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:41.710579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 05/10/23 06:31:42.586
  STEP: Ensuring resource quota status released usage @ 05/10/23 06:31:42.592
  E0510 06:31:42.710696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:43.710930      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:31:44.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9040" for this suite. @ 05/10/23 06:31:44.598
• [16.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 05/10/23 06:31:44.604
  May 10 06:31:44.604: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename container-runtime @ 05/10/23 06:31:44.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:31:44.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:31:44.62
  STEP: create the container @ 05/10/23 06:31:44.622
  W0510 06:31:44.630680      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/10/23 06:31:44.63
  E0510 06:31:44.711815      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:45.712330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:46.712909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/10/23 06:31:47.643
  STEP: the container should be terminated @ 05/10/23 06:31:47.646
  STEP: the termination message should be set @ 05/10/23 06:31:47.646
  May 10 06:31:47.646: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/10/23 06:31:47.646
  May 10 06:31:47.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7012" for this suite. @ 05/10/23 06:31:47.668
• [3.077 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 05/10/23 06:31:47.681
  May 10 06:31:47.681: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename emptydir @ 05/10/23 06:31:47.682
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:31:47.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:31:47.692
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/10/23 06:31:47.694
  E0510 06:31:47.713478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:48.713781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:49.714760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:50.714980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/10/23 06:31:51.709
  May 10 06:31:51.711: INFO: Trying to get logs from node node-02 pod pod-6de8f623-88ca-4e9c-a9b7-08b589d99807 container test-container: <nil>
  E0510 06:31:51.715036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 05/10/23 06:31:51.715
  May 10 06:31:51.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5153" for this suite. @ 05/10/23 06:31:51.734
• [4.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 05/10/23 06:31:51.741
  May 10 06:31:51.741: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename secrets @ 05/10/23 06:31:51.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:31:51.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:31:51.754
  STEP: Creating secret with name s-test-opt-del-d9de8f37-c100-4437-95da-f57d87eafc31 @ 05/10/23 06:31:51.758
  STEP: Creating secret with name s-test-opt-upd-035e5a59-44c7-462a-869f-93d509a5b371 @ 05/10/23 06:31:51.765
  STEP: Creating the pod @ 05/10/23 06:31:51.769
  E0510 06:31:52.715937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:53.716304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:54.716823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:55.717020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-d9de8f37-c100-4437-95da-f57d87eafc31 @ 05/10/23 06:31:55.846
  STEP: Updating secret s-test-opt-upd-035e5a59-44c7-462a-869f-93d509a5b371 @ 05/10/23 06:31:55.85
  STEP: Creating secret with name s-test-opt-create-dee7472a-a651-481e-a8c3-9208a0880dbb @ 05/10/23 06:31:55.857
  STEP: waiting to observe update in volume @ 05/10/23 06:31:55.863
  E0510 06:31:56.717476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:57.717964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:58.718983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:31:59.719380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:31:59.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9025" for this suite. @ 05/10/23 06:31:59.888
• [8.249 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 05/10/23 06:31:59.991
  May 10 06:31:59.991: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename daemonsets @ 05/10/23 06:31:59.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:32:00.01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:32:00.012
  STEP: Creating simple DaemonSet "daemon-set" @ 05/10/23 06:32:00.031
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/10/23 06:32:00.039
  May 10 06:32:00.051: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:32:00.051: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 06:32:00.720129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:01.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:32:01.057: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 06:32:01.721129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:02.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 10 06:32:02.057: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 05/10/23 06:32:02.059
  May 10 06:32:02.061: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 05/10/23 06:32:02.061
  May 10 06:32:02.069: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 05/10/23 06:32:02.07
  May 10 06:32:02.071: INFO: Observed &DaemonSet event: ADDED
  May 10 06:32:02.071: INFO: Observed &DaemonSet event: MODIFIED
  May 10 06:32:02.071: INFO: Observed &DaemonSet event: MODIFIED
  May 10 06:32:02.071: INFO: Observed &DaemonSet event: MODIFIED
  May 10 06:32:02.072: INFO: Observed &DaemonSet event: MODIFIED
  May 10 06:32:02.072: INFO: Observed &DaemonSet event: MODIFIED
  May 10 06:32:02.072: INFO: Found daemon set daemon-set in namespace daemonsets-4331 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 10 06:32:02.072: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 05/10/23 06:32:02.072
  STEP: watching for the daemon set status to be patched @ 05/10/23 06:32:02.087
  May 10 06:32:02.088: INFO: Observed &DaemonSet event: ADDED
  May 10 06:32:02.088: INFO: Observed &DaemonSet event: MODIFIED
  May 10 06:32:02.088: INFO: Observed &DaemonSet event: MODIFIED
  May 10 06:32:02.089: INFO: Observed &DaemonSet event: MODIFIED
  May 10 06:32:02.089: INFO: Observed &DaemonSet event: MODIFIED
  May 10 06:32:02.089: INFO: Observed &DaemonSet event: MODIFIED
  May 10 06:32:02.089: INFO: Observed daemon set daemon-set in namespace daemonsets-4331 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 10 06:32:02.089: INFO: Observed &DaemonSet event: MODIFIED
  May 10 06:32:02.089: INFO: Found daemon set daemon-set in namespace daemonsets-4331 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  May 10 06:32:02.089: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 05/10/23 06:32:02.091
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4331, will wait for the garbage collector to delete the pods @ 05/10/23 06:32:02.091
  May 10 06:32:02.148: INFO: Deleting DaemonSet.extensions daemon-set took: 4.368473ms
  May 10 06:32:02.249: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.007761ms
  E0510 06:32:02.721526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:03.722602      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:04.723045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:05.723305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:05.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:32:05.951: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 10 06:32:05.952: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1096438"},"items":null}

  May 10 06:32:05.956: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1096439"},"items":null}

  May 10 06:32:05.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4331" for this suite. @ 05/10/23 06:32:05.968
• [5.980 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 05/10/23 06:32:05.972
  May 10 06:32:05.972: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/10/23 06:32:05.973
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:32:05.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:32:05.984
  STEP: creating @ 05/10/23 06:32:05.985
  STEP: getting @ 05/10/23 06:32:06.006
  STEP: listing in namespace @ 05/10/23 06:32:06.008
  STEP: patching @ 05/10/23 06:32:06.013
  STEP: deleting @ 05/10/23 06:32:06.017
  May 10 06:32:06.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-690" for this suite. @ 05/10/23 06:32:06.029
• [0.063 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 05/10/23 06:32:06.035
  May 10 06:32:06.035: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename crd-webhook @ 05/10/23 06:32:06.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:32:06.043
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:32:06.046
  STEP: Setting up server cert @ 05/10/23 06:32:06.048
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/10/23 06:32:06.384
  STEP: Deploying the custom resource conversion webhook pod @ 05/10/23 06:32:06.393
  STEP: Wait for the deployment to be ready @ 05/10/23 06:32:06.404
  May 10 06:32:06.413: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0510 06:32:06.723797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:07.724734      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/10/23 06:32:08.436
  STEP: Verifying the service has paired with the endpoint @ 05/10/23 06:32:08.447
  E0510 06:32:08.724929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:09.447: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 10 06:32:09.450: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 06:32:09.725782      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:10.726653      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:11.726976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 05/10/23 06:32:12.002
  STEP: Create a v2 custom resource @ 05/10/23 06:32:12.017
  STEP: List CRs in v1 @ 05/10/23 06:32:12.052
  STEP: List CRs in v2 @ 05/10/23 06:32:12.061
  May 10 06:32:12.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6104" for this suite. @ 05/10/23 06:32:12.611
• [6.585 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 05/10/23 06:32:12.621
  May 10 06:32:12.621: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename dns @ 05/10/23 06:32:12.622
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:32:12.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:32:12.64
  STEP: Creating a test headless service @ 05/10/23 06:32:12.642
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9940 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9940;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9940 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9940;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9940.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9940.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9940.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9940.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9940.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9940.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9940.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9940.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9940.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9940.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9940.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9940.svc;check="$$(dig +notcp +noall +answer +search 245.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.245_udp@PTR;check="$$(dig +tcp +noall +answer +search 245.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.245_tcp@PTR;sleep 1; done
   @ 05/10/23 06:32:12.657
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9940 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9940;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9940 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9940;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9940.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9940.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9940.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9940.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9940.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9940.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9940.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9940.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9940.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9940.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9940.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9940.svc;check="$$(dig +notcp +noall +answer +search 245.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.245_udp@PTR;check="$$(dig +tcp +noall +answer +search 245.2.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.2.245_tcp@PTR;sleep 1; done
   @ 05/10/23 06:32:12.657
  STEP: creating a pod to probe DNS @ 05/10/23 06:32:12.657
  STEP: submitting the pod to kubernetes @ 05/10/23 06:32:12.657
  E0510 06:32:12.727947      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:13.728703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:14.729274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:15.729758      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/10/23 06:32:16.682
  STEP: looking for the results for each expected name from probers @ 05/10/23 06:32:16.684
  May 10 06:32:16.691: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.693: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.696: INFO: Unable to read wheezy_udp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.698: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.700: INFO: Unable to read wheezy_udp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.702: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.704: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.705: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.707: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.710: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.712: INFO: Unable to read 10.96.2.245_udp@PTR from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.714: INFO: Unable to read 10.96.2.245_tcp@PTR from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.716: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.718: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.720: INFO: Unable to read jessie_udp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.722: INFO: Unable to read jessie_tcp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.723: INFO: Unable to read jessie_udp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.725: INFO: Unable to read jessie_tcp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  E0510 06:32:16.729853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:16.731: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.733: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.734: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.736: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.742: INFO: Unable to read 10.96.2.245_udp@PTR from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.744: INFO: Unable to read 10.96.2.245_tcp@PTR from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:16.744: INFO: Lookups using dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9940 wheezy_tcp@dns-test-service.dns-9940 wheezy_udp@dns-test-service.dns-9940.svc wheezy_tcp@dns-test-service.dns-9940.svc wheezy_udp@_http._tcp.dns-test-service.dns-9940.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9940.svc wheezy_udp@_http._tcp.test-service-2.dns-9940.svc wheezy_tcp@_http._tcp.test-service-2.dns-9940.svc 10.96.2.245_udp@PTR 10.96.2.245_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9940 jessie_tcp@dns-test-service.dns-9940 jessie_udp@dns-test-service.dns-9940.svc jessie_tcp@dns-test-service.dns-9940.svc jessie_udp@_http._tcp.dns-test-service.dns-9940.svc jessie_tcp@_http._tcp.dns-test-service.dns-9940.svc jessie_udp@_http._tcp.test-service-2.dns-9940.svc jessie_tcp@_http._tcp.test-service-2.dns-9940.svc 10.96.2.245_udp@PTR 10.96.2.245_tcp@PTR]

  E0510 06:32:17.730012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:18.730351      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:19.730577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:20.730794      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:21.730973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:21.748: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:21.751: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:21.754: INFO: Unable to read wheezy_udp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:21.756: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:21.759: INFO: Unable to read wheezy_udp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:21.761: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:21.774: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:21.776: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:21.778: INFO: Unable to read jessie_udp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:21.780: INFO: Unable to read jessie_tcp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:21.782: INFO: Unable to read jessie_udp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:21.784: INFO: Unable to read jessie_tcp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:21.799: INFO: Lookups using dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9940 wheezy_tcp@dns-test-service.dns-9940 wheezy_udp@dns-test-service.dns-9940.svc wheezy_tcp@dns-test-service.dns-9940.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9940 jessie_tcp@dns-test-service.dns-9940 jessie_udp@dns-test-service.dns-9940.svc jessie_tcp@dns-test-service.dns-9940.svc]

  E0510 06:32:22.731929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:23.732223      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:24.732327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:25.732469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:26.732732      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:26.749: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:26.751: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:26.754: INFO: Unable to read wheezy_udp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:26.756: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:26.758: INFO: Unable to read wheezy_udp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:26.759: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:26.774: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:26.776: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:26.778: INFO: Unable to read jessie_udp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:26.780: INFO: Unable to read jessie_tcp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:26.784: INFO: Unable to read jessie_udp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:26.786: INFO: Unable to read jessie_tcp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:26.797: INFO: Lookups using dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9940 wheezy_tcp@dns-test-service.dns-9940 wheezy_udp@dns-test-service.dns-9940.svc wheezy_tcp@dns-test-service.dns-9940.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9940 jessie_tcp@dns-test-service.dns-9940 jessie_udp@dns-test-service.dns-9940.svc jessie_tcp@dns-test-service.dns-9940.svc]

  E0510 06:32:27.732881      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:28.733121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:29.733698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:30.733933      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:31.734376      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:31.748: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:31.751: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:31.754: INFO: Unable to read wheezy_udp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:31.756: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:31.758: INFO: Unable to read wheezy_udp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:31.760: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:31.778: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:31.780: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:31.782: INFO: Unable to read jessie_udp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:31.783: INFO: Unable to read jessie_tcp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:31.785: INFO: Unable to read jessie_udp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:31.787: INFO: Unable to read jessie_tcp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:31.797: INFO: Lookups using dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9940 wheezy_tcp@dns-test-service.dns-9940 wheezy_udp@dns-test-service.dns-9940.svc wheezy_tcp@dns-test-service.dns-9940.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9940 jessie_tcp@dns-test-service.dns-9940 jessie_udp@dns-test-service.dns-9940.svc jessie_tcp@dns-test-service.dns-9940.svc]

  E0510 06:32:32.734458      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:33.734670      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:34.734895      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:35.735011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:36.735218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:36.748: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:36.750: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:36.752: INFO: Unable to read wheezy_udp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:36.754: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:36.756: INFO: Unable to read wheezy_udp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:36.763: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:36.776: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:36.778: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:36.779: INFO: Unable to read jessie_udp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:36.781: INFO: Unable to read jessie_tcp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:36.783: INFO: Unable to read jessie_udp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:36.784: INFO: Unable to read jessie_tcp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:36.795: INFO: Lookups using dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9940 wheezy_tcp@dns-test-service.dns-9940 wheezy_udp@dns-test-service.dns-9940.svc wheezy_tcp@dns-test-service.dns-9940.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9940 jessie_tcp@dns-test-service.dns-9940 jessie_udp@dns-test-service.dns-9940.svc jessie_tcp@dns-test-service.dns-9940.svc]

  E0510 06:32:37.735631      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:38.735879      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:39.736222      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:40.736347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:41.736548      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:41.748: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:41.751: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:41.754: INFO: Unable to read wheezy_udp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:41.756: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:41.758: INFO: Unable to read wheezy_udp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:41.760: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:41.776: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:41.778: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:41.780: INFO: Unable to read jessie_udp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:41.782: INFO: Unable to read jessie_tcp@dns-test-service.dns-9940 from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:41.784: INFO: Unable to read jessie_udp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:41.786: INFO: Unable to read jessie_tcp@dns-test-service.dns-9940.svc from pod dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67: the server could not find the requested resource (get pods dns-test-74f490bf-581a-4144-b36d-05847fdd7a67)
  May 10 06:32:41.796: INFO: Lookups using dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9940 wheezy_tcp@dns-test-service.dns-9940 wheezy_udp@dns-test-service.dns-9940.svc wheezy_tcp@dns-test-service.dns-9940.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9940 jessie_tcp@dns-test-service.dns-9940 jessie_udp@dns-test-service.dns-9940.svc jessie_tcp@dns-test-service.dns-9940.svc]

  E0510 06:32:42.736665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:43.736786      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:44.737196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:45.737424      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:46.737625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:46.787: INFO: DNS probes using dns-9940/dns-test-74f490bf-581a-4144-b36d-05847fdd7a67 succeeded

  May 10 06:32:46.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/10/23 06:32:46.789
  STEP: deleting the test service @ 05/10/23 06:32:46.813
  STEP: deleting the test headless service @ 05/10/23 06:32:46.831
  STEP: Destroying namespace "dns-9940" for this suite. @ 05/10/23 06:32:46.838
• [34.223 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 05/10/23 06:32:46.844
  May 10 06:32:46.844: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename services @ 05/10/23 06:32:46.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:32:46.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:32:46.862
  STEP: creating service in namespace services-4307 @ 05/10/23 06:32:46.864
  STEP: creating service affinity-clusterip-transition in namespace services-4307 @ 05/10/23 06:32:46.864
  STEP: creating replication controller affinity-clusterip-transition in namespace services-4307 @ 05/10/23 06:32:46.87
  I0510 06:32:46.878835      22 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-4307, replica count: 3
  E0510 06:32:47.737795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:48.738170      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:49.738488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0510 06:32:49.929988      22 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 10 06:32:49.933: INFO: Creating new exec pod
  E0510 06:32:50.738946      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:51.739544      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:52.740026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:52.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-4307 exec execpod-affinitytx7mq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  May 10 06:32:53.074: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  May 10 06:32:53.074: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 06:32:53.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-4307 exec execpod-affinitytx7mq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.2.45 80'
  May 10 06:32:53.195: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.2.45 80\nConnection to 10.96.2.45 80 port [tcp/http] succeeded!\n"
  May 10 06:32:53.195: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 10 06:32:53.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-4307 exec execpod-affinitytx7mq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.2.45:80/ ; done'
  May 10 06:32:53.383: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n"
  May 10 06:32:53.383: INFO: stdout: "\naffinity-clusterip-transition-xgqpl\naffinity-clusterip-transition-h4gjd\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-xgqpl\naffinity-clusterip-transition-h4gjd\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-xgqpl\naffinity-clusterip-transition-h4gjd\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-xgqpl\naffinity-clusterip-transition-h4gjd\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-xgqpl\naffinity-clusterip-transition-h4gjd\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-xgqpl"
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-xgqpl
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-h4gjd
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-xgqpl
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-h4gjd
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-xgqpl
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-h4gjd
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-xgqpl
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-h4gjd
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-xgqpl
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-h4gjd
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.383: INFO: Received response from host: affinity-clusterip-transition-xgqpl
  May 10 06:32:53.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=services-4307 exec execpod-affinitytx7mq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.2.45:80/ ; done'
  May 10 06:32:53.564: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.2.45:80/\n"
  May 10 06:32:53.564: INFO: stdout: "\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn\naffinity-clusterip-transition-gvjwn"
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Received response from host: affinity-clusterip-transition-gvjwn
  May 10 06:32:53.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 10 06:32:53.567: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4307, will wait for the garbage collector to delete the pods @ 05/10/23 06:32:53.574
  May 10 06:32:53.637: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.530556ms
  May 10 06:32:53.738: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.92662ms
  E0510 06:32:53.740437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:54.741406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:32:55.742336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4307" for this suite. @ 05/10/23 06:32:56.69
• [9.857 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 05/10/23 06:32:56.701
  May 10 06:32:56.701: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/10/23 06:32:56.702
  E0510 06:32:56.742508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:32:56.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:32:56.782
  May 10 06:32:56.784: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  E0510 06:32:57.743551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/10/23 06:32:58.295
  May 10 06:32:58.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-325 --namespace=crd-publish-openapi-325 create -f -'
  E0510 06:32:58.744499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:59.230: INFO: stderr: ""
  May 10 06:32:59.230: INFO: stdout: "e2e-test-crd-publish-openapi-8974-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 10 06:32:59.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-325 --namespace=crd-publish-openapi-325 delete e2e-test-crd-publish-openapi-8974-crds test-cr'
  May 10 06:32:59.293: INFO: stderr: ""
  May 10 06:32:59.293: INFO: stdout: "e2e-test-crd-publish-openapi-8974-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  May 10 06:32:59.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-325 --namespace=crd-publish-openapi-325 apply -f -'
  May 10 06:32:59.557: INFO: stderr: ""
  May 10 06:32:59.557: INFO: stdout: "e2e-test-crd-publish-openapi-8974-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 10 06:32:59.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-325 --namespace=crd-publish-openapi-325 delete e2e-test-crd-publish-openapi-8974-crds test-cr'
  May 10 06:32:59.621: INFO: stderr: ""
  May 10 06:32:59.621: INFO: stdout: "e2e-test-crd-publish-openapi-8974-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 05/10/23 06:32:59.621
  May 10 06:32:59.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=crd-publish-openapi-325 explain e2e-test-crd-publish-openapi-8974-crds'
  E0510 06:32:59.744911      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:32:59.914: INFO: stderr: ""
  May 10 06:32:59.914: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-8974-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0510 06:33:00.745149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:33:01.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-325" for this suite. @ 05/10/23 06:33:01.355
• [4.659 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 05/10/23 06:33:01.36
  May 10 06:33:01.360: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename replicaset @ 05/10/23 06:33:01.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:33:01.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:33:01.534
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 05/10/23 06:33:01.543
  E0510 06:33:01.746105      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0510 06:33:02.746823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 05/10/23 06:33:03.618
  STEP: Then the orphan pod is adopted @ 05/10/23 06:33:03.623
  E0510 06:33:03.747003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 05/10/23 06:33:04.628
  May 10 06:33:04.631: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/10/23 06:33:04.643
  E0510 06:33:04.747339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:33:05.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4131" for this suite. @ 05/10/23 06:33:05.651
• [4.301 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 05/10/23 06:33:05.662
  May 10 06:33:05.662: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename kubectl @ 05/10/23 06:33:05.662
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:33:05.675
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:33:05.677
  May 10 06:33:05.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-993842567 --namespace=kubectl-7765 version'
  May 10 06:33:05.730: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  May 10 06:33:05.730: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:14:42Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  May 10 06:33:05.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7765" for this suite. @ 05/10/23 06:33:05.734
• [0.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 05/10/23 06:33:05.742
  May 10 06:33:05.742: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename daemonsets @ 05/10/23 06:33:05.743
  E0510 06:33:05.747681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:33:05.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:33:05.758
  STEP: Creating simple DaemonSet "daemon-set" @ 05/10/23 06:33:05.776
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/10/23 06:33:05.784
  May 10 06:33:05.788: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:33:05.788: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 06:33:06.747944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:33:06.793: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 10 06:33:06.793: INFO: Node node-01 is running 0 daemon pod, expected 1
  E0510 06:33:07.748199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 10 06:33:07.793: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 10 06:33:07.794: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 05/10/23 06:33:07.795
  STEP: DeleteCollection of the DaemonSets @ 05/10/23 06:33:07.797
  STEP: Verify that ReplicaSets have been deleted @ 05/10/23 06:33:07.802
  May 10 06:33:07.813: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1097062"},"items":null}

  May 10 06:33:07.818: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1097064"},"items":[{"metadata":{"name":"daemon-set-gv6pd","generateName":"daemon-set-","namespace":"daemonsets-2808","uid":"56217859-3d1a-46bd-89d6-6cdfd82ffab5","resourceVersion":"1097062","creationTimestamp":"2023-05-10T06:33:05Z","deletionTimestamp":"2023-05-10T06:33:37Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"e0778806071b13d462caf1d6bb7784979066f6dd24b1622cfd8c267adfd3619c","cni.projectcalico.org/podIP":"100.114.252.148/32","cni.projectcalico.org/podIPs":"100.114.252.148/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"823adcde-3acf-4453-b225-feb90aaa9cc4","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-10T06:33:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"823adcde-3acf-4453-b225-feb90aaa9cc4\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-10T06:33:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-10T06:33:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.114.252.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7khrk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7khrk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node-02","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node-02"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-10T06:33:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-10T06:33:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-10T06:33:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-10T06:33:05Z"}],"hostIP":"192.168.60.6","podIP":"100.114.252.148","podIPs":[{"ip":"100.114.252.148"}],"startTime":"2023-05-10T06:33:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-10T06:33:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"containerd://795ec1d189a84a75eedb517cb1563e369595510baea5d75021cb651143a3b366","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hztcv","generateName":"daemon-set-","namespace":"daemonsets-2808","uid":"58424b5d-f9cb-4ba9-8d05-c9e6c15dba75","resourceVersion":"1097064","creationTimestamp":"2023-05-10T06:33:05Z","deletionTimestamp":"2023-05-10T06:33:37Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"cca427c0eaa8891c2960266fe3083c5b7f0617fe56bc8982e3e9c6d84a8ef9f6","cni.projectcalico.org/podIP":"100.74.79.54/32","cni.projectcalico.org/podIPs":"100.74.79.54/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"823adcde-3acf-4453-b225-feb90aaa9cc4","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-10T06:33:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"823adcde-3acf-4453-b225-feb90aaa9cc4\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-10T06:33:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-10T06:33:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.74.79.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-trjkw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-trjkw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ub-test","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ub-test"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-10T06:33:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-10T06:33:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-10T06:33:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-10T06:33:05Z"}],"hostIP":"192.168.60.152","podIP":"100.74.79.54","podIPs":[{"ip":"100.74.79.54"}],"startTime":"2023-05-10T06:33:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-10T06:33:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"containerd://e2c8a6778a84bb047925bbcaa630d79e8f87f673308e9234a868aca7fc697f3a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-z9d8n","generateName":"daemon-set-","namespace":"daemonsets-2808","uid":"f53e1c2f-a1d8-4425-86aa-3d0dcbaff9bc","resourceVersion":"1097061","creationTimestamp":"2023-05-10T06:33:05Z","deletionTimestamp":"2023-05-10T06:33:37Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"bd0756a6ad9e81e1cf94067698e063ad1e945ed7f5255d5519ef62b70900f5db","cni.projectcalico.org/podIP":"100.67.79.153/32","cni.projectcalico.org/podIPs":"100.67.79.153/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"823adcde-3acf-4453-b225-feb90aaa9cc4","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-10T06:33:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"823adcde-3acf-4453-b225-feb90aaa9cc4\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-10T06:33:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-10T06:33:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.67.79.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-sc7cq","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-sc7cq","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node-01","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node-01"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-10T06:33:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-10T06:33:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-10T06:33:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-10T06:33:05Z"}],"hostIP":"192.168.60.83","podIP":"100.67.79.153","podIPs":[{"ip":"100.67.79.153"}],"startTime":"2023-05-10T06:33:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-10T06:33:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"containerd://f51043824f95bfda45ed40e3f1164be0f2bd84b18b445b68d793321a52870abf","started":true}],"qosClass":"BestEffort"}}]}

  May 10 06:33:07.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2808" for this suite. @ 05/10/23 06:33:07.829
• [2.091 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 05/10/23 06:33:07.833
  May 10 06:33:07.833: INFO: >>> kubeConfig: /tmp/kubeconfig-993842567
  STEP: Building a namespace api object, basename resourcequota @ 05/10/23 06:33:07.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/10/23 06:33:07.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/10/23 06:33:07.846
  STEP: Creating a ResourceQuota @ 05/10/23 06:33:07.848
  STEP: Getting a ResourceQuota @ 05/10/23 06:33:07.852
  STEP: Listing all ResourceQuotas with LabelSelector @ 05/10/23 06:33:07.853
  STEP: Patching the ResourceQuota @ 05/10/23 06:33:07.857
  STEP: Deleting a Collection of ResourceQuotas @ 05/10/23 06:33:07.864
  STEP: Verifying the deleted ResourceQuota @ 05/10/23 06:33:07.87
  May 10 06:33:07.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5906" for this suite. @ 05/10/23 06:33:07.874
• [0.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  May 10 06:33:07.882: INFO: Running AfterSuite actions on node 1
  May 10 06:33:07.882: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.039 seconds]
------------------------------

Ran 378 of 7207 Specs in 6170.816 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h42m51.128408385s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

