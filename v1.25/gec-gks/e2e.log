I0509 12:28:01.849321      24 e2e.go:116] Starting e2e run "e3816ee9-df1e-4df6-89d7-c4dc2982cde5" on Ginkgo node 1
May  9 12:28:01.859: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1683635281 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
May  9 12:28:01.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:28:01.917: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0509 12:28:01.917722      24 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0509 12:28:01.917722      24 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
May  9 12:28:01.930: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May  9 12:28:01.959: INFO: 48 / 48 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May  9 12:28:01.959: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
May  9 12:28:01.959: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May  9 12:28:01.965: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
May  9 12:28:01.965: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-controllerplugin' (0 seconds elapsed)
May  9 12:28:01.965: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
May  9 12:28:01.965: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May  9 12:28:01.965: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
May  9 12:28:01.965: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
May  9 12:28:01.965: INFO: e2e test version: v1.25.8
May  9 12:28:01.966: INFO: kube-apiserver version: v1.25.8
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
May  9 12:28:01.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:28:01.968: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.053 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    May  9 12:28:01.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:28:01.917: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0509 12:28:01.917722      24 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    May  9 12:28:01.930: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    May  9 12:28:01.959: INFO: 48 / 48 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    May  9 12:28:01.959: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
    May  9 12:28:01.959: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    May  9 12:28:01.965: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    May  9 12:28:01.965: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-controllerplugin' (0 seconds elapsed)
    May  9 12:28:01.965: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
    May  9 12:28:01.965: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    May  9 12:28:01.965: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    May  9 12:28:01.965: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
    May  9 12:28:01.965: INFO: e2e test version: v1.25.8
    May  9 12:28:01.966: INFO: kube-apiserver version: v1.25.8
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    May  9 12:28:01.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:28:01.968: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:28:01.988
May  9 12:28:01.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 12:28:01.988
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:02.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:02.007
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
May  9 12:28:02.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 create -f -'
May  9 12:28:02.199: INFO: stderr: ""
May  9 12:28:02.199: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May  9 12:28:02.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 create -f -'
May  9 12:28:02.344: INFO: stderr: ""
May  9 12:28:02.344: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/09/23 12:28:02.344
May  9 12:28:03.347: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 12:28:03.347: INFO: Found 0 / 1
May  9 12:28:04.347: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 12:28:04.347: INFO: Found 0 / 1
May  9 12:28:05.347: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 12:28:05.347: INFO: Found 0 / 1
May  9 12:28:06.347: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 12:28:06.347: INFO: Found 1 / 1
May  9 12:28:06.347: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  9 12:28:06.349: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 12:28:06.349: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  9 12:28:06.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 describe pod agnhost-primary-snhjz'
May  9 12:28:06.408: INFO: stderr: ""
May  9 12:28:06.408: INFO: stdout: "Name:             agnhost-primary-snhjz\nNamespace:        kubectl-6269\nPriority:         0\nService Account:  default\nNode:             cl-gks-cncf-ix1-md-0-48ljh/192.168.1.64\nStart Time:       Tue, 09 May 2023 12:28:02 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: d0214f3e039bf368676cf296e97a372b958b670871269a7743911c495cb555af\n                  cni.projectcalico.org/podIP: 172.25.124.194/32\n                  cni.projectcalico.org/podIPs: 172.25.124.194/32\nStatus:           Running\nIP:               172.25.124.194\nIPs:\n  IP:           172.25.124.194\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://f6db4f308e876edcf6e721cea9e5ec562b9b1e364e0bd205e5fffe2170ce0fa7\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 09 May 2023 12:28:04 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-64zpt (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-64zpt:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-6269/agnhost-primary-snhjz to cl-gks-cncf-ix1-md-0-48ljh\n  Normal  Pulling    4s    kubelet            Pulling image \"registry.k8s.io/e2e-test-images/agnhost:2.40\"\n  Normal  Pulled     2s    kubelet            Successfully pulled image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" in 1.995803465s (1.995812541s including waiting)\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
May  9 12:28:06.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 describe rc agnhost-primary'
May  9 12:28:06.468: INFO: stderr: ""
May  9 12:28:06.468: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6269\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-snhjz\n"
May  9 12:28:06.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 describe service agnhost-primary'
May  9 12:28:06.525: INFO: stderr: ""
May  9 12:28:06.525: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6269\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.98.249.35\nIPs:               10.98.249.35\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.124.194:6379\nSession Affinity:  None\nEvents:            <none>\n"
May  9 12:28:06.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 describe node cl-gks-cncf-control-plane-7p8m8'
May  9 12:28:06.609: INFO: stderr: ""
May  9 12:28:06.609: INFO: stdout: "Name:               cl-gks-cncf-control-plane-7p8m8\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=s1.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=fra\n                    failure-domain.beta.kubernetes.io/zone=ix1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cl-gks-cncf-control-plane-7p8m8\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=s1.medium\n                    topology.cinder.csi.openstack.org/zone=ix1\n                    topology.kubernetes.io/region=fra\n                    topology.kubernetes.io/zone=ix1\nAnnotations:        cluster.x-k8s.io/cluster-name: cl-gks-cncf\n                    cluster.x-k8s.io/cluster-namespace: cl-gks-cncf\n                    cluster.x-k8s.io/labels-from-machine: \n                    cluster.x-k8s.io/machine: cl-gks-cncf-control-plane-9t7lb\n                    cluster.x-k8s.io/owner-kind: KubeadmControlPlane\n                    cluster.x-k8s.io/owner-name: cl-gks-cncf-control-plane\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"4283e539-5bc0-4351-abea-2d52be253f85\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.1.33/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.165.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 09 May 2023 11:58:13 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  cl-gks-cncf-control-plane-7p8m8\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 09 May 2023 12:28:04 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 09 May 2023 12:00:14 +0000   Tue, 09 May 2023 12:00:14 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 09 May 2023 12:26:49 +0000   Tue, 09 May 2023 11:58:13 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 09 May 2023 12:26:49 +0000   Tue, 09 May 2023 11:58:13 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 09 May 2023 12:26:49 +0000   Tue, 09 May 2023 11:58:13 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 09 May 2023 12:26:49 +0000   Tue, 09 May 2023 12:00:06 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.1.33\n  Hostname:    cl-gks-cncf-control-plane-7p8m8\nCapacity:\n  cpu:                4\n  ephemeral-storage:  17885708Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8140840Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  16483468466\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8038440Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 4283e5395bc04351abea2d52be253f85\n  System UUID:                4283e539-5bc0-4351-abea-2d52be253f85\n  Boot ID:                    94989398-1c0f-4077-a8f7-17df888e8e84\n  Kernel Version:             5.15.106-flatcar\n  OS Image:                   Flatcar Container Linux by Kinvolk 3510.2.1 (Oklo)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.16\n  Kubelet Version:            v1.25.8\n  Kube-Proxy Version:         v1.25.8\nPodCIDR:                      172.25.0.0/24\nPodCIDRs:                     172.25.0.0/24\nProviderID:                   openstack:///4283e539-5bc0-4351-abea-2d52be253f85\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-zhcmm                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 csi-cinder-controllerplugin-pm8tx                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\n  kube-system                 csi-cinder-nodeplugin-q6b88                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 etcd-cl-gks-cncf-control-plane-7p8m8                       100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         29m\n  kube-system                 kube-apiserver-cl-gks-cncf-control-plane-7p8m8             250m (6%)     0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-controller-manager-cl-gks-cncf-control-plane-7p8m8    200m (5%)     0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-proxy-5g58n                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-scheduler-cl-gks-cncf-control-plane-7p8m8             100m (2%)     0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 node-local-dns-w92j4                                       25m (0%)      0 (0%)      5Mi (0%)         0 (0%)         29m\n  kube-system                 openstack-cloud-controller-manager-g5rr6                   200m (5%)     0 (0%)      0 (0%)           0 (0%)         28m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-zk6d5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1125m (28%)  0 (0%)\n  memory             105Mi (1%)   0 (0%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:\n  Type     Reason                   Age   From                   Message\n  ----     ------                   ----  ----                   -------\n  Normal   Starting                 29m   kube-proxy             \n  Normal   Starting                 29m   kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity      29m   kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  29m   kubelet                Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  29m   kubelet                Node cl-gks-cncf-control-plane-7p8m8 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    29m   kubelet                Node cl-gks-cncf-control-plane-7p8m8 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     29m   kubelet                Node cl-gks-cncf-control-plane-7p8m8 status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           29m   node-controller        Node cl-gks-cncf-control-plane-7p8m8 event: Registered Node cl-gks-cncf-control-plane-7p8m8 in Controller\n  Normal   NodeReady                28m   kubelet                Node cl-gks-cncf-control-plane-7p8m8 status is now: NodeReady\n  Normal   Synced                   27m   cloud-node-controller  Node synced successfully\n"
May  9 12:28:06.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 describe namespace kubectl-6269'
May  9 12:28:06.663: INFO: stderr: ""
May  9 12:28:06.663: INFO: stdout: "Name:         kubectl-6269\nLabels:       e2e-framework=kubectl\n              e2e-run=e3816ee9-df1e-4df6-89d7-c4dc2982cde5\n              kubernetes.io/metadata.name=kubectl-6269\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 12:28:06.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6269" for this suite. 05/09/23 12:28:06.665
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":1,"skipped":4,"failed":0}
------------------------------
• [4.683 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:28:01.988
    May  9 12:28:01.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 12:28:01.988
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:02.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:02.007
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    May  9 12:28:02.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 create -f -'
    May  9 12:28:02.199: INFO: stderr: ""
    May  9 12:28:02.199: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    May  9 12:28:02.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 create -f -'
    May  9 12:28:02.344: INFO: stderr: ""
    May  9 12:28:02.344: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/09/23 12:28:02.344
    May  9 12:28:03.347: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 12:28:03.347: INFO: Found 0 / 1
    May  9 12:28:04.347: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 12:28:04.347: INFO: Found 0 / 1
    May  9 12:28:05.347: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 12:28:05.347: INFO: Found 0 / 1
    May  9 12:28:06.347: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 12:28:06.347: INFO: Found 1 / 1
    May  9 12:28:06.347: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May  9 12:28:06.349: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 12:28:06.349: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  9 12:28:06.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 describe pod agnhost-primary-snhjz'
    May  9 12:28:06.408: INFO: stderr: ""
    May  9 12:28:06.408: INFO: stdout: "Name:             agnhost-primary-snhjz\nNamespace:        kubectl-6269\nPriority:         0\nService Account:  default\nNode:             cl-gks-cncf-ix1-md-0-48ljh/192.168.1.64\nStart Time:       Tue, 09 May 2023 12:28:02 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: d0214f3e039bf368676cf296e97a372b958b670871269a7743911c495cb555af\n                  cni.projectcalico.org/podIP: 172.25.124.194/32\n                  cni.projectcalico.org/podIPs: 172.25.124.194/32\nStatus:           Running\nIP:               172.25.124.194\nIPs:\n  IP:           172.25.124.194\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://f6db4f308e876edcf6e721cea9e5ec562b9b1e364e0bd205e5fffe2170ce0fa7\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 09 May 2023 12:28:04 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-64zpt (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-64zpt:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-6269/agnhost-primary-snhjz to cl-gks-cncf-ix1-md-0-48ljh\n  Normal  Pulling    4s    kubelet            Pulling image \"registry.k8s.io/e2e-test-images/agnhost:2.40\"\n  Normal  Pulled     2s    kubelet            Successfully pulled image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" in 1.995803465s (1.995812541s including waiting)\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    May  9 12:28:06.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 describe rc agnhost-primary'
    May  9 12:28:06.468: INFO: stderr: ""
    May  9 12:28:06.468: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6269\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-snhjz\n"
    May  9 12:28:06.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 describe service agnhost-primary'
    May  9 12:28:06.525: INFO: stderr: ""
    May  9 12:28:06.525: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6269\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.98.249.35\nIPs:               10.98.249.35\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.124.194:6379\nSession Affinity:  None\nEvents:            <none>\n"
    May  9 12:28:06.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 describe node cl-gks-cncf-control-plane-7p8m8'
    May  9 12:28:06.609: INFO: stderr: ""
    May  9 12:28:06.609: INFO: stdout: "Name:               cl-gks-cncf-control-plane-7p8m8\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=s1.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=fra\n                    failure-domain.beta.kubernetes.io/zone=ix1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cl-gks-cncf-control-plane-7p8m8\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=s1.medium\n                    topology.cinder.csi.openstack.org/zone=ix1\n                    topology.kubernetes.io/region=fra\n                    topology.kubernetes.io/zone=ix1\nAnnotations:        cluster.x-k8s.io/cluster-name: cl-gks-cncf\n                    cluster.x-k8s.io/cluster-namespace: cl-gks-cncf\n                    cluster.x-k8s.io/labels-from-machine: \n                    cluster.x-k8s.io/machine: cl-gks-cncf-control-plane-9t7lb\n                    cluster.x-k8s.io/owner-kind: KubeadmControlPlane\n                    cluster.x-k8s.io/owner-name: cl-gks-cncf-control-plane\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"4283e539-5bc0-4351-abea-2d52be253f85\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.1.33/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.165.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 09 May 2023 11:58:13 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  cl-gks-cncf-control-plane-7p8m8\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 09 May 2023 12:28:04 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 09 May 2023 12:00:14 +0000   Tue, 09 May 2023 12:00:14 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 09 May 2023 12:26:49 +0000   Tue, 09 May 2023 11:58:13 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 09 May 2023 12:26:49 +0000   Tue, 09 May 2023 11:58:13 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 09 May 2023 12:26:49 +0000   Tue, 09 May 2023 11:58:13 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 09 May 2023 12:26:49 +0000   Tue, 09 May 2023 12:00:06 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.1.33\n  Hostname:    cl-gks-cncf-control-plane-7p8m8\nCapacity:\n  cpu:                4\n  ephemeral-storage:  17885708Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8140840Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  16483468466\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8038440Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 4283e5395bc04351abea2d52be253f85\n  System UUID:                4283e539-5bc0-4351-abea-2d52be253f85\n  Boot ID:                    94989398-1c0f-4077-a8f7-17df888e8e84\n  Kernel Version:             5.15.106-flatcar\n  OS Image:                   Flatcar Container Linux by Kinvolk 3510.2.1 (Oklo)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.16\n  Kubelet Version:            v1.25.8\n  Kube-Proxy Version:         v1.25.8\nPodCIDR:                      172.25.0.0/24\nPodCIDRs:                     172.25.0.0/24\nProviderID:                   openstack:///4283e539-5bc0-4351-abea-2d52be253f85\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-zhcmm                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 csi-cinder-controllerplugin-pm8tx                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\n  kube-system                 csi-cinder-nodeplugin-q6b88                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 etcd-cl-gks-cncf-control-plane-7p8m8                       100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         29m\n  kube-system                 kube-apiserver-cl-gks-cncf-control-plane-7p8m8             250m (6%)     0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-controller-manager-cl-gks-cncf-control-plane-7p8m8    200m (5%)     0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-proxy-5g58n                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-scheduler-cl-gks-cncf-control-plane-7p8m8             100m (2%)     0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 node-local-dns-w92j4                                       25m (0%)      0 (0%)      5Mi (0%)         0 (0%)         29m\n  kube-system                 openstack-cloud-controller-manager-g5rr6                   200m (5%)     0 (0%)      0 (0%)           0 (0%)         28m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-zk6d5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1125m (28%)  0 (0%)\n  memory             105Mi (1%)   0 (0%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:\n  Type     Reason                   Age   From                   Message\n  ----     ------                   ----  ----                   -------\n  Normal   Starting                 29m   kube-proxy             \n  Normal   Starting                 29m   kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity      29m   kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  29m   kubelet                Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  29m   kubelet                Node cl-gks-cncf-control-plane-7p8m8 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    29m   kubelet                Node cl-gks-cncf-control-plane-7p8m8 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     29m   kubelet                Node cl-gks-cncf-control-plane-7p8m8 status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           29m   node-controller        Node cl-gks-cncf-control-plane-7p8m8 event: Registered Node cl-gks-cncf-control-plane-7p8m8 in Controller\n  Normal   NodeReady                28m   kubelet                Node cl-gks-cncf-control-plane-7p8m8 status is now: NodeReady\n  Normal   Synced                   27m   cloud-node-controller  Node synced successfully\n"
    May  9 12:28:06.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6269 describe namespace kubectl-6269'
    May  9 12:28:06.663: INFO: stderr: ""
    May  9 12:28:06.663: INFO: stdout: "Name:         kubectl-6269\nLabels:       e2e-framework=kubectl\n              e2e-run=e3816ee9-df1e-4df6-89d7-c4dc2982cde5\n              kubernetes.io/metadata.name=kubectl-6269\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 12:28:06.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6269" for this suite. 05/09/23 12:28:06.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:28:06.673
May  9 12:28:06.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename var-expansion 05/09/23 12:28:06.673
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:06.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:06.69
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 05/09/23 12:28:06.691
May  9 12:28:06.698: INFO: Waiting up to 5m0s for pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06" in namespace "var-expansion-1941" to be "Succeeded or Failed"
May  9 12:28:06.702: INFO: Pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.647012ms
May  9 12:28:08.706: INFO: Pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06": Phase="Running", Reason="", readiness=true. Elapsed: 2.008474649s
May  9 12:28:10.707: INFO: Pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06": Phase="Running", Reason="", readiness=true. Elapsed: 4.008821734s
May  9 12:28:12.705: INFO: Pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06": Phase="Running", Reason="", readiness=false. Elapsed: 6.007533052s
May  9 12:28:14.707: INFO: Pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009211403s
STEP: Saw pod success 05/09/23 12:28:14.707
May  9 12:28:14.707: INFO: Pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06" satisfied condition "Succeeded or Failed"
May  9 12:28:14.709: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06 container dapi-container: <nil>
STEP: delete the pod 05/09/23 12:28:14.721
May  9 12:28:14.733: INFO: Waiting for pod var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06 to disappear
May  9 12:28:14.735: INFO: Pod var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  9 12:28:14.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1941" for this suite. 05/09/23 12:28:14.738
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":2,"skipped":55,"failed":0}
------------------------------
• [SLOW TEST] [8.071 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:28:06.673
    May  9 12:28:06.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename var-expansion 05/09/23 12:28:06.673
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:06.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:06.69
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 05/09/23 12:28:06.691
    May  9 12:28:06.698: INFO: Waiting up to 5m0s for pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06" in namespace "var-expansion-1941" to be "Succeeded or Failed"
    May  9 12:28:06.702: INFO: Pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.647012ms
    May  9 12:28:08.706: INFO: Pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06": Phase="Running", Reason="", readiness=true. Elapsed: 2.008474649s
    May  9 12:28:10.707: INFO: Pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06": Phase="Running", Reason="", readiness=true. Elapsed: 4.008821734s
    May  9 12:28:12.705: INFO: Pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06": Phase="Running", Reason="", readiness=false. Elapsed: 6.007533052s
    May  9 12:28:14.707: INFO: Pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009211403s
    STEP: Saw pod success 05/09/23 12:28:14.707
    May  9 12:28:14.707: INFO: Pod "var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06" satisfied condition "Succeeded or Failed"
    May  9 12:28:14.709: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06 container dapi-container: <nil>
    STEP: delete the pod 05/09/23 12:28:14.721
    May  9 12:28:14.733: INFO: Waiting for pod var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06 to disappear
    May  9 12:28:14.735: INFO: Pod var-expansion-73f7ac38-53cf-48ea-881f-0bc880efac06 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  9 12:28:14.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1941" for this suite. 05/09/23 12:28:14.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:28:14.744
May  9 12:28:14.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename var-expansion 05/09/23 12:28:14.745
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:14.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:14.759
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 05/09/23 12:28:14.761
May  9 12:28:14.766: INFO: Waiting up to 5m0s for pod "var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13" in namespace "var-expansion-2191" to be "Succeeded or Failed"
May  9 12:28:14.769: INFO: Pod "var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.422428ms
May  9 12:28:16.774: INFO: Pod "var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007304882s
May  9 12:28:18.772: INFO: Pod "var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005161782s
STEP: Saw pod success 05/09/23 12:28:18.772
May  9 12:28:18.772: INFO: Pod "var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13" satisfied condition "Succeeded or Failed"
May  9 12:28:18.774: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13 container dapi-container: <nil>
STEP: delete the pod 05/09/23 12:28:18.778
May  9 12:28:18.788: INFO: Waiting for pod var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13 to disappear
May  9 12:28:18.790: INFO: Pod var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  9 12:28:18.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2191" for this suite. 05/09/23 12:28:18.793
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":3,"skipped":79,"failed":0}
------------------------------
• [4.053 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:28:14.744
    May  9 12:28:14.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename var-expansion 05/09/23 12:28:14.745
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:14.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:14.759
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 05/09/23 12:28:14.761
    May  9 12:28:14.766: INFO: Waiting up to 5m0s for pod "var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13" in namespace "var-expansion-2191" to be "Succeeded or Failed"
    May  9 12:28:14.769: INFO: Pod "var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.422428ms
    May  9 12:28:16.774: INFO: Pod "var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007304882s
    May  9 12:28:18.772: INFO: Pod "var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005161782s
    STEP: Saw pod success 05/09/23 12:28:18.772
    May  9 12:28:18.772: INFO: Pod "var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13" satisfied condition "Succeeded or Failed"
    May  9 12:28:18.774: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13 container dapi-container: <nil>
    STEP: delete the pod 05/09/23 12:28:18.778
    May  9 12:28:18.788: INFO: Waiting for pod var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13 to disappear
    May  9 12:28:18.790: INFO: Pod var-expansion-e635349e-a5ff-4ad4-9b4f-f10b4a577b13 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  9 12:28:18.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2191" for this suite. 05/09/23 12:28:18.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:28:18.798
May  9 12:28:18.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 12:28:18.798
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:18.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:18.812
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 05/09/23 12:28:18.814
May  9 12:28:18.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 create -f -'
May  9 12:28:18.962: INFO: stderr: ""
May  9 12:28:18.962: INFO: stdout: "pod/pause created\n"
May  9 12:28:18.962: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May  9 12:28:18.962: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2971" to be "running and ready"
May  9 12:28:18.965: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.401349ms
May  9 12:28:18.965: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'cl-gks-cncf-ix1-md-0-48ljh' to be 'Running' but was 'Pending'
May  9 12:28:20.968: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006389685s
May  9 12:28:20.968: INFO: Pod "pause" satisfied condition "running and ready"
May  9 12:28:20.968: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 05/09/23 12:28:20.968
May  9 12:28:20.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 label pods pause testing-label=testing-label-value'
May  9 12:28:21.131: INFO: stderr: ""
May  9 12:28:21.131: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 05/09/23 12:28:21.131
May  9 12:28:21.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 get pod pause -L testing-label'
May  9 12:28:21.181: INFO: stderr: ""
May  9 12:28:21.181: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 05/09/23 12:28:21.181
May  9 12:28:21.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 label pods pause testing-label-'
May  9 12:28:21.388: INFO: stderr: ""
May  9 12:28:21.388: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 05/09/23 12:28:21.388
May  9 12:28:21.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 get pod pause -L testing-label'
May  9 12:28:21.445: INFO: stderr: ""
May  9 12:28:21.445: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 05/09/23 12:28:21.445
May  9 12:28:21.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 delete --grace-period=0 --force -f -'
May  9 12:28:21.505: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 12:28:21.505: INFO: stdout: "pod \"pause\" force deleted\n"
May  9 12:28:21.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 get rc,svc -l name=pause --no-headers'
May  9 12:28:21.559: INFO: stderr: "No resources found in kubectl-2971 namespace.\n"
May  9 12:28:21.559: INFO: stdout: ""
May  9 12:28:21.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  9 12:28:21.609: INFO: stderr: ""
May  9 12:28:21.609: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 12:28:21.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2971" for this suite. 05/09/23 12:28:21.613
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":4,"skipped":86,"failed":0}
------------------------------
• [2.822 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:28:18.798
    May  9 12:28:18.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 12:28:18.798
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:18.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:18.812
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 05/09/23 12:28:18.814
    May  9 12:28:18.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 create -f -'
    May  9 12:28:18.962: INFO: stderr: ""
    May  9 12:28:18.962: INFO: stdout: "pod/pause created\n"
    May  9 12:28:18.962: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    May  9 12:28:18.962: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2971" to be "running and ready"
    May  9 12:28:18.965: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.401349ms
    May  9 12:28:18.965: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'cl-gks-cncf-ix1-md-0-48ljh' to be 'Running' but was 'Pending'
    May  9 12:28:20.968: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006389685s
    May  9 12:28:20.968: INFO: Pod "pause" satisfied condition "running and ready"
    May  9 12:28:20.968: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 05/09/23 12:28:20.968
    May  9 12:28:20.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 label pods pause testing-label=testing-label-value'
    May  9 12:28:21.131: INFO: stderr: ""
    May  9 12:28:21.131: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 05/09/23 12:28:21.131
    May  9 12:28:21.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 get pod pause -L testing-label'
    May  9 12:28:21.181: INFO: stderr: ""
    May  9 12:28:21.181: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 05/09/23 12:28:21.181
    May  9 12:28:21.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 label pods pause testing-label-'
    May  9 12:28:21.388: INFO: stderr: ""
    May  9 12:28:21.388: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 05/09/23 12:28:21.388
    May  9 12:28:21.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 get pod pause -L testing-label'
    May  9 12:28:21.445: INFO: stderr: ""
    May  9 12:28:21.445: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 05/09/23 12:28:21.445
    May  9 12:28:21.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 delete --grace-period=0 --force -f -'
    May  9 12:28:21.505: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 12:28:21.505: INFO: stdout: "pod \"pause\" force deleted\n"
    May  9 12:28:21.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 get rc,svc -l name=pause --no-headers'
    May  9 12:28:21.559: INFO: stderr: "No resources found in kubectl-2971 namespace.\n"
    May  9 12:28:21.559: INFO: stdout: ""
    May  9 12:28:21.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2971 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  9 12:28:21.609: INFO: stderr: ""
    May  9 12:28:21.609: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 12:28:21.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2971" for this suite. 05/09/23 12:28:21.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:28:21.62
May  9 12:28:21.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename watch 05/09/23 12:28:21.621
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:21.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:21.633
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 05/09/23 12:28:21.635
STEP: modifying the configmap once 05/09/23 12:28:21.638
STEP: modifying the configmap a second time 05/09/23 12:28:21.649
STEP: deleting the configmap 05/09/23 12:28:21.655
STEP: creating a watch on configmaps from the resource version returned by the first update 05/09/23 12:28:21.659
STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/09/23 12:28:21.66
May  9 12:28:21.660: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7501  f6168ccc-7702-46e5-9a2e-61cce8535472 7774 0 2023-05-09 12:28:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-09 12:28:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 12:28:21.660: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7501  f6168ccc-7702-46e5-9a2e-61cce8535472 7775 0 2023-05-09 12:28:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-09 12:28:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  9 12:28:21.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7501" for this suite. 05/09/23 12:28:21.663
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":5,"skipped":92,"failed":0}
------------------------------
• [0.047 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:28:21.62
    May  9 12:28:21.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename watch 05/09/23 12:28:21.621
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:21.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:21.633
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 05/09/23 12:28:21.635
    STEP: modifying the configmap once 05/09/23 12:28:21.638
    STEP: modifying the configmap a second time 05/09/23 12:28:21.649
    STEP: deleting the configmap 05/09/23 12:28:21.655
    STEP: creating a watch on configmaps from the resource version returned by the first update 05/09/23 12:28:21.659
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/09/23 12:28:21.66
    May  9 12:28:21.660: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7501  f6168ccc-7702-46e5-9a2e-61cce8535472 7774 0 2023-05-09 12:28:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-09 12:28:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 12:28:21.660: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7501  f6168ccc-7702-46e5-9a2e-61cce8535472 7775 0 2023-05-09 12:28:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-09 12:28:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  9 12:28:21.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7501" for this suite. 05/09/23 12:28:21.663
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:28:21.667
May  9 12:28:21.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 12:28:21.668
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:21.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:21.681
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-42c07805-fafd-4d74-b542-2cc93cbe0c61 05/09/23 12:28:21.682
STEP: Creating a pod to test consume configMaps 05/09/23 12:28:21.686
May  9 12:28:21.691: INFO: Waiting up to 5m0s for pod "pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357" in namespace "configmap-1833" to be "Succeeded or Failed"
May  9 12:28:21.696: INFO: Pod "pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357": Phase="Pending", Reason="", readiness=false. Elapsed: 4.963351ms
May  9 12:28:23.700: INFO: Pod "pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009113337s
May  9 12:28:25.700: INFO: Pod "pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009139809s
STEP: Saw pod success 05/09/23 12:28:25.7
May  9 12:28:25.700: INFO: Pod "pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357" satisfied condition "Succeeded or Failed"
May  9 12:28:25.702: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 12:28:25.714
May  9 12:28:25.723: INFO: Waiting for pod pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357 to disappear
May  9 12:28:25.728: INFO: Pod pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  9 12:28:25.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1833" for this suite. 05/09/23 12:28:25.732
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":6,"skipped":94,"failed":0}
------------------------------
• [4.070 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:28:21.667
    May  9 12:28:21.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 12:28:21.668
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:21.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:21.681
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-42c07805-fafd-4d74-b542-2cc93cbe0c61 05/09/23 12:28:21.682
    STEP: Creating a pod to test consume configMaps 05/09/23 12:28:21.686
    May  9 12:28:21.691: INFO: Waiting up to 5m0s for pod "pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357" in namespace "configmap-1833" to be "Succeeded or Failed"
    May  9 12:28:21.696: INFO: Pod "pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357": Phase="Pending", Reason="", readiness=false. Elapsed: 4.963351ms
    May  9 12:28:23.700: INFO: Pod "pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009113337s
    May  9 12:28:25.700: INFO: Pod "pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009139809s
    STEP: Saw pod success 05/09/23 12:28:25.7
    May  9 12:28:25.700: INFO: Pod "pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357" satisfied condition "Succeeded or Failed"
    May  9 12:28:25.702: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 12:28:25.714
    May  9 12:28:25.723: INFO: Waiting for pod pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357 to disappear
    May  9 12:28:25.728: INFO: Pod pod-configmaps-cbf9c4f4-37cb-483a-8936-7ddb944ce357 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 12:28:25.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1833" for this suite. 05/09/23 12:28:25.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:28:25.738
May  9 12:28:25.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 12:28:25.739
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:25.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:25.756
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 05/09/23 12:28:25.758
May  9 12:28:25.766: INFO: Waiting up to 5m0s for pod "pod-f1d7621c-90cb-47a4-9af7-d5957f11f897" in namespace "emptydir-5266" to be "Succeeded or Failed"
May  9 12:28:25.773: INFO: Pod "pod-f1d7621c-90cb-47a4-9af7-d5957f11f897": Phase="Pending", Reason="", readiness=false. Elapsed: 6.541694ms
May  9 12:28:27.775: INFO: Pod "pod-f1d7621c-90cb-47a4-9af7-d5957f11f897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008934751s
May  9 12:28:29.776: INFO: Pod "pod-f1d7621c-90cb-47a4-9af7-d5957f11f897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009756218s
STEP: Saw pod success 05/09/23 12:28:29.776
May  9 12:28:29.776: INFO: Pod "pod-f1d7621c-90cb-47a4-9af7-d5957f11f897" satisfied condition "Succeeded or Failed"
May  9 12:28:29.778: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-f1d7621c-90cb-47a4-9af7-d5957f11f897 container test-container: <nil>
STEP: delete the pod 05/09/23 12:28:29.782
May  9 12:28:29.791: INFO: Waiting for pod pod-f1d7621c-90cb-47a4-9af7-d5957f11f897 to disappear
May  9 12:28:29.792: INFO: Pod pod-f1d7621c-90cb-47a4-9af7-d5957f11f897 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 12:28:29.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5266" for this suite. 05/09/23 12:28:29.795
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":7,"skipped":124,"failed":0}
------------------------------
• [4.061 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:28:25.738
    May  9 12:28:25.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 12:28:25.739
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:25.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:25.756
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/09/23 12:28:25.758
    May  9 12:28:25.766: INFO: Waiting up to 5m0s for pod "pod-f1d7621c-90cb-47a4-9af7-d5957f11f897" in namespace "emptydir-5266" to be "Succeeded or Failed"
    May  9 12:28:25.773: INFO: Pod "pod-f1d7621c-90cb-47a4-9af7-d5957f11f897": Phase="Pending", Reason="", readiness=false. Elapsed: 6.541694ms
    May  9 12:28:27.775: INFO: Pod "pod-f1d7621c-90cb-47a4-9af7-d5957f11f897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008934751s
    May  9 12:28:29.776: INFO: Pod "pod-f1d7621c-90cb-47a4-9af7-d5957f11f897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009756218s
    STEP: Saw pod success 05/09/23 12:28:29.776
    May  9 12:28:29.776: INFO: Pod "pod-f1d7621c-90cb-47a4-9af7-d5957f11f897" satisfied condition "Succeeded or Failed"
    May  9 12:28:29.778: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-f1d7621c-90cb-47a4-9af7-d5957f11f897 container test-container: <nil>
    STEP: delete the pod 05/09/23 12:28:29.782
    May  9 12:28:29.791: INFO: Waiting for pod pod-f1d7621c-90cb-47a4-9af7-d5957f11f897 to disappear
    May  9 12:28:29.792: INFO: Pod pod-f1d7621c-90cb-47a4-9af7-d5957f11f897 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 12:28:29.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5266" for this suite. 05/09/23 12:28:29.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:28:29.801
May  9 12:28:29.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-runtime 05/09/23 12:28:29.801
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:29.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:29.815
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/09/23 12:28:29.822
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/09/23 12:28:45.876
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/09/23 12:28:45.878
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/09/23 12:28:45.883
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/09/23 12:28:45.883
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/09/23 12:28:45.9
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/09/23 12:28:47.908
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/09/23 12:28:49.915
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/09/23 12:28:49.92
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/09/23 12:28:49.92
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/09/23 12:28:49.934
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/09/23 12:28:50.939
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/09/23 12:28:52.948
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/09/23 12:28:52.952
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/09/23 12:28:52.952
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  9 12:28:52.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8421" for this suite. 05/09/23 12:28:52.969
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":8,"skipped":149,"failed":0}
------------------------------
• [SLOW TEST] [23.174 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:28:29.801
    May  9 12:28:29.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-runtime 05/09/23 12:28:29.801
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:29.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:29.815
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/09/23 12:28:29.822
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/09/23 12:28:45.876
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/09/23 12:28:45.878
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/09/23 12:28:45.883
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/09/23 12:28:45.883
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/09/23 12:28:45.9
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/09/23 12:28:47.908
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/09/23 12:28:49.915
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/09/23 12:28:49.92
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/09/23 12:28:49.92
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/09/23 12:28:49.934
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/09/23 12:28:50.939
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/09/23 12:28:52.948
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/09/23 12:28:52.952
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/09/23 12:28:52.952
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  9 12:28:52.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8421" for this suite. 05/09/23 12:28:52.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:28:52.976
May  9 12:28:52.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 12:28:52.976
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:52.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:52.99
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 05/09/23 12:28:52.991
May  9 12:28:52.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-9702 create -f -'
May  9 12:28:53.124: INFO: stderr: ""
May  9 12:28:53.124: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 05/09/23 12:28:53.124
May  9 12:28:53.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-9702 diff -f -'
May  9 12:28:53.247: INFO: rc: 1
May  9 12:28:53.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-9702 delete -f -'
May  9 12:28:53.300: INFO: stderr: ""
May  9 12:28:53.300: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 12:28:53.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9702" for this suite. 05/09/23 12:28:53.304
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":9,"skipped":183,"failed":0}
------------------------------
• [0.334 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:28:52.976
    May  9 12:28:52.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 12:28:52.976
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:52.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:52.99
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 05/09/23 12:28:52.991
    May  9 12:28:52.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-9702 create -f -'
    May  9 12:28:53.124: INFO: stderr: ""
    May  9 12:28:53.124: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 05/09/23 12:28:53.124
    May  9 12:28:53.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-9702 diff -f -'
    May  9 12:28:53.247: INFO: rc: 1
    May  9 12:28:53.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-9702 delete -f -'
    May  9 12:28:53.300: INFO: stderr: ""
    May  9 12:28:53.300: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 12:28:53.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9702" for this suite. 05/09/23 12:28:53.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:28:53.31
May  9 12:28:53.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 12:28:53.311
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:53.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:53.326
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 05/09/23 12:28:53.328
May  9 12:28:53.334: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-409282a5-1a30-4f15-910b-987cfe5a95aa" in namespace "emptydir-9241" to be "running"
May  9 12:28:53.336: INFO: Pod "pod-sharedvolume-409282a5-1a30-4f15-910b-987cfe5a95aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.727175ms
May  9 12:28:55.340: INFO: Pod "pod-sharedvolume-409282a5-1a30-4f15-910b-987cfe5a95aa": Phase="Running", Reason="", readiness=false. Elapsed: 2.005382641s
May  9 12:28:55.340: INFO: Pod "pod-sharedvolume-409282a5-1a30-4f15-910b-987cfe5a95aa" satisfied condition "running"
STEP: Reading file content from the nginx-container 05/09/23 12:28:55.34
May  9 12:28:55.340: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9241 PodName:pod-sharedvolume-409282a5-1a30-4f15-910b-987cfe5a95aa ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 12:28:55.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:28:55.340: INFO: ExecWithOptions: Clientset creation
May  9 12:28:55.340: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-9241/pods/pod-sharedvolume-409282a5-1a30-4f15-910b-987cfe5a95aa/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
May  9 12:28:55.390: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 12:28:55.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9241" for this suite. 05/09/23 12:28:55.393
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":10,"skipped":205,"failed":0}
------------------------------
• [2.087 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:28:53.31
    May  9 12:28:53.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 12:28:53.311
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:53.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:53.326
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 05/09/23 12:28:53.328
    May  9 12:28:53.334: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-409282a5-1a30-4f15-910b-987cfe5a95aa" in namespace "emptydir-9241" to be "running"
    May  9 12:28:53.336: INFO: Pod "pod-sharedvolume-409282a5-1a30-4f15-910b-987cfe5a95aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.727175ms
    May  9 12:28:55.340: INFO: Pod "pod-sharedvolume-409282a5-1a30-4f15-910b-987cfe5a95aa": Phase="Running", Reason="", readiness=false. Elapsed: 2.005382641s
    May  9 12:28:55.340: INFO: Pod "pod-sharedvolume-409282a5-1a30-4f15-910b-987cfe5a95aa" satisfied condition "running"
    STEP: Reading file content from the nginx-container 05/09/23 12:28:55.34
    May  9 12:28:55.340: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9241 PodName:pod-sharedvolume-409282a5-1a30-4f15-910b-987cfe5a95aa ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 12:28:55.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:28:55.340: INFO: ExecWithOptions: Clientset creation
    May  9 12:28:55.340: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-9241/pods/pod-sharedvolume-409282a5-1a30-4f15-910b-987cfe5a95aa/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    May  9 12:28:55.390: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 12:28:55.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9241" for this suite. 05/09/23 12:28:55.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:28:55.399
May  9 12:28:55.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/09/23 12:28:55.4
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:55.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:55.414
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 05/09/23 12:28:55.415
STEP: Creating hostNetwork=false pod 05/09/23 12:28:55.415
May  9 12:28:55.424: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4158" to be "running and ready"
May  9 12:28:55.430: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.482399ms
May  9 12:28:55.430: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  9 12:28:57.433: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008634743s
May  9 12:28:57.433: INFO: The phase of Pod test-pod is Running (Ready = true)
May  9 12:28:57.433: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 05/09/23 12:28:57.435
May  9 12:28:57.441: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4158" to be "running and ready"
May  9 12:28:57.445: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.2633ms
May  9 12:28:57.445: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May  9 12:28:59.449: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008198863s
May  9 12:28:59.449: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May  9 12:29:01.450: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008621862s
May  9 12:29:01.450: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
May  9 12:29:01.450: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 05/09/23 12:29:01.451
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/09/23 12:29:01.452
May  9 12:29:01.452: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 12:29:01.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:29:01.452: INFO: ExecWithOptions: Clientset creation
May  9 12:29:01.452: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  9 12:29:01.496: INFO: Exec stderr: ""
May  9 12:29:01.496: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 12:29:01.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:29:01.496: INFO: ExecWithOptions: Clientset creation
May  9 12:29:01.496: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  9 12:29:01.544: INFO: Exec stderr: ""
May  9 12:29:01.544: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 12:29:01.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:29:01.545: INFO: ExecWithOptions: Clientset creation
May  9 12:29:01.545: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  9 12:29:01.590: INFO: Exec stderr: ""
May  9 12:29:01.590: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 12:29:01.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:29:01.591: INFO: ExecWithOptions: Clientset creation
May  9 12:29:01.591: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  9 12:29:01.637: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/09/23 12:29:01.637
May  9 12:29:01.637: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 12:29:01.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:29:01.637: INFO: ExecWithOptions: Clientset creation
May  9 12:29:01.637: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May  9 12:29:01.682: INFO: Exec stderr: ""
May  9 12:29:01.682: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 12:29:01.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:29:01.683: INFO: ExecWithOptions: Clientset creation
May  9 12:29:01.683: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May  9 12:29:01.725: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/09/23 12:29:01.725
May  9 12:29:01.725: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 12:29:01.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:29:01.725: INFO: ExecWithOptions: Clientset creation
May  9 12:29:01.725: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  9 12:29:01.772: INFO: Exec stderr: ""
May  9 12:29:01.772: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 12:29:01.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:29:01.772: INFO: ExecWithOptions: Clientset creation
May  9 12:29:01.772: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  9 12:29:01.825: INFO: Exec stderr: ""
May  9 12:29:01.825: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 12:29:01.825: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:29:01.825: INFO: ExecWithOptions: Clientset creation
May  9 12:29:01.825: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  9 12:29:01.873: INFO: Exec stderr: ""
May  9 12:29:01.873: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 12:29:01.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:29:01.873: INFO: ExecWithOptions: Clientset creation
May  9 12:29:01.873: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  9 12:29:01.918: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
May  9 12:29:01.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4158" for this suite. 05/09/23 12:29:01.922
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":11,"skipped":254,"failed":0}
------------------------------
• [SLOW TEST] [6.529 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:28:55.399
    May  9 12:28:55.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/09/23 12:28:55.4
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:28:55.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:28:55.414
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 05/09/23 12:28:55.415
    STEP: Creating hostNetwork=false pod 05/09/23 12:28:55.415
    May  9 12:28:55.424: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4158" to be "running and ready"
    May  9 12:28:55.430: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.482399ms
    May  9 12:28:55.430: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:28:57.433: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008634743s
    May  9 12:28:57.433: INFO: The phase of Pod test-pod is Running (Ready = true)
    May  9 12:28:57.433: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 05/09/23 12:28:57.435
    May  9 12:28:57.441: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4158" to be "running and ready"
    May  9 12:28:57.445: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.2633ms
    May  9 12:28:57.445: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:28:59.449: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008198863s
    May  9 12:28:59.449: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:29:01.450: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008621862s
    May  9 12:29:01.450: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    May  9 12:29:01.450: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 05/09/23 12:29:01.451
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/09/23 12:29:01.452
    May  9 12:29:01.452: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 12:29:01.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:29:01.452: INFO: ExecWithOptions: Clientset creation
    May  9 12:29:01.452: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  9 12:29:01.496: INFO: Exec stderr: ""
    May  9 12:29:01.496: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 12:29:01.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:29:01.496: INFO: ExecWithOptions: Clientset creation
    May  9 12:29:01.496: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  9 12:29:01.544: INFO: Exec stderr: ""
    May  9 12:29:01.544: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 12:29:01.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:29:01.545: INFO: ExecWithOptions: Clientset creation
    May  9 12:29:01.545: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  9 12:29:01.590: INFO: Exec stderr: ""
    May  9 12:29:01.590: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 12:29:01.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:29:01.591: INFO: ExecWithOptions: Clientset creation
    May  9 12:29:01.591: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  9 12:29:01.637: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/09/23 12:29:01.637
    May  9 12:29:01.637: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 12:29:01.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:29:01.637: INFO: ExecWithOptions: Clientset creation
    May  9 12:29:01.637: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May  9 12:29:01.682: INFO: Exec stderr: ""
    May  9 12:29:01.682: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 12:29:01.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:29:01.683: INFO: ExecWithOptions: Clientset creation
    May  9 12:29:01.683: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May  9 12:29:01.725: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/09/23 12:29:01.725
    May  9 12:29:01.725: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 12:29:01.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:29:01.725: INFO: ExecWithOptions: Clientset creation
    May  9 12:29:01.725: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  9 12:29:01.772: INFO: Exec stderr: ""
    May  9 12:29:01.772: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 12:29:01.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:29:01.772: INFO: ExecWithOptions: Clientset creation
    May  9 12:29:01.772: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  9 12:29:01.825: INFO: Exec stderr: ""
    May  9 12:29:01.825: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 12:29:01.825: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:29:01.825: INFO: ExecWithOptions: Clientset creation
    May  9 12:29:01.825: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  9 12:29:01.873: INFO: Exec stderr: ""
    May  9 12:29:01.873: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 12:29:01.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:29:01.873: INFO: ExecWithOptions: Clientset creation
    May  9 12:29:01.873: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  9 12:29:01.918: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    May  9 12:29:01.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-4158" for this suite. 05/09/23 12:29:01.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:29:01.929
May  9 12:29:01.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename replication-controller 05/09/23 12:29:01.93
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:01.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:01.943
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 05/09/23 12:29:01.947
STEP: waiting for RC to be added 05/09/23 12:29:01.951
STEP: waiting for available Replicas 05/09/23 12:29:01.951
STEP: patching ReplicationController 05/09/23 12:29:03.942
STEP: waiting for RC to be modified 05/09/23 12:29:03.949
STEP: patching ReplicationController status 05/09/23 12:29:03.949
STEP: waiting for RC to be modified 05/09/23 12:29:03.953
STEP: waiting for available Replicas 05/09/23 12:29:03.954
STEP: fetching ReplicationController status 05/09/23 12:29:03.958
STEP: patching ReplicationController scale 05/09/23 12:29:03.96
STEP: waiting for RC to be modified 05/09/23 12:29:03.965
STEP: waiting for ReplicationController's scale to be the max amount 05/09/23 12:29:03.965
STEP: fetching ReplicationController; ensuring that it's patched 05/09/23 12:29:05.742
STEP: updating ReplicationController status 05/09/23 12:29:05.746
STEP: waiting for RC to be modified 05/09/23 12:29:05.751
STEP: listing all ReplicationControllers 05/09/23 12:29:05.752
STEP: checking that ReplicationController has expected values 05/09/23 12:29:05.757
STEP: deleting ReplicationControllers by collection 05/09/23 12:29:05.757
STEP: waiting for ReplicationController to have a DELETED watchEvent 05/09/23 12:29:05.765
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  9 12:29:05.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-820" for this suite. 05/09/23 12:29:05.798
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":12,"skipped":261,"failed":0}
------------------------------
• [3.874 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:29:01.929
    May  9 12:29:01.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename replication-controller 05/09/23 12:29:01.93
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:01.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:01.943
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 05/09/23 12:29:01.947
    STEP: waiting for RC to be added 05/09/23 12:29:01.951
    STEP: waiting for available Replicas 05/09/23 12:29:01.951
    STEP: patching ReplicationController 05/09/23 12:29:03.942
    STEP: waiting for RC to be modified 05/09/23 12:29:03.949
    STEP: patching ReplicationController status 05/09/23 12:29:03.949
    STEP: waiting for RC to be modified 05/09/23 12:29:03.953
    STEP: waiting for available Replicas 05/09/23 12:29:03.954
    STEP: fetching ReplicationController status 05/09/23 12:29:03.958
    STEP: patching ReplicationController scale 05/09/23 12:29:03.96
    STEP: waiting for RC to be modified 05/09/23 12:29:03.965
    STEP: waiting for ReplicationController's scale to be the max amount 05/09/23 12:29:03.965
    STEP: fetching ReplicationController; ensuring that it's patched 05/09/23 12:29:05.742
    STEP: updating ReplicationController status 05/09/23 12:29:05.746
    STEP: waiting for RC to be modified 05/09/23 12:29:05.751
    STEP: listing all ReplicationControllers 05/09/23 12:29:05.752
    STEP: checking that ReplicationController has expected values 05/09/23 12:29:05.757
    STEP: deleting ReplicationControllers by collection 05/09/23 12:29:05.757
    STEP: waiting for ReplicationController to have a DELETED watchEvent 05/09/23 12:29:05.765
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  9 12:29:05.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-820" for this suite. 05/09/23 12:29:05.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:29:05.806
May  9 12:29:05.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 12:29:05.807
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:05.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:05.825
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 05/09/23 12:29:05.827
May  9 12:29:05.833: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1" in namespace "downward-api-7654" to be "Succeeded or Failed"
May  9 12:29:05.838: INFO: Pod "downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.44576ms
May  9 12:29:07.842: INFO: Pod "downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008794408s
May  9 12:29:09.843: INFO: Pod "downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009800885s
STEP: Saw pod success 05/09/23 12:29:09.843
May  9 12:29:09.843: INFO: Pod "downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1" satisfied condition "Succeeded or Failed"
May  9 12:29:09.845: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1 container client-container: <nil>
STEP: delete the pod 05/09/23 12:29:09.849
May  9 12:29:09.857: INFO: Waiting for pod downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1 to disappear
May  9 12:29:09.859: INFO: Pod downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  9 12:29:09.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7654" for this suite. 05/09/23 12:29:09.862
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":13,"skipped":360,"failed":0}
------------------------------
• [4.060 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:29:05.806
    May  9 12:29:05.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 12:29:05.807
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:05.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:05.825
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 05/09/23 12:29:05.827
    May  9 12:29:05.833: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1" in namespace "downward-api-7654" to be "Succeeded or Failed"
    May  9 12:29:05.838: INFO: Pod "downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.44576ms
    May  9 12:29:07.842: INFO: Pod "downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008794408s
    May  9 12:29:09.843: INFO: Pod "downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009800885s
    STEP: Saw pod success 05/09/23 12:29:09.843
    May  9 12:29:09.843: INFO: Pod "downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1" satisfied condition "Succeeded or Failed"
    May  9 12:29:09.845: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1 container client-container: <nil>
    STEP: delete the pod 05/09/23 12:29:09.849
    May  9 12:29:09.857: INFO: Waiting for pod downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1 to disappear
    May  9 12:29:09.859: INFO: Pod downwardapi-volume-0a94a734-8f90-4770-9fbc-f7f21aa3b9c1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  9 12:29:09.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7654" for this suite. 05/09/23 12:29:09.862
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:29:09.867
May  9 12:29:09.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 12:29:09.868
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:09.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:09.88
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-8158 05/09/23 12:29:09.882
STEP: creating service affinity-clusterip-transition in namespace services-8158 05/09/23 12:29:09.882
STEP: creating replication controller affinity-clusterip-transition in namespace services-8158 05/09/23 12:29:09.893
I0509 12:29:09.899099      24 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8158, replica count: 3
I0509 12:29:12.949633      24 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0509 12:29:15.950539      24 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 12:29:15.955: INFO: Creating new exec pod
May  9 12:29:15.961: INFO: Waiting up to 5m0s for pod "execpod-affinity5l7bq" in namespace "services-8158" to be "running"
May  9 12:29:15.963: INFO: Pod "execpod-affinity5l7bq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129581ms
May  9 12:29:17.967: INFO: Pod "execpod-affinity5l7bq": Phase="Running", Reason="", readiness=true. Elapsed: 2.005926482s
May  9 12:29:17.967: INFO: Pod "execpod-affinity5l7bq" satisfied condition "running"
May  9 12:29:18.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8158 exec execpod-affinity5l7bq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
May  9 12:29:19.070: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May  9 12:29:19.070: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 12:29:19.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8158 exec execpod-affinity5l7bq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.51.57 80'
May  9 12:29:19.171: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.51.57 80\nConnection to 10.106.51.57 80 port [tcp/http] succeeded!\n"
May  9 12:29:19.171: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 12:29:19.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8158 exec execpod-affinity5l7bq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.51.57:80/ ; done'
May  9 12:29:19.320: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n"
May  9 12:29:19.320: INFO: stdout: "\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-fxtcw\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-fxtcw\naffinity-clusterip-transition-fxtcw\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-fxtcw\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-78d65"
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-fxtcw
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-fxtcw
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-fxtcw
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-fxtcw
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
May  9 12:29:19.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8158 exec execpod-affinity5l7bq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.51.57:80/ ; done'
May  9 12:29:19.477: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n"
May  9 12:29:19.477: INFO: stdout: "\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c"
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
May  9 12:29:19.477: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8158, will wait for the garbage collector to delete the pods 05/09/23 12:29:19.485
May  9 12:29:19.545: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.035599ms
May  9 12:29:19.646: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.939396ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 12:29:22.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8158" for this suite. 05/09/23 12:29:22.177
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":14,"skipped":362,"failed":0}
------------------------------
• [SLOW TEST] [12.318 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:29:09.867
    May  9 12:29:09.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 12:29:09.868
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:09.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:09.88
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-8158 05/09/23 12:29:09.882
    STEP: creating service affinity-clusterip-transition in namespace services-8158 05/09/23 12:29:09.882
    STEP: creating replication controller affinity-clusterip-transition in namespace services-8158 05/09/23 12:29:09.893
    I0509 12:29:09.899099      24 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8158, replica count: 3
    I0509 12:29:12.949633      24 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0509 12:29:15.950539      24 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 12:29:15.955: INFO: Creating new exec pod
    May  9 12:29:15.961: INFO: Waiting up to 5m0s for pod "execpod-affinity5l7bq" in namespace "services-8158" to be "running"
    May  9 12:29:15.963: INFO: Pod "execpod-affinity5l7bq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129581ms
    May  9 12:29:17.967: INFO: Pod "execpod-affinity5l7bq": Phase="Running", Reason="", readiness=true. Elapsed: 2.005926482s
    May  9 12:29:17.967: INFO: Pod "execpod-affinity5l7bq" satisfied condition "running"
    May  9 12:29:18.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8158 exec execpod-affinity5l7bq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    May  9 12:29:19.070: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    May  9 12:29:19.070: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 12:29:19.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8158 exec execpod-affinity5l7bq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.51.57 80'
    May  9 12:29:19.171: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.51.57 80\nConnection to 10.106.51.57 80 port [tcp/http] succeeded!\n"
    May  9 12:29:19.171: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 12:29:19.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8158 exec execpod-affinity5l7bq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.51.57:80/ ; done'
    May  9 12:29:19.320: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n"
    May  9 12:29:19.320: INFO: stdout: "\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-fxtcw\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-fxtcw\naffinity-clusterip-transition-fxtcw\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-fxtcw\naffinity-clusterip-transition-78d65\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-78d65"
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-fxtcw
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-fxtcw
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-fxtcw
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-fxtcw
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.320: INFO: Received response from host: affinity-clusterip-transition-78d65
    May  9 12:29:19.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8158 exec execpod-affinity5l7bq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.51.57:80/ ; done'
    May  9 12:29:19.477: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.51.57:80/\n"
    May  9 12:29:19.477: INFO: stdout: "\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c\naffinity-clusterip-transition-2m94c"
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Received response from host: affinity-clusterip-transition-2m94c
    May  9 12:29:19.477: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8158, will wait for the garbage collector to delete the pods 05/09/23 12:29:19.485
    May  9 12:29:19.545: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.035599ms
    May  9 12:29:19.646: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.939396ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 12:29:22.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8158" for this suite. 05/09/23 12:29:22.177
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:29:22.185
May  9 12:29:22.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename replication-controller 05/09/23 12:29:22.186
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:22.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:22.201
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
May  9 12:29:22.202: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/09/23 12:29:23.21
STEP: Checking rc "condition-test" has the desired failure condition set 05/09/23 12:29:23.215
STEP: Scaling down rc "condition-test" to satisfy pod quota 05/09/23 12:29:24.22
May  9 12:29:24.228: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 05/09/23 12:29:24.228
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  9 12:29:25.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9362" for this suite. 05/09/23 12:29:25.241
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":15,"skipped":362,"failed":0}
------------------------------
• [3.062 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:29:22.185
    May  9 12:29:22.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename replication-controller 05/09/23 12:29:22.186
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:22.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:22.201
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    May  9 12:29:22.202: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/09/23 12:29:23.21
    STEP: Checking rc "condition-test" has the desired failure condition set 05/09/23 12:29:23.215
    STEP: Scaling down rc "condition-test" to satisfy pod quota 05/09/23 12:29:24.22
    May  9 12:29:24.228: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 05/09/23 12:29:24.228
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  9 12:29:25.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9362" for this suite. 05/09/23 12:29:25.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:29:25.248
May  9 12:29:25.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 12:29:25.249
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:25.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:25.262
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 05/09/23 12:29:25.263
May  9 12:29:25.270: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877" in namespace "downward-api-8062" to be "Succeeded or Failed"
May  9 12:29:25.274: INFO: Pod "downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877": Phase="Pending", Reason="", readiness=false. Elapsed: 3.419345ms
May  9 12:29:27.279: INFO: Pod "downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008745772s
May  9 12:29:29.278: INFO: Pod "downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007298327s
STEP: Saw pod success 05/09/23 12:29:29.278
May  9 12:29:29.278: INFO: Pod "downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877" satisfied condition "Succeeded or Failed"
May  9 12:29:29.280: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877 container client-container: <nil>
STEP: delete the pod 05/09/23 12:29:29.284
May  9 12:29:29.292: INFO: Waiting for pod downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877 to disappear
May  9 12:29:29.294: INFO: Pod downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  9 12:29:29.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8062" for this suite. 05/09/23 12:29:29.298
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":16,"skipped":389,"failed":0}
------------------------------
• [4.054 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:29:25.248
    May  9 12:29:25.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 12:29:25.249
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:25.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:25.262
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 05/09/23 12:29:25.263
    May  9 12:29:25.270: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877" in namespace "downward-api-8062" to be "Succeeded or Failed"
    May  9 12:29:25.274: INFO: Pod "downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877": Phase="Pending", Reason="", readiness=false. Elapsed: 3.419345ms
    May  9 12:29:27.279: INFO: Pod "downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008745772s
    May  9 12:29:29.278: INFO: Pod "downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007298327s
    STEP: Saw pod success 05/09/23 12:29:29.278
    May  9 12:29:29.278: INFO: Pod "downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877" satisfied condition "Succeeded or Failed"
    May  9 12:29:29.280: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877 container client-container: <nil>
    STEP: delete the pod 05/09/23 12:29:29.284
    May  9 12:29:29.292: INFO: Waiting for pod downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877 to disappear
    May  9 12:29:29.294: INFO: Pod downwardapi-volume-4ca741c4-69a1-46d6-b17e-4ae8a8583877 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  9 12:29:29.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8062" for this suite. 05/09/23 12:29:29.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:29:29.303
May  9 12:29:29.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename sysctl 05/09/23 12:29:29.304
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:29.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:29.318
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/09/23 12:29:29.319
STEP: Watching for error events or started pod 05/09/23 12:29:29.324
STEP: Waiting for pod completion 05/09/23 12:29:31.328
May  9 12:29:31.328: INFO: Waiting up to 3m0s for pod "sysctl-e1fb750a-fe2e-4ec3-af89-37287cd50d61" in namespace "sysctl-2399" to be "completed"
May  9 12:29:31.330: INFO: Pod "sysctl-e1fb750a-fe2e-4ec3-af89-37287cd50d61": Phase="Running", Reason="", readiness=true. Elapsed: 2.310445ms
May  9 12:29:33.334: INFO: Pod "sysctl-e1fb750a-fe2e-4ec3-af89-37287cd50d61": Phase="Running", Reason="", readiness=false. Elapsed: 2.005905536s
May  9 12:29:35.333: INFO: Pod "sysctl-e1fb750a-fe2e-4ec3-af89-37287cd50d61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00539241s
May  9 12:29:35.333: INFO: Pod "sysctl-e1fb750a-fe2e-4ec3-af89-37287cd50d61" satisfied condition "completed"
STEP: Checking that the pod succeeded 05/09/23 12:29:35.335
STEP: Getting logs from the pod 05/09/23 12:29:35.335
STEP: Checking that the sysctl is actually updated 05/09/23 12:29:35.339
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
May  9 12:29:35.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2399" for this suite. 05/09/23 12:29:35.343
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":17,"skipped":404,"failed":0}
------------------------------
• [SLOW TEST] [6.045 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:29:29.303
    May  9 12:29:29.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename sysctl 05/09/23 12:29:29.304
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:29.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:29.318
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/09/23 12:29:29.319
    STEP: Watching for error events or started pod 05/09/23 12:29:29.324
    STEP: Waiting for pod completion 05/09/23 12:29:31.328
    May  9 12:29:31.328: INFO: Waiting up to 3m0s for pod "sysctl-e1fb750a-fe2e-4ec3-af89-37287cd50d61" in namespace "sysctl-2399" to be "completed"
    May  9 12:29:31.330: INFO: Pod "sysctl-e1fb750a-fe2e-4ec3-af89-37287cd50d61": Phase="Running", Reason="", readiness=true. Elapsed: 2.310445ms
    May  9 12:29:33.334: INFO: Pod "sysctl-e1fb750a-fe2e-4ec3-af89-37287cd50d61": Phase="Running", Reason="", readiness=false. Elapsed: 2.005905536s
    May  9 12:29:35.333: INFO: Pod "sysctl-e1fb750a-fe2e-4ec3-af89-37287cd50d61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00539241s
    May  9 12:29:35.333: INFO: Pod "sysctl-e1fb750a-fe2e-4ec3-af89-37287cd50d61" satisfied condition "completed"
    STEP: Checking that the pod succeeded 05/09/23 12:29:35.335
    STEP: Getting logs from the pod 05/09/23 12:29:35.335
    STEP: Checking that the sysctl is actually updated 05/09/23 12:29:35.339
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    May  9 12:29:35.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-2399" for this suite. 05/09/23 12:29:35.343
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:29:35.349
May  9 12:29:35.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename daemonsets 05/09/23 12:29:35.349
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:35.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:35.363
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 05/09/23 12:29:35.378
STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 12:29:35.383
May  9 12:29:35.387: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:35.387: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:35.387: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:35.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 12:29:35.389: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
May  9 12:29:36.392: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:36.392: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:36.392: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:36.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  9 12:29:36.395: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
May  9 12:29:37.393: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:37.393: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:37.393: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:37.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 12:29:37.396: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 12:29:38.394: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:38.394: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:38.394: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:38.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 12:29:38.396: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 12:29:39.393: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:39.393: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:39.393: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:39.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 12:29:39.395: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 05/09/23 12:29:39.397
May  9 12:29:39.409: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:39.409: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:39.409: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:39.411: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 12:29:39.411: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 12:29:40.419: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:40.419: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:40.419: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:40.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 12:29:40.422: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 12:29:41.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:41.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:41.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:41.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 12:29:41.418: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 12:29:42.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:42.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:42.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:42.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 12:29:42.418: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 12:29:43.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:43.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:43.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:43.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 12:29:43.418: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 12:29:44.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:44.416: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:44.416: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:29:44.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 12:29:44.418: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/09/23 12:29:44.42
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8354, will wait for the garbage collector to delete the pods 05/09/23 12:29:44.42
May  9 12:29:44.478: INFO: Deleting DaemonSet.extensions daemon-set took: 5.21453ms
May  9 12:29:44.579: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.535195ms
May  9 12:29:47.182: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 12:29:47.182: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  9 12:29:47.184: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8888"},"items":null}

May  9 12:29:47.186: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8888"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  9 12:29:47.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8354" for this suite. 05/09/23 12:29:47.199
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":18,"skipped":408,"failed":0}
------------------------------
• [SLOW TEST] [11.855 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:29:35.349
    May  9 12:29:35.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename daemonsets 05/09/23 12:29:35.349
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:35.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:35.363
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 05/09/23 12:29:35.378
    STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 12:29:35.383
    May  9 12:29:35.387: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:35.387: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:35.387: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:35.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 12:29:35.389: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
    May  9 12:29:36.392: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:36.392: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:36.392: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:36.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  9 12:29:36.395: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
    May  9 12:29:37.393: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:37.393: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:37.393: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:37.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 12:29:37.396: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 12:29:38.394: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:38.394: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:38.394: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:38.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 12:29:38.396: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 12:29:39.393: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:39.393: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:39.393: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:39.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 12:29:39.395: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 05/09/23 12:29:39.397
    May  9 12:29:39.409: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:39.409: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:39.409: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:39.411: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 12:29:39.411: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 12:29:40.419: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:40.419: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:40.419: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:40.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 12:29:40.422: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 12:29:41.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:41.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:41.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:41.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 12:29:41.418: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 12:29:42.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:42.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:42.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:42.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 12:29:42.418: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 12:29:43.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:43.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:43.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:43.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 12:29:43.418: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 12:29:44.415: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:44.416: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:44.416: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:29:44.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 12:29:44.418: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/09/23 12:29:44.42
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8354, will wait for the garbage collector to delete the pods 05/09/23 12:29:44.42
    May  9 12:29:44.478: INFO: Deleting DaemonSet.extensions daemon-set took: 5.21453ms
    May  9 12:29:44.579: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.535195ms
    May  9 12:29:47.182: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 12:29:47.182: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  9 12:29:47.184: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8888"},"items":null}

    May  9 12:29:47.186: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8888"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  9 12:29:47.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8354" for this suite. 05/09/23 12:29:47.199
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:29:47.204
May  9 12:29:47.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 12:29:47.205
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:47.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:47.225
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/09/23 12:29:47.23
May  9 12:29:47.236: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7111" to be "running and ready"
May  9 12:29:47.240: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.931731ms
May  9 12:29:47.240: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  9 12:29:49.244: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.00812979s
May  9 12:29:49.244: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  9 12:29:49.244: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 05/09/23 12:29:49.246
May  9 12:29:49.251: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7111" to be "running and ready"
May  9 12:29:49.255: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.987045ms
May  9 12:29:49.255: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  9 12:29:51.258: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007349307s
May  9 12:29:51.258: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
May  9 12:29:51.258: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/09/23 12:29:51.26
May  9 12:29:51.268: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  9 12:29:51.270: INFO: Pod pod-with-prestop-exec-hook still exists
May  9 12:29:53.270: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  9 12:29:53.273: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 05/09/23 12:29:53.273
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May  9 12:29:53.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7111" for this suite. 05/09/23 12:29:53.281
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":19,"skipped":408,"failed":0}
------------------------------
• [SLOW TEST] [6.082 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:29:47.204
    May  9 12:29:47.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 12:29:47.205
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:47.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:47.225
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/09/23 12:29:47.23
    May  9 12:29:47.236: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7111" to be "running and ready"
    May  9 12:29:47.240: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.931731ms
    May  9 12:29:47.240: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:29:49.244: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.00812979s
    May  9 12:29:49.244: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  9 12:29:49.244: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 05/09/23 12:29:49.246
    May  9 12:29:49.251: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7111" to be "running and ready"
    May  9 12:29:49.255: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.987045ms
    May  9 12:29:49.255: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:29:51.258: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007349307s
    May  9 12:29:51.258: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    May  9 12:29:51.258: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/09/23 12:29:51.26
    May  9 12:29:51.268: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  9 12:29:51.270: INFO: Pod pod-with-prestop-exec-hook still exists
    May  9 12:29:53.270: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  9 12:29:53.273: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 05/09/23 12:29:53.273
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May  9 12:29:53.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7111" for this suite. 05/09/23 12:29:53.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:29:53.287
May  9 12:29:53.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 12:29:53.287
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:53.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:53.304
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 12:29:53.317
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 12:29:53.572
STEP: Deploying the webhook pod 05/09/23 12:29:53.579
STEP: Wait for the deployment to be ready 05/09/23 12:29:53.589
May  9 12:29:53.599: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 12:29:55.606
STEP: Verifying the service has paired with the endpoint 05/09/23 12:29:55.618
May  9 12:29:56.618: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
May  9 12:29:56.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4125-crds.webhook.example.com via the AdmissionRegistration API 05/09/23 12:29:57.129
STEP: Creating a custom resource that should be mutated by the webhook 05/09/23 12:29:57.144
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 12:29:59.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2237" for this suite. 05/09/23 12:29:59.696
STEP: Destroying namespace "webhook-2237-markers" for this suite. 05/09/23 12:29:59.704
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":20,"skipped":419,"failed":0}
------------------------------
• [SLOW TEST] [6.483 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:29:53.287
    May  9 12:29:53.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 12:29:53.287
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:53.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:53.304
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 12:29:53.317
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 12:29:53.572
    STEP: Deploying the webhook pod 05/09/23 12:29:53.579
    STEP: Wait for the deployment to be ready 05/09/23 12:29:53.589
    May  9 12:29:53.599: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 12:29:55.606
    STEP: Verifying the service has paired with the endpoint 05/09/23 12:29:55.618
    May  9 12:29:56.618: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    May  9 12:29:56.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4125-crds.webhook.example.com via the AdmissionRegistration API 05/09/23 12:29:57.129
    STEP: Creating a custom resource that should be mutated by the webhook 05/09/23 12:29:57.144
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 12:29:59.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2237" for this suite. 05/09/23 12:29:59.696
    STEP: Destroying namespace "webhook-2237-markers" for this suite. 05/09/23 12:29:59.704
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:29:59.77
May  9 12:29:59.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 12:29:59.771
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:59.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:59.791
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-47e00ca4-6d7e-4595-85ff-2f3ef1af6a05 05/09/23 12:29:59.793
STEP: Creating a pod to test consume secrets 05/09/23 12:29:59.798
May  9 12:29:59.805: INFO: Waiting up to 5m0s for pod "pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2" in namespace "secrets-4241" to be "Succeeded or Failed"
May  9 12:29:59.811: INFO: Pod "pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.391238ms
May  9 12:30:01.815: INFO: Pod "pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00935379s
May  9 12:30:03.813: INFO: Pod "pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008140051s
STEP: Saw pod success 05/09/23 12:30:03.813
May  9 12:30:03.814: INFO: Pod "pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2" satisfied condition "Succeeded or Failed"
May  9 12:30:03.816: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2 container secret-env-test: <nil>
STEP: delete the pod 05/09/23 12:30:03.821
May  9 12:30:03.832: INFO: Waiting for pod pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2 to disappear
May  9 12:30:03.833: INFO: Pod pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May  9 12:30:03.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4241" for this suite. 05/09/23 12:30:03.837
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":21,"skipped":431,"failed":0}
------------------------------
• [4.071 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:29:59.77
    May  9 12:29:59.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 12:29:59.771
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:29:59.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:29:59.791
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-47e00ca4-6d7e-4595-85ff-2f3ef1af6a05 05/09/23 12:29:59.793
    STEP: Creating a pod to test consume secrets 05/09/23 12:29:59.798
    May  9 12:29:59.805: INFO: Waiting up to 5m0s for pod "pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2" in namespace "secrets-4241" to be "Succeeded or Failed"
    May  9 12:29:59.811: INFO: Pod "pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.391238ms
    May  9 12:30:01.815: INFO: Pod "pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00935379s
    May  9 12:30:03.813: INFO: Pod "pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008140051s
    STEP: Saw pod success 05/09/23 12:30:03.813
    May  9 12:30:03.814: INFO: Pod "pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2" satisfied condition "Succeeded or Failed"
    May  9 12:30:03.816: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2 container secret-env-test: <nil>
    STEP: delete the pod 05/09/23 12:30:03.821
    May  9 12:30:03.832: INFO: Waiting for pod pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2 to disappear
    May  9 12:30:03.833: INFO: Pod pod-secrets-fddff48c-2481-4423-8294-6c43fc0f6db2 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May  9 12:30:03.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4241" for this suite. 05/09/23 12:30:03.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:03.842
May  9 12:30:03.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 12:30:03.842
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:03.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:03.855
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/09/23 12:30:03.857
May  9 12:30:03.865: INFO: Waiting up to 5m0s for pod "pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f" in namespace "emptydir-7797" to be "Succeeded or Failed"
May  9 12:30:03.867: INFO: Pod "pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.656161ms
May  9 12:30:05.870: INFO: Pod "pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004421065s
May  9 12:30:07.870: INFO: Pod "pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00505233s
STEP: Saw pod success 05/09/23 12:30:07.871
May  9 12:30:07.871: INFO: Pod "pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f" satisfied condition "Succeeded or Failed"
May  9 12:30:07.873: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f container test-container: <nil>
STEP: delete the pod 05/09/23 12:30:07.877
May  9 12:30:07.886: INFO: Waiting for pod pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f to disappear
May  9 12:30:07.889: INFO: Pod pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 12:30:07.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7797" for this suite. 05/09/23 12:30:07.892
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":22,"skipped":439,"failed":0}
------------------------------
• [4.055 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:03.842
    May  9 12:30:03.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 12:30:03.842
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:03.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:03.855
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/09/23 12:30:03.857
    May  9 12:30:03.865: INFO: Waiting up to 5m0s for pod "pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f" in namespace "emptydir-7797" to be "Succeeded or Failed"
    May  9 12:30:03.867: INFO: Pod "pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.656161ms
    May  9 12:30:05.870: INFO: Pod "pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004421065s
    May  9 12:30:07.870: INFO: Pod "pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00505233s
    STEP: Saw pod success 05/09/23 12:30:07.871
    May  9 12:30:07.871: INFO: Pod "pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f" satisfied condition "Succeeded or Failed"
    May  9 12:30:07.873: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f container test-container: <nil>
    STEP: delete the pod 05/09/23 12:30:07.877
    May  9 12:30:07.886: INFO: Waiting for pod pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f to disappear
    May  9 12:30:07.889: INFO: Pod pod-8dcad73f-b6c8-4b07-b767-4d4232ac081f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 12:30:07.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7797" for this suite. 05/09/23 12:30:07.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:07.897
May  9 12:30:07.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 12:30:07.898
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:07.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:07.913
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 05/09/23 12:30:07.915
May  9 12:30:07.920: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3" in namespace "downward-api-1809" to be "Succeeded or Failed"
May  9 12:30:07.926: INFO: Pod "downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.361982ms
May  9 12:30:09.929: INFO: Pod "downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008879746s
May  9 12:30:11.929: INFO: Pod "downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008917316s
STEP: Saw pod success 05/09/23 12:30:11.929
May  9 12:30:11.929: INFO: Pod "downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3" satisfied condition "Succeeded or Failed"
May  9 12:30:11.931: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3 container client-container: <nil>
STEP: delete the pod 05/09/23 12:30:11.936
May  9 12:30:11.945: INFO: Waiting for pod downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3 to disappear
May  9 12:30:11.947: INFO: Pod downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  9 12:30:11.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1809" for this suite. 05/09/23 12:30:11.95
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":23,"skipped":451,"failed":0}
------------------------------
• [4.058 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:07.897
    May  9 12:30:07.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 12:30:07.898
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:07.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:07.913
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 05/09/23 12:30:07.915
    May  9 12:30:07.920: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3" in namespace "downward-api-1809" to be "Succeeded or Failed"
    May  9 12:30:07.926: INFO: Pod "downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.361982ms
    May  9 12:30:09.929: INFO: Pod "downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008879746s
    May  9 12:30:11.929: INFO: Pod "downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008917316s
    STEP: Saw pod success 05/09/23 12:30:11.929
    May  9 12:30:11.929: INFO: Pod "downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3" satisfied condition "Succeeded or Failed"
    May  9 12:30:11.931: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3 container client-container: <nil>
    STEP: delete the pod 05/09/23 12:30:11.936
    May  9 12:30:11.945: INFO: Waiting for pod downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3 to disappear
    May  9 12:30:11.947: INFO: Pod downwardapi-volume-dc7419e0-1113-479d-916d-9c54f1546af3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  9 12:30:11.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1809" for this suite. 05/09/23 12:30:11.95
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:11.955
May  9 12:30:11.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename replicaset 05/09/23 12:30:11.956
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:11.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:11.971
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
May  9 12:30:11.980: INFO: Pod name sample-pod: Found 0 pods out of 1
May  9 12:30:16.983: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/09/23 12:30:16.983
STEP: Scaling up "test-rs" replicaset  05/09/23 12:30:16.983
May  9 12:30:16.993: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 05/09/23 12:30:16.993
W0509 12:30:17.004825      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  9 12:30:17.005: INFO: observed ReplicaSet test-rs in namespace replicaset-5597 with ReadyReplicas 1, AvailableReplicas 1
May  9 12:30:17.014: INFO: observed ReplicaSet test-rs in namespace replicaset-5597 with ReadyReplicas 1, AvailableReplicas 1
May  9 12:30:17.027: INFO: observed ReplicaSet test-rs in namespace replicaset-5597 with ReadyReplicas 1, AvailableReplicas 1
May  9 12:30:17.032: INFO: observed ReplicaSet test-rs in namespace replicaset-5597 with ReadyReplicas 1, AvailableReplicas 1
May  9 12:30:18.102: INFO: observed ReplicaSet test-rs in namespace replicaset-5597 with ReadyReplicas 2, AvailableReplicas 2
May  9 12:30:18.194: INFO: observed Replicaset test-rs in namespace replicaset-5597 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  9 12:30:18.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5597" for this suite. 05/09/23 12:30:18.199
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":24,"skipped":454,"failed":0}
------------------------------
• [SLOW TEST] [6.249 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:11.955
    May  9 12:30:11.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename replicaset 05/09/23 12:30:11.956
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:11.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:11.971
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    May  9 12:30:11.980: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  9 12:30:16.983: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/09/23 12:30:16.983
    STEP: Scaling up "test-rs" replicaset  05/09/23 12:30:16.983
    May  9 12:30:16.993: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 05/09/23 12:30:16.993
    W0509 12:30:17.004825      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  9 12:30:17.005: INFO: observed ReplicaSet test-rs in namespace replicaset-5597 with ReadyReplicas 1, AvailableReplicas 1
    May  9 12:30:17.014: INFO: observed ReplicaSet test-rs in namespace replicaset-5597 with ReadyReplicas 1, AvailableReplicas 1
    May  9 12:30:17.027: INFO: observed ReplicaSet test-rs in namespace replicaset-5597 with ReadyReplicas 1, AvailableReplicas 1
    May  9 12:30:17.032: INFO: observed ReplicaSet test-rs in namespace replicaset-5597 with ReadyReplicas 1, AvailableReplicas 1
    May  9 12:30:18.102: INFO: observed ReplicaSet test-rs in namespace replicaset-5597 with ReadyReplicas 2, AvailableReplicas 2
    May  9 12:30:18.194: INFO: observed Replicaset test-rs in namespace replicaset-5597 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  9 12:30:18.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5597" for this suite. 05/09/23 12:30:18.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:18.204
May  9 12:30:18.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename server-version 05/09/23 12:30:18.205
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:18.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:18.22
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 05/09/23 12:30:18.221
STEP: Confirm major version 05/09/23 12:30:18.222
May  9 12:30:18.222: INFO: Major version: 1
STEP: Confirm minor version 05/09/23 12:30:18.222
May  9 12:30:18.222: INFO: cleanMinorVersion: 25
May  9 12:30:18.222: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
May  9 12:30:18.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-4181" for this suite. 05/09/23 12:30:18.225
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":25,"skipped":464,"failed":0}
------------------------------
• [0.025 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:18.204
    May  9 12:30:18.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename server-version 05/09/23 12:30:18.205
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:18.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:18.22
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 05/09/23 12:30:18.221
    STEP: Confirm major version 05/09/23 12:30:18.222
    May  9 12:30:18.222: INFO: Major version: 1
    STEP: Confirm minor version 05/09/23 12:30:18.222
    May  9 12:30:18.222: INFO: cleanMinorVersion: 25
    May  9 12:30:18.222: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    May  9 12:30:18.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-4181" for this suite. 05/09/23 12:30:18.225
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:18.23
May  9 12:30:18.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename discovery 05/09/23 12:30:18.231
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:18.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:18.244
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 05/09/23 12:30:18.246
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
May  9 12:30:18.466: INFO: Checking APIGroup: apiregistration.k8s.io
May  9 12:30:18.467: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May  9 12:30:18.467: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
May  9 12:30:18.467: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May  9 12:30:18.467: INFO: Checking APIGroup: apps
May  9 12:30:18.467: INFO: PreferredVersion.GroupVersion: apps/v1
May  9 12:30:18.467: INFO: Versions found [{apps/v1 v1}]
May  9 12:30:18.467: INFO: apps/v1 matches apps/v1
May  9 12:30:18.468: INFO: Checking APIGroup: events.k8s.io
May  9 12:30:18.468: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May  9 12:30:18.468: INFO: Versions found [{events.k8s.io/v1 v1}]
May  9 12:30:18.468: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May  9 12:30:18.468: INFO: Checking APIGroup: authentication.k8s.io
May  9 12:30:18.469: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May  9 12:30:18.469: INFO: Versions found [{authentication.k8s.io/v1 v1}]
May  9 12:30:18.469: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May  9 12:30:18.469: INFO: Checking APIGroup: authorization.k8s.io
May  9 12:30:18.469: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May  9 12:30:18.469: INFO: Versions found [{authorization.k8s.io/v1 v1}]
May  9 12:30:18.469: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May  9 12:30:18.470: INFO: Checking APIGroup: autoscaling
May  9 12:30:18.470: INFO: PreferredVersion.GroupVersion: autoscaling/v2
May  9 12:30:18.470: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
May  9 12:30:18.470: INFO: autoscaling/v2 matches autoscaling/v2
May  9 12:30:18.470: INFO: Checking APIGroup: batch
May  9 12:30:18.471: INFO: PreferredVersion.GroupVersion: batch/v1
May  9 12:30:18.471: INFO: Versions found [{batch/v1 v1}]
May  9 12:30:18.471: INFO: batch/v1 matches batch/v1
May  9 12:30:18.471: INFO: Checking APIGroup: certificates.k8s.io
May  9 12:30:18.471: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May  9 12:30:18.471: INFO: Versions found [{certificates.k8s.io/v1 v1}]
May  9 12:30:18.471: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May  9 12:30:18.471: INFO: Checking APIGroup: networking.k8s.io
May  9 12:30:18.472: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May  9 12:30:18.472: INFO: Versions found [{networking.k8s.io/v1 v1}]
May  9 12:30:18.472: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May  9 12:30:18.472: INFO: Checking APIGroup: policy
May  9 12:30:18.473: INFO: PreferredVersion.GroupVersion: policy/v1
May  9 12:30:18.473: INFO: Versions found [{policy/v1 v1}]
May  9 12:30:18.473: INFO: policy/v1 matches policy/v1
May  9 12:30:18.473: INFO: Checking APIGroup: rbac.authorization.k8s.io
May  9 12:30:18.473: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May  9 12:30:18.473: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
May  9 12:30:18.473: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May  9 12:30:18.473: INFO: Checking APIGroup: storage.k8s.io
May  9 12:30:18.474: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May  9 12:30:18.474: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May  9 12:30:18.474: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May  9 12:30:18.474: INFO: Checking APIGroup: admissionregistration.k8s.io
May  9 12:30:18.474: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May  9 12:30:18.474: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
May  9 12:30:18.474: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May  9 12:30:18.474: INFO: Checking APIGroup: apiextensions.k8s.io
May  9 12:30:18.475: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May  9 12:30:18.475: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
May  9 12:30:18.475: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May  9 12:30:18.475: INFO: Checking APIGroup: scheduling.k8s.io
May  9 12:30:18.475: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May  9 12:30:18.475: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
May  9 12:30:18.475: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May  9 12:30:18.475: INFO: Checking APIGroup: coordination.k8s.io
May  9 12:30:18.476: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May  9 12:30:18.476: INFO: Versions found [{coordination.k8s.io/v1 v1}]
May  9 12:30:18.476: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May  9 12:30:18.476: INFO: Checking APIGroup: node.k8s.io
May  9 12:30:18.476: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May  9 12:30:18.476: INFO: Versions found [{node.k8s.io/v1 v1}]
May  9 12:30:18.476: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May  9 12:30:18.476: INFO: Checking APIGroup: discovery.k8s.io
May  9 12:30:18.477: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May  9 12:30:18.477: INFO: Versions found [{discovery.k8s.io/v1 v1}]
May  9 12:30:18.477: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May  9 12:30:18.477: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May  9 12:30:18.477: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
May  9 12:30:18.477: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May  9 12:30:18.477: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
May  9 12:30:18.477: INFO: Checking APIGroup: crd.projectcalico.org
May  9 12:30:18.478: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
May  9 12:30:18.478: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
May  9 12:30:18.478: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
May  9 12:30:18.478: INFO: Checking APIGroup: snapshot.storage.k8s.io
May  9 12:30:18.478: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
May  9 12:30:18.478: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
May  9 12:30:18.478: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
May  9 12:30:18.478: INFO: Checking APIGroup: metrics.k8s.io
May  9 12:30:18.479: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May  9 12:30:18.479: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May  9 12:30:18.479: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
May  9 12:30:18.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7123" for this suite. 05/09/23 12:30:18.482
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":26,"skipped":468,"failed":0}
------------------------------
• [0.256 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:18.23
    May  9 12:30:18.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename discovery 05/09/23 12:30:18.231
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:18.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:18.244
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 05/09/23 12:30:18.246
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    May  9 12:30:18.466: INFO: Checking APIGroup: apiregistration.k8s.io
    May  9 12:30:18.467: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    May  9 12:30:18.467: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    May  9 12:30:18.467: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    May  9 12:30:18.467: INFO: Checking APIGroup: apps
    May  9 12:30:18.467: INFO: PreferredVersion.GroupVersion: apps/v1
    May  9 12:30:18.467: INFO: Versions found [{apps/v1 v1}]
    May  9 12:30:18.467: INFO: apps/v1 matches apps/v1
    May  9 12:30:18.468: INFO: Checking APIGroup: events.k8s.io
    May  9 12:30:18.468: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    May  9 12:30:18.468: INFO: Versions found [{events.k8s.io/v1 v1}]
    May  9 12:30:18.468: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    May  9 12:30:18.468: INFO: Checking APIGroup: authentication.k8s.io
    May  9 12:30:18.469: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    May  9 12:30:18.469: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    May  9 12:30:18.469: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    May  9 12:30:18.469: INFO: Checking APIGroup: authorization.k8s.io
    May  9 12:30:18.469: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    May  9 12:30:18.469: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    May  9 12:30:18.469: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    May  9 12:30:18.470: INFO: Checking APIGroup: autoscaling
    May  9 12:30:18.470: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    May  9 12:30:18.470: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    May  9 12:30:18.470: INFO: autoscaling/v2 matches autoscaling/v2
    May  9 12:30:18.470: INFO: Checking APIGroup: batch
    May  9 12:30:18.471: INFO: PreferredVersion.GroupVersion: batch/v1
    May  9 12:30:18.471: INFO: Versions found [{batch/v1 v1}]
    May  9 12:30:18.471: INFO: batch/v1 matches batch/v1
    May  9 12:30:18.471: INFO: Checking APIGroup: certificates.k8s.io
    May  9 12:30:18.471: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    May  9 12:30:18.471: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    May  9 12:30:18.471: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    May  9 12:30:18.471: INFO: Checking APIGroup: networking.k8s.io
    May  9 12:30:18.472: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    May  9 12:30:18.472: INFO: Versions found [{networking.k8s.io/v1 v1}]
    May  9 12:30:18.472: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    May  9 12:30:18.472: INFO: Checking APIGroup: policy
    May  9 12:30:18.473: INFO: PreferredVersion.GroupVersion: policy/v1
    May  9 12:30:18.473: INFO: Versions found [{policy/v1 v1}]
    May  9 12:30:18.473: INFO: policy/v1 matches policy/v1
    May  9 12:30:18.473: INFO: Checking APIGroup: rbac.authorization.k8s.io
    May  9 12:30:18.473: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    May  9 12:30:18.473: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    May  9 12:30:18.473: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    May  9 12:30:18.473: INFO: Checking APIGroup: storage.k8s.io
    May  9 12:30:18.474: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    May  9 12:30:18.474: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    May  9 12:30:18.474: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    May  9 12:30:18.474: INFO: Checking APIGroup: admissionregistration.k8s.io
    May  9 12:30:18.474: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    May  9 12:30:18.474: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    May  9 12:30:18.474: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    May  9 12:30:18.474: INFO: Checking APIGroup: apiextensions.k8s.io
    May  9 12:30:18.475: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    May  9 12:30:18.475: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    May  9 12:30:18.475: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    May  9 12:30:18.475: INFO: Checking APIGroup: scheduling.k8s.io
    May  9 12:30:18.475: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    May  9 12:30:18.475: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    May  9 12:30:18.475: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    May  9 12:30:18.475: INFO: Checking APIGroup: coordination.k8s.io
    May  9 12:30:18.476: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    May  9 12:30:18.476: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    May  9 12:30:18.476: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    May  9 12:30:18.476: INFO: Checking APIGroup: node.k8s.io
    May  9 12:30:18.476: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    May  9 12:30:18.476: INFO: Versions found [{node.k8s.io/v1 v1}]
    May  9 12:30:18.476: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    May  9 12:30:18.476: INFO: Checking APIGroup: discovery.k8s.io
    May  9 12:30:18.477: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    May  9 12:30:18.477: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    May  9 12:30:18.477: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    May  9 12:30:18.477: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    May  9 12:30:18.477: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    May  9 12:30:18.477: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    May  9 12:30:18.477: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    May  9 12:30:18.477: INFO: Checking APIGroup: crd.projectcalico.org
    May  9 12:30:18.478: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    May  9 12:30:18.478: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    May  9 12:30:18.478: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    May  9 12:30:18.478: INFO: Checking APIGroup: snapshot.storage.k8s.io
    May  9 12:30:18.478: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    May  9 12:30:18.478: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    May  9 12:30:18.478: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    May  9 12:30:18.478: INFO: Checking APIGroup: metrics.k8s.io
    May  9 12:30:18.479: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    May  9 12:30:18.479: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    May  9 12:30:18.479: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    May  9 12:30:18.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-7123" for this suite. 05/09/23 12:30:18.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:18.487
May  9 12:30:18.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 12:30:18.487
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:18.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:18.507
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 05/09/23 12:30:18.509
May  9 12:30:18.516: INFO: Waiting up to 5m0s for pod "pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a" in namespace "emptydir-3813" to be "Succeeded or Failed"
May  9 12:30:18.517: INFO: Pod "pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.683402ms
May  9 12:30:20.521: INFO: Pod "pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005304261s
May  9 12:30:22.521: INFO: Pod "pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005089398s
STEP: Saw pod success 05/09/23 12:30:22.521
May  9 12:30:22.521: INFO: Pod "pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a" satisfied condition "Succeeded or Failed"
May  9 12:30:22.523: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a container test-container: <nil>
STEP: delete the pod 05/09/23 12:30:22.53
May  9 12:30:22.540: INFO: Waiting for pod pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a to disappear
May  9 12:30:22.542: INFO: Pod pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 12:30:22.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3813" for this suite. 05/09/23 12:30:22.545
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":27,"skipped":484,"failed":0}
------------------------------
• [4.065 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:18.487
    May  9 12:30:18.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 12:30:18.487
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:18.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:18.507
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/09/23 12:30:18.509
    May  9 12:30:18.516: INFO: Waiting up to 5m0s for pod "pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a" in namespace "emptydir-3813" to be "Succeeded or Failed"
    May  9 12:30:18.517: INFO: Pod "pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.683402ms
    May  9 12:30:20.521: INFO: Pod "pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005304261s
    May  9 12:30:22.521: INFO: Pod "pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005089398s
    STEP: Saw pod success 05/09/23 12:30:22.521
    May  9 12:30:22.521: INFO: Pod "pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a" satisfied condition "Succeeded or Failed"
    May  9 12:30:22.523: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a container test-container: <nil>
    STEP: delete the pod 05/09/23 12:30:22.53
    May  9 12:30:22.540: INFO: Waiting for pod pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a to disappear
    May  9 12:30:22.542: INFO: Pod pod-ffda9fe7-8aa4-4f17-8a94-a27dd25cc22a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 12:30:22.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3813" for this suite. 05/09/23 12:30:22.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:22.552
May  9 12:30:22.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename replicaset 05/09/23 12:30:22.553
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:22.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:22.576
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/09/23 12:30:22.577
May  9 12:30:22.585: INFO: Pod name sample-pod: Found 0 pods out of 1
May  9 12:30:27.590: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/09/23 12:30:27.59
STEP: getting scale subresource 05/09/23 12:30:27.59
STEP: updating a scale subresource 05/09/23 12:30:27.595
STEP: verifying the replicaset Spec.Replicas was modified 05/09/23 12:30:27.6
STEP: Patch a scale subresource 05/09/23 12:30:27.606
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  9 12:30:27.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7788" for this suite. 05/09/23 12:30:27.62
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":28,"skipped":492,"failed":0}
------------------------------
• [SLOW TEST] [5.076 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:22.552
    May  9 12:30:22.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename replicaset 05/09/23 12:30:22.553
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:22.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:22.576
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/09/23 12:30:22.577
    May  9 12:30:22.585: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  9 12:30:27.590: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/09/23 12:30:27.59
    STEP: getting scale subresource 05/09/23 12:30:27.59
    STEP: updating a scale subresource 05/09/23 12:30:27.595
    STEP: verifying the replicaset Spec.Replicas was modified 05/09/23 12:30:27.6
    STEP: Patch a scale subresource 05/09/23 12:30:27.606
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  9 12:30:27.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7788" for this suite. 05/09/23 12:30:27.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:27.631
May  9 12:30:27.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename containers 05/09/23 12:30:27.631
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:27.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:27.648
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
May  9 12:30:27.655: INFO: Waiting up to 5m0s for pod "client-containers-89f79873-66a1-43c5-b459-abdf4a25e854" in namespace "containers-7648" to be "running"
May  9 12:30:27.660: INFO: Pod "client-containers-89f79873-66a1-43c5-b459-abdf4a25e854": Phase="Pending", Reason="", readiness=false. Elapsed: 4.980956ms
May  9 12:30:29.663: INFO: Pod "client-containers-89f79873-66a1-43c5-b459-abdf4a25e854": Phase="Running", Reason="", readiness=true. Elapsed: 2.007880791s
May  9 12:30:29.663: INFO: Pod "client-containers-89f79873-66a1-43c5-b459-abdf4a25e854" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May  9 12:30:29.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7648" for this suite. 05/09/23 12:30:29.671
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":29,"skipped":607,"failed":0}
------------------------------
• [2.047 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:27.631
    May  9 12:30:27.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename containers 05/09/23 12:30:27.631
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:27.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:27.648
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    May  9 12:30:27.655: INFO: Waiting up to 5m0s for pod "client-containers-89f79873-66a1-43c5-b459-abdf4a25e854" in namespace "containers-7648" to be "running"
    May  9 12:30:27.660: INFO: Pod "client-containers-89f79873-66a1-43c5-b459-abdf4a25e854": Phase="Pending", Reason="", readiness=false. Elapsed: 4.980956ms
    May  9 12:30:29.663: INFO: Pod "client-containers-89f79873-66a1-43c5-b459-abdf4a25e854": Phase="Running", Reason="", readiness=true. Elapsed: 2.007880791s
    May  9 12:30:29.663: INFO: Pod "client-containers-89f79873-66a1-43c5-b459-abdf4a25e854" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May  9 12:30:29.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7648" for this suite. 05/09/23 12:30:29.671
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:29.678
May  9 12:30:29.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 12:30:29.678
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:29.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:29.691
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 05/09/23 12:30:29.692
May  9 12:30:29.700: INFO: Waiting up to 5m0s for pod "annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22" in namespace "projected-2378" to be "running and ready"
May  9 12:30:29.704: INFO: Pod "annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.537674ms
May  9 12:30:29.704: INFO: The phase of Pod annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22 is Pending, waiting for it to be Running (with Ready = true)
May  9 12:30:31.708: INFO: Pod "annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22": Phase="Running", Reason="", readiness=true. Elapsed: 2.008353282s
May  9 12:30:31.708: INFO: The phase of Pod annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22 is Running (Ready = true)
May  9 12:30:31.708: INFO: Pod "annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22" satisfied condition "running and ready"
May  9 12:30:32.227: INFO: Successfully updated pod "annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  9 12:30:36.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2378" for this suite. 05/09/23 12:30:36.252
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":30,"skipped":607,"failed":0}
------------------------------
• [SLOW TEST] [6.584 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:29.678
    May  9 12:30:29.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 12:30:29.678
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:29.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:29.691
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 05/09/23 12:30:29.692
    May  9 12:30:29.700: INFO: Waiting up to 5m0s for pod "annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22" in namespace "projected-2378" to be "running and ready"
    May  9 12:30:29.704: INFO: Pod "annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.537674ms
    May  9 12:30:29.704: INFO: The phase of Pod annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22 is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:30:31.708: INFO: Pod "annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22": Phase="Running", Reason="", readiness=true. Elapsed: 2.008353282s
    May  9 12:30:31.708: INFO: The phase of Pod annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22 is Running (Ready = true)
    May  9 12:30:31.708: INFO: Pod "annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22" satisfied condition "running and ready"
    May  9 12:30:32.227: INFO: Successfully updated pod "annotationupdateb350eeb8-c707-4a8b-94f4-2459626d4d22"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  9 12:30:36.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2378" for this suite. 05/09/23 12:30:36.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:36.262
May  9 12:30:36.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 12:30:36.262
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:36.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:36.279
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/09/23 12:30:36.283
May  9 12:30:36.290: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9668" to be "running and ready"
May  9 12:30:36.294: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.967709ms
May  9 12:30:36.294: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  9 12:30:38.297: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006853864s
May  9 12:30:38.297: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  9 12:30:38.297: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 05/09/23 12:30:38.299
May  9 12:30:38.304: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9668" to be "running and ready"
May  9 12:30:38.311: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.782745ms
May  9 12:30:38.311: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  9 12:30:40.314: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009998335s
May  9 12:30:40.314: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
May  9 12:30:40.314: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/09/23 12:30:40.319
May  9 12:30:40.326: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  9 12:30:40.328: INFO: Pod pod-with-prestop-http-hook still exists
May  9 12:30:42.328: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  9 12:30:42.331: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 05/09/23 12:30:42.331
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May  9 12:30:42.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9668" for this suite. 05/09/23 12:30:42.346
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":31,"skipped":624,"failed":0}
------------------------------
• [SLOW TEST] [6.090 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:36.262
    May  9 12:30:36.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 12:30:36.262
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:36.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:36.279
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/09/23 12:30:36.283
    May  9 12:30:36.290: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9668" to be "running and ready"
    May  9 12:30:36.294: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.967709ms
    May  9 12:30:36.294: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:30:38.297: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006853864s
    May  9 12:30:38.297: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  9 12:30:38.297: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 05/09/23 12:30:38.299
    May  9 12:30:38.304: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9668" to be "running and ready"
    May  9 12:30:38.311: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.782745ms
    May  9 12:30:38.311: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:30:40.314: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009998335s
    May  9 12:30:40.314: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    May  9 12:30:40.314: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/09/23 12:30:40.319
    May  9 12:30:40.326: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  9 12:30:40.328: INFO: Pod pod-with-prestop-http-hook still exists
    May  9 12:30:42.328: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  9 12:30:42.331: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 05/09/23 12:30:42.331
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May  9 12:30:42.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9668" for this suite. 05/09/23 12:30:42.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:42.352
May  9 12:30:42.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename deployment 05/09/23 12:30:42.353
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:42.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:42.37
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 05/09/23 12:30:42.373
STEP: waiting for Deployment to be created 05/09/23 12:30:42.378
STEP: waiting for all Replicas to be Ready 05/09/23 12:30:42.379
May  9 12:30:42.379: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 12:30:42.379: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 12:30:42.388: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 12:30:42.388: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 12:30:42.402: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 12:30:42.402: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 12:30:42.424: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 12:30:42.424: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 12:30:43.154: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May  9 12:30:43.154: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May  9 12:30:43.962: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 05/09/23 12:30:43.962
W0509 12:30:43.967787      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  9 12:30:43.968: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 05/09/23 12:30:43.968
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:43.980: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:43.980: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:43.997: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:43.997: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:44.010: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
May  9 12:30:44.010: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
May  9 12:30:44.021: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
May  9 12:30:44.021: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
May  9 12:30:44.973: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:44.973: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:44.992: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
STEP: listing Deployments 05/09/23 12:30:44.992
May  9 12:30:44.994: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 05/09/23 12:30:44.994
May  9 12:30:45.006: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 05/09/23 12:30:45.006
May  9 12:30:45.011: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  9 12:30:45.022: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  9 12:30:45.036: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  9 12:30:45.047: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  9 12:30:45.058: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  9 12:30:45.981: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  9 12:30:46.006: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  9 12:30:46.013: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  9 12:30:47.167: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 05/09/23 12:30:47.182
STEP: fetching the DeploymentStatus 05/09/23 12:30:47.187
May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 3
STEP: deleting the Deployment 05/09/23 12:30:47.191
May  9 12:30:47.196: INFO: observed event type MODIFIED
May  9 12:30:47.196: INFO: observed event type MODIFIED
May  9 12:30:47.197: INFO: observed event type MODIFIED
May  9 12:30:47.197: INFO: observed event type MODIFIED
May  9 12:30:47.197: INFO: observed event type MODIFIED
May  9 12:30:47.197: INFO: observed event type MODIFIED
May  9 12:30:47.197: INFO: observed event type MODIFIED
May  9 12:30:47.197: INFO: observed event type MODIFIED
May  9 12:30:47.197: INFO: observed event type MODIFIED
May  9 12:30:47.197: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 12:30:47.201: INFO: Log out all the ReplicaSets if there is no deployment created
May  9 12:30:47.203: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-1535  4544bf8b-3642-4f0a-967d-fdb49b493c7c 9822 4 2023-05-09 12:30:43 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment cc9bf103-6bb1-4594-912e-6f566a95be90 0xc001ff47d7 0xc001ff47d8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 12:30:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc9bf103-6bb1-4594-912e-6f566a95be90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 12:30:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ff4860 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May  9 12:30:47.205: INFO: pod: "test-deployment-54cc775c4b-q74rp":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-q74rp test-deployment-54cc775c4b- deployment-1535  c906134c-b194-41ec-9a18-d013cb169976 9818 0 2023-05-09 12:30:43 +0000 UTC 2023-05-09 12:30:48 +0000 UTC 0xc003e16e90 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:419a57d5f44786f3949a4f9b231a86e54b89b8ba915171516daf196fa95d4388 cni.projectcalico.org/podIP:172.25.124.216/32 cni.projectcalico.org/podIPs:172.25.124.216/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 4544bf8b-3642-4f0a-967d-fdb49b493c7c 0xc003e16ec7 0xc003e16ec8}] [] [{kube-controller-manager Update v1 2023-05-09 12:30:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4544bf8b-3642-4f0a-967d-fdb49b493c7c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 12:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 12:30:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxlcr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxlcr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.216,StartTime:2023-05-09 12:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 12:30:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://3b7a5cf9351c837119a33f69f9b4210ccb0a7a7c9b7626e492f84261f674edc0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  9 12:30:47.205: INFO: pod: "test-deployment-54cc775c4b-wjb7r":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-wjb7r test-deployment-54cc775c4b- deployment-1535  6e21837f-d8f6-4f08-afff-3e2b1727e4d4 9815 0 2023-05-09 12:30:44 +0000 UTC 2023-05-09 12:30:46 +0000 UTC 0xc003e170a0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:3e969558170c0d1dca6bdd21f32513f4676a8cb732e4c4735971434e646772ce cni.projectcalico.org/podIP:172.25.53.145/32 cni.projectcalico.org/podIPs:172.25.53.145/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 4544bf8b-3642-4f0a-967d-fdb49b493c7c 0xc003e170f7 0xc003e170f8}] [] [{kube-controller-manager Update v1 2023-05-09 12:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4544bf8b-3642-4f0a-967d-fdb49b493c7c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 12:30:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 12:30:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2bqc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2bqc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.145,StartTime:2023-05-09 12:30:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 12:30:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://89a16dad9d66cd576913b8175652b1874f123b7046592bd9ecddd6a9237fccc8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  9 12:30:47.205: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-1535  08704419-1584-43bc-8cb3-7f0f9ad10879 9813 2 2023-05-09 12:30:44 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment cc9bf103-6bb1-4594-912e-6f566a95be90 0xc001ff48c7 0xc001ff48c8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 12:30:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc9bf103-6bb1-4594-912e-6f566a95be90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 12:30:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ff4950 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

May  9 12:30:47.211: INFO: pod: "test-deployment-7c7d8d58c8-lzkp9":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-lzkp9 test-deployment-7c7d8d58c8- deployment-1535  cca27ea2-912d-4079-81e4-9f977bf7fd71 9812 0 2023-05-09 12:30:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:d04e36ffd8bf88797a226194649957a744454e0710d318e1d81e6bb286de5152 cni.projectcalico.org/podIP:172.25.53.146/32 cni.projectcalico.org/podIPs:172.25.53.146/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 08704419-1584-43bc-8cb3-7f0f9ad10879 0xc00339a307 0xc00339a308}] [] [{kube-controller-manager Update v1 2023-05-09 12:30:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08704419-1584-43bc-8cb3-7f0f9ad10879\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 12:30:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 12:30:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vxgln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vxgln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.146,StartTime:2023-05-09 12:30:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 12:30:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://176b6e6bf38d83ec86becec8ae7c671bf72c57e807db16d74d7d65ef259a889b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  9 12:30:47.211: INFO: pod: "test-deployment-7c7d8d58c8-x5k6w":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-x5k6w test-deployment-7c7d8d58c8- deployment-1535  7ccd8e9f-3168-4be3-b13a-6b04373ac57f 9766 0 2023-05-09 12:30:44 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:a4e5379c2fc4bc2fad760cc31f7ac79aa9db1707661a16288d78cf9539c524ef cni.projectcalico.org/podIP:172.25.124.217/32 cni.projectcalico.org/podIPs:172.25.124.217/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 08704419-1584-43bc-8cb3-7f0f9ad10879 0xc00339a517 0xc00339a518}] [] [{kube-controller-manager Update v1 2023-05-09 12:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08704419-1584-43bc-8cb3-7f0f9ad10879\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 12:30:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 12:30:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8k4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8k4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.217,StartTime:2023-05-09 12:30:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 12:30:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7510bf8130c0a064a5a1ba93d5c0783487bf7cb60959e01b81b594bfaf632e9f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  9 12:30:47.211: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-1535  aaaecc7e-67db-45f6-b4ed-89fa34ce7374 9722 3 2023-05-09 12:30:42 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment cc9bf103-6bb1-4594-912e-6f566a95be90 0xc001ff49b7 0xc001ff49b8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 12:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc9bf103-6bb1-4594-912e-6f566a95be90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 12:30:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ff4a40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  9 12:30:47.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1535" for this suite. 05/09/23 12:30:47.219
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":32,"skipped":630,"failed":0}
------------------------------
• [4.871 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:42.352
    May  9 12:30:42.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename deployment 05/09/23 12:30:42.353
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:42.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:42.37
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 05/09/23 12:30:42.373
    STEP: waiting for Deployment to be created 05/09/23 12:30:42.378
    STEP: waiting for all Replicas to be Ready 05/09/23 12:30:42.379
    May  9 12:30:42.379: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 12:30:42.379: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 12:30:42.388: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 12:30:42.388: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 12:30:42.402: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 12:30:42.402: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 12:30:42.424: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 12:30:42.424: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 12:30:43.154: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May  9 12:30:43.154: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May  9 12:30:43.962: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 05/09/23 12:30:43.962
    W0509 12:30:43.967787      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  9 12:30:43.968: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 05/09/23 12:30:43.968
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 0
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:43.969: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:43.980: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:43.980: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:43.997: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:43.997: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:44.010: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    May  9 12:30:44.010: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    May  9 12:30:44.021: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    May  9 12:30:44.021: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    May  9 12:30:44.973: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:44.973: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:44.992: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    STEP: listing Deployments 05/09/23 12:30:44.992
    May  9 12:30:44.994: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 05/09/23 12:30:44.994
    May  9 12:30:45.006: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 05/09/23 12:30:45.006
    May  9 12:30:45.011: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 12:30:45.022: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 12:30:45.036: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 12:30:45.047: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 12:30:45.058: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 12:30:45.981: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 12:30:46.006: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 12:30:46.013: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 12:30:47.167: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 05/09/23 12:30:47.182
    STEP: fetching the DeploymentStatus 05/09/23 12:30:47.187
    May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 1
    May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 2
    May  9 12:30:47.190: INFO: observed Deployment test-deployment in namespace deployment-1535 with ReadyReplicas 3
    STEP: deleting the Deployment 05/09/23 12:30:47.191
    May  9 12:30:47.196: INFO: observed event type MODIFIED
    May  9 12:30:47.196: INFO: observed event type MODIFIED
    May  9 12:30:47.197: INFO: observed event type MODIFIED
    May  9 12:30:47.197: INFO: observed event type MODIFIED
    May  9 12:30:47.197: INFO: observed event type MODIFIED
    May  9 12:30:47.197: INFO: observed event type MODIFIED
    May  9 12:30:47.197: INFO: observed event type MODIFIED
    May  9 12:30:47.197: INFO: observed event type MODIFIED
    May  9 12:30:47.197: INFO: observed event type MODIFIED
    May  9 12:30:47.197: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 12:30:47.201: INFO: Log out all the ReplicaSets if there is no deployment created
    May  9 12:30:47.203: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-1535  4544bf8b-3642-4f0a-967d-fdb49b493c7c 9822 4 2023-05-09 12:30:43 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment cc9bf103-6bb1-4594-912e-6f566a95be90 0xc001ff47d7 0xc001ff47d8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 12:30:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc9bf103-6bb1-4594-912e-6f566a95be90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 12:30:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ff4860 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    May  9 12:30:47.205: INFO: pod: "test-deployment-54cc775c4b-q74rp":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-q74rp test-deployment-54cc775c4b- deployment-1535  c906134c-b194-41ec-9a18-d013cb169976 9818 0 2023-05-09 12:30:43 +0000 UTC 2023-05-09 12:30:48 +0000 UTC 0xc003e16e90 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:419a57d5f44786f3949a4f9b231a86e54b89b8ba915171516daf196fa95d4388 cni.projectcalico.org/podIP:172.25.124.216/32 cni.projectcalico.org/podIPs:172.25.124.216/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 4544bf8b-3642-4f0a-967d-fdb49b493c7c 0xc003e16ec7 0xc003e16ec8}] [] [{kube-controller-manager Update v1 2023-05-09 12:30:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4544bf8b-3642-4f0a-967d-fdb49b493c7c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 12:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 12:30:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxlcr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxlcr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.216,StartTime:2023-05-09 12:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 12:30:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://3b7a5cf9351c837119a33f69f9b4210ccb0a7a7c9b7626e492f84261f674edc0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  9 12:30:47.205: INFO: pod: "test-deployment-54cc775c4b-wjb7r":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-wjb7r test-deployment-54cc775c4b- deployment-1535  6e21837f-d8f6-4f08-afff-3e2b1727e4d4 9815 0 2023-05-09 12:30:44 +0000 UTC 2023-05-09 12:30:46 +0000 UTC 0xc003e170a0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:3e969558170c0d1dca6bdd21f32513f4676a8cb732e4c4735971434e646772ce cni.projectcalico.org/podIP:172.25.53.145/32 cni.projectcalico.org/podIPs:172.25.53.145/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 4544bf8b-3642-4f0a-967d-fdb49b493c7c 0xc003e170f7 0xc003e170f8}] [] [{kube-controller-manager Update v1 2023-05-09 12:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4544bf8b-3642-4f0a-967d-fdb49b493c7c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 12:30:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 12:30:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2bqc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2bqc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.145,StartTime:2023-05-09 12:30:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 12:30:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://89a16dad9d66cd576913b8175652b1874f123b7046592bd9ecddd6a9237fccc8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  9 12:30:47.205: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-1535  08704419-1584-43bc-8cb3-7f0f9ad10879 9813 2 2023-05-09 12:30:44 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment cc9bf103-6bb1-4594-912e-6f566a95be90 0xc001ff48c7 0xc001ff48c8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 12:30:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc9bf103-6bb1-4594-912e-6f566a95be90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 12:30:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ff4950 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    May  9 12:30:47.211: INFO: pod: "test-deployment-7c7d8d58c8-lzkp9":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-lzkp9 test-deployment-7c7d8d58c8- deployment-1535  cca27ea2-912d-4079-81e4-9f977bf7fd71 9812 0 2023-05-09 12:30:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:d04e36ffd8bf88797a226194649957a744454e0710d318e1d81e6bb286de5152 cni.projectcalico.org/podIP:172.25.53.146/32 cni.projectcalico.org/podIPs:172.25.53.146/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 08704419-1584-43bc-8cb3-7f0f9ad10879 0xc00339a307 0xc00339a308}] [] [{kube-controller-manager Update v1 2023-05-09 12:30:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08704419-1584-43bc-8cb3-7f0f9ad10879\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 12:30:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 12:30:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vxgln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vxgln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.146,StartTime:2023-05-09 12:30:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 12:30:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://176b6e6bf38d83ec86becec8ae7c671bf72c57e807db16d74d7d65ef259a889b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  9 12:30:47.211: INFO: pod: "test-deployment-7c7d8d58c8-x5k6w":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-x5k6w test-deployment-7c7d8d58c8- deployment-1535  7ccd8e9f-3168-4be3-b13a-6b04373ac57f 9766 0 2023-05-09 12:30:44 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:a4e5379c2fc4bc2fad760cc31f7ac79aa9db1707661a16288d78cf9539c524ef cni.projectcalico.org/podIP:172.25.124.217/32 cni.projectcalico.org/podIPs:172.25.124.217/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 08704419-1584-43bc-8cb3-7f0f9ad10879 0xc00339a517 0xc00339a518}] [] [{kube-controller-manager Update v1 2023-05-09 12:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08704419-1584-43bc-8cb3-7f0f9ad10879\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 12:30:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 12:30:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8k4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8k4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 12:30:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.217,StartTime:2023-05-09 12:30:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 12:30:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7510bf8130c0a064a5a1ba93d5c0783487bf7cb60959e01b81b594bfaf632e9f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  9 12:30:47.211: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-1535  aaaecc7e-67db-45f6-b4ed-89fa34ce7374 9722 3 2023-05-09 12:30:42 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment cc9bf103-6bb1-4594-912e-6f566a95be90 0xc001ff49b7 0xc001ff49b8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 12:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc9bf103-6bb1-4594-912e-6f566a95be90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 12:30:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ff4a40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  9 12:30:47.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1535" for this suite. 05/09/23 12:30:47.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:47.226
May  9 12:30:47.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 12:30:47.227
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:47.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:47.239
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-b7cbeb9b-4dcb-4db1-a5dd-8dfd5d3897ee 05/09/23 12:30:47.242
STEP: Creating the pod 05/09/23 12:30:47.246
May  9 12:30:47.251: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7b64add-48fc-4bf1-9e77-cd120582a905" in namespace "configmap-2531" to be "running"
May  9 12:30:47.253: INFO: Pod "pod-configmaps-b7b64add-48fc-4bf1-9e77-cd120582a905": Phase="Pending", Reason="", readiness=false. Elapsed: 1.584024ms
May  9 12:30:49.257: INFO: Pod "pod-configmaps-b7b64add-48fc-4bf1-9e77-cd120582a905": Phase="Running", Reason="", readiness=true. Elapsed: 2.005740465s
May  9 12:30:49.257: INFO: Pod "pod-configmaps-b7b64add-48fc-4bf1-9e77-cd120582a905" satisfied condition "running"
STEP: Waiting for pod with text data 05/09/23 12:30:49.257
STEP: Waiting for pod with binary data 05/09/23 12:30:49.275
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  9 12:30:49.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2531" for this suite. 05/09/23 12:30:49.288
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":33,"skipped":640,"failed":0}
------------------------------
• [2.067 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:47.226
    May  9 12:30:47.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 12:30:47.227
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:47.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:47.239
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-b7cbeb9b-4dcb-4db1-a5dd-8dfd5d3897ee 05/09/23 12:30:47.242
    STEP: Creating the pod 05/09/23 12:30:47.246
    May  9 12:30:47.251: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7b64add-48fc-4bf1-9e77-cd120582a905" in namespace "configmap-2531" to be "running"
    May  9 12:30:47.253: INFO: Pod "pod-configmaps-b7b64add-48fc-4bf1-9e77-cd120582a905": Phase="Pending", Reason="", readiness=false. Elapsed: 1.584024ms
    May  9 12:30:49.257: INFO: Pod "pod-configmaps-b7b64add-48fc-4bf1-9e77-cd120582a905": Phase="Running", Reason="", readiness=true. Elapsed: 2.005740465s
    May  9 12:30:49.257: INFO: Pod "pod-configmaps-b7b64add-48fc-4bf1-9e77-cd120582a905" satisfied condition "running"
    STEP: Waiting for pod with text data 05/09/23 12:30:49.257
    STEP: Waiting for pod with binary data 05/09/23 12:30:49.275
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 12:30:49.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2531" for this suite. 05/09/23 12:30:49.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:49.296
May  9 12:30:49.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 12:30:49.296
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:49.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:49.314
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-3756 05/09/23 12:30:49.316
STEP: creating service affinity-nodeport-transition in namespace services-3756 05/09/23 12:30:49.316
STEP: creating replication controller affinity-nodeport-transition in namespace services-3756 05/09/23 12:30:49.33
I0509 12:30:49.338224      24 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3756, replica count: 3
I0509 12:30:52.389701      24 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 12:30:52.400: INFO: Creating new exec pod
May  9 12:30:52.407: INFO: Waiting up to 5m0s for pod "execpod-affinityfj2lm" in namespace "services-3756" to be "running"
May  9 12:30:52.411: INFO: Pod "execpod-affinityfj2lm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107736ms
May  9 12:30:54.414: INFO: Pod "execpod-affinityfj2lm": Phase="Running", Reason="", readiness=true. Elapsed: 2.007027727s
May  9 12:30:54.414: INFO: Pod "execpod-affinityfj2lm" satisfied condition "running"
May  9 12:30:55.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3756 exec execpod-affinityfj2lm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
May  9 12:30:56.522: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May  9 12:30:56.522: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 12:30:56.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3756 exec execpod-affinityfj2lm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.190.215 80'
May  9 12:30:56.626: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.190.215 80\nConnection to 10.98.190.215 80 port [tcp/http] succeeded!\n"
May  9 12:30:56.626: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 12:30:56.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3756 exec execpod-affinityfj2lm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.89 32050'
May  9 12:30:56.732: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.89 32050\nConnection to 192.168.1.89 32050 port [tcp/*] succeeded!\n"
May  9 12:30:56.732: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 12:30:56.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3756 exec execpod-affinityfj2lm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.73 32050'
May  9 12:30:56.833: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.73 32050\nConnection to 192.168.1.73 32050 port [tcp/*] succeeded!\n"
May  9 12:30:56.833: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 12:30:56.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3756 exec execpod-affinityfj2lm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.64:32050/ ; done'
May  9 12:30:56.992: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n"
May  9 12:30:56.992: INFO: stdout: "\naffinity-nodeport-transition-srxq9\naffinity-nodeport-transition-pswhd\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-srxq9\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-pswhd\naffinity-nodeport-transition-srxq9\naffinity-nodeport-transition-srxq9\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-pswhd\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-srxq9\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-srxq9"
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-srxq9
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-pswhd
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-srxq9
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-pswhd
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-srxq9
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-srxq9
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-pswhd
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-srxq9
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-srxq9
May  9 12:30:57.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3756 exec execpod-affinityfj2lm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.64:32050/ ; done'
May  9 12:30:57.164: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n"
May  9 12:30:57.164: INFO: stdout: "\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp"
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
May  9 12:30:57.164: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3756, will wait for the garbage collector to delete the pods 05/09/23 12:30:57.178
May  9 12:30:57.236: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.270578ms
May  9 12:30:57.336: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.389589ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 12:30:59.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3756" for this suite. 05/09/23 12:30:59.363
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":34,"skipped":726,"failed":0}
------------------------------
• [SLOW TEST] [10.072 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:49.296
    May  9 12:30:49.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 12:30:49.296
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:49.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:49.314
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-3756 05/09/23 12:30:49.316
    STEP: creating service affinity-nodeport-transition in namespace services-3756 05/09/23 12:30:49.316
    STEP: creating replication controller affinity-nodeport-transition in namespace services-3756 05/09/23 12:30:49.33
    I0509 12:30:49.338224      24 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3756, replica count: 3
    I0509 12:30:52.389701      24 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 12:30:52.400: INFO: Creating new exec pod
    May  9 12:30:52.407: INFO: Waiting up to 5m0s for pod "execpod-affinityfj2lm" in namespace "services-3756" to be "running"
    May  9 12:30:52.411: INFO: Pod "execpod-affinityfj2lm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107736ms
    May  9 12:30:54.414: INFO: Pod "execpod-affinityfj2lm": Phase="Running", Reason="", readiness=true. Elapsed: 2.007027727s
    May  9 12:30:54.414: INFO: Pod "execpod-affinityfj2lm" satisfied condition "running"
    May  9 12:30:55.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3756 exec execpod-affinityfj2lm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    May  9 12:30:56.522: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    May  9 12:30:56.522: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 12:30:56.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3756 exec execpod-affinityfj2lm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.190.215 80'
    May  9 12:30:56.626: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.190.215 80\nConnection to 10.98.190.215 80 port [tcp/http] succeeded!\n"
    May  9 12:30:56.626: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 12:30:56.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3756 exec execpod-affinityfj2lm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.89 32050'
    May  9 12:30:56.732: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.89 32050\nConnection to 192.168.1.89 32050 port [tcp/*] succeeded!\n"
    May  9 12:30:56.732: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 12:30:56.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3756 exec execpod-affinityfj2lm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.73 32050'
    May  9 12:30:56.833: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.73 32050\nConnection to 192.168.1.73 32050 port [tcp/*] succeeded!\n"
    May  9 12:30:56.833: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 12:30:56.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3756 exec execpod-affinityfj2lm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.64:32050/ ; done'
    May  9 12:30:56.992: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n"
    May  9 12:30:56.992: INFO: stdout: "\naffinity-nodeport-transition-srxq9\naffinity-nodeport-transition-pswhd\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-srxq9\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-pswhd\naffinity-nodeport-transition-srxq9\naffinity-nodeport-transition-srxq9\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-pswhd\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-srxq9\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-srxq9"
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-srxq9
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-pswhd
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-srxq9
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-pswhd
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-srxq9
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-srxq9
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-pswhd
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-srxq9
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:56.992: INFO: Received response from host: affinity-nodeport-transition-srxq9
    May  9 12:30:57.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3756 exec execpod-affinityfj2lm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.64:32050/ ; done'
    May  9 12:30:57.164: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32050/\n"
    May  9 12:30:57.164: INFO: stdout: "\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp\naffinity-nodeport-transition-ncqnp"
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Received response from host: affinity-nodeport-transition-ncqnp
    May  9 12:30:57.164: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3756, will wait for the garbage collector to delete the pods 05/09/23 12:30:57.178
    May  9 12:30:57.236: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.270578ms
    May  9 12:30:57.336: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.389589ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 12:30:59.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3756" for this suite. 05/09/23 12:30:59.363
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:30:59.369
May  9 12:30:59.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename crd-webhook 05/09/23 12:30:59.37
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:59.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:59.386
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/09/23 12:30:59.388
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/09/23 12:30:59.793
STEP: Deploying the custom resource conversion webhook pod 05/09/23 12:30:59.799
STEP: Wait for the deployment to be ready 05/09/23 12:30:59.808
May  9 12:30:59.814: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 12:31:01.821
STEP: Verifying the service has paired with the endpoint 05/09/23 12:31:01.834
May  9 12:31:02.834: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
May  9 12:31:02.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Creating a v1 custom resource 05/09/23 12:31:05.399
STEP: v2 custom resource should be converted 05/09/23 12:31:05.404
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 12:31:05.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8699" for this suite. 05/09/23 12:31:05.92
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":35,"skipped":754,"failed":0}
------------------------------
• [SLOW TEST] [6.591 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:30:59.369
    May  9 12:30:59.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename crd-webhook 05/09/23 12:30:59.37
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:30:59.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:30:59.386
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/09/23 12:30:59.388
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/09/23 12:30:59.793
    STEP: Deploying the custom resource conversion webhook pod 05/09/23 12:30:59.799
    STEP: Wait for the deployment to be ready 05/09/23 12:30:59.808
    May  9 12:30:59.814: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 12:31:01.821
    STEP: Verifying the service has paired with the endpoint 05/09/23 12:31:01.834
    May  9 12:31:02.834: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    May  9 12:31:02.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Creating a v1 custom resource 05/09/23 12:31:05.399
    STEP: v2 custom resource should be converted 05/09/23 12:31:05.404
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 12:31:05.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-8699" for this suite. 05/09/23 12:31:05.92
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:31:05.961
May  9 12:31:05.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename endpointslice 05/09/23 12:31:05.961
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:05.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:05.978
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 05/09/23 12:31:11.056
STEP: referencing matching pods with named port 05/09/23 12:31:16.067
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/09/23 12:31:21.076
STEP: recreating EndpointSlices after they've been deleted 05/09/23 12:31:26.086
May  9 12:31:26.101: INFO: EndpointSlice for Service endpointslice-2042/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May  9 12:31:36.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2042" for this suite. 05/09/23 12:31:36.119
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":36,"skipped":757,"failed":0}
------------------------------
• [SLOW TEST] [30.163 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:31:05.961
    May  9 12:31:05.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename endpointslice 05/09/23 12:31:05.961
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:05.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:05.978
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 05/09/23 12:31:11.056
    STEP: referencing matching pods with named port 05/09/23 12:31:16.067
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/09/23 12:31:21.076
    STEP: recreating EndpointSlices after they've been deleted 05/09/23 12:31:26.086
    May  9 12:31:26.101: INFO: EndpointSlice for Service endpointslice-2042/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May  9 12:31:36.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2042" for this suite. 05/09/23 12:31:36.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:31:36.124
May  9 12:31:36.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 12:31:36.124
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:36.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:36.144
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-e185ce62-295f-4d4c-a92b-844fb514a89e 05/09/23 12:31:36.145
STEP: Creating a pod to test consume secrets 05/09/23 12:31:36.149
May  9 12:31:36.155: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3" in namespace "projected-9977" to be "Succeeded or Failed"
May  9 12:31:36.157: INFO: Pod "pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.743457ms
May  9 12:31:38.161: INFO: Pod "pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0057644s
May  9 12:31:40.162: INFO: Pod "pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006270254s
STEP: Saw pod success 05/09/23 12:31:40.162
May  9 12:31:40.162: INFO: Pod "pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3" satisfied condition "Succeeded or Failed"
May  9 12:31:40.163: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/09/23 12:31:40.168
May  9 12:31:40.177: INFO: Waiting for pod pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3 to disappear
May  9 12:31:40.179: INFO: Pod pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  9 12:31:40.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9977" for this suite. 05/09/23 12:31:40.182
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":37,"skipped":763,"failed":0}
------------------------------
• [4.062 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:31:36.124
    May  9 12:31:36.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 12:31:36.124
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:36.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:36.144
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-e185ce62-295f-4d4c-a92b-844fb514a89e 05/09/23 12:31:36.145
    STEP: Creating a pod to test consume secrets 05/09/23 12:31:36.149
    May  9 12:31:36.155: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3" in namespace "projected-9977" to be "Succeeded or Failed"
    May  9 12:31:36.157: INFO: Pod "pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.743457ms
    May  9 12:31:38.161: INFO: Pod "pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0057644s
    May  9 12:31:40.162: INFO: Pod "pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006270254s
    STEP: Saw pod success 05/09/23 12:31:40.162
    May  9 12:31:40.162: INFO: Pod "pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3" satisfied condition "Succeeded or Failed"
    May  9 12:31:40.163: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 12:31:40.168
    May  9 12:31:40.177: INFO: Waiting for pod pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3 to disappear
    May  9 12:31:40.179: INFO: Pod pod-projected-secrets-f1dff68d-ff8b-4cba-8af9-9e1c2a5fb7f3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  9 12:31:40.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9977" for this suite. 05/09/23 12:31:40.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:31:40.186
May  9 12:31:40.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename svcaccounts 05/09/23 12:31:40.187
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:40.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:40.199
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  05/09/23 12:31:40.201
May  9 12:31:40.206: INFO: Waiting up to 5m0s for pod "test-pod-5ff6b119-6d12-44b1-8087-572f320c5364" in namespace "svcaccounts-70" to be "Succeeded or Failed"
May  9 12:31:40.207: INFO: Pod "test-pod-5ff6b119-6d12-44b1-8087-572f320c5364": Phase="Pending", Reason="", readiness=false. Elapsed: 1.832517ms
May  9 12:31:42.210: INFO: Pod "test-pod-5ff6b119-6d12-44b1-8087-572f320c5364": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004658746s
May  9 12:31:44.212: INFO: Pod "test-pod-5ff6b119-6d12-44b1-8087-572f320c5364": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006246129s
STEP: Saw pod success 05/09/23 12:31:44.212
May  9 12:31:44.212: INFO: Pod "test-pod-5ff6b119-6d12-44b1-8087-572f320c5364" satisfied condition "Succeeded or Failed"
May  9 12:31:44.214: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod test-pod-5ff6b119-6d12-44b1-8087-572f320c5364 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 12:31:44.218
May  9 12:31:44.227: INFO: Waiting for pod test-pod-5ff6b119-6d12-44b1-8087-572f320c5364 to disappear
May  9 12:31:44.228: INFO: Pod test-pod-5ff6b119-6d12-44b1-8087-572f320c5364 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  9 12:31:44.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-70" for this suite. 05/09/23 12:31:44.231
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":38,"skipped":772,"failed":0}
------------------------------
• [4.050 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:31:40.186
    May  9 12:31:40.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 12:31:40.187
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:40.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:40.199
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  05/09/23 12:31:40.201
    May  9 12:31:40.206: INFO: Waiting up to 5m0s for pod "test-pod-5ff6b119-6d12-44b1-8087-572f320c5364" in namespace "svcaccounts-70" to be "Succeeded or Failed"
    May  9 12:31:40.207: INFO: Pod "test-pod-5ff6b119-6d12-44b1-8087-572f320c5364": Phase="Pending", Reason="", readiness=false. Elapsed: 1.832517ms
    May  9 12:31:42.210: INFO: Pod "test-pod-5ff6b119-6d12-44b1-8087-572f320c5364": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004658746s
    May  9 12:31:44.212: INFO: Pod "test-pod-5ff6b119-6d12-44b1-8087-572f320c5364": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006246129s
    STEP: Saw pod success 05/09/23 12:31:44.212
    May  9 12:31:44.212: INFO: Pod "test-pod-5ff6b119-6d12-44b1-8087-572f320c5364" satisfied condition "Succeeded or Failed"
    May  9 12:31:44.214: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod test-pod-5ff6b119-6d12-44b1-8087-572f320c5364 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 12:31:44.218
    May  9 12:31:44.227: INFO: Waiting for pod test-pod-5ff6b119-6d12-44b1-8087-572f320c5364 to disappear
    May  9 12:31:44.228: INFO: Pod test-pod-5ff6b119-6d12-44b1-8087-572f320c5364 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  9 12:31:44.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-70" for this suite. 05/09/23 12:31:44.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:31:44.238
May  9 12:31:44.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename replicaset 05/09/23 12:31:44.238
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:44.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:44.252
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
May  9 12:31:44.254: INFO: Creating ReplicaSet my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383
May  9 12:31:44.262: INFO: Pod name my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383: Found 0 pods out of 1
May  9 12:31:49.268: INFO: Pod name my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383: Found 1 pods out of 1
May  9 12:31:49.268: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383" is running
May  9 12:31:49.268: INFO: Waiting up to 5m0s for pod "my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383-zjwl7" in namespace "replicaset-927" to be "running"
May  9 12:31:49.270: INFO: Pod "my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383-zjwl7": Phase="Running", Reason="", readiness=true. Elapsed: 2.221658ms
May  9 12:31:49.270: INFO: Pod "my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383-zjwl7" satisfied condition "running"
May  9 12:31:49.270: INFO: Pod "my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383-zjwl7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 12:31:44 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 12:31:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 12:31:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 12:31:44 +0000 UTC Reason: Message:}])
May  9 12:31:49.270: INFO: Trying to dial the pod
May  9 12:31:54.277: INFO: Controller my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383: Got expected result from replica 1 [my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383-zjwl7]: "my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383-zjwl7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  9 12:31:54.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-927" for this suite. 05/09/23 12:31:54.28
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":39,"skipped":811,"failed":0}
------------------------------
• [SLOW TEST] [10.048 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:31:44.238
    May  9 12:31:44.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename replicaset 05/09/23 12:31:44.238
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:44.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:44.252
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    May  9 12:31:44.254: INFO: Creating ReplicaSet my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383
    May  9 12:31:44.262: INFO: Pod name my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383: Found 0 pods out of 1
    May  9 12:31:49.268: INFO: Pod name my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383: Found 1 pods out of 1
    May  9 12:31:49.268: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383" is running
    May  9 12:31:49.268: INFO: Waiting up to 5m0s for pod "my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383-zjwl7" in namespace "replicaset-927" to be "running"
    May  9 12:31:49.270: INFO: Pod "my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383-zjwl7": Phase="Running", Reason="", readiness=true. Elapsed: 2.221658ms
    May  9 12:31:49.270: INFO: Pod "my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383-zjwl7" satisfied condition "running"
    May  9 12:31:49.270: INFO: Pod "my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383-zjwl7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 12:31:44 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 12:31:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 12:31:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 12:31:44 +0000 UTC Reason: Message:}])
    May  9 12:31:49.270: INFO: Trying to dial the pod
    May  9 12:31:54.277: INFO: Controller my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383: Got expected result from replica 1 [my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383-zjwl7]: "my-hostname-basic-e3c89b83-5a48-437c-aa19-d58984321383-zjwl7", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  9 12:31:54.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-927" for this suite. 05/09/23 12:31:54.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:31:54.286
May  9 12:31:54.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename gc 05/09/23 12:31:54.287
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:54.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:54.303
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 05/09/23 12:31:54.305
STEP: Wait for the Deployment to create new ReplicaSet 05/09/23 12:31:54.31
STEP: delete the deployment 05/09/23 12:31:54.816
STEP: wait for all rs to be garbage collected 05/09/23 12:31:54.82
STEP: expected 0 rs, got 1 rs 05/09/23 12:31:54.825
STEP: expected 0 pods, got 2 pods 05/09/23 12:31:54.832
STEP: Gathering metrics 05/09/23 12:31:55.338
May  9 12:31:55.360: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" in namespace "kube-system" to be "running and ready"
May  9 12:31:55.364: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv": Phase="Running", Reason="", readiness=true. Elapsed: 3.789601ms
May  9 12:31:55.364: INFO: The phase of Pod kube-controller-manager-cl-gks-cncf-control-plane-gv9sv is Running (Ready = true)
May  9 12:31:55.364: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" satisfied condition "running and ready"
May  9 12:31:55.403: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  9 12:31:55.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8249" for this suite. 05/09/23 12:31:55.406
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":40,"skipped":833,"failed":0}
------------------------------
• [1.128 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:31:54.286
    May  9 12:31:54.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename gc 05/09/23 12:31:54.287
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:54.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:54.303
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 05/09/23 12:31:54.305
    STEP: Wait for the Deployment to create new ReplicaSet 05/09/23 12:31:54.31
    STEP: delete the deployment 05/09/23 12:31:54.816
    STEP: wait for all rs to be garbage collected 05/09/23 12:31:54.82
    STEP: expected 0 rs, got 1 rs 05/09/23 12:31:54.825
    STEP: expected 0 pods, got 2 pods 05/09/23 12:31:54.832
    STEP: Gathering metrics 05/09/23 12:31:55.338
    May  9 12:31:55.360: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" in namespace "kube-system" to be "running and ready"
    May  9 12:31:55.364: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv": Phase="Running", Reason="", readiness=true. Elapsed: 3.789601ms
    May  9 12:31:55.364: INFO: The phase of Pod kube-controller-manager-cl-gks-cncf-control-plane-gv9sv is Running (Ready = true)
    May  9 12:31:55.364: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" satisfied condition "running and ready"
    May  9 12:31:55.403: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  9 12:31:55.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8249" for this suite. 05/09/23 12:31:55.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:31:55.414
May  9 12:31:55.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 12:31:55.415
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:55.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:55.435
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 05/09/23 12:31:55.436
STEP: listing secrets in all namespaces to ensure that there are more than zero 05/09/23 12:31:55.441
STEP: patching the secret 05/09/23 12:31:55.443
STEP: deleting the secret using a LabelSelector 05/09/23 12:31:55.449
STEP: listing secrets in all namespaces, searching for label name and value in patch 05/09/23 12:31:55.454
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May  9 12:31:55.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5368" for this suite. 05/09/23 12:31:55.459
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":41,"skipped":858,"failed":0}
------------------------------
• [0.049 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:31:55.414
    May  9 12:31:55.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 12:31:55.415
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:55.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:55.435
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 05/09/23 12:31:55.436
    STEP: listing secrets in all namespaces to ensure that there are more than zero 05/09/23 12:31:55.441
    STEP: patching the secret 05/09/23 12:31:55.443
    STEP: deleting the secret using a LabelSelector 05/09/23 12:31:55.449
    STEP: listing secrets in all namespaces, searching for label name and value in patch 05/09/23 12:31:55.454
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May  9 12:31:55.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5368" for this suite. 05/09/23 12:31:55.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:31:55.463
May  9 12:31:55.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename replicaset 05/09/23 12:31:55.464
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:55.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:55.477
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/09/23 12:31:55.479
May  9 12:31:55.484: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7716" to be "running and ready"
May  9 12:31:55.490: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.69734ms
May  9 12:31:55.490: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May  9 12:31:57.493: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009205709s
May  9 12:31:57.493: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
May  9 12:31:57.493: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 05/09/23 12:31:57.496
STEP: Then the orphan pod is adopted 05/09/23 12:31:57.501
STEP: When the matched label of one of its pods change 05/09/23 12:31:58.505
May  9 12:31:58.507: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 05/09/23 12:31:58.516
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  9 12:31:59.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7716" for this suite. 05/09/23 12:31:59.526
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":42,"skipped":877,"failed":0}
------------------------------
• [4.068 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:31:55.463
    May  9 12:31:55.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename replicaset 05/09/23 12:31:55.464
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:55.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:55.477
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/09/23 12:31:55.479
    May  9 12:31:55.484: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7716" to be "running and ready"
    May  9 12:31:55.490: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.69734ms
    May  9 12:31:55.490: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:31:57.493: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009205709s
    May  9 12:31:57.493: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    May  9 12:31:57.493: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 05/09/23 12:31:57.496
    STEP: Then the orphan pod is adopted 05/09/23 12:31:57.501
    STEP: When the matched label of one of its pods change 05/09/23 12:31:58.505
    May  9 12:31:58.507: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/09/23 12:31:58.516
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  9 12:31:59.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7716" for this suite. 05/09/23 12:31:59.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:31:59.532
May  9 12:31:59.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename resourcequota 05/09/23 12:31:59.532
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:59.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:59.548
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 05/09/23 12:31:59.549
STEP: Creating a ResourceQuota 05/09/23 12:32:04.556
STEP: Ensuring resource quota status is calculated 05/09/23 12:32:04.56
STEP: Creating a Service 05/09/23 12:32:06.563
STEP: Creating a NodePort Service 05/09/23 12:32:06.584
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/09/23 12:32:06.604
STEP: Ensuring resource quota status captures service creation 05/09/23 12:32:06.631
STEP: Deleting Services 05/09/23 12:32:08.634
STEP: Ensuring resource quota status released usage 05/09/23 12:32:08.675
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  9 12:32:10.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6130" for this suite. 05/09/23 12:32:10.682
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":43,"skipped":903,"failed":0}
------------------------------
• [SLOW TEST] [11.156 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:31:59.532
    May  9 12:31:59.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename resourcequota 05/09/23 12:31:59.532
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:31:59.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:31:59.548
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 05/09/23 12:31:59.549
    STEP: Creating a ResourceQuota 05/09/23 12:32:04.556
    STEP: Ensuring resource quota status is calculated 05/09/23 12:32:04.56
    STEP: Creating a Service 05/09/23 12:32:06.563
    STEP: Creating a NodePort Service 05/09/23 12:32:06.584
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/09/23 12:32:06.604
    STEP: Ensuring resource quota status captures service creation 05/09/23 12:32:06.631
    STEP: Deleting Services 05/09/23 12:32:08.634
    STEP: Ensuring resource quota status released usage 05/09/23 12:32:08.675
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  9 12:32:10.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6130" for this suite. 05/09/23 12:32:10.682
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:32:10.687
May  9 12:32:10.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 12:32:10.688
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:32:10.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:32:10.702
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/09/23 12:32:10.703
May  9 12:32:10.708: INFO: Waiting up to 5m0s for pod "pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475" in namespace "emptydir-8184" to be "Succeeded or Failed"
May  9 12:32:10.713: INFO: Pod "pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475": Phase="Pending", Reason="", readiness=false. Elapsed: 5.078774ms
May  9 12:32:12.716: INFO: Pod "pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007800275s
May  9 12:32:14.717: INFO: Pod "pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009175471s
STEP: Saw pod success 05/09/23 12:32:14.717
May  9 12:32:14.717: INFO: Pod "pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475" satisfied condition "Succeeded or Failed"
May  9 12:32:14.719: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475 container test-container: <nil>
STEP: delete the pod 05/09/23 12:32:14.723
May  9 12:32:14.734: INFO: Waiting for pod pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475 to disappear
May  9 12:32:14.736: INFO: Pod pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 12:32:14.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8184" for this suite. 05/09/23 12:32:14.739
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":44,"skipped":904,"failed":0}
------------------------------
• [4.056 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:32:10.687
    May  9 12:32:10.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 12:32:10.688
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:32:10.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:32:10.702
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/09/23 12:32:10.703
    May  9 12:32:10.708: INFO: Waiting up to 5m0s for pod "pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475" in namespace "emptydir-8184" to be "Succeeded or Failed"
    May  9 12:32:10.713: INFO: Pod "pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475": Phase="Pending", Reason="", readiness=false. Elapsed: 5.078774ms
    May  9 12:32:12.716: INFO: Pod "pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007800275s
    May  9 12:32:14.717: INFO: Pod "pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009175471s
    STEP: Saw pod success 05/09/23 12:32:14.717
    May  9 12:32:14.717: INFO: Pod "pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475" satisfied condition "Succeeded or Failed"
    May  9 12:32:14.719: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475 container test-container: <nil>
    STEP: delete the pod 05/09/23 12:32:14.723
    May  9 12:32:14.734: INFO: Waiting for pod pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475 to disappear
    May  9 12:32:14.736: INFO: Pod pod-b53a4ba3-d8ed-47b2-b1b8-39c8dc70d475 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 12:32:14.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8184" for this suite. 05/09/23 12:32:14.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:32:14.745
May  9 12:32:14.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-probe 05/09/23 12:32:14.751
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:32:14.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:32:14.766
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-4eab9b74-d244-442a-accd-784c348e174b in namespace container-probe-6889 05/09/23 12:32:14.767
May  9 12:32:14.773: INFO: Waiting up to 5m0s for pod "test-webserver-4eab9b74-d244-442a-accd-784c348e174b" in namespace "container-probe-6889" to be "not pending"
May  9 12:32:14.776: INFO: Pod "test-webserver-4eab9b74-d244-442a-accd-784c348e174b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.680882ms
May  9 12:32:16.780: INFO: Pod "test-webserver-4eab9b74-d244-442a-accd-784c348e174b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007093232s
May  9 12:32:16.780: INFO: Pod "test-webserver-4eab9b74-d244-442a-accd-784c348e174b" satisfied condition "not pending"
May  9 12:32:16.780: INFO: Started pod test-webserver-4eab9b74-d244-442a-accd-784c348e174b in namespace container-probe-6889
STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 12:32:16.78
May  9 12:32:16.782: INFO: Initial restart count of pod test-webserver-4eab9b74-d244-442a-accd-784c348e174b is 0
STEP: deleting the pod 05/09/23 12:36:17.233
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  9 12:36:17.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6889" for this suite. 05/09/23 12:36:17.246
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":45,"skipped":941,"failed":0}
------------------------------
• [SLOW TEST] [242.506 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:32:14.745
    May  9 12:32:14.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-probe 05/09/23 12:32:14.751
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:32:14.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:32:14.766
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-4eab9b74-d244-442a-accd-784c348e174b in namespace container-probe-6889 05/09/23 12:32:14.767
    May  9 12:32:14.773: INFO: Waiting up to 5m0s for pod "test-webserver-4eab9b74-d244-442a-accd-784c348e174b" in namespace "container-probe-6889" to be "not pending"
    May  9 12:32:14.776: INFO: Pod "test-webserver-4eab9b74-d244-442a-accd-784c348e174b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.680882ms
    May  9 12:32:16.780: INFO: Pod "test-webserver-4eab9b74-d244-442a-accd-784c348e174b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007093232s
    May  9 12:32:16.780: INFO: Pod "test-webserver-4eab9b74-d244-442a-accd-784c348e174b" satisfied condition "not pending"
    May  9 12:32:16.780: INFO: Started pod test-webserver-4eab9b74-d244-442a-accd-784c348e174b in namespace container-probe-6889
    STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 12:32:16.78
    May  9 12:32:16.782: INFO: Initial restart count of pod test-webserver-4eab9b74-d244-442a-accd-784c348e174b is 0
    STEP: deleting the pod 05/09/23 12:36:17.233
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  9 12:36:17.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6889" for this suite. 05/09/23 12:36:17.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:36:17.253
May  9 12:36:17.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename proxy 05/09/23 12:36:17.253
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:36:17.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:36:17.266
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
May  9 12:36:17.267: INFO: Creating pod...
May  9 12:36:17.273: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6368" to be "running"
May  9 12:36:17.277: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.747325ms
May  9 12:36:19.280: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007349647s
May  9 12:36:19.280: INFO: Pod "agnhost" satisfied condition "running"
May  9 12:36:19.280: INFO: Creating service...
May  9 12:36:19.301: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/DELETE
May  9 12:36:19.307: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  9 12:36:19.307: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/GET
May  9 12:36:19.309: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May  9 12:36:19.309: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/HEAD
May  9 12:36:19.311: INFO: http.Client request:HEAD | StatusCode:200
May  9 12:36:19.311: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/OPTIONS
May  9 12:36:19.313: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  9 12:36:19.313: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/PATCH
May  9 12:36:19.315: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  9 12:36:19.315: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/POST
May  9 12:36:19.317: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  9 12:36:19.317: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/PUT
May  9 12:36:19.323: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  9 12:36:19.323: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/DELETE
May  9 12:36:19.327: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  9 12:36:19.327: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/GET
May  9 12:36:19.330: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May  9 12:36:19.330: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/HEAD
May  9 12:36:19.332: INFO: http.Client request:HEAD | StatusCode:200
May  9 12:36:19.332: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/OPTIONS
May  9 12:36:19.335: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  9 12:36:19.335: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/PATCH
May  9 12:36:19.338: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  9 12:36:19.338: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/POST
May  9 12:36:19.341: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  9 12:36:19.341: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/PUT
May  9 12:36:19.344: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
May  9 12:36:19.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6368" for this suite. 05/09/23 12:36:19.347
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":46,"skipped":950,"failed":0}
------------------------------
• [2.100 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:36:17.253
    May  9 12:36:17.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename proxy 05/09/23 12:36:17.253
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:36:17.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:36:17.266
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    May  9 12:36:17.267: INFO: Creating pod...
    May  9 12:36:17.273: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6368" to be "running"
    May  9 12:36:17.277: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.747325ms
    May  9 12:36:19.280: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007349647s
    May  9 12:36:19.280: INFO: Pod "agnhost" satisfied condition "running"
    May  9 12:36:19.280: INFO: Creating service...
    May  9 12:36:19.301: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/DELETE
    May  9 12:36:19.307: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  9 12:36:19.307: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/GET
    May  9 12:36:19.309: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May  9 12:36:19.309: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/HEAD
    May  9 12:36:19.311: INFO: http.Client request:HEAD | StatusCode:200
    May  9 12:36:19.311: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/OPTIONS
    May  9 12:36:19.313: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  9 12:36:19.313: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/PATCH
    May  9 12:36:19.315: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  9 12:36:19.315: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/POST
    May  9 12:36:19.317: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  9 12:36:19.317: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/pods/agnhost/proxy/some/path/with/PUT
    May  9 12:36:19.323: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  9 12:36:19.323: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/DELETE
    May  9 12:36:19.327: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  9 12:36:19.327: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/GET
    May  9 12:36:19.330: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May  9 12:36:19.330: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/HEAD
    May  9 12:36:19.332: INFO: http.Client request:HEAD | StatusCode:200
    May  9 12:36:19.332: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/OPTIONS
    May  9 12:36:19.335: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  9 12:36:19.335: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/PATCH
    May  9 12:36:19.338: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  9 12:36:19.338: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/POST
    May  9 12:36:19.341: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  9 12:36:19.341: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6368/services/test-service/proxy/some/path/with/PUT
    May  9 12:36:19.344: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    May  9 12:36:19.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6368" for this suite. 05/09/23 12:36:19.347
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:36:19.352
May  9 12:36:19.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename replication-controller 05/09/23 12:36:19.353
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:36:19.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:36:19.368
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 05/09/23 12:36:19.37
May  9 12:36:19.375: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1877" to be "running and ready"
May  9 12:36:19.378: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.514537ms
May  9 12:36:19.378: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May  9 12:36:21.382: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.006720628s
May  9 12:36:21.382: INFO: The phase of Pod pod-adoption is Running (Ready = true)
May  9 12:36:21.382: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 05/09/23 12:36:21.384
STEP: Then the orphan pod is adopted 05/09/23 12:36:21.389
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  9 12:36:22.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1877" for this suite. 05/09/23 12:36:22.397
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":47,"skipped":951,"failed":0}
------------------------------
• [3.049 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:36:19.352
    May  9 12:36:19.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename replication-controller 05/09/23 12:36:19.353
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:36:19.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:36:19.368
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 05/09/23 12:36:19.37
    May  9 12:36:19.375: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1877" to be "running and ready"
    May  9 12:36:19.378: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.514537ms
    May  9 12:36:19.378: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:36:21.382: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.006720628s
    May  9 12:36:21.382: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    May  9 12:36:21.382: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 05/09/23 12:36:21.384
    STEP: Then the orphan pod is adopted 05/09/23 12:36:21.389
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  9 12:36:22.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1877" for this suite. 05/09/23 12:36:22.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:36:22.402
May  9 12:36:22.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename resourcequota 05/09/23 12:36:22.403
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:36:22.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:36:22.416
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 05/09/23 12:36:22.417
STEP: Ensuring ResourceQuota status is calculated 05/09/23 12:36:22.421
STEP: Creating a ResourceQuota with not best effort scope 05/09/23 12:36:24.425
STEP: Ensuring ResourceQuota status is calculated 05/09/23 12:36:24.429
STEP: Creating a best-effort pod 05/09/23 12:36:26.433
STEP: Ensuring resource quota with best effort scope captures the pod usage 05/09/23 12:36:26.443
STEP: Ensuring resource quota with not best effort ignored the pod usage 05/09/23 12:36:28.447
STEP: Deleting the pod 05/09/23 12:36:30.451
STEP: Ensuring resource quota status released the pod usage 05/09/23 12:36:30.465
STEP: Creating a not best-effort pod 05/09/23 12:36:32.469
STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/09/23 12:36:32.481
STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/09/23 12:36:34.484
STEP: Deleting the pod 05/09/23 12:36:36.488
STEP: Ensuring resource quota status released the pod usage 05/09/23 12:36:36.5
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  9 12:36:38.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5411" for this suite. 05/09/23 12:36:38.508
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":48,"skipped":978,"failed":0}
------------------------------
• [SLOW TEST] [16.111 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:36:22.402
    May  9 12:36:22.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename resourcequota 05/09/23 12:36:22.403
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:36:22.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:36:22.416
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 05/09/23 12:36:22.417
    STEP: Ensuring ResourceQuota status is calculated 05/09/23 12:36:22.421
    STEP: Creating a ResourceQuota with not best effort scope 05/09/23 12:36:24.425
    STEP: Ensuring ResourceQuota status is calculated 05/09/23 12:36:24.429
    STEP: Creating a best-effort pod 05/09/23 12:36:26.433
    STEP: Ensuring resource quota with best effort scope captures the pod usage 05/09/23 12:36:26.443
    STEP: Ensuring resource quota with not best effort ignored the pod usage 05/09/23 12:36:28.447
    STEP: Deleting the pod 05/09/23 12:36:30.451
    STEP: Ensuring resource quota status released the pod usage 05/09/23 12:36:30.465
    STEP: Creating a not best-effort pod 05/09/23 12:36:32.469
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/09/23 12:36:32.481
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/09/23 12:36:34.484
    STEP: Deleting the pod 05/09/23 12:36:36.488
    STEP: Ensuring resource quota status released the pod usage 05/09/23 12:36:36.5
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  9 12:36:38.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5411" for this suite. 05/09/23 12:36:38.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:36:38.514
May  9 12:36:38.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename svc-latency 05/09/23 12:36:38.514
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:36:38.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:36:38.53
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
May  9 12:36:38.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4998 05/09/23 12:36:38.532
I0509 12:36:38.536688      24 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4998, replica count: 1
I0509 12:36:39.587442      24 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 12:36:39.699: INFO: Created: latency-svc-lrb95
May  9 12:36:39.705: INFO: Got endpoints: latency-svc-lrb95 [17.479511ms]
May  9 12:36:39.719: INFO: Created: latency-svc-5wr2w
May  9 12:36:39.726: INFO: Got endpoints: latency-svc-5wr2w [21.150157ms]
May  9 12:36:39.729: INFO: Created: latency-svc-4t88v
May  9 12:36:39.736: INFO: Got endpoints: latency-svc-4t88v [31.128395ms]
May  9 12:36:39.739: INFO: Created: latency-svc-b7j25
May  9 12:36:39.743: INFO: Got endpoints: latency-svc-b7j25 [38.029717ms]
May  9 12:36:39.750: INFO: Created: latency-svc-8764z
May  9 12:36:39.757: INFO: Got endpoints: latency-svc-8764z [52.486508ms]
May  9 12:36:39.762: INFO: Created: latency-svc-cdnqn
May  9 12:36:39.764: INFO: Got endpoints: latency-svc-cdnqn [59.522206ms]
May  9 12:36:39.771: INFO: Created: latency-svc-pvlf8
May  9 12:36:39.776: INFO: Got endpoints: latency-svc-pvlf8 [71.566084ms]
May  9 12:36:39.783: INFO: Created: latency-svc-rpr8j
May  9 12:36:39.790: INFO: Got endpoints: latency-svc-rpr8j [85.593968ms]
May  9 12:36:39.794: INFO: Created: latency-svc-llhxr
May  9 12:36:39.803: INFO: Got endpoints: latency-svc-llhxr [97.911106ms]
May  9 12:36:39.806: INFO: Created: latency-svc-cmr9d
May  9 12:36:39.812: INFO: Got endpoints: latency-svc-cmr9d [106.939458ms]
May  9 12:36:39.831: INFO: Created: latency-svc-l5sz7
May  9 12:36:39.834: INFO: Got endpoints: latency-svc-l5sz7 [128.892853ms]
May  9 12:36:39.843: INFO: Created: latency-svc-dxbc8
May  9 12:36:39.849: INFO: Got endpoints: latency-svc-dxbc8 [143.750125ms]
May  9 12:36:39.851: INFO: Created: latency-svc-2zk58
May  9 12:36:39.857: INFO: Got endpoints: latency-svc-2zk58 [152.093833ms]
May  9 12:36:39.860: INFO: Created: latency-svc-lwvvn
May  9 12:36:39.863: INFO: Got endpoints: latency-svc-lwvvn [158.468573ms]
May  9 12:36:39.871: INFO: Created: latency-svc-8jpsk
May  9 12:36:39.880: INFO: Got endpoints: latency-svc-8jpsk [174.913997ms]
May  9 12:36:39.884: INFO: Created: latency-svc-wpdg8
May  9 12:36:39.889: INFO: Got endpoints: latency-svc-wpdg8 [184.355324ms]
May  9 12:36:39.895: INFO: Created: latency-svc-ct7hc
May  9 12:36:39.902: INFO: Got endpoints: latency-svc-ct7hc [176.415895ms]
May  9 12:36:39.907: INFO: Created: latency-svc-xxz6t
May  9 12:36:39.917: INFO: Got endpoints: latency-svc-xxz6t [181.096132ms]
May  9 12:36:39.939: INFO: Created: latency-svc-cn24f
May  9 12:36:39.939: INFO: Got endpoints: latency-svc-cn24f [196.473713ms]
May  9 12:36:39.952: INFO: Created: latency-svc-bkxr6
May  9 12:36:39.956: INFO: Got endpoints: latency-svc-bkxr6 [199.076327ms]
May  9 12:36:39.964: INFO: Created: latency-svc-jtq5p
May  9 12:36:39.968: INFO: Got endpoints: latency-svc-jtq5p [203.341633ms]
May  9 12:36:39.974: INFO: Created: latency-svc-s7kqc
May  9 12:36:39.981: INFO: Got endpoints: latency-svc-s7kqc [204.407871ms]
May  9 12:36:39.986: INFO: Created: latency-svc-4hdrl
May  9 12:36:39.993: INFO: Got endpoints: latency-svc-4hdrl [202.422234ms]
May  9 12:36:40.000: INFO: Created: latency-svc-4xwnn
May  9 12:36:40.005: INFO: Created: latency-svc-98wcj
May  9 12:36:40.006: INFO: Got endpoints: latency-svc-4xwnn [202.974755ms]
May  9 12:36:40.013: INFO: Got endpoints: latency-svc-98wcj [201.071964ms]
May  9 12:36:40.018: INFO: Created: latency-svc-kkzzf
May  9 12:36:40.024: INFO: Got endpoints: latency-svc-kkzzf [190.292833ms]
May  9 12:36:40.027: INFO: Created: latency-svc-6nn8r
May  9 12:36:40.036: INFO: Got endpoints: latency-svc-6nn8r [187.766615ms]
May  9 12:36:40.051: INFO: Created: latency-svc-z76dc
May  9 12:36:40.053: INFO: Got endpoints: latency-svc-z76dc [196.050378ms]
May  9 12:36:40.060: INFO: Created: latency-svc-p8jfg
May  9 12:36:40.068: INFO: Got endpoints: latency-svc-p8jfg [204.646767ms]
May  9 12:36:40.071: INFO: Created: latency-svc-lgqc9
May  9 12:36:40.077: INFO: Got endpoints: latency-svc-lgqc9 [196.845021ms]
May  9 12:36:40.081: INFO: Created: latency-svc-474gz
May  9 12:36:40.085: INFO: Got endpoints: latency-svc-474gz [195.933456ms]
May  9 12:36:40.091: INFO: Created: latency-svc-pk5qt
May  9 12:36:40.097: INFO: Got endpoints: latency-svc-pk5qt [194.841487ms]
May  9 12:36:40.100: INFO: Created: latency-svc-65wt9
May  9 12:36:40.106: INFO: Got endpoints: latency-svc-65wt9 [188.766659ms]
May  9 12:36:40.128: INFO: Created: latency-svc-9j44x
May  9 12:36:40.132: INFO: Got endpoints: latency-svc-9j44x [164.604371ms]
May  9 12:36:40.139: INFO: Created: latency-svc-t6nt2
May  9 12:36:40.146: INFO: Got endpoints: latency-svc-t6nt2 [165.441802ms]
May  9 12:36:40.161: INFO: Created: latency-svc-94q8x
May  9 12:36:40.161: INFO: Got endpoints: latency-svc-94q8x [168.449497ms]
May  9 12:36:40.170: INFO: Created: latency-svc-8vcxd
May  9 12:36:40.177: INFO: Got endpoints: latency-svc-8vcxd [170.860666ms]
May  9 12:36:40.181: INFO: Created: latency-svc-kh7dt
May  9 12:36:40.188: INFO: Got endpoints: latency-svc-kh7dt [175.460289ms]
May  9 12:36:40.191: INFO: Created: latency-svc-2r2s9
May  9 12:36:40.198: INFO: Got endpoints: latency-svc-2r2s9 [174.156438ms]
May  9 12:36:40.202: INFO: Created: latency-svc-6ptd6
May  9 12:36:40.206: INFO: Got endpoints: latency-svc-6ptd6 [169.337228ms]
May  9 12:36:40.214: INFO: Created: latency-svc-bmr9q
May  9 12:36:40.221: INFO: Created: latency-svc-zjt5t
May  9 12:36:40.234: INFO: Created: latency-svc-7zxxb
May  9 12:36:40.241: INFO: Created: latency-svc-g9ssq
May  9 12:36:40.250: INFO: Created: latency-svc-p7qzz
May  9 12:36:40.271: INFO: Got endpoints: latency-svc-bmr9q [218.093676ms]
May  9 12:36:40.275: INFO: Created: latency-svc-j4jq8
May  9 12:36:40.276: INFO: Created: latency-svc-kj9lj
May  9 12:36:40.276: INFO: Created: latency-svc-rnhfd
May  9 12:36:40.282: INFO: Created: latency-svc-dnjb5
May  9 12:36:40.291: INFO: Created: latency-svc-fj5tg
May  9 12:36:40.299: INFO: Created: latency-svc-7cr8j
May  9 12:36:40.306: INFO: Got endpoints: latency-svc-zjt5t [238.156265ms]
May  9 12:36:40.309: INFO: Created: latency-svc-bjrw9
May  9 12:36:40.319: INFO: Created: latency-svc-dwpv4
May  9 12:36:40.328: INFO: Created: latency-svc-pvgvg
May  9 12:36:40.337: INFO: Created: latency-svc-2tz7d
May  9 12:36:40.355: INFO: Got endpoints: latency-svc-7zxxb [278.594456ms]
May  9 12:36:40.357: INFO: Created: latency-svc-tstzw
May  9 12:36:40.383: INFO: Created: latency-svc-9f877
May  9 12:36:40.392: INFO: Created: latency-svc-fddbf
May  9 12:36:40.405: INFO: Got endpoints: latency-svc-g9ssq [319.711288ms]
May  9 12:36:40.416: INFO: Created: latency-svc-knqwb
May  9 12:36:40.455: INFO: Got endpoints: latency-svc-p7qzz [357.985139ms]
May  9 12:36:40.467: INFO: Created: latency-svc-49hcd
May  9 12:36:40.504: INFO: Got endpoints: latency-svc-j4jq8 [397.921244ms]
May  9 12:36:40.515: INFO: Created: latency-svc-2x567
May  9 12:36:40.555: INFO: Got endpoints: latency-svc-rnhfd [615.680133ms]
May  9 12:36:40.567: INFO: Created: latency-svc-6xpm2
May  9 12:36:40.606: INFO: Got endpoints: latency-svc-kj9lj [649.560044ms]
May  9 12:36:40.617: INFO: Created: latency-svc-p8zld
May  9 12:36:40.656: INFO: Got endpoints: latency-svc-dnjb5 [523.382018ms]
May  9 12:36:40.668: INFO: Created: latency-svc-tjmf6
May  9 12:36:40.706: INFO: Got endpoints: latency-svc-fj5tg [559.231355ms]
May  9 12:36:40.718: INFO: Created: latency-svc-dcgdj
May  9 12:36:40.756: INFO: Got endpoints: latency-svc-7cr8j [594.743754ms]
May  9 12:36:40.766: INFO: Created: latency-svc-m8wtb
May  9 12:36:40.810: INFO: Got endpoints: latency-svc-bjrw9 [633.499633ms]
May  9 12:36:40.823: INFO: Created: latency-svc-w5n8f
May  9 12:36:40.855: INFO: Got endpoints: latency-svc-dwpv4 [666.572688ms]
May  9 12:36:40.868: INFO: Created: latency-svc-5vjzl
May  9 12:36:40.906: INFO: Got endpoints: latency-svc-pvgvg [707.569403ms]
May  9 12:36:40.919: INFO: Created: latency-svc-fzdhk
May  9 12:36:40.956: INFO: Got endpoints: latency-svc-2tz7d [749.781367ms]
May  9 12:36:40.968: INFO: Created: latency-svc-dqdpf
May  9 12:36:41.005: INFO: Got endpoints: latency-svc-tstzw [734.356165ms]
May  9 12:36:41.024: INFO: Created: latency-svc-qmp5v
May  9 12:36:41.056: INFO: Got endpoints: latency-svc-9f877 [749.778783ms]
May  9 12:36:41.068: INFO: Created: latency-svc-6d75r
May  9 12:36:41.105: INFO: Got endpoints: latency-svc-fddbf [749.579754ms]
May  9 12:36:41.118: INFO: Created: latency-svc-qwgjl
May  9 12:36:41.155: INFO: Got endpoints: latency-svc-knqwb [750.217108ms]
May  9 12:36:41.167: INFO: Created: latency-svc-4s77f
May  9 12:36:41.205: INFO: Got endpoints: latency-svc-49hcd [750.118851ms]
May  9 12:36:41.219: INFO: Created: latency-svc-rkbqt
May  9 12:36:41.255: INFO: Got endpoints: latency-svc-2x567 [751.248771ms]
May  9 12:36:41.267: INFO: Created: latency-svc-cspw4
May  9 12:36:41.305: INFO: Got endpoints: latency-svc-6xpm2 [750.413272ms]
May  9 12:36:41.322: INFO: Created: latency-svc-hmdmt
May  9 12:36:41.356: INFO: Got endpoints: latency-svc-p8zld [749.961163ms]
May  9 12:36:41.368: INFO: Created: latency-svc-2n8xb
May  9 12:36:41.403: INFO: Got endpoints: latency-svc-tjmf6 [747.545065ms]
May  9 12:36:41.415: INFO: Created: latency-svc-58ngd
May  9 12:36:41.455: INFO: Got endpoints: latency-svc-dcgdj [749.315083ms]
May  9 12:36:41.466: INFO: Created: latency-svc-2zqnw
May  9 12:36:41.505: INFO: Got endpoints: latency-svc-m8wtb [749.348917ms]
May  9 12:36:41.518: INFO: Created: latency-svc-dxgkh
May  9 12:36:41.554: INFO: Got endpoints: latency-svc-w5n8f [743.819937ms]
May  9 12:36:41.565: INFO: Created: latency-svc-6wznr
May  9 12:36:41.606: INFO: Got endpoints: latency-svc-5vjzl [750.489409ms]
May  9 12:36:41.617: INFO: Created: latency-svc-jjsts
May  9 12:36:41.658: INFO: Got endpoints: latency-svc-fzdhk [752.268615ms]
May  9 12:36:41.669: INFO: Created: latency-svc-cb2nc
May  9 12:36:41.706: INFO: Got endpoints: latency-svc-dqdpf [750.102183ms]
May  9 12:36:41.720: INFO: Created: latency-svc-r454k
May  9 12:36:41.756: INFO: Got endpoints: latency-svc-qmp5v [750.462869ms]
May  9 12:36:41.773: INFO: Created: latency-svc-jnvjg
May  9 12:36:41.805: INFO: Got endpoints: latency-svc-6d75r [748.707409ms]
May  9 12:36:41.816: INFO: Created: latency-svc-jqj9w
May  9 12:36:41.856: INFO: Got endpoints: latency-svc-qwgjl [751.125882ms]
May  9 12:36:41.872: INFO: Created: latency-svc-fbtwv
May  9 12:36:41.905: INFO: Got endpoints: latency-svc-4s77f [750.317033ms]
May  9 12:36:41.918: INFO: Created: latency-svc-8v9bf
May  9 12:36:41.954: INFO: Got endpoints: latency-svc-rkbqt [748.216836ms]
May  9 12:36:41.965: INFO: Created: latency-svc-zkr86
May  9 12:36:42.005: INFO: Got endpoints: latency-svc-cspw4 [749.551275ms]
May  9 12:36:42.016: INFO: Created: latency-svc-b5jxr
May  9 12:36:42.054: INFO: Got endpoints: latency-svc-hmdmt [748.733098ms]
May  9 12:36:42.067: INFO: Created: latency-svc-8zqrj
May  9 12:36:42.104: INFO: Got endpoints: latency-svc-2n8xb [747.678592ms]
May  9 12:36:42.114: INFO: Created: latency-svc-4smsm
May  9 12:36:42.153: INFO: Got endpoints: latency-svc-58ngd [749.297414ms]
May  9 12:36:42.167: INFO: Created: latency-svc-4vws8
May  9 12:36:42.204: INFO: Got endpoints: latency-svc-2zqnw [748.856816ms]
May  9 12:36:42.230: INFO: Created: latency-svc-pxzkc
May  9 12:36:42.254: INFO: Got endpoints: latency-svc-dxgkh [748.992513ms]
May  9 12:36:42.266: INFO: Created: latency-svc-dg4dr
May  9 12:36:42.309: INFO: Got endpoints: latency-svc-6wznr [755.272625ms]
May  9 12:36:42.322: INFO: Created: latency-svc-5hwzr
May  9 12:36:42.353: INFO: Got endpoints: latency-svc-jjsts [747.414902ms]
May  9 12:36:42.368: INFO: Created: latency-svc-q68np
May  9 12:36:42.405: INFO: Got endpoints: latency-svc-cb2nc [746.624689ms]
May  9 12:36:42.415: INFO: Created: latency-svc-p29lh
May  9 12:36:42.456: INFO: Got endpoints: latency-svc-r454k [750.55081ms]
May  9 12:36:42.468: INFO: Created: latency-svc-x9scr
May  9 12:36:42.506: INFO: Got endpoints: latency-svc-jnvjg [750.43506ms]
May  9 12:36:42.517: INFO: Created: latency-svc-mkxrp
May  9 12:36:42.554: INFO: Got endpoints: latency-svc-jqj9w [748.870031ms]
May  9 12:36:42.565: INFO: Created: latency-svc-lwhxk
May  9 12:36:42.613: INFO: Got endpoints: latency-svc-fbtwv [756.658323ms]
May  9 12:36:42.628: INFO: Created: latency-svc-ktljg
May  9 12:36:42.655: INFO: Got endpoints: latency-svc-8v9bf [749.13132ms]
May  9 12:36:42.666: INFO: Created: latency-svc-rks65
May  9 12:36:42.707: INFO: Got endpoints: latency-svc-zkr86 [753.408008ms]
May  9 12:36:42.724: INFO: Created: latency-svc-zk9xm
May  9 12:36:42.755: INFO: Got endpoints: latency-svc-b5jxr [750.00903ms]
May  9 12:36:42.769: INFO: Created: latency-svc-nlchg
May  9 12:36:42.805: INFO: Got endpoints: latency-svc-8zqrj [750.476931ms]
May  9 12:36:42.817: INFO: Created: latency-svc-f47zt
May  9 12:36:42.856: INFO: Got endpoints: latency-svc-4smsm [751.972998ms]
May  9 12:36:42.869: INFO: Created: latency-svc-bcjfn
May  9 12:36:42.905: INFO: Got endpoints: latency-svc-4vws8 [752.745467ms]
May  9 12:36:42.917: INFO: Created: latency-svc-blnkl
May  9 12:36:42.956: INFO: Got endpoints: latency-svc-pxzkc [752.371185ms]
May  9 12:36:42.968: INFO: Created: latency-svc-sr6kx
May  9 12:36:43.005: INFO: Got endpoints: latency-svc-dg4dr [750.509793ms]
May  9 12:36:43.017: INFO: Created: latency-svc-wznlb
May  9 12:36:43.056: INFO: Got endpoints: latency-svc-5hwzr [746.315211ms]
May  9 12:36:43.067: INFO: Created: latency-svc-fcswd
May  9 12:36:43.104: INFO: Got endpoints: latency-svc-q68np [751.234133ms]
May  9 12:36:43.116: INFO: Created: latency-svc-g69l8
May  9 12:36:43.155: INFO: Got endpoints: latency-svc-p29lh [750.740623ms]
May  9 12:36:43.168: INFO: Created: latency-svc-55qqn
May  9 12:36:43.208: INFO: Got endpoints: latency-svc-x9scr [751.239022ms]
May  9 12:36:43.219: INFO: Created: latency-svc-l955q
May  9 12:36:43.255: INFO: Got endpoints: latency-svc-mkxrp [748.910512ms]
May  9 12:36:43.269: INFO: Created: latency-svc-nkkgs
May  9 12:36:43.305: INFO: Got endpoints: latency-svc-lwhxk [751.366535ms]
May  9 12:36:43.319: INFO: Created: latency-svc-jrlqf
May  9 12:36:43.359: INFO: Got endpoints: latency-svc-ktljg [746.540904ms]
May  9 12:36:43.371: INFO: Created: latency-svc-gjhtj
May  9 12:36:43.404: INFO: Got endpoints: latency-svc-rks65 [749.496397ms]
May  9 12:36:43.417: INFO: Created: latency-svc-4hwv6
May  9 12:36:43.457: INFO: Got endpoints: latency-svc-zk9xm [750.334323ms]
May  9 12:36:43.474: INFO: Created: latency-svc-sqdc6
May  9 12:36:43.507: INFO: Got endpoints: latency-svc-nlchg [752.590466ms]
May  9 12:36:43.524: INFO: Created: latency-svc-q7jzr
May  9 12:36:43.558: INFO: Got endpoints: latency-svc-f47zt [753.641395ms]
May  9 12:36:43.597: INFO: Created: latency-svc-scw9m
May  9 12:36:43.604: INFO: Got endpoints: latency-svc-bcjfn [748.841372ms]
May  9 12:36:43.617: INFO: Created: latency-svc-s5zfk
May  9 12:36:43.655: INFO: Got endpoints: latency-svc-blnkl [749.578034ms]
May  9 12:36:43.668: INFO: Created: latency-svc-6djjp
May  9 12:36:43.706: INFO: Got endpoints: latency-svc-sr6kx [750.195209ms]
May  9 12:36:43.720: INFO: Created: latency-svc-phphf
May  9 12:36:43.758: INFO: Got endpoints: latency-svc-wznlb [752.945463ms]
May  9 12:36:43.770: INFO: Created: latency-svc-bcss6
May  9 12:36:43.805: INFO: Got endpoints: latency-svc-fcswd [749.519043ms]
May  9 12:36:43.817: INFO: Created: latency-svc-zv4f4
May  9 12:36:43.856: INFO: Got endpoints: latency-svc-g69l8 [752.126765ms]
May  9 12:36:43.870: INFO: Created: latency-svc-jjf7r
May  9 12:36:43.912: INFO: Got endpoints: latency-svc-55qqn [756.751835ms]
May  9 12:36:43.923: INFO: Created: latency-svc-qdj5f
May  9 12:36:43.957: INFO: Got endpoints: latency-svc-l955q [749.373084ms]
May  9 12:36:43.971: INFO: Created: latency-svc-6k2xj
May  9 12:36:44.005: INFO: Got endpoints: latency-svc-nkkgs [749.805788ms]
May  9 12:36:44.023: INFO: Created: latency-svc-m5l6c
May  9 12:36:44.055: INFO: Got endpoints: latency-svc-jrlqf [750.38998ms]
May  9 12:36:44.067: INFO: Created: latency-svc-qm24v
May  9 12:36:44.106: INFO: Got endpoints: latency-svc-gjhtj [746.204535ms]
May  9 12:36:44.118: INFO: Created: latency-svc-x7rxf
May  9 12:36:44.155: INFO: Got endpoints: latency-svc-4hwv6 [750.687548ms]
May  9 12:36:44.167: INFO: Created: latency-svc-hpzxv
May  9 12:36:44.206: INFO: Got endpoints: latency-svc-sqdc6 [748.682172ms]
May  9 12:36:44.218: INFO: Created: latency-svc-grj24
May  9 12:36:44.254: INFO: Got endpoints: latency-svc-q7jzr [746.397473ms]
May  9 12:36:44.264: INFO: Created: latency-svc-qkws4
May  9 12:36:44.305: INFO: Got endpoints: latency-svc-scw9m [746.329375ms]
May  9 12:36:44.316: INFO: Created: latency-svc-ggml4
May  9 12:36:44.354: INFO: Got endpoints: latency-svc-s5zfk [749.527311ms]
May  9 12:36:44.365: INFO: Created: latency-svc-kdkpw
May  9 12:36:44.405: INFO: Got endpoints: latency-svc-6djjp [749.525779ms]
May  9 12:36:44.415: INFO: Created: latency-svc-hfslm
May  9 12:36:44.456: INFO: Got endpoints: latency-svc-phphf [749.454443ms]
May  9 12:36:44.466: INFO: Created: latency-svc-w2gqf
May  9 12:36:44.505: INFO: Got endpoints: latency-svc-bcss6 [746.999612ms]
May  9 12:36:44.517: INFO: Created: latency-svc-hx95g
May  9 12:36:44.559: INFO: Got endpoints: latency-svc-zv4f4 [754.044629ms]
May  9 12:36:44.570: INFO: Created: latency-svc-dc8mm
May  9 12:36:44.604: INFO: Got endpoints: latency-svc-jjf7r [748.05474ms]
May  9 12:36:44.617: INFO: Created: latency-svc-8tqsv
May  9 12:36:44.654: INFO: Got endpoints: latency-svc-qdj5f [741.905058ms]
May  9 12:36:44.674: INFO: Created: latency-svc-wgwxz
May  9 12:36:44.705: INFO: Got endpoints: latency-svc-6k2xj [747.897623ms]
May  9 12:36:44.720: INFO: Created: latency-svc-mk5kf
May  9 12:36:44.756: INFO: Got endpoints: latency-svc-m5l6c [751.062073ms]
May  9 12:36:44.767: INFO: Created: latency-svc-svb7h
May  9 12:36:44.804: INFO: Got endpoints: latency-svc-qm24v [748.419646ms]
May  9 12:36:44.816: INFO: Created: latency-svc-kn59q
May  9 12:36:44.854: INFO: Got endpoints: latency-svc-x7rxf [748.483467ms]
May  9 12:36:44.865: INFO: Created: latency-svc-lx6bs
May  9 12:36:44.908: INFO: Got endpoints: latency-svc-hpzxv [753.028156ms]
May  9 12:36:44.920: INFO: Created: latency-svc-42c9k
May  9 12:36:44.955: INFO: Got endpoints: latency-svc-grj24 [748.85327ms]
May  9 12:36:44.966: INFO: Created: latency-svc-bzpvm
May  9 12:36:45.006: INFO: Got endpoints: latency-svc-qkws4 [751.919946ms]
May  9 12:36:45.016: INFO: Created: latency-svc-z9jks
May  9 12:36:45.053: INFO: Got endpoints: latency-svc-ggml4 [748.703976ms]
May  9 12:36:45.065: INFO: Created: latency-svc-7cwth
May  9 12:36:45.105: INFO: Got endpoints: latency-svc-kdkpw [750.667614ms]
May  9 12:36:45.115: INFO: Created: latency-svc-fwgtq
May  9 12:36:45.156: INFO: Got endpoints: latency-svc-hfslm [751.499348ms]
May  9 12:36:45.168: INFO: Created: latency-svc-gljwt
May  9 12:36:45.208: INFO: Got endpoints: latency-svc-w2gqf [751.718024ms]
May  9 12:36:45.219: INFO: Created: latency-svc-ncm2p
May  9 12:36:45.255: INFO: Got endpoints: latency-svc-hx95g [750.337797ms]
May  9 12:36:45.267: INFO: Created: latency-svc-lgtsc
May  9 12:36:45.305: INFO: Got endpoints: latency-svc-dc8mm [745.198206ms]
May  9 12:36:45.318: INFO: Created: latency-svc-khk7g
May  9 12:36:45.355: INFO: Got endpoints: latency-svc-8tqsv [750.694728ms]
May  9 12:36:45.366: INFO: Created: latency-svc-gjl85
May  9 12:36:45.404: INFO: Got endpoints: latency-svc-wgwxz [750.13917ms]
May  9 12:36:45.420: INFO: Created: latency-svc-p5tc5
May  9 12:36:45.455: INFO: Got endpoints: latency-svc-mk5kf [750.021475ms]
May  9 12:36:45.468: INFO: Created: latency-svc-hzbqx
May  9 12:36:45.504: INFO: Got endpoints: latency-svc-svb7h [748.172597ms]
May  9 12:36:45.515: INFO: Created: latency-svc-kdckl
May  9 12:36:45.556: INFO: Got endpoints: latency-svc-kn59q [752.139147ms]
May  9 12:36:45.568: INFO: Created: latency-svc-4szht
May  9 12:36:45.604: INFO: Got endpoints: latency-svc-lx6bs [749.46589ms]
May  9 12:36:45.615: INFO: Created: latency-svc-m24fh
May  9 12:36:45.656: INFO: Got endpoints: latency-svc-42c9k [747.701822ms]
May  9 12:36:45.667: INFO: Created: latency-svc-tjdn6
May  9 12:36:45.704: INFO: Got endpoints: latency-svc-bzpvm [748.599141ms]
May  9 12:36:45.720: INFO: Created: latency-svc-ftpwm
May  9 12:36:45.759: INFO: Got endpoints: latency-svc-z9jks [753.351324ms]
May  9 12:36:45.779: INFO: Created: latency-svc-j4j7h
May  9 12:36:45.805: INFO: Got endpoints: latency-svc-7cwth [751.600373ms]
May  9 12:36:45.818: INFO: Created: latency-svc-j8ch4
May  9 12:36:45.855: INFO: Got endpoints: latency-svc-fwgtq [749.967155ms]
May  9 12:36:45.866: INFO: Created: latency-svc-zl2bq
May  9 12:36:45.906: INFO: Got endpoints: latency-svc-gljwt [749.695376ms]
May  9 12:36:45.919: INFO: Created: latency-svc-j2t2j
May  9 12:36:45.955: INFO: Got endpoints: latency-svc-ncm2p [747.238362ms]
May  9 12:36:45.968: INFO: Created: latency-svc-84qxs
May  9 12:36:46.008: INFO: Got endpoints: latency-svc-lgtsc [752.489394ms]
May  9 12:36:46.023: INFO: Created: latency-svc-t8kjt
May  9 12:36:46.058: INFO: Got endpoints: latency-svc-khk7g [753.434212ms]
May  9 12:36:46.071: INFO: Created: latency-svc-mhfmt
May  9 12:36:46.106: INFO: Got endpoints: latency-svc-gjl85 [750.641828ms]
May  9 12:36:46.116: INFO: Created: latency-svc-4pd8b
May  9 12:36:46.155: INFO: Got endpoints: latency-svc-p5tc5 [750.501502ms]
May  9 12:36:46.171: INFO: Created: latency-svc-vwz82
May  9 12:36:46.204: INFO: Got endpoints: latency-svc-hzbqx [749.410097ms]
May  9 12:36:46.215: INFO: Created: latency-svc-flg67
May  9 12:36:46.256: INFO: Got endpoints: latency-svc-kdckl [751.949529ms]
May  9 12:36:46.269: INFO: Created: latency-svc-w279q
May  9 12:36:46.303: INFO: Got endpoints: latency-svc-4szht [746.867829ms]
May  9 12:36:46.315: INFO: Created: latency-svc-7cb6p
May  9 12:36:46.356: INFO: Got endpoints: latency-svc-m24fh [752.155099ms]
May  9 12:36:46.368: INFO: Created: latency-svc-8f4r4
May  9 12:36:46.404: INFO: Got endpoints: latency-svc-tjdn6 [748.707489ms]
May  9 12:36:46.416: INFO: Created: latency-svc-7dwhv
May  9 12:36:46.455: INFO: Got endpoints: latency-svc-ftpwm [751.787591ms]
May  9 12:36:46.467: INFO: Created: latency-svc-4k4c8
May  9 12:36:46.504: INFO: Got endpoints: latency-svc-j4j7h [745.194255ms]
May  9 12:36:46.523: INFO: Created: latency-svc-62sl4
May  9 12:36:46.556: INFO: Got endpoints: latency-svc-j8ch4 [750.403227ms]
May  9 12:36:46.569: INFO: Created: latency-svc-tdzk6
May  9 12:36:46.606: INFO: Got endpoints: latency-svc-zl2bq [751.474466ms]
May  9 12:36:46.619: INFO: Created: latency-svc-mh6zv
May  9 12:36:46.655: INFO: Got endpoints: latency-svc-j2t2j [749.305209ms]
May  9 12:36:46.667: INFO: Created: latency-svc-nzgs7
May  9 12:36:46.706: INFO: Got endpoints: latency-svc-84qxs [750.746732ms]
May  9 12:36:46.719: INFO: Created: latency-svc-kxnqb
May  9 12:36:46.756: INFO: Got endpoints: latency-svc-t8kjt [747.804333ms]
May  9 12:36:46.769: INFO: Created: latency-svc-5wj8h
May  9 12:36:46.805: INFO: Got endpoints: latency-svc-mhfmt [747.254536ms]
May  9 12:36:46.823: INFO: Created: latency-svc-rdlp7
May  9 12:36:46.857: INFO: Got endpoints: latency-svc-4pd8b [751.244529ms]
May  9 12:36:46.876: INFO: Created: latency-svc-44dfk
May  9 12:36:46.912: INFO: Got endpoints: latency-svc-vwz82 [757.433757ms]
May  9 12:36:46.927: INFO: Created: latency-svc-pr7m7
May  9 12:36:46.954: INFO: Got endpoints: latency-svc-flg67 [749.693077ms]
May  9 12:36:46.973: INFO: Created: latency-svc-xzbqq
May  9 12:36:47.004: INFO: Got endpoints: latency-svc-w279q [747.798953ms]
May  9 12:36:47.023: INFO: Created: latency-svc-w6ltf
May  9 12:36:47.059: INFO: Got endpoints: latency-svc-7cb6p [755.966946ms]
May  9 12:36:47.073: INFO: Created: latency-svc-s7djj
May  9 12:36:47.106: INFO: Got endpoints: latency-svc-8f4r4 [750.461801ms]
May  9 12:36:47.117: INFO: Created: latency-svc-jl6rb
May  9 12:36:47.161: INFO: Got endpoints: latency-svc-7dwhv [756.539797ms]
May  9 12:36:47.173: INFO: Created: latency-svc-b54pm
May  9 12:36:47.205: INFO: Got endpoints: latency-svc-4k4c8 [749.087967ms]
May  9 12:36:47.217: INFO: Created: latency-svc-r2zdw
May  9 12:36:47.255: INFO: Got endpoints: latency-svc-62sl4 [750.28764ms]
May  9 12:36:47.266: INFO: Created: latency-svc-7b2ng
May  9 12:36:47.306: INFO: Got endpoints: latency-svc-tdzk6 [750.002197ms]
May  9 12:36:47.317: INFO: Created: latency-svc-sgtsk
May  9 12:36:47.355: INFO: Got endpoints: latency-svc-mh6zv [749.054412ms]
May  9 12:36:47.374: INFO: Created: latency-svc-gb65v
May  9 12:36:47.403: INFO: Got endpoints: latency-svc-nzgs7 [748.098545ms]
May  9 12:36:47.421: INFO: Created: latency-svc-7vkxp
May  9 12:36:47.458: INFO: Got endpoints: latency-svc-kxnqb [751.946848ms]
May  9 12:36:47.476: INFO: Created: latency-svc-ztp59
May  9 12:36:47.506: INFO: Got endpoints: latency-svc-5wj8h [750.358756ms]
May  9 12:36:47.517: INFO: Created: latency-svc-4kvrt
May  9 12:36:47.556: INFO: Got endpoints: latency-svc-rdlp7 [750.995017ms]
May  9 12:36:47.605: INFO: Got endpoints: latency-svc-44dfk [747.638851ms]
May  9 12:36:47.656: INFO: Got endpoints: latency-svc-pr7m7 [743.844912ms]
May  9 12:36:47.709: INFO: Got endpoints: latency-svc-xzbqq [755.03729ms]
May  9 12:36:47.756: INFO: Got endpoints: latency-svc-w6ltf [751.983008ms]
May  9 12:36:47.807: INFO: Got endpoints: latency-svc-s7djj [748.186444ms]
May  9 12:36:47.855: INFO: Got endpoints: latency-svc-jl6rb [748.296754ms]
May  9 12:36:47.905: INFO: Got endpoints: latency-svc-b54pm [744.129374ms]
May  9 12:36:47.954: INFO: Got endpoints: latency-svc-r2zdw [749.260498ms]
May  9 12:36:48.012: INFO: Got endpoints: latency-svc-7b2ng [757.12898ms]
May  9 12:36:48.056: INFO: Got endpoints: latency-svc-sgtsk [750.00262ms]
May  9 12:36:48.106: INFO: Got endpoints: latency-svc-gb65v [750.73807ms]
May  9 12:36:48.155: INFO: Got endpoints: latency-svc-7vkxp [751.242119ms]
May  9 12:36:48.205: INFO: Got endpoints: latency-svc-ztp59 [747.925077ms]
May  9 12:36:48.256: INFO: Got endpoints: latency-svc-4kvrt [749.668296ms]
May  9 12:36:48.256: INFO: Latencies: [21.150157ms 31.128395ms 38.029717ms 52.486508ms 59.522206ms 71.566084ms 85.593968ms 97.911106ms 106.939458ms 128.892853ms 143.750125ms 152.093833ms 158.468573ms 164.604371ms 165.441802ms 168.449497ms 169.337228ms 170.860666ms 174.156438ms 174.913997ms 175.460289ms 176.415895ms 181.096132ms 184.355324ms 187.766615ms 188.766659ms 190.292833ms 194.841487ms 195.933456ms 196.050378ms 196.473713ms 196.845021ms 199.076327ms 201.071964ms 202.422234ms 202.974755ms 203.341633ms 204.407871ms 204.646767ms 218.093676ms 238.156265ms 278.594456ms 319.711288ms 357.985139ms 397.921244ms 523.382018ms 559.231355ms 594.743754ms 615.680133ms 633.499633ms 649.560044ms 666.572688ms 707.569403ms 734.356165ms 741.905058ms 743.819937ms 743.844912ms 744.129374ms 745.194255ms 745.198206ms 746.204535ms 746.315211ms 746.329375ms 746.397473ms 746.540904ms 746.624689ms 746.867829ms 746.999612ms 747.238362ms 747.254536ms 747.414902ms 747.545065ms 747.638851ms 747.678592ms 747.701822ms 747.798953ms 747.804333ms 747.897623ms 747.925077ms 748.05474ms 748.098545ms 748.172597ms 748.186444ms 748.216836ms 748.296754ms 748.419646ms 748.483467ms 748.599141ms 748.682172ms 748.703976ms 748.707409ms 748.707489ms 748.733098ms 748.841372ms 748.85327ms 748.856816ms 748.870031ms 748.910512ms 748.992513ms 749.054412ms 749.087967ms 749.13132ms 749.260498ms 749.297414ms 749.305209ms 749.315083ms 749.348917ms 749.373084ms 749.410097ms 749.454443ms 749.46589ms 749.496397ms 749.519043ms 749.525779ms 749.527311ms 749.551275ms 749.578034ms 749.579754ms 749.668296ms 749.693077ms 749.695376ms 749.778783ms 749.781367ms 749.805788ms 749.961163ms 749.967155ms 750.002197ms 750.00262ms 750.00903ms 750.021475ms 750.102183ms 750.118851ms 750.13917ms 750.195209ms 750.217108ms 750.28764ms 750.317033ms 750.334323ms 750.337797ms 750.358756ms 750.38998ms 750.403227ms 750.413272ms 750.43506ms 750.461801ms 750.462869ms 750.476931ms 750.489409ms 750.501502ms 750.509793ms 750.55081ms 750.641828ms 750.667614ms 750.687548ms 750.694728ms 750.73807ms 750.740623ms 750.746732ms 750.995017ms 751.062073ms 751.125882ms 751.234133ms 751.239022ms 751.242119ms 751.244529ms 751.248771ms 751.366535ms 751.474466ms 751.499348ms 751.600373ms 751.718024ms 751.787591ms 751.919946ms 751.946848ms 751.949529ms 751.972998ms 751.983008ms 752.126765ms 752.139147ms 752.155099ms 752.268615ms 752.371185ms 752.489394ms 752.590466ms 752.745467ms 752.945463ms 753.028156ms 753.351324ms 753.408008ms 753.434212ms 753.641395ms 754.044629ms 755.03729ms 755.272625ms 755.966946ms 756.539797ms 756.658323ms 756.751835ms 757.12898ms 757.433757ms]
May  9 12:36:48.256: INFO: 50 %ile: 749.087967ms
May  9 12:36:48.256: INFO: 90 %ile: 752.268615ms
May  9 12:36:48.256: INFO: 99 %ile: 757.12898ms
May  9 12:36:48.256: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
May  9 12:36:48.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4998" for this suite. 05/09/23 12:36:48.263
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":49,"skipped":989,"failed":0}
------------------------------
• [SLOW TEST] [9.755 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:36:38.514
    May  9 12:36:38.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename svc-latency 05/09/23 12:36:38.514
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:36:38.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:36:38.53
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    May  9 12:36:38.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-4998 05/09/23 12:36:38.532
    I0509 12:36:38.536688      24 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4998, replica count: 1
    I0509 12:36:39.587442      24 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 12:36:39.699: INFO: Created: latency-svc-lrb95
    May  9 12:36:39.705: INFO: Got endpoints: latency-svc-lrb95 [17.479511ms]
    May  9 12:36:39.719: INFO: Created: latency-svc-5wr2w
    May  9 12:36:39.726: INFO: Got endpoints: latency-svc-5wr2w [21.150157ms]
    May  9 12:36:39.729: INFO: Created: latency-svc-4t88v
    May  9 12:36:39.736: INFO: Got endpoints: latency-svc-4t88v [31.128395ms]
    May  9 12:36:39.739: INFO: Created: latency-svc-b7j25
    May  9 12:36:39.743: INFO: Got endpoints: latency-svc-b7j25 [38.029717ms]
    May  9 12:36:39.750: INFO: Created: latency-svc-8764z
    May  9 12:36:39.757: INFO: Got endpoints: latency-svc-8764z [52.486508ms]
    May  9 12:36:39.762: INFO: Created: latency-svc-cdnqn
    May  9 12:36:39.764: INFO: Got endpoints: latency-svc-cdnqn [59.522206ms]
    May  9 12:36:39.771: INFO: Created: latency-svc-pvlf8
    May  9 12:36:39.776: INFO: Got endpoints: latency-svc-pvlf8 [71.566084ms]
    May  9 12:36:39.783: INFO: Created: latency-svc-rpr8j
    May  9 12:36:39.790: INFO: Got endpoints: latency-svc-rpr8j [85.593968ms]
    May  9 12:36:39.794: INFO: Created: latency-svc-llhxr
    May  9 12:36:39.803: INFO: Got endpoints: latency-svc-llhxr [97.911106ms]
    May  9 12:36:39.806: INFO: Created: latency-svc-cmr9d
    May  9 12:36:39.812: INFO: Got endpoints: latency-svc-cmr9d [106.939458ms]
    May  9 12:36:39.831: INFO: Created: latency-svc-l5sz7
    May  9 12:36:39.834: INFO: Got endpoints: latency-svc-l5sz7 [128.892853ms]
    May  9 12:36:39.843: INFO: Created: latency-svc-dxbc8
    May  9 12:36:39.849: INFO: Got endpoints: latency-svc-dxbc8 [143.750125ms]
    May  9 12:36:39.851: INFO: Created: latency-svc-2zk58
    May  9 12:36:39.857: INFO: Got endpoints: latency-svc-2zk58 [152.093833ms]
    May  9 12:36:39.860: INFO: Created: latency-svc-lwvvn
    May  9 12:36:39.863: INFO: Got endpoints: latency-svc-lwvvn [158.468573ms]
    May  9 12:36:39.871: INFO: Created: latency-svc-8jpsk
    May  9 12:36:39.880: INFO: Got endpoints: latency-svc-8jpsk [174.913997ms]
    May  9 12:36:39.884: INFO: Created: latency-svc-wpdg8
    May  9 12:36:39.889: INFO: Got endpoints: latency-svc-wpdg8 [184.355324ms]
    May  9 12:36:39.895: INFO: Created: latency-svc-ct7hc
    May  9 12:36:39.902: INFO: Got endpoints: latency-svc-ct7hc [176.415895ms]
    May  9 12:36:39.907: INFO: Created: latency-svc-xxz6t
    May  9 12:36:39.917: INFO: Got endpoints: latency-svc-xxz6t [181.096132ms]
    May  9 12:36:39.939: INFO: Created: latency-svc-cn24f
    May  9 12:36:39.939: INFO: Got endpoints: latency-svc-cn24f [196.473713ms]
    May  9 12:36:39.952: INFO: Created: latency-svc-bkxr6
    May  9 12:36:39.956: INFO: Got endpoints: latency-svc-bkxr6 [199.076327ms]
    May  9 12:36:39.964: INFO: Created: latency-svc-jtq5p
    May  9 12:36:39.968: INFO: Got endpoints: latency-svc-jtq5p [203.341633ms]
    May  9 12:36:39.974: INFO: Created: latency-svc-s7kqc
    May  9 12:36:39.981: INFO: Got endpoints: latency-svc-s7kqc [204.407871ms]
    May  9 12:36:39.986: INFO: Created: latency-svc-4hdrl
    May  9 12:36:39.993: INFO: Got endpoints: latency-svc-4hdrl [202.422234ms]
    May  9 12:36:40.000: INFO: Created: latency-svc-4xwnn
    May  9 12:36:40.005: INFO: Created: latency-svc-98wcj
    May  9 12:36:40.006: INFO: Got endpoints: latency-svc-4xwnn [202.974755ms]
    May  9 12:36:40.013: INFO: Got endpoints: latency-svc-98wcj [201.071964ms]
    May  9 12:36:40.018: INFO: Created: latency-svc-kkzzf
    May  9 12:36:40.024: INFO: Got endpoints: latency-svc-kkzzf [190.292833ms]
    May  9 12:36:40.027: INFO: Created: latency-svc-6nn8r
    May  9 12:36:40.036: INFO: Got endpoints: latency-svc-6nn8r [187.766615ms]
    May  9 12:36:40.051: INFO: Created: latency-svc-z76dc
    May  9 12:36:40.053: INFO: Got endpoints: latency-svc-z76dc [196.050378ms]
    May  9 12:36:40.060: INFO: Created: latency-svc-p8jfg
    May  9 12:36:40.068: INFO: Got endpoints: latency-svc-p8jfg [204.646767ms]
    May  9 12:36:40.071: INFO: Created: latency-svc-lgqc9
    May  9 12:36:40.077: INFO: Got endpoints: latency-svc-lgqc9 [196.845021ms]
    May  9 12:36:40.081: INFO: Created: latency-svc-474gz
    May  9 12:36:40.085: INFO: Got endpoints: latency-svc-474gz [195.933456ms]
    May  9 12:36:40.091: INFO: Created: latency-svc-pk5qt
    May  9 12:36:40.097: INFO: Got endpoints: latency-svc-pk5qt [194.841487ms]
    May  9 12:36:40.100: INFO: Created: latency-svc-65wt9
    May  9 12:36:40.106: INFO: Got endpoints: latency-svc-65wt9 [188.766659ms]
    May  9 12:36:40.128: INFO: Created: latency-svc-9j44x
    May  9 12:36:40.132: INFO: Got endpoints: latency-svc-9j44x [164.604371ms]
    May  9 12:36:40.139: INFO: Created: latency-svc-t6nt2
    May  9 12:36:40.146: INFO: Got endpoints: latency-svc-t6nt2 [165.441802ms]
    May  9 12:36:40.161: INFO: Created: latency-svc-94q8x
    May  9 12:36:40.161: INFO: Got endpoints: latency-svc-94q8x [168.449497ms]
    May  9 12:36:40.170: INFO: Created: latency-svc-8vcxd
    May  9 12:36:40.177: INFO: Got endpoints: latency-svc-8vcxd [170.860666ms]
    May  9 12:36:40.181: INFO: Created: latency-svc-kh7dt
    May  9 12:36:40.188: INFO: Got endpoints: latency-svc-kh7dt [175.460289ms]
    May  9 12:36:40.191: INFO: Created: latency-svc-2r2s9
    May  9 12:36:40.198: INFO: Got endpoints: latency-svc-2r2s9 [174.156438ms]
    May  9 12:36:40.202: INFO: Created: latency-svc-6ptd6
    May  9 12:36:40.206: INFO: Got endpoints: latency-svc-6ptd6 [169.337228ms]
    May  9 12:36:40.214: INFO: Created: latency-svc-bmr9q
    May  9 12:36:40.221: INFO: Created: latency-svc-zjt5t
    May  9 12:36:40.234: INFO: Created: latency-svc-7zxxb
    May  9 12:36:40.241: INFO: Created: latency-svc-g9ssq
    May  9 12:36:40.250: INFO: Created: latency-svc-p7qzz
    May  9 12:36:40.271: INFO: Got endpoints: latency-svc-bmr9q [218.093676ms]
    May  9 12:36:40.275: INFO: Created: latency-svc-j4jq8
    May  9 12:36:40.276: INFO: Created: latency-svc-kj9lj
    May  9 12:36:40.276: INFO: Created: latency-svc-rnhfd
    May  9 12:36:40.282: INFO: Created: latency-svc-dnjb5
    May  9 12:36:40.291: INFO: Created: latency-svc-fj5tg
    May  9 12:36:40.299: INFO: Created: latency-svc-7cr8j
    May  9 12:36:40.306: INFO: Got endpoints: latency-svc-zjt5t [238.156265ms]
    May  9 12:36:40.309: INFO: Created: latency-svc-bjrw9
    May  9 12:36:40.319: INFO: Created: latency-svc-dwpv4
    May  9 12:36:40.328: INFO: Created: latency-svc-pvgvg
    May  9 12:36:40.337: INFO: Created: latency-svc-2tz7d
    May  9 12:36:40.355: INFO: Got endpoints: latency-svc-7zxxb [278.594456ms]
    May  9 12:36:40.357: INFO: Created: latency-svc-tstzw
    May  9 12:36:40.383: INFO: Created: latency-svc-9f877
    May  9 12:36:40.392: INFO: Created: latency-svc-fddbf
    May  9 12:36:40.405: INFO: Got endpoints: latency-svc-g9ssq [319.711288ms]
    May  9 12:36:40.416: INFO: Created: latency-svc-knqwb
    May  9 12:36:40.455: INFO: Got endpoints: latency-svc-p7qzz [357.985139ms]
    May  9 12:36:40.467: INFO: Created: latency-svc-49hcd
    May  9 12:36:40.504: INFO: Got endpoints: latency-svc-j4jq8 [397.921244ms]
    May  9 12:36:40.515: INFO: Created: latency-svc-2x567
    May  9 12:36:40.555: INFO: Got endpoints: latency-svc-rnhfd [615.680133ms]
    May  9 12:36:40.567: INFO: Created: latency-svc-6xpm2
    May  9 12:36:40.606: INFO: Got endpoints: latency-svc-kj9lj [649.560044ms]
    May  9 12:36:40.617: INFO: Created: latency-svc-p8zld
    May  9 12:36:40.656: INFO: Got endpoints: latency-svc-dnjb5 [523.382018ms]
    May  9 12:36:40.668: INFO: Created: latency-svc-tjmf6
    May  9 12:36:40.706: INFO: Got endpoints: latency-svc-fj5tg [559.231355ms]
    May  9 12:36:40.718: INFO: Created: latency-svc-dcgdj
    May  9 12:36:40.756: INFO: Got endpoints: latency-svc-7cr8j [594.743754ms]
    May  9 12:36:40.766: INFO: Created: latency-svc-m8wtb
    May  9 12:36:40.810: INFO: Got endpoints: latency-svc-bjrw9 [633.499633ms]
    May  9 12:36:40.823: INFO: Created: latency-svc-w5n8f
    May  9 12:36:40.855: INFO: Got endpoints: latency-svc-dwpv4 [666.572688ms]
    May  9 12:36:40.868: INFO: Created: latency-svc-5vjzl
    May  9 12:36:40.906: INFO: Got endpoints: latency-svc-pvgvg [707.569403ms]
    May  9 12:36:40.919: INFO: Created: latency-svc-fzdhk
    May  9 12:36:40.956: INFO: Got endpoints: latency-svc-2tz7d [749.781367ms]
    May  9 12:36:40.968: INFO: Created: latency-svc-dqdpf
    May  9 12:36:41.005: INFO: Got endpoints: latency-svc-tstzw [734.356165ms]
    May  9 12:36:41.024: INFO: Created: latency-svc-qmp5v
    May  9 12:36:41.056: INFO: Got endpoints: latency-svc-9f877 [749.778783ms]
    May  9 12:36:41.068: INFO: Created: latency-svc-6d75r
    May  9 12:36:41.105: INFO: Got endpoints: latency-svc-fddbf [749.579754ms]
    May  9 12:36:41.118: INFO: Created: latency-svc-qwgjl
    May  9 12:36:41.155: INFO: Got endpoints: latency-svc-knqwb [750.217108ms]
    May  9 12:36:41.167: INFO: Created: latency-svc-4s77f
    May  9 12:36:41.205: INFO: Got endpoints: latency-svc-49hcd [750.118851ms]
    May  9 12:36:41.219: INFO: Created: latency-svc-rkbqt
    May  9 12:36:41.255: INFO: Got endpoints: latency-svc-2x567 [751.248771ms]
    May  9 12:36:41.267: INFO: Created: latency-svc-cspw4
    May  9 12:36:41.305: INFO: Got endpoints: latency-svc-6xpm2 [750.413272ms]
    May  9 12:36:41.322: INFO: Created: latency-svc-hmdmt
    May  9 12:36:41.356: INFO: Got endpoints: latency-svc-p8zld [749.961163ms]
    May  9 12:36:41.368: INFO: Created: latency-svc-2n8xb
    May  9 12:36:41.403: INFO: Got endpoints: latency-svc-tjmf6 [747.545065ms]
    May  9 12:36:41.415: INFO: Created: latency-svc-58ngd
    May  9 12:36:41.455: INFO: Got endpoints: latency-svc-dcgdj [749.315083ms]
    May  9 12:36:41.466: INFO: Created: latency-svc-2zqnw
    May  9 12:36:41.505: INFO: Got endpoints: latency-svc-m8wtb [749.348917ms]
    May  9 12:36:41.518: INFO: Created: latency-svc-dxgkh
    May  9 12:36:41.554: INFO: Got endpoints: latency-svc-w5n8f [743.819937ms]
    May  9 12:36:41.565: INFO: Created: latency-svc-6wznr
    May  9 12:36:41.606: INFO: Got endpoints: latency-svc-5vjzl [750.489409ms]
    May  9 12:36:41.617: INFO: Created: latency-svc-jjsts
    May  9 12:36:41.658: INFO: Got endpoints: latency-svc-fzdhk [752.268615ms]
    May  9 12:36:41.669: INFO: Created: latency-svc-cb2nc
    May  9 12:36:41.706: INFO: Got endpoints: latency-svc-dqdpf [750.102183ms]
    May  9 12:36:41.720: INFO: Created: latency-svc-r454k
    May  9 12:36:41.756: INFO: Got endpoints: latency-svc-qmp5v [750.462869ms]
    May  9 12:36:41.773: INFO: Created: latency-svc-jnvjg
    May  9 12:36:41.805: INFO: Got endpoints: latency-svc-6d75r [748.707409ms]
    May  9 12:36:41.816: INFO: Created: latency-svc-jqj9w
    May  9 12:36:41.856: INFO: Got endpoints: latency-svc-qwgjl [751.125882ms]
    May  9 12:36:41.872: INFO: Created: latency-svc-fbtwv
    May  9 12:36:41.905: INFO: Got endpoints: latency-svc-4s77f [750.317033ms]
    May  9 12:36:41.918: INFO: Created: latency-svc-8v9bf
    May  9 12:36:41.954: INFO: Got endpoints: latency-svc-rkbqt [748.216836ms]
    May  9 12:36:41.965: INFO: Created: latency-svc-zkr86
    May  9 12:36:42.005: INFO: Got endpoints: latency-svc-cspw4 [749.551275ms]
    May  9 12:36:42.016: INFO: Created: latency-svc-b5jxr
    May  9 12:36:42.054: INFO: Got endpoints: latency-svc-hmdmt [748.733098ms]
    May  9 12:36:42.067: INFO: Created: latency-svc-8zqrj
    May  9 12:36:42.104: INFO: Got endpoints: latency-svc-2n8xb [747.678592ms]
    May  9 12:36:42.114: INFO: Created: latency-svc-4smsm
    May  9 12:36:42.153: INFO: Got endpoints: latency-svc-58ngd [749.297414ms]
    May  9 12:36:42.167: INFO: Created: latency-svc-4vws8
    May  9 12:36:42.204: INFO: Got endpoints: latency-svc-2zqnw [748.856816ms]
    May  9 12:36:42.230: INFO: Created: latency-svc-pxzkc
    May  9 12:36:42.254: INFO: Got endpoints: latency-svc-dxgkh [748.992513ms]
    May  9 12:36:42.266: INFO: Created: latency-svc-dg4dr
    May  9 12:36:42.309: INFO: Got endpoints: latency-svc-6wznr [755.272625ms]
    May  9 12:36:42.322: INFO: Created: latency-svc-5hwzr
    May  9 12:36:42.353: INFO: Got endpoints: latency-svc-jjsts [747.414902ms]
    May  9 12:36:42.368: INFO: Created: latency-svc-q68np
    May  9 12:36:42.405: INFO: Got endpoints: latency-svc-cb2nc [746.624689ms]
    May  9 12:36:42.415: INFO: Created: latency-svc-p29lh
    May  9 12:36:42.456: INFO: Got endpoints: latency-svc-r454k [750.55081ms]
    May  9 12:36:42.468: INFO: Created: latency-svc-x9scr
    May  9 12:36:42.506: INFO: Got endpoints: latency-svc-jnvjg [750.43506ms]
    May  9 12:36:42.517: INFO: Created: latency-svc-mkxrp
    May  9 12:36:42.554: INFO: Got endpoints: latency-svc-jqj9w [748.870031ms]
    May  9 12:36:42.565: INFO: Created: latency-svc-lwhxk
    May  9 12:36:42.613: INFO: Got endpoints: latency-svc-fbtwv [756.658323ms]
    May  9 12:36:42.628: INFO: Created: latency-svc-ktljg
    May  9 12:36:42.655: INFO: Got endpoints: latency-svc-8v9bf [749.13132ms]
    May  9 12:36:42.666: INFO: Created: latency-svc-rks65
    May  9 12:36:42.707: INFO: Got endpoints: latency-svc-zkr86 [753.408008ms]
    May  9 12:36:42.724: INFO: Created: latency-svc-zk9xm
    May  9 12:36:42.755: INFO: Got endpoints: latency-svc-b5jxr [750.00903ms]
    May  9 12:36:42.769: INFO: Created: latency-svc-nlchg
    May  9 12:36:42.805: INFO: Got endpoints: latency-svc-8zqrj [750.476931ms]
    May  9 12:36:42.817: INFO: Created: latency-svc-f47zt
    May  9 12:36:42.856: INFO: Got endpoints: latency-svc-4smsm [751.972998ms]
    May  9 12:36:42.869: INFO: Created: latency-svc-bcjfn
    May  9 12:36:42.905: INFO: Got endpoints: latency-svc-4vws8 [752.745467ms]
    May  9 12:36:42.917: INFO: Created: latency-svc-blnkl
    May  9 12:36:42.956: INFO: Got endpoints: latency-svc-pxzkc [752.371185ms]
    May  9 12:36:42.968: INFO: Created: latency-svc-sr6kx
    May  9 12:36:43.005: INFO: Got endpoints: latency-svc-dg4dr [750.509793ms]
    May  9 12:36:43.017: INFO: Created: latency-svc-wznlb
    May  9 12:36:43.056: INFO: Got endpoints: latency-svc-5hwzr [746.315211ms]
    May  9 12:36:43.067: INFO: Created: latency-svc-fcswd
    May  9 12:36:43.104: INFO: Got endpoints: latency-svc-q68np [751.234133ms]
    May  9 12:36:43.116: INFO: Created: latency-svc-g69l8
    May  9 12:36:43.155: INFO: Got endpoints: latency-svc-p29lh [750.740623ms]
    May  9 12:36:43.168: INFO: Created: latency-svc-55qqn
    May  9 12:36:43.208: INFO: Got endpoints: latency-svc-x9scr [751.239022ms]
    May  9 12:36:43.219: INFO: Created: latency-svc-l955q
    May  9 12:36:43.255: INFO: Got endpoints: latency-svc-mkxrp [748.910512ms]
    May  9 12:36:43.269: INFO: Created: latency-svc-nkkgs
    May  9 12:36:43.305: INFO: Got endpoints: latency-svc-lwhxk [751.366535ms]
    May  9 12:36:43.319: INFO: Created: latency-svc-jrlqf
    May  9 12:36:43.359: INFO: Got endpoints: latency-svc-ktljg [746.540904ms]
    May  9 12:36:43.371: INFO: Created: latency-svc-gjhtj
    May  9 12:36:43.404: INFO: Got endpoints: latency-svc-rks65 [749.496397ms]
    May  9 12:36:43.417: INFO: Created: latency-svc-4hwv6
    May  9 12:36:43.457: INFO: Got endpoints: latency-svc-zk9xm [750.334323ms]
    May  9 12:36:43.474: INFO: Created: latency-svc-sqdc6
    May  9 12:36:43.507: INFO: Got endpoints: latency-svc-nlchg [752.590466ms]
    May  9 12:36:43.524: INFO: Created: latency-svc-q7jzr
    May  9 12:36:43.558: INFO: Got endpoints: latency-svc-f47zt [753.641395ms]
    May  9 12:36:43.597: INFO: Created: latency-svc-scw9m
    May  9 12:36:43.604: INFO: Got endpoints: latency-svc-bcjfn [748.841372ms]
    May  9 12:36:43.617: INFO: Created: latency-svc-s5zfk
    May  9 12:36:43.655: INFO: Got endpoints: latency-svc-blnkl [749.578034ms]
    May  9 12:36:43.668: INFO: Created: latency-svc-6djjp
    May  9 12:36:43.706: INFO: Got endpoints: latency-svc-sr6kx [750.195209ms]
    May  9 12:36:43.720: INFO: Created: latency-svc-phphf
    May  9 12:36:43.758: INFO: Got endpoints: latency-svc-wznlb [752.945463ms]
    May  9 12:36:43.770: INFO: Created: latency-svc-bcss6
    May  9 12:36:43.805: INFO: Got endpoints: latency-svc-fcswd [749.519043ms]
    May  9 12:36:43.817: INFO: Created: latency-svc-zv4f4
    May  9 12:36:43.856: INFO: Got endpoints: latency-svc-g69l8 [752.126765ms]
    May  9 12:36:43.870: INFO: Created: latency-svc-jjf7r
    May  9 12:36:43.912: INFO: Got endpoints: latency-svc-55qqn [756.751835ms]
    May  9 12:36:43.923: INFO: Created: latency-svc-qdj5f
    May  9 12:36:43.957: INFO: Got endpoints: latency-svc-l955q [749.373084ms]
    May  9 12:36:43.971: INFO: Created: latency-svc-6k2xj
    May  9 12:36:44.005: INFO: Got endpoints: latency-svc-nkkgs [749.805788ms]
    May  9 12:36:44.023: INFO: Created: latency-svc-m5l6c
    May  9 12:36:44.055: INFO: Got endpoints: latency-svc-jrlqf [750.38998ms]
    May  9 12:36:44.067: INFO: Created: latency-svc-qm24v
    May  9 12:36:44.106: INFO: Got endpoints: latency-svc-gjhtj [746.204535ms]
    May  9 12:36:44.118: INFO: Created: latency-svc-x7rxf
    May  9 12:36:44.155: INFO: Got endpoints: latency-svc-4hwv6 [750.687548ms]
    May  9 12:36:44.167: INFO: Created: latency-svc-hpzxv
    May  9 12:36:44.206: INFO: Got endpoints: latency-svc-sqdc6 [748.682172ms]
    May  9 12:36:44.218: INFO: Created: latency-svc-grj24
    May  9 12:36:44.254: INFO: Got endpoints: latency-svc-q7jzr [746.397473ms]
    May  9 12:36:44.264: INFO: Created: latency-svc-qkws4
    May  9 12:36:44.305: INFO: Got endpoints: latency-svc-scw9m [746.329375ms]
    May  9 12:36:44.316: INFO: Created: latency-svc-ggml4
    May  9 12:36:44.354: INFO: Got endpoints: latency-svc-s5zfk [749.527311ms]
    May  9 12:36:44.365: INFO: Created: latency-svc-kdkpw
    May  9 12:36:44.405: INFO: Got endpoints: latency-svc-6djjp [749.525779ms]
    May  9 12:36:44.415: INFO: Created: latency-svc-hfslm
    May  9 12:36:44.456: INFO: Got endpoints: latency-svc-phphf [749.454443ms]
    May  9 12:36:44.466: INFO: Created: latency-svc-w2gqf
    May  9 12:36:44.505: INFO: Got endpoints: latency-svc-bcss6 [746.999612ms]
    May  9 12:36:44.517: INFO: Created: latency-svc-hx95g
    May  9 12:36:44.559: INFO: Got endpoints: latency-svc-zv4f4 [754.044629ms]
    May  9 12:36:44.570: INFO: Created: latency-svc-dc8mm
    May  9 12:36:44.604: INFO: Got endpoints: latency-svc-jjf7r [748.05474ms]
    May  9 12:36:44.617: INFO: Created: latency-svc-8tqsv
    May  9 12:36:44.654: INFO: Got endpoints: latency-svc-qdj5f [741.905058ms]
    May  9 12:36:44.674: INFO: Created: latency-svc-wgwxz
    May  9 12:36:44.705: INFO: Got endpoints: latency-svc-6k2xj [747.897623ms]
    May  9 12:36:44.720: INFO: Created: latency-svc-mk5kf
    May  9 12:36:44.756: INFO: Got endpoints: latency-svc-m5l6c [751.062073ms]
    May  9 12:36:44.767: INFO: Created: latency-svc-svb7h
    May  9 12:36:44.804: INFO: Got endpoints: latency-svc-qm24v [748.419646ms]
    May  9 12:36:44.816: INFO: Created: latency-svc-kn59q
    May  9 12:36:44.854: INFO: Got endpoints: latency-svc-x7rxf [748.483467ms]
    May  9 12:36:44.865: INFO: Created: latency-svc-lx6bs
    May  9 12:36:44.908: INFO: Got endpoints: latency-svc-hpzxv [753.028156ms]
    May  9 12:36:44.920: INFO: Created: latency-svc-42c9k
    May  9 12:36:44.955: INFO: Got endpoints: latency-svc-grj24 [748.85327ms]
    May  9 12:36:44.966: INFO: Created: latency-svc-bzpvm
    May  9 12:36:45.006: INFO: Got endpoints: latency-svc-qkws4 [751.919946ms]
    May  9 12:36:45.016: INFO: Created: latency-svc-z9jks
    May  9 12:36:45.053: INFO: Got endpoints: latency-svc-ggml4 [748.703976ms]
    May  9 12:36:45.065: INFO: Created: latency-svc-7cwth
    May  9 12:36:45.105: INFO: Got endpoints: latency-svc-kdkpw [750.667614ms]
    May  9 12:36:45.115: INFO: Created: latency-svc-fwgtq
    May  9 12:36:45.156: INFO: Got endpoints: latency-svc-hfslm [751.499348ms]
    May  9 12:36:45.168: INFO: Created: latency-svc-gljwt
    May  9 12:36:45.208: INFO: Got endpoints: latency-svc-w2gqf [751.718024ms]
    May  9 12:36:45.219: INFO: Created: latency-svc-ncm2p
    May  9 12:36:45.255: INFO: Got endpoints: latency-svc-hx95g [750.337797ms]
    May  9 12:36:45.267: INFO: Created: latency-svc-lgtsc
    May  9 12:36:45.305: INFO: Got endpoints: latency-svc-dc8mm [745.198206ms]
    May  9 12:36:45.318: INFO: Created: latency-svc-khk7g
    May  9 12:36:45.355: INFO: Got endpoints: latency-svc-8tqsv [750.694728ms]
    May  9 12:36:45.366: INFO: Created: latency-svc-gjl85
    May  9 12:36:45.404: INFO: Got endpoints: latency-svc-wgwxz [750.13917ms]
    May  9 12:36:45.420: INFO: Created: latency-svc-p5tc5
    May  9 12:36:45.455: INFO: Got endpoints: latency-svc-mk5kf [750.021475ms]
    May  9 12:36:45.468: INFO: Created: latency-svc-hzbqx
    May  9 12:36:45.504: INFO: Got endpoints: latency-svc-svb7h [748.172597ms]
    May  9 12:36:45.515: INFO: Created: latency-svc-kdckl
    May  9 12:36:45.556: INFO: Got endpoints: latency-svc-kn59q [752.139147ms]
    May  9 12:36:45.568: INFO: Created: latency-svc-4szht
    May  9 12:36:45.604: INFO: Got endpoints: latency-svc-lx6bs [749.46589ms]
    May  9 12:36:45.615: INFO: Created: latency-svc-m24fh
    May  9 12:36:45.656: INFO: Got endpoints: latency-svc-42c9k [747.701822ms]
    May  9 12:36:45.667: INFO: Created: latency-svc-tjdn6
    May  9 12:36:45.704: INFO: Got endpoints: latency-svc-bzpvm [748.599141ms]
    May  9 12:36:45.720: INFO: Created: latency-svc-ftpwm
    May  9 12:36:45.759: INFO: Got endpoints: latency-svc-z9jks [753.351324ms]
    May  9 12:36:45.779: INFO: Created: latency-svc-j4j7h
    May  9 12:36:45.805: INFO: Got endpoints: latency-svc-7cwth [751.600373ms]
    May  9 12:36:45.818: INFO: Created: latency-svc-j8ch4
    May  9 12:36:45.855: INFO: Got endpoints: latency-svc-fwgtq [749.967155ms]
    May  9 12:36:45.866: INFO: Created: latency-svc-zl2bq
    May  9 12:36:45.906: INFO: Got endpoints: latency-svc-gljwt [749.695376ms]
    May  9 12:36:45.919: INFO: Created: latency-svc-j2t2j
    May  9 12:36:45.955: INFO: Got endpoints: latency-svc-ncm2p [747.238362ms]
    May  9 12:36:45.968: INFO: Created: latency-svc-84qxs
    May  9 12:36:46.008: INFO: Got endpoints: latency-svc-lgtsc [752.489394ms]
    May  9 12:36:46.023: INFO: Created: latency-svc-t8kjt
    May  9 12:36:46.058: INFO: Got endpoints: latency-svc-khk7g [753.434212ms]
    May  9 12:36:46.071: INFO: Created: latency-svc-mhfmt
    May  9 12:36:46.106: INFO: Got endpoints: latency-svc-gjl85 [750.641828ms]
    May  9 12:36:46.116: INFO: Created: latency-svc-4pd8b
    May  9 12:36:46.155: INFO: Got endpoints: latency-svc-p5tc5 [750.501502ms]
    May  9 12:36:46.171: INFO: Created: latency-svc-vwz82
    May  9 12:36:46.204: INFO: Got endpoints: latency-svc-hzbqx [749.410097ms]
    May  9 12:36:46.215: INFO: Created: latency-svc-flg67
    May  9 12:36:46.256: INFO: Got endpoints: latency-svc-kdckl [751.949529ms]
    May  9 12:36:46.269: INFO: Created: latency-svc-w279q
    May  9 12:36:46.303: INFO: Got endpoints: latency-svc-4szht [746.867829ms]
    May  9 12:36:46.315: INFO: Created: latency-svc-7cb6p
    May  9 12:36:46.356: INFO: Got endpoints: latency-svc-m24fh [752.155099ms]
    May  9 12:36:46.368: INFO: Created: latency-svc-8f4r4
    May  9 12:36:46.404: INFO: Got endpoints: latency-svc-tjdn6 [748.707489ms]
    May  9 12:36:46.416: INFO: Created: latency-svc-7dwhv
    May  9 12:36:46.455: INFO: Got endpoints: latency-svc-ftpwm [751.787591ms]
    May  9 12:36:46.467: INFO: Created: latency-svc-4k4c8
    May  9 12:36:46.504: INFO: Got endpoints: latency-svc-j4j7h [745.194255ms]
    May  9 12:36:46.523: INFO: Created: latency-svc-62sl4
    May  9 12:36:46.556: INFO: Got endpoints: latency-svc-j8ch4 [750.403227ms]
    May  9 12:36:46.569: INFO: Created: latency-svc-tdzk6
    May  9 12:36:46.606: INFO: Got endpoints: latency-svc-zl2bq [751.474466ms]
    May  9 12:36:46.619: INFO: Created: latency-svc-mh6zv
    May  9 12:36:46.655: INFO: Got endpoints: latency-svc-j2t2j [749.305209ms]
    May  9 12:36:46.667: INFO: Created: latency-svc-nzgs7
    May  9 12:36:46.706: INFO: Got endpoints: latency-svc-84qxs [750.746732ms]
    May  9 12:36:46.719: INFO: Created: latency-svc-kxnqb
    May  9 12:36:46.756: INFO: Got endpoints: latency-svc-t8kjt [747.804333ms]
    May  9 12:36:46.769: INFO: Created: latency-svc-5wj8h
    May  9 12:36:46.805: INFO: Got endpoints: latency-svc-mhfmt [747.254536ms]
    May  9 12:36:46.823: INFO: Created: latency-svc-rdlp7
    May  9 12:36:46.857: INFO: Got endpoints: latency-svc-4pd8b [751.244529ms]
    May  9 12:36:46.876: INFO: Created: latency-svc-44dfk
    May  9 12:36:46.912: INFO: Got endpoints: latency-svc-vwz82 [757.433757ms]
    May  9 12:36:46.927: INFO: Created: latency-svc-pr7m7
    May  9 12:36:46.954: INFO: Got endpoints: latency-svc-flg67 [749.693077ms]
    May  9 12:36:46.973: INFO: Created: latency-svc-xzbqq
    May  9 12:36:47.004: INFO: Got endpoints: latency-svc-w279q [747.798953ms]
    May  9 12:36:47.023: INFO: Created: latency-svc-w6ltf
    May  9 12:36:47.059: INFO: Got endpoints: latency-svc-7cb6p [755.966946ms]
    May  9 12:36:47.073: INFO: Created: latency-svc-s7djj
    May  9 12:36:47.106: INFO: Got endpoints: latency-svc-8f4r4 [750.461801ms]
    May  9 12:36:47.117: INFO: Created: latency-svc-jl6rb
    May  9 12:36:47.161: INFO: Got endpoints: latency-svc-7dwhv [756.539797ms]
    May  9 12:36:47.173: INFO: Created: latency-svc-b54pm
    May  9 12:36:47.205: INFO: Got endpoints: latency-svc-4k4c8 [749.087967ms]
    May  9 12:36:47.217: INFO: Created: latency-svc-r2zdw
    May  9 12:36:47.255: INFO: Got endpoints: latency-svc-62sl4 [750.28764ms]
    May  9 12:36:47.266: INFO: Created: latency-svc-7b2ng
    May  9 12:36:47.306: INFO: Got endpoints: latency-svc-tdzk6 [750.002197ms]
    May  9 12:36:47.317: INFO: Created: latency-svc-sgtsk
    May  9 12:36:47.355: INFO: Got endpoints: latency-svc-mh6zv [749.054412ms]
    May  9 12:36:47.374: INFO: Created: latency-svc-gb65v
    May  9 12:36:47.403: INFO: Got endpoints: latency-svc-nzgs7 [748.098545ms]
    May  9 12:36:47.421: INFO: Created: latency-svc-7vkxp
    May  9 12:36:47.458: INFO: Got endpoints: latency-svc-kxnqb [751.946848ms]
    May  9 12:36:47.476: INFO: Created: latency-svc-ztp59
    May  9 12:36:47.506: INFO: Got endpoints: latency-svc-5wj8h [750.358756ms]
    May  9 12:36:47.517: INFO: Created: latency-svc-4kvrt
    May  9 12:36:47.556: INFO: Got endpoints: latency-svc-rdlp7 [750.995017ms]
    May  9 12:36:47.605: INFO: Got endpoints: latency-svc-44dfk [747.638851ms]
    May  9 12:36:47.656: INFO: Got endpoints: latency-svc-pr7m7 [743.844912ms]
    May  9 12:36:47.709: INFO: Got endpoints: latency-svc-xzbqq [755.03729ms]
    May  9 12:36:47.756: INFO: Got endpoints: latency-svc-w6ltf [751.983008ms]
    May  9 12:36:47.807: INFO: Got endpoints: latency-svc-s7djj [748.186444ms]
    May  9 12:36:47.855: INFO: Got endpoints: latency-svc-jl6rb [748.296754ms]
    May  9 12:36:47.905: INFO: Got endpoints: latency-svc-b54pm [744.129374ms]
    May  9 12:36:47.954: INFO: Got endpoints: latency-svc-r2zdw [749.260498ms]
    May  9 12:36:48.012: INFO: Got endpoints: latency-svc-7b2ng [757.12898ms]
    May  9 12:36:48.056: INFO: Got endpoints: latency-svc-sgtsk [750.00262ms]
    May  9 12:36:48.106: INFO: Got endpoints: latency-svc-gb65v [750.73807ms]
    May  9 12:36:48.155: INFO: Got endpoints: latency-svc-7vkxp [751.242119ms]
    May  9 12:36:48.205: INFO: Got endpoints: latency-svc-ztp59 [747.925077ms]
    May  9 12:36:48.256: INFO: Got endpoints: latency-svc-4kvrt [749.668296ms]
    May  9 12:36:48.256: INFO: Latencies: [21.150157ms 31.128395ms 38.029717ms 52.486508ms 59.522206ms 71.566084ms 85.593968ms 97.911106ms 106.939458ms 128.892853ms 143.750125ms 152.093833ms 158.468573ms 164.604371ms 165.441802ms 168.449497ms 169.337228ms 170.860666ms 174.156438ms 174.913997ms 175.460289ms 176.415895ms 181.096132ms 184.355324ms 187.766615ms 188.766659ms 190.292833ms 194.841487ms 195.933456ms 196.050378ms 196.473713ms 196.845021ms 199.076327ms 201.071964ms 202.422234ms 202.974755ms 203.341633ms 204.407871ms 204.646767ms 218.093676ms 238.156265ms 278.594456ms 319.711288ms 357.985139ms 397.921244ms 523.382018ms 559.231355ms 594.743754ms 615.680133ms 633.499633ms 649.560044ms 666.572688ms 707.569403ms 734.356165ms 741.905058ms 743.819937ms 743.844912ms 744.129374ms 745.194255ms 745.198206ms 746.204535ms 746.315211ms 746.329375ms 746.397473ms 746.540904ms 746.624689ms 746.867829ms 746.999612ms 747.238362ms 747.254536ms 747.414902ms 747.545065ms 747.638851ms 747.678592ms 747.701822ms 747.798953ms 747.804333ms 747.897623ms 747.925077ms 748.05474ms 748.098545ms 748.172597ms 748.186444ms 748.216836ms 748.296754ms 748.419646ms 748.483467ms 748.599141ms 748.682172ms 748.703976ms 748.707409ms 748.707489ms 748.733098ms 748.841372ms 748.85327ms 748.856816ms 748.870031ms 748.910512ms 748.992513ms 749.054412ms 749.087967ms 749.13132ms 749.260498ms 749.297414ms 749.305209ms 749.315083ms 749.348917ms 749.373084ms 749.410097ms 749.454443ms 749.46589ms 749.496397ms 749.519043ms 749.525779ms 749.527311ms 749.551275ms 749.578034ms 749.579754ms 749.668296ms 749.693077ms 749.695376ms 749.778783ms 749.781367ms 749.805788ms 749.961163ms 749.967155ms 750.002197ms 750.00262ms 750.00903ms 750.021475ms 750.102183ms 750.118851ms 750.13917ms 750.195209ms 750.217108ms 750.28764ms 750.317033ms 750.334323ms 750.337797ms 750.358756ms 750.38998ms 750.403227ms 750.413272ms 750.43506ms 750.461801ms 750.462869ms 750.476931ms 750.489409ms 750.501502ms 750.509793ms 750.55081ms 750.641828ms 750.667614ms 750.687548ms 750.694728ms 750.73807ms 750.740623ms 750.746732ms 750.995017ms 751.062073ms 751.125882ms 751.234133ms 751.239022ms 751.242119ms 751.244529ms 751.248771ms 751.366535ms 751.474466ms 751.499348ms 751.600373ms 751.718024ms 751.787591ms 751.919946ms 751.946848ms 751.949529ms 751.972998ms 751.983008ms 752.126765ms 752.139147ms 752.155099ms 752.268615ms 752.371185ms 752.489394ms 752.590466ms 752.745467ms 752.945463ms 753.028156ms 753.351324ms 753.408008ms 753.434212ms 753.641395ms 754.044629ms 755.03729ms 755.272625ms 755.966946ms 756.539797ms 756.658323ms 756.751835ms 757.12898ms 757.433757ms]
    May  9 12:36:48.256: INFO: 50 %ile: 749.087967ms
    May  9 12:36:48.256: INFO: 90 %ile: 752.268615ms
    May  9 12:36:48.256: INFO: 99 %ile: 757.12898ms
    May  9 12:36:48.256: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    May  9 12:36:48.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-4998" for this suite. 05/09/23 12:36:48.263
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:36:48.269
May  9 12:36:48.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename subpath 05/09/23 12:36:48.27
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:36:48.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:36:48.284
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/09/23 12:36:48.285
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-t6ns 05/09/23 12:36:48.295
STEP: Creating a pod to test atomic-volume-subpath 05/09/23 12:36:48.295
May  9 12:36:48.301: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-t6ns" in namespace "subpath-3520" to be "Succeeded or Failed"
May  9 12:36:48.303: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Pending", Reason="", readiness=false. Elapsed: 1.678043ms
May  9 12:36:50.307: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 2.005582717s
May  9 12:36:52.305: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 4.004416757s
May  9 12:36:54.309: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 6.007617762s
May  9 12:36:56.309: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 8.007817097s
May  9 12:36:58.306: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 10.004773876s
May  9 12:37:00.307: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 12.00607316s
May  9 12:37:02.306: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 14.005042083s
May  9 12:37:04.306: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 16.004461285s
May  9 12:37:06.307: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 18.005564647s
May  9 12:37:08.306: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 20.005330696s
May  9 12:37:10.306: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=false. Elapsed: 22.004452512s
May  9 12:37:12.307: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00561768s
STEP: Saw pod success 05/09/23 12:37:12.307
May  9 12:37:12.307: INFO: Pod "pod-subpath-test-configmap-t6ns" satisfied condition "Succeeded or Failed"
May  9 12:37:12.309: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-subpath-test-configmap-t6ns container test-container-subpath-configmap-t6ns: <nil>
STEP: delete the pod 05/09/23 12:37:12.322
May  9 12:37:12.331: INFO: Waiting for pod pod-subpath-test-configmap-t6ns to disappear
May  9 12:37:12.333: INFO: Pod pod-subpath-test-configmap-t6ns no longer exists
STEP: Deleting pod pod-subpath-test-configmap-t6ns 05/09/23 12:37:12.333
May  9 12:37:12.333: INFO: Deleting pod "pod-subpath-test-configmap-t6ns" in namespace "subpath-3520"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  9 12:37:12.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3520" for this suite. 05/09/23 12:37:12.338
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":50,"skipped":992,"failed":0}
------------------------------
• [SLOW TEST] [24.074 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:36:48.269
    May  9 12:36:48.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename subpath 05/09/23 12:36:48.27
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:36:48.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:36:48.284
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/09/23 12:36:48.285
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-t6ns 05/09/23 12:36:48.295
    STEP: Creating a pod to test atomic-volume-subpath 05/09/23 12:36:48.295
    May  9 12:36:48.301: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-t6ns" in namespace "subpath-3520" to be "Succeeded or Failed"
    May  9 12:36:48.303: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Pending", Reason="", readiness=false. Elapsed: 1.678043ms
    May  9 12:36:50.307: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 2.005582717s
    May  9 12:36:52.305: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 4.004416757s
    May  9 12:36:54.309: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 6.007617762s
    May  9 12:36:56.309: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 8.007817097s
    May  9 12:36:58.306: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 10.004773876s
    May  9 12:37:00.307: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 12.00607316s
    May  9 12:37:02.306: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 14.005042083s
    May  9 12:37:04.306: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 16.004461285s
    May  9 12:37:06.307: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 18.005564647s
    May  9 12:37:08.306: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=true. Elapsed: 20.005330696s
    May  9 12:37:10.306: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Running", Reason="", readiness=false. Elapsed: 22.004452512s
    May  9 12:37:12.307: INFO: Pod "pod-subpath-test-configmap-t6ns": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00561768s
    STEP: Saw pod success 05/09/23 12:37:12.307
    May  9 12:37:12.307: INFO: Pod "pod-subpath-test-configmap-t6ns" satisfied condition "Succeeded or Failed"
    May  9 12:37:12.309: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-subpath-test-configmap-t6ns container test-container-subpath-configmap-t6ns: <nil>
    STEP: delete the pod 05/09/23 12:37:12.322
    May  9 12:37:12.331: INFO: Waiting for pod pod-subpath-test-configmap-t6ns to disappear
    May  9 12:37:12.333: INFO: Pod pod-subpath-test-configmap-t6ns no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-t6ns 05/09/23 12:37:12.333
    May  9 12:37:12.333: INFO: Deleting pod "pod-subpath-test-configmap-t6ns" in namespace "subpath-3520"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  9 12:37:12.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3520" for this suite. 05/09/23 12:37:12.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:37:12.343
May  9 12:37:12.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 12:37:12.344
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:12.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:12.356
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/09/23 12:37:12.357
May  9 12:37:12.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2869 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May  9 12:37:12.414: INFO: stderr: ""
May  9 12:37:12.414: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 05/09/23 12:37:12.414
May  9 12:37:12.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2869 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
May  9 12:37:13.019: INFO: stderr: ""
May  9 12:37:13.019: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/09/23 12:37:13.019
May  9 12:37:13.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2869 delete pods e2e-test-httpd-pod'
May  9 12:37:15.628: INFO: stderr: ""
May  9 12:37:15.628: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 12:37:15.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2869" for this suite. 05/09/23 12:37:15.632
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":51,"skipped":1002,"failed":0}
------------------------------
• [3.294 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:37:12.343
    May  9 12:37:12.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 12:37:12.344
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:12.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:12.356
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/09/23 12:37:12.357
    May  9 12:37:12.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2869 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May  9 12:37:12.414: INFO: stderr: ""
    May  9 12:37:12.414: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 05/09/23 12:37:12.414
    May  9 12:37:12.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2869 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    May  9 12:37:13.019: INFO: stderr: ""
    May  9 12:37:13.019: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/09/23 12:37:13.019
    May  9 12:37:13.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2869 delete pods e2e-test-httpd-pod'
    May  9 12:37:15.628: INFO: stderr: ""
    May  9 12:37:15.628: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 12:37:15.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2869" for this suite. 05/09/23 12:37:15.632
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:37:15.637
May  9 12:37:15.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename csistoragecapacity 05/09/23 12:37:15.638
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:15.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:15.652
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 05/09/23 12:37:15.653
STEP: getting /apis/storage.k8s.io 05/09/23 12:37:15.654
STEP: getting /apis/storage.k8s.io/v1 05/09/23 12:37:15.655
STEP: creating 05/09/23 12:37:15.656
STEP: watching 05/09/23 12:37:15.668
May  9 12:37:15.668: INFO: starting watch
STEP: getting 05/09/23 12:37:15.672
STEP: listing in namespace 05/09/23 12:37:15.674
STEP: listing across namespaces 05/09/23 12:37:15.676
STEP: patching 05/09/23 12:37:15.678
STEP: updating 05/09/23 12:37:15.682
May  9 12:37:15.687: INFO: waiting for watch events with expected annotations in namespace
May  9 12:37:15.687: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 05/09/23 12:37:15.687
STEP: deleting a collection 05/09/23 12:37:15.695
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
May  9 12:37:15.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-3974" for this suite. 05/09/23 12:37:15.717
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":52,"skipped":1003,"failed":0}
------------------------------
• [0.084 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:37:15.637
    May  9 12:37:15.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename csistoragecapacity 05/09/23 12:37:15.638
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:15.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:15.652
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 05/09/23 12:37:15.653
    STEP: getting /apis/storage.k8s.io 05/09/23 12:37:15.654
    STEP: getting /apis/storage.k8s.io/v1 05/09/23 12:37:15.655
    STEP: creating 05/09/23 12:37:15.656
    STEP: watching 05/09/23 12:37:15.668
    May  9 12:37:15.668: INFO: starting watch
    STEP: getting 05/09/23 12:37:15.672
    STEP: listing in namespace 05/09/23 12:37:15.674
    STEP: listing across namespaces 05/09/23 12:37:15.676
    STEP: patching 05/09/23 12:37:15.678
    STEP: updating 05/09/23 12:37:15.682
    May  9 12:37:15.687: INFO: waiting for watch events with expected annotations in namespace
    May  9 12:37:15.687: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 05/09/23 12:37:15.687
    STEP: deleting a collection 05/09/23 12:37:15.695
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    May  9 12:37:15.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-3974" for this suite. 05/09/23 12:37:15.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:37:15.722
May  9 12:37:15.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 12:37:15.722
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:15.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:15.737
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-9073 05/09/23 12:37:15.739
STEP: creating replication controller nodeport-test in namespace services-9073 05/09/23 12:37:15.759
I0509 12:37:15.767383      24 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9073, replica count: 2
I0509 12:37:18.819524      24 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 12:37:18.819: INFO: Creating new exec pod
May  9 12:37:18.827: INFO: Waiting up to 5m0s for pod "execpodph5g2" in namespace "services-9073" to be "running"
May  9 12:37:18.834: INFO: Pod "execpodph5g2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913498ms
May  9 12:37:20.838: INFO: Pod "execpodph5g2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010811271s
May  9 12:37:20.838: INFO: Pod "execpodph5g2" satisfied condition "running"
May  9 12:37:21.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9073 exec execpodph5g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May  9 12:37:21.943: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  9 12:37:21.943: INFO: stdout: "nodeport-test-nw2lp"
May  9 12:37:21.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9073 exec execpodph5g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.193.36 80'
May  9 12:37:22.043: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.193.36 80\nConnection to 10.102.193.36 80 port [tcp/http] succeeded!\n"
May  9 12:37:22.043: INFO: stdout: "nodeport-test-thxhw"
May  9 12:37:22.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9073 exec execpodph5g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.64 30006'
May  9 12:37:22.135: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.64 30006\nConnection to 192.168.1.64 30006 port [tcp/*] succeeded!\n"
May  9 12:37:22.135: INFO: stdout: "nodeport-test-thxhw"
May  9 12:37:22.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9073 exec execpodph5g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.73 30006'
May  9 12:37:22.234: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.73 30006\nConnection to 192.168.1.73 30006 port [tcp/*] succeeded!\n"
May  9 12:37:22.234: INFO: stdout: "nodeport-test-nw2lp"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 12:37:22.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9073" for this suite. 05/09/23 12:37:22.238
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":53,"skipped":1013,"failed":0}
------------------------------
• [SLOW TEST] [6.523 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:37:15.722
    May  9 12:37:15.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 12:37:15.722
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:15.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:15.737
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-9073 05/09/23 12:37:15.739
    STEP: creating replication controller nodeport-test in namespace services-9073 05/09/23 12:37:15.759
    I0509 12:37:15.767383      24 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9073, replica count: 2
    I0509 12:37:18.819524      24 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 12:37:18.819: INFO: Creating new exec pod
    May  9 12:37:18.827: INFO: Waiting up to 5m0s for pod "execpodph5g2" in namespace "services-9073" to be "running"
    May  9 12:37:18.834: INFO: Pod "execpodph5g2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913498ms
    May  9 12:37:20.838: INFO: Pod "execpodph5g2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010811271s
    May  9 12:37:20.838: INFO: Pod "execpodph5g2" satisfied condition "running"
    May  9 12:37:21.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9073 exec execpodph5g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    May  9 12:37:21.943: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    May  9 12:37:21.943: INFO: stdout: "nodeport-test-nw2lp"
    May  9 12:37:21.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9073 exec execpodph5g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.193.36 80'
    May  9 12:37:22.043: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.193.36 80\nConnection to 10.102.193.36 80 port [tcp/http] succeeded!\n"
    May  9 12:37:22.043: INFO: stdout: "nodeport-test-thxhw"
    May  9 12:37:22.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9073 exec execpodph5g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.64 30006'
    May  9 12:37:22.135: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.64 30006\nConnection to 192.168.1.64 30006 port [tcp/*] succeeded!\n"
    May  9 12:37:22.135: INFO: stdout: "nodeport-test-thxhw"
    May  9 12:37:22.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9073 exec execpodph5g2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.73 30006'
    May  9 12:37:22.234: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.73 30006\nConnection to 192.168.1.73 30006 port [tcp/*] succeeded!\n"
    May  9 12:37:22.234: INFO: stdout: "nodeport-test-nw2lp"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 12:37:22.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9073" for this suite. 05/09/23 12:37:22.238
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:37:22.245
May  9 12:37:22.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 12:37:22.246
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:22.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:22.26
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-5dfc27e7-8a38-4e77-bb40-ef74dbdcfcde 05/09/23 12:37:22.266
STEP: Creating configMap with name cm-test-opt-upd-2cf7ff81-8b5d-4ae0-adc1-7796766f826e 05/09/23 12:37:22.269
STEP: Creating the pod 05/09/23 12:37:22.273
May  9 12:37:22.280: INFO: Waiting up to 5m0s for pod "pod-configmaps-626a453b-0b8b-4811-b0eb-396036f90156" in namespace "configmap-7102" to be "running and ready"
May  9 12:37:22.282: INFO: Pod "pod-configmaps-626a453b-0b8b-4811-b0eb-396036f90156": Phase="Pending", Reason="", readiness=false. Elapsed: 1.97552ms
May  9 12:37:22.282: INFO: The phase of Pod pod-configmaps-626a453b-0b8b-4811-b0eb-396036f90156 is Pending, waiting for it to be Running (with Ready = true)
May  9 12:37:24.286: INFO: Pod "pod-configmaps-626a453b-0b8b-4811-b0eb-396036f90156": Phase="Running", Reason="", readiness=true. Elapsed: 2.006198086s
May  9 12:37:24.286: INFO: The phase of Pod pod-configmaps-626a453b-0b8b-4811-b0eb-396036f90156 is Running (Ready = true)
May  9 12:37:24.286: INFO: Pod "pod-configmaps-626a453b-0b8b-4811-b0eb-396036f90156" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-5dfc27e7-8a38-4e77-bb40-ef74dbdcfcde 05/09/23 12:37:24.32
STEP: Updating configmap cm-test-opt-upd-2cf7ff81-8b5d-4ae0-adc1-7796766f826e 05/09/23 12:37:24.324
STEP: Creating configMap with name cm-test-opt-create-eae83eef-8977-43d7-94b2-acb015a05dcf 05/09/23 12:37:24.329
STEP: waiting to observe update in volume 05/09/23 12:37:24.334
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  9 12:37:26.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7102" for this suite. 05/09/23 12:37:26.355
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":54,"skipped":1024,"failed":0}
------------------------------
• [4.116 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:37:22.245
    May  9 12:37:22.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 12:37:22.246
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:22.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:22.26
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-5dfc27e7-8a38-4e77-bb40-ef74dbdcfcde 05/09/23 12:37:22.266
    STEP: Creating configMap with name cm-test-opt-upd-2cf7ff81-8b5d-4ae0-adc1-7796766f826e 05/09/23 12:37:22.269
    STEP: Creating the pod 05/09/23 12:37:22.273
    May  9 12:37:22.280: INFO: Waiting up to 5m0s for pod "pod-configmaps-626a453b-0b8b-4811-b0eb-396036f90156" in namespace "configmap-7102" to be "running and ready"
    May  9 12:37:22.282: INFO: Pod "pod-configmaps-626a453b-0b8b-4811-b0eb-396036f90156": Phase="Pending", Reason="", readiness=false. Elapsed: 1.97552ms
    May  9 12:37:22.282: INFO: The phase of Pod pod-configmaps-626a453b-0b8b-4811-b0eb-396036f90156 is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:37:24.286: INFO: Pod "pod-configmaps-626a453b-0b8b-4811-b0eb-396036f90156": Phase="Running", Reason="", readiness=true. Elapsed: 2.006198086s
    May  9 12:37:24.286: INFO: The phase of Pod pod-configmaps-626a453b-0b8b-4811-b0eb-396036f90156 is Running (Ready = true)
    May  9 12:37:24.286: INFO: Pod "pod-configmaps-626a453b-0b8b-4811-b0eb-396036f90156" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-5dfc27e7-8a38-4e77-bb40-ef74dbdcfcde 05/09/23 12:37:24.32
    STEP: Updating configmap cm-test-opt-upd-2cf7ff81-8b5d-4ae0-adc1-7796766f826e 05/09/23 12:37:24.324
    STEP: Creating configMap with name cm-test-opt-create-eae83eef-8977-43d7-94b2-acb015a05dcf 05/09/23 12:37:24.329
    STEP: waiting to observe update in volume 05/09/23 12:37:24.334
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 12:37:26.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7102" for this suite. 05/09/23 12:37:26.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:37:26.361
May  9 12:37:26.361: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename containers 05/09/23 12:37:26.362
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:26.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:26.376
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 05/09/23 12:37:26.377
May  9 12:37:26.384: INFO: Waiting up to 5m0s for pod "client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39" in namespace "containers-6440" to be "Succeeded or Failed"
May  9 12:37:26.387: INFO: Pod "client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39": Phase="Pending", Reason="", readiness=false. Elapsed: 3.271385ms
May  9 12:37:28.390: INFO: Pod "client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005999402s
May  9 12:37:30.391: INFO: Pod "client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007340501s
STEP: Saw pod success 05/09/23 12:37:30.391
May  9 12:37:30.391: INFO: Pod "client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39" satisfied condition "Succeeded or Failed"
May  9 12:37:30.393: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 12:37:30.398
May  9 12:37:30.408: INFO: Waiting for pod client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39 to disappear
May  9 12:37:30.410: INFO: Pod client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May  9 12:37:30.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6440" for this suite. 05/09/23 12:37:30.413
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":55,"skipped":1037,"failed":0}
------------------------------
• [4.056 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:37:26.361
    May  9 12:37:26.361: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename containers 05/09/23 12:37:26.362
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:26.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:26.376
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 05/09/23 12:37:26.377
    May  9 12:37:26.384: INFO: Waiting up to 5m0s for pod "client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39" in namespace "containers-6440" to be "Succeeded or Failed"
    May  9 12:37:26.387: INFO: Pod "client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39": Phase="Pending", Reason="", readiness=false. Elapsed: 3.271385ms
    May  9 12:37:28.390: INFO: Pod "client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005999402s
    May  9 12:37:30.391: INFO: Pod "client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007340501s
    STEP: Saw pod success 05/09/23 12:37:30.391
    May  9 12:37:30.391: INFO: Pod "client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39" satisfied condition "Succeeded or Failed"
    May  9 12:37:30.393: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 12:37:30.398
    May  9 12:37:30.408: INFO: Waiting for pod client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39 to disappear
    May  9 12:37:30.410: INFO: Pod client-containers-20fd2413-2137-4e71-a5d6-29cac4bd9e39 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May  9 12:37:30.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-6440" for this suite. 05/09/23 12:37:30.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:37:30.418
May  9 12:37:30.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename watch 05/09/23 12:37:30.419
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:30.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:30.433
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 05/09/23 12:37:30.434
STEP: starting a background goroutine to produce watch events 05/09/23 12:37:30.437
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/09/23 12:37:30.437
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  9 12:37:33.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6601" for this suite. 05/09/23 12:37:33.273
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":56,"skipped":1059,"failed":0}
------------------------------
• [2.906 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:37:30.418
    May  9 12:37:30.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename watch 05/09/23 12:37:30.419
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:30.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:30.433
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 05/09/23 12:37:30.434
    STEP: starting a background goroutine to produce watch events 05/09/23 12:37:30.437
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/09/23 12:37:30.437
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  9 12:37:33.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6601" for this suite. 05/09/23 12:37:33.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:37:33.325
May  9 12:37:33.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubelet-test 05/09/23 12:37:33.325
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:33.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:33.338
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  9 12:37:37.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1230" for this suite. 05/09/23 12:37:37.353
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":57,"skipped":1102,"failed":0}
------------------------------
• [4.032 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:37:33.325
    May  9 12:37:33.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubelet-test 05/09/23 12:37:33.325
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:33.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:33.338
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  9 12:37:37.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-1230" for this suite. 05/09/23 12:37:37.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:37:37.358
May  9 12:37:37.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 12:37:37.358
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:37.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:37.372
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/09/23 12:37:37.373
May  9 12:37:37.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/09/23 12:37:48.595
May  9 12:37:48.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 12:37:52.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 12:38:01.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6513" for this suite. 05/09/23 12:38:01.94
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":58,"skipped":1109,"failed":0}
------------------------------
• [SLOW TEST] [24.587 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:37:37.358
    May  9 12:37:37.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 12:37:37.358
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:37:37.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:37:37.372
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/09/23 12:37:37.373
    May  9 12:37:37.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/09/23 12:37:48.595
    May  9 12:37:48.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 12:37:52.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 12:38:01.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6513" for this suite. 05/09/23 12:38:01.94
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:38:01.945
May  9 12:38:01.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename dns 05/09/23 12:38:01.946
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:38:01.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:38:01.962
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 05/09/23 12:38:01.963
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 72.114.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.114.72_udp@PTR;check="$$(dig +tcp +noall +answer +search 72.114.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.114.72_tcp@PTR;sleep 1; done
 05/09/23 12:38:01.98
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 72.114.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.114.72_udp@PTR;check="$$(dig +tcp +noall +answer +search 72.114.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.114.72_tcp@PTR;sleep 1; done
 05/09/23 12:38:01.98
STEP: creating a pod to probe DNS 05/09/23 12:38:01.98
STEP: submitting the pod to kubernetes 05/09/23 12:38:01.98
May  9 12:38:01.989: INFO: Waiting up to 15m0s for pod "dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271" in namespace "dns-3961" to be "running"
May  9 12:38:01.992: INFO: Pod "dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271": Phase="Pending", Reason="", readiness=false. Elapsed: 2.595731ms
May  9 12:38:03.995: INFO: Pod "dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005670193s
May  9 12:38:05.996: INFO: Pod "dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006257212s
May  9 12:38:07.996: INFO: Pod "dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271": Phase="Running", Reason="", readiness=true. Elapsed: 6.006535907s
May  9 12:38:07.996: INFO: Pod "dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271" satisfied condition "running"
STEP: retrieving the pod 05/09/23 12:38:07.996
STEP: looking for the results for each expected name from probers 05/09/23 12:38:07.998
May  9 12:38:08.001: INFO: Unable to read wheezy_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:08.004: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:08.005: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:08.007: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:08.019: INFO: Unable to read jessie_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:08.021: INFO: Unable to read jessie_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:08.023: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:08.026: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:08.034: INFO: Lookups using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 failed for: [wheezy_udp@dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_udp@dns-test-service.dns-3961.svc.cluster.local jessie_tcp@dns-test-service.dns-3961.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local]

May  9 12:38:13.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:13.039: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:13.042: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:13.044: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:13.055: INFO: Unable to read jessie_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:13.057: INFO: Unable to read jessie_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:13.059: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:13.061: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:13.069: INFO: Lookups using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 failed for: [wheezy_udp@dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_udp@dns-test-service.dns-3961.svc.cluster.local jessie_tcp@dns-test-service.dns-3961.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local]

May  9 12:38:18.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:18.039: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:18.041: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:18.043: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:18.052: INFO: Unable to read jessie_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:18.054: INFO: Unable to read jessie_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:18.056: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:18.058: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:18.066: INFO: Lookups using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 failed for: [wheezy_udp@dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_udp@dns-test-service.dns-3961.svc.cluster.local jessie_tcp@dns-test-service.dns-3961.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local]

May  9 12:38:23.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:23.040: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:23.042: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:23.045: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:23.056: INFO: Unable to read jessie_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:23.058: INFO: Unable to read jessie_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:23.060: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:23.062: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:23.070: INFO: Lookups using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 failed for: [wheezy_udp@dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_udp@dns-test-service.dns-3961.svc.cluster.local jessie_tcp@dns-test-service.dns-3961.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local]

May  9 12:38:28.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:28.040: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:28.043: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:28.045: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:28.057: INFO: Unable to read jessie_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:28.059: INFO: Unable to read jessie_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:28.061: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:28.063: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:28.070: INFO: Lookups using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 failed for: [wheezy_udp@dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_udp@dns-test-service.dns-3961.svc.cluster.local jessie_tcp@dns-test-service.dns-3961.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local]

May  9 12:38:33.038: INFO: Unable to read wheezy_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:33.040: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:33.042: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:33.044: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:33.054: INFO: Unable to read jessie_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:33.056: INFO: Unable to read jessie_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:33.058: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:33.060: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
May  9 12:38:33.068: INFO: Lookups using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 failed for: [wheezy_udp@dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_udp@dns-test-service.dns-3961.svc.cluster.local jessie_tcp@dns-test-service.dns-3961.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local]

May  9 12:38:38.066: INFO: DNS probes using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 succeeded

STEP: deleting the pod 05/09/23 12:38:38.066
STEP: deleting the test service 05/09/23 12:38:38.076
STEP: deleting the test headless service 05/09/23 12:38:38.107
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  9 12:38:38.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3961" for this suite. 05/09/23 12:38:38.125
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":59,"skipped":1109,"failed":0}
------------------------------
• [SLOW TEST] [36.184 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:38:01.945
    May  9 12:38:01.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename dns 05/09/23 12:38:01.946
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:38:01.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:38:01.962
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 05/09/23 12:38:01.963
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 72.114.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.114.72_udp@PTR;check="$$(dig +tcp +noall +answer +search 72.114.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.114.72_tcp@PTR;sleep 1; done
     05/09/23 12:38:01.98
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3961.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 72.114.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.114.72_udp@PTR;check="$$(dig +tcp +noall +answer +search 72.114.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.114.72_tcp@PTR;sleep 1; done
     05/09/23 12:38:01.98
    STEP: creating a pod to probe DNS 05/09/23 12:38:01.98
    STEP: submitting the pod to kubernetes 05/09/23 12:38:01.98
    May  9 12:38:01.989: INFO: Waiting up to 15m0s for pod "dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271" in namespace "dns-3961" to be "running"
    May  9 12:38:01.992: INFO: Pod "dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271": Phase="Pending", Reason="", readiness=false. Elapsed: 2.595731ms
    May  9 12:38:03.995: INFO: Pod "dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005670193s
    May  9 12:38:05.996: INFO: Pod "dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006257212s
    May  9 12:38:07.996: INFO: Pod "dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271": Phase="Running", Reason="", readiness=true. Elapsed: 6.006535907s
    May  9 12:38:07.996: INFO: Pod "dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 12:38:07.996
    STEP: looking for the results for each expected name from probers 05/09/23 12:38:07.998
    May  9 12:38:08.001: INFO: Unable to read wheezy_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:08.004: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:08.005: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:08.007: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:08.019: INFO: Unable to read jessie_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:08.021: INFO: Unable to read jessie_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:08.023: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:08.026: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:08.034: INFO: Lookups using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 failed for: [wheezy_udp@dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_udp@dns-test-service.dns-3961.svc.cluster.local jessie_tcp@dns-test-service.dns-3961.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local]

    May  9 12:38:13.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:13.039: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:13.042: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:13.044: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:13.055: INFO: Unable to read jessie_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:13.057: INFO: Unable to read jessie_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:13.059: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:13.061: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:13.069: INFO: Lookups using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 failed for: [wheezy_udp@dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_udp@dns-test-service.dns-3961.svc.cluster.local jessie_tcp@dns-test-service.dns-3961.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local]

    May  9 12:38:18.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:18.039: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:18.041: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:18.043: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:18.052: INFO: Unable to read jessie_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:18.054: INFO: Unable to read jessie_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:18.056: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:18.058: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:18.066: INFO: Lookups using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 failed for: [wheezy_udp@dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_udp@dns-test-service.dns-3961.svc.cluster.local jessie_tcp@dns-test-service.dns-3961.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local]

    May  9 12:38:23.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:23.040: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:23.042: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:23.045: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:23.056: INFO: Unable to read jessie_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:23.058: INFO: Unable to read jessie_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:23.060: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:23.062: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:23.070: INFO: Lookups using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 failed for: [wheezy_udp@dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_udp@dns-test-service.dns-3961.svc.cluster.local jessie_tcp@dns-test-service.dns-3961.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local]

    May  9 12:38:28.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:28.040: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:28.043: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:28.045: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:28.057: INFO: Unable to read jessie_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:28.059: INFO: Unable to read jessie_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:28.061: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:28.063: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:28.070: INFO: Lookups using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 failed for: [wheezy_udp@dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_udp@dns-test-service.dns-3961.svc.cluster.local jessie_tcp@dns-test-service.dns-3961.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local]

    May  9 12:38:33.038: INFO: Unable to read wheezy_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:33.040: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:33.042: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:33.044: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:33.054: INFO: Unable to read jessie_udp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:33.056: INFO: Unable to read jessie_tcp@dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:33.058: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:33.060: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local from pod dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271: the server could not find the requested resource (get pods dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271)
    May  9 12:38:33.068: INFO: Lookups using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 failed for: [wheezy_udp@dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@dns-test-service.dns-3961.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_udp@dns-test-service.dns-3961.svc.cluster.local jessie_tcp@dns-test-service.dns-3961.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3961.svc.cluster.local]

    May  9 12:38:38.066: INFO: DNS probes using dns-3961/dns-test-11c507dc-fe2c-43c6-a1f3-e22b7aff8271 succeeded

    STEP: deleting the pod 05/09/23 12:38:38.066
    STEP: deleting the test service 05/09/23 12:38:38.076
    STEP: deleting the test headless service 05/09/23 12:38:38.107
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  9 12:38:38.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3961" for this suite. 05/09/23 12:38:38.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:38:38.131
May  9 12:38:38.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename resourcequota 05/09/23 12:38:38.131
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:38:38.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:38:38.151
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 05/09/23 12:38:55.156
STEP: Creating a ResourceQuota 05/09/23 12:39:00.159
STEP: Ensuring resource quota status is calculated 05/09/23 12:39:00.165
STEP: Creating a ConfigMap 05/09/23 12:39:02.168
STEP: Ensuring resource quota status captures configMap creation 05/09/23 12:39:02.177
STEP: Deleting a ConfigMap 05/09/23 12:39:04.181
STEP: Ensuring resource quota status released usage 05/09/23 12:39:04.186
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  9 12:39:06.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1381" for this suite. 05/09/23 12:39:06.193
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":60,"skipped":1156,"failed":0}
------------------------------
• [SLOW TEST] [28.069 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:38:38.131
    May  9 12:38:38.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename resourcequota 05/09/23 12:38:38.131
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:38:38.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:38:38.151
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 05/09/23 12:38:55.156
    STEP: Creating a ResourceQuota 05/09/23 12:39:00.159
    STEP: Ensuring resource quota status is calculated 05/09/23 12:39:00.165
    STEP: Creating a ConfigMap 05/09/23 12:39:02.168
    STEP: Ensuring resource quota status captures configMap creation 05/09/23 12:39:02.177
    STEP: Deleting a ConfigMap 05/09/23 12:39:04.181
    STEP: Ensuring resource quota status released usage 05/09/23 12:39:04.186
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  9 12:39:06.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1381" for this suite. 05/09/23 12:39:06.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:39:06.201
May  9 12:39:06.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 12:39:06.201
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:39:06.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:39:06.215
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 05/09/23 12:39:06.216
May  9 12:39:06.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
May  9 12:39:06.276: INFO: stderr: ""
May  9 12:39:06.276: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 05/09/23 12:39:06.276
May  9 12:39:06.277: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May  9 12:39:06.277: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3615" to be "running and ready, or succeeded"
May  9 12:39:06.279: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.9103ms
May  9 12:39:06.279: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'cl-gks-cncf-ix1-md-0-48ljh' to be 'Running' but was 'Pending'
May  9 12:39:08.282: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.00572918s
May  9 12:39:08.282: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May  9 12:39:08.282: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 05/09/23 12:39:08.282
May  9 12:39:08.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 logs logs-generator logs-generator'
May  9 12:39:08.344: INFO: stderr: ""
May  9 12:39:08.344: INFO: stdout: "I0509 12:39:06.878340       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/z2lj 452\nI0509 12:39:07.078487       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/fbfq 423\nI0509 12:39:07.278966       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/gfw 597\nI0509 12:39:07.479280       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/jdf 557\nI0509 12:39:07.678509       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/d5ql 262\nI0509 12:39:07.878829       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/b76t 422\nI0509 12:39:08.079139       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/st4 513\nI0509 12:39:08.278409       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/6xg 372\n"
STEP: limiting log lines 05/09/23 12:39:08.344
May  9 12:39:08.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 logs logs-generator logs-generator --tail=1'
May  9 12:39:08.396: INFO: stderr: ""
May  9 12:39:08.396: INFO: stdout: "I0509 12:39:08.278409       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/6xg 372\n"
May  9 12:39:08.396: INFO: got output "I0509 12:39:08.278409       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/6xg 372\n"
STEP: limiting log bytes 05/09/23 12:39:08.396
May  9 12:39:08.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 logs logs-generator logs-generator --limit-bytes=1'
May  9 12:39:08.457: INFO: stderr: ""
May  9 12:39:08.457: INFO: stdout: "I"
May  9 12:39:08.457: INFO: got output "I"
STEP: exposing timestamps 05/09/23 12:39:08.457
May  9 12:39:08.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 logs logs-generator logs-generator --tail=1 --timestamps'
May  9 12:39:08.508: INFO: stderr: ""
May  9 12:39:08.508: INFO: stdout: "2023-05-09T12:39:08.478819896Z I0509 12:39:08.478726       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/wlb7 231\n"
May  9 12:39:08.508: INFO: got output "2023-05-09T12:39:08.478819896Z I0509 12:39:08.478726       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/wlb7 231\n"
STEP: restricting to a time range 05/09/23 12:39:08.508
May  9 12:39:11.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 logs logs-generator logs-generator --since=1s'
May  9 12:39:11.067: INFO: stderr: ""
May  9 12:39:11.067: INFO: stdout: "I0509 12:39:10.278649       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/x25 442\nI0509 12:39:10.478979       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/n7zm 391\nI0509 12:39:10.679293       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/2ms2 572\nI0509 12:39:10.878542       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/kzzq 490\nI0509 12:39:11.078855       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/znl 473\n"
May  9 12:39:11.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 logs logs-generator logs-generator --since=24h'
May  9 12:39:11.121: INFO: stderr: ""
May  9 12:39:11.121: INFO: stdout: "I0509 12:39:06.878340       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/z2lj 452\nI0509 12:39:07.078487       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/fbfq 423\nI0509 12:39:07.278966       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/gfw 597\nI0509 12:39:07.479280       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/jdf 557\nI0509 12:39:07.678509       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/d5ql 262\nI0509 12:39:07.878829       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/b76t 422\nI0509 12:39:08.079139       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/st4 513\nI0509 12:39:08.278409       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/6xg 372\nI0509 12:39:08.478726       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/wlb7 231\nI0509 12:39:08.679059       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/dcz 216\nI0509 12:39:08.879385       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/8dzb 531\nI0509 12:39:09.078733       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/wpcs 362\nI0509 12:39:09.279065       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/67lx 457\nI0509 12:39:09.479395       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/z997 364\nI0509 12:39:09.678703       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/kgl 385\nI0509 12:39:09.879014       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/9r5 385\nI0509 12:39:10.079342       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/g7w 451\nI0509 12:39:10.278649       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/x25 442\nI0509 12:39:10.478979       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/n7zm 391\nI0509 12:39:10.679293       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/2ms2 572\nI0509 12:39:10.878542       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/kzzq 490\nI0509 12:39:11.078855       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/znl 473\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
May  9 12:39:11.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 delete pod logs-generator'
May  9 12:39:11.844: INFO: stderr: ""
May  9 12:39:11.844: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 12:39:11.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3615" for this suite. 05/09/23 12:39:11.848
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":61,"skipped":1195,"failed":0}
------------------------------
• [SLOW TEST] [5.653 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:39:06.201
    May  9 12:39:06.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 12:39:06.201
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:39:06.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:39:06.215
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 05/09/23 12:39:06.216
    May  9 12:39:06.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    May  9 12:39:06.276: INFO: stderr: ""
    May  9 12:39:06.276: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 05/09/23 12:39:06.276
    May  9 12:39:06.277: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    May  9 12:39:06.277: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3615" to be "running and ready, or succeeded"
    May  9 12:39:06.279: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.9103ms
    May  9 12:39:06.279: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'cl-gks-cncf-ix1-md-0-48ljh' to be 'Running' but was 'Pending'
    May  9 12:39:08.282: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.00572918s
    May  9 12:39:08.282: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    May  9 12:39:08.282: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 05/09/23 12:39:08.282
    May  9 12:39:08.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 logs logs-generator logs-generator'
    May  9 12:39:08.344: INFO: stderr: ""
    May  9 12:39:08.344: INFO: stdout: "I0509 12:39:06.878340       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/z2lj 452\nI0509 12:39:07.078487       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/fbfq 423\nI0509 12:39:07.278966       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/gfw 597\nI0509 12:39:07.479280       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/jdf 557\nI0509 12:39:07.678509       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/d5ql 262\nI0509 12:39:07.878829       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/b76t 422\nI0509 12:39:08.079139       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/st4 513\nI0509 12:39:08.278409       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/6xg 372\n"
    STEP: limiting log lines 05/09/23 12:39:08.344
    May  9 12:39:08.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 logs logs-generator logs-generator --tail=1'
    May  9 12:39:08.396: INFO: stderr: ""
    May  9 12:39:08.396: INFO: stdout: "I0509 12:39:08.278409       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/6xg 372\n"
    May  9 12:39:08.396: INFO: got output "I0509 12:39:08.278409       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/6xg 372\n"
    STEP: limiting log bytes 05/09/23 12:39:08.396
    May  9 12:39:08.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 logs logs-generator logs-generator --limit-bytes=1'
    May  9 12:39:08.457: INFO: stderr: ""
    May  9 12:39:08.457: INFO: stdout: "I"
    May  9 12:39:08.457: INFO: got output "I"
    STEP: exposing timestamps 05/09/23 12:39:08.457
    May  9 12:39:08.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 logs logs-generator logs-generator --tail=1 --timestamps'
    May  9 12:39:08.508: INFO: stderr: ""
    May  9 12:39:08.508: INFO: stdout: "2023-05-09T12:39:08.478819896Z I0509 12:39:08.478726       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/wlb7 231\n"
    May  9 12:39:08.508: INFO: got output "2023-05-09T12:39:08.478819896Z I0509 12:39:08.478726       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/wlb7 231\n"
    STEP: restricting to a time range 05/09/23 12:39:08.508
    May  9 12:39:11.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 logs logs-generator logs-generator --since=1s'
    May  9 12:39:11.067: INFO: stderr: ""
    May  9 12:39:11.067: INFO: stdout: "I0509 12:39:10.278649       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/x25 442\nI0509 12:39:10.478979       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/n7zm 391\nI0509 12:39:10.679293       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/2ms2 572\nI0509 12:39:10.878542       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/kzzq 490\nI0509 12:39:11.078855       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/znl 473\n"
    May  9 12:39:11.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 logs logs-generator logs-generator --since=24h'
    May  9 12:39:11.121: INFO: stderr: ""
    May  9 12:39:11.121: INFO: stdout: "I0509 12:39:06.878340       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/z2lj 452\nI0509 12:39:07.078487       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/fbfq 423\nI0509 12:39:07.278966       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/gfw 597\nI0509 12:39:07.479280       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/jdf 557\nI0509 12:39:07.678509       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/d5ql 262\nI0509 12:39:07.878829       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/b76t 422\nI0509 12:39:08.079139       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/st4 513\nI0509 12:39:08.278409       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/6xg 372\nI0509 12:39:08.478726       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/wlb7 231\nI0509 12:39:08.679059       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/dcz 216\nI0509 12:39:08.879385       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/8dzb 531\nI0509 12:39:09.078733       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/wpcs 362\nI0509 12:39:09.279065       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/67lx 457\nI0509 12:39:09.479395       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/z997 364\nI0509 12:39:09.678703       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/kgl 385\nI0509 12:39:09.879014       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/9r5 385\nI0509 12:39:10.079342       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/g7w 451\nI0509 12:39:10.278649       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/x25 442\nI0509 12:39:10.478979       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/n7zm 391\nI0509 12:39:10.679293       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/2ms2 572\nI0509 12:39:10.878542       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/kzzq 490\nI0509 12:39:11.078855       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/znl 473\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    May  9 12:39:11.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-3615 delete pod logs-generator'
    May  9 12:39:11.844: INFO: stderr: ""
    May  9 12:39:11.844: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 12:39:11.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3615" for this suite. 05/09/23 12:39:11.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:39:11.855
May  9 12:39:11.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename limitrange 05/09/23 12:39:11.855
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:39:11.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:39:11.871
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 05/09/23 12:39:11.873
STEP: Setting up watch 05/09/23 12:39:11.873
STEP: Submitting a LimitRange 05/09/23 12:39:11.976
STEP: Verifying LimitRange creation was observed 05/09/23 12:39:11.981
STEP: Fetching the LimitRange to ensure it has proper values 05/09/23 12:39:11.981
May  9 12:39:11.983: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May  9 12:39:11.983: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 05/09/23 12:39:11.983
STEP: Ensuring Pod has resource requirements applied from LimitRange 05/09/23 12:39:11.987
May  9 12:39:11.991: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May  9 12:39:11.991: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 05/09/23 12:39:11.991
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/09/23 12:39:11.998
May  9 12:39:12.006: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May  9 12:39:12.006: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 05/09/23 12:39:12.006
STEP: Failing to create a Pod with more than max resources 05/09/23 12:39:12.007
STEP: Updating a LimitRange 05/09/23 12:39:12.008
STEP: Verifying LimitRange updating is effective 05/09/23 12:39:12.013
STEP: Creating a Pod with less than former min resources 05/09/23 12:39:14.016
STEP: Failing to create a Pod with more than max resources 05/09/23 12:39:14.021
STEP: Deleting a LimitRange 05/09/23 12:39:14.023
STEP: Verifying the LimitRange was deleted 05/09/23 12:39:14.03
May  9 12:39:19.034: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 05/09/23 12:39:19.034
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
May  9 12:39:19.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4119" for this suite. 05/09/23 12:39:19.045
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":62,"skipped":1211,"failed":0}
------------------------------
• [SLOW TEST] [7.196 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:39:11.855
    May  9 12:39:11.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename limitrange 05/09/23 12:39:11.855
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:39:11.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:39:11.871
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 05/09/23 12:39:11.873
    STEP: Setting up watch 05/09/23 12:39:11.873
    STEP: Submitting a LimitRange 05/09/23 12:39:11.976
    STEP: Verifying LimitRange creation was observed 05/09/23 12:39:11.981
    STEP: Fetching the LimitRange to ensure it has proper values 05/09/23 12:39:11.981
    May  9 12:39:11.983: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May  9 12:39:11.983: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 05/09/23 12:39:11.983
    STEP: Ensuring Pod has resource requirements applied from LimitRange 05/09/23 12:39:11.987
    May  9 12:39:11.991: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May  9 12:39:11.991: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 05/09/23 12:39:11.991
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/09/23 12:39:11.998
    May  9 12:39:12.006: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    May  9 12:39:12.006: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 05/09/23 12:39:12.006
    STEP: Failing to create a Pod with more than max resources 05/09/23 12:39:12.007
    STEP: Updating a LimitRange 05/09/23 12:39:12.008
    STEP: Verifying LimitRange updating is effective 05/09/23 12:39:12.013
    STEP: Creating a Pod with less than former min resources 05/09/23 12:39:14.016
    STEP: Failing to create a Pod with more than max resources 05/09/23 12:39:14.021
    STEP: Deleting a LimitRange 05/09/23 12:39:14.023
    STEP: Verifying the LimitRange was deleted 05/09/23 12:39:14.03
    May  9 12:39:19.034: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 05/09/23 12:39:19.034
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    May  9 12:39:19.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-4119" for this suite. 05/09/23 12:39:19.045
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:39:19.051
May  9 12:39:19.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-probe 05/09/23 12:39:19.052
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:39:19.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:39:19.064
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb in namespace container-probe-9189 05/09/23 12:39:19.065
May  9 12:39:19.072: INFO: Waiting up to 5m0s for pod "busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb" in namespace "container-probe-9189" to be "not pending"
May  9 12:39:19.076: INFO: Pod "busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.323196ms
May  9 12:39:21.079: INFO: Pod "busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007113529s
May  9 12:39:21.079: INFO: Pod "busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb" satisfied condition "not pending"
May  9 12:39:21.079: INFO: Started pod busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb in namespace container-probe-9189
STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 12:39:21.079
May  9 12:39:21.081: INFO: Initial restart count of pod busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb is 0
STEP: deleting the pod 05/09/23 12:43:21.517
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  9 12:43:21.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9189" for this suite. 05/09/23 12:43:21.537
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":63,"skipped":1215,"failed":0}
------------------------------
• [SLOW TEST] [242.490 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:39:19.051
    May  9 12:39:19.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-probe 05/09/23 12:39:19.052
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:39:19.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:39:19.064
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb in namespace container-probe-9189 05/09/23 12:39:19.065
    May  9 12:39:19.072: INFO: Waiting up to 5m0s for pod "busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb" in namespace "container-probe-9189" to be "not pending"
    May  9 12:39:19.076: INFO: Pod "busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.323196ms
    May  9 12:39:21.079: INFO: Pod "busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007113529s
    May  9 12:39:21.079: INFO: Pod "busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb" satisfied condition "not pending"
    May  9 12:39:21.079: INFO: Started pod busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb in namespace container-probe-9189
    STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 12:39:21.079
    May  9 12:39:21.081: INFO: Initial restart count of pod busybox-9c879b69-cf90-4809-a0c2-5f58dfe2badb is 0
    STEP: deleting the pod 05/09/23 12:43:21.517
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  9 12:43:21.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9189" for this suite. 05/09/23 12:43:21.537
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:43:21.541
May  9 12:43:21.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 12:43:21.542
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:43:21.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:43:21.556
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 12:43:21.568
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 12:43:21.839
STEP: Deploying the webhook pod 05/09/23 12:43:21.845
STEP: Wait for the deployment to be ready 05/09/23 12:43:21.856
May  9 12:43:21.865: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 12:43:23.873
STEP: Verifying the service has paired with the endpoint 05/09/23 12:43:23.888
May  9 12:43:24.888: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
May  9 12:43:24.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/09/23 12:43:25.4
STEP: Creating a custom resource that should be denied by the webhook 05/09/23 12:43:25.414
STEP: Creating a custom resource whose deletion would be denied by the webhook 05/09/23 12:43:27.439
STEP: Updating the custom resource with disallowed data should be denied 05/09/23 12:43:27.445
STEP: Deleting the custom resource should be denied 05/09/23 12:43:27.45
STEP: Remove the offending key and value from the custom resource data 05/09/23 12:43:27.454
STEP: Deleting the updated custom resource should be successful 05/09/23 12:43:27.461
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 12:43:27.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6439" for this suite. 05/09/23 12:43:27.991
STEP: Destroying namespace "webhook-6439-markers" for this suite. 05/09/23 12:43:27.998
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":64,"skipped":1218,"failed":0}
------------------------------
• [SLOW TEST] [6.509 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:43:21.541
    May  9 12:43:21.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 12:43:21.542
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:43:21.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:43:21.556
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 12:43:21.568
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 12:43:21.839
    STEP: Deploying the webhook pod 05/09/23 12:43:21.845
    STEP: Wait for the deployment to be ready 05/09/23 12:43:21.856
    May  9 12:43:21.865: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 12:43:23.873
    STEP: Verifying the service has paired with the endpoint 05/09/23 12:43:23.888
    May  9 12:43:24.888: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    May  9 12:43:24.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/09/23 12:43:25.4
    STEP: Creating a custom resource that should be denied by the webhook 05/09/23 12:43:25.414
    STEP: Creating a custom resource whose deletion would be denied by the webhook 05/09/23 12:43:27.439
    STEP: Updating the custom resource with disallowed data should be denied 05/09/23 12:43:27.445
    STEP: Deleting the custom resource should be denied 05/09/23 12:43:27.45
    STEP: Remove the offending key and value from the custom resource data 05/09/23 12:43:27.454
    STEP: Deleting the updated custom resource should be successful 05/09/23 12:43:27.461
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 12:43:27.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6439" for this suite. 05/09/23 12:43:27.991
    STEP: Destroying namespace "webhook-6439-markers" for this suite. 05/09/23 12:43:27.998
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:43:28.051
May  9 12:43:28.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename disruption 05/09/23 12:43:28.052
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:43:28.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:43:28.073
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 05/09/23 12:43:28.078
STEP: Waiting for all pods to be running 05/09/23 12:43:30.11
May  9 12:43:30.117: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  9 12:43:32.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7991" for this suite. 05/09/23 12:43:32.126
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":65,"skipped":1229,"failed":0}
------------------------------
• [4.080 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:43:28.051
    May  9 12:43:28.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename disruption 05/09/23 12:43:28.052
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:43:28.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:43:28.073
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 05/09/23 12:43:28.078
    STEP: Waiting for all pods to be running 05/09/23 12:43:30.11
    May  9 12:43:30.117: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  9 12:43:32.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7991" for this suite. 05/09/23 12:43:32.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:43:32.131
May  9 12:43:32.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename security-context 05/09/23 12:43:32.132
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:43:32.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:43:32.148
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/09/23 12:43:32.15
May  9 12:43:32.156: INFO: Waiting up to 5m0s for pod "security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8" in namespace "security-context-1919" to be "Succeeded or Failed"
May  9 12:43:32.159: INFO: Pod "security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.589802ms
May  9 12:43:34.163: INFO: Pod "security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007547527s
May  9 12:43:36.164: INFO: Pod "security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008273272s
STEP: Saw pod success 05/09/23 12:43:36.164
May  9 12:43:36.164: INFO: Pod "security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8" satisfied condition "Succeeded or Failed"
May  9 12:43:36.166: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8 container test-container: <nil>
STEP: delete the pod 05/09/23 12:43:36.177
May  9 12:43:36.191: INFO: Waiting for pod security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8 to disappear
May  9 12:43:36.193: INFO: Pod security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  9 12:43:36.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1919" for this suite. 05/09/23 12:43:36.196
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":66,"skipped":1235,"failed":0}
------------------------------
• [4.070 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:43:32.131
    May  9 12:43:32.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename security-context 05/09/23 12:43:32.132
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:43:32.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:43:32.148
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/09/23 12:43:32.15
    May  9 12:43:32.156: INFO: Waiting up to 5m0s for pod "security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8" in namespace "security-context-1919" to be "Succeeded or Failed"
    May  9 12:43:32.159: INFO: Pod "security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.589802ms
    May  9 12:43:34.163: INFO: Pod "security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007547527s
    May  9 12:43:36.164: INFO: Pod "security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008273272s
    STEP: Saw pod success 05/09/23 12:43:36.164
    May  9 12:43:36.164: INFO: Pod "security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8" satisfied condition "Succeeded or Failed"
    May  9 12:43:36.166: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8 container test-container: <nil>
    STEP: delete the pod 05/09/23 12:43:36.177
    May  9 12:43:36.191: INFO: Waiting for pod security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8 to disappear
    May  9 12:43:36.193: INFO: Pod security-context-1fea82ba-0a4e-4298-bb0a-d50f03bf63b8 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  9 12:43:36.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-1919" for this suite. 05/09/23 12:43:36.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:43:36.203
May  9 12:43:36.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename containers 05/09/23 12:43:36.203
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:43:36.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:43:36.222
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 05/09/23 12:43:36.224
May  9 12:43:36.232: INFO: Waiting up to 5m0s for pod "client-containers-24db3110-8b70-48db-b677-26862a360eca" in namespace "containers-7562" to be "Succeeded or Failed"
May  9 12:43:36.235: INFO: Pod "client-containers-24db3110-8b70-48db-b677-26862a360eca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383219ms
May  9 12:43:38.238: INFO: Pod "client-containers-24db3110-8b70-48db-b677-26862a360eca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006541461s
May  9 12:43:40.240: INFO: Pod "client-containers-24db3110-8b70-48db-b677-26862a360eca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007677279s
STEP: Saw pod success 05/09/23 12:43:40.24
May  9 12:43:40.240: INFO: Pod "client-containers-24db3110-8b70-48db-b677-26862a360eca" satisfied condition "Succeeded or Failed"
May  9 12:43:40.242: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod client-containers-24db3110-8b70-48db-b677-26862a360eca container agnhost-container: <nil>
STEP: delete the pod 05/09/23 12:43:40.253
May  9 12:43:40.264: INFO: Waiting for pod client-containers-24db3110-8b70-48db-b677-26862a360eca to disappear
May  9 12:43:40.265: INFO: Pod client-containers-24db3110-8b70-48db-b677-26862a360eca no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May  9 12:43:40.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7562" for this suite. 05/09/23 12:43:40.269
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":67,"skipped":1278,"failed":0}
------------------------------
• [4.070 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:43:36.203
    May  9 12:43:36.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename containers 05/09/23 12:43:36.203
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:43:36.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:43:36.222
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 05/09/23 12:43:36.224
    May  9 12:43:36.232: INFO: Waiting up to 5m0s for pod "client-containers-24db3110-8b70-48db-b677-26862a360eca" in namespace "containers-7562" to be "Succeeded or Failed"
    May  9 12:43:36.235: INFO: Pod "client-containers-24db3110-8b70-48db-b677-26862a360eca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383219ms
    May  9 12:43:38.238: INFO: Pod "client-containers-24db3110-8b70-48db-b677-26862a360eca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006541461s
    May  9 12:43:40.240: INFO: Pod "client-containers-24db3110-8b70-48db-b677-26862a360eca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007677279s
    STEP: Saw pod success 05/09/23 12:43:40.24
    May  9 12:43:40.240: INFO: Pod "client-containers-24db3110-8b70-48db-b677-26862a360eca" satisfied condition "Succeeded or Failed"
    May  9 12:43:40.242: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod client-containers-24db3110-8b70-48db-b677-26862a360eca container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 12:43:40.253
    May  9 12:43:40.264: INFO: Waiting for pod client-containers-24db3110-8b70-48db-b677-26862a360eca to disappear
    May  9 12:43:40.265: INFO: Pod client-containers-24db3110-8b70-48db-b677-26862a360eca no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May  9 12:43:40.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7562" for this suite. 05/09/23 12:43:40.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:43:40.275
May  9 12:43:40.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename watch 05/09/23 12:43:40.275
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:43:40.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:43:40.29
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 05/09/23 12:43:40.292
STEP: creating a new configmap 05/09/23 12:43:40.292
STEP: modifying the configmap once 05/09/23 12:43:40.297
STEP: changing the label value of the configmap 05/09/23 12:43:40.303
STEP: Expecting to observe a delete notification for the watched object 05/09/23 12:43:40.31
May  9 12:43:40.310: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  a8e7cb17-7a25-4478-adda-969358e889af 15962 0 2023-05-09 12:43:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 12:43:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 12:43:40.310: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  a8e7cb17-7a25-4478-adda-969358e889af 15963 0 2023-05-09 12:43:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 12:43:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 12:43:40.310: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  a8e7cb17-7a25-4478-adda-969358e889af 15964 0 2023-05-09 12:43:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 12:43:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 05/09/23 12:43:40.31
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/09/23 12:43:40.316
STEP: changing the label value of the configmap back 05/09/23 12:43:50.317
STEP: modifying the configmap a third time 05/09/23 12:43:50.324
STEP: deleting the configmap 05/09/23 12:43:50.331
STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/09/23 12:43:50.335
May  9 12:43:50.335: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  a8e7cb17-7a25-4478-adda-969358e889af 16021 0 2023-05-09 12:43:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 12:43:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 12:43:50.335: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  a8e7cb17-7a25-4478-adda-969358e889af 16022 0 2023-05-09 12:43:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 12:43:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 12:43:50.335: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  a8e7cb17-7a25-4478-adda-969358e889af 16023 0 2023-05-09 12:43:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 12:43:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  9 12:43:50.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9983" for this suite. 05/09/23 12:43:50.339
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":68,"skipped":1319,"failed":0}
------------------------------
• [SLOW TEST] [10.068 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:43:40.275
    May  9 12:43:40.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename watch 05/09/23 12:43:40.275
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:43:40.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:43:40.29
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 05/09/23 12:43:40.292
    STEP: creating a new configmap 05/09/23 12:43:40.292
    STEP: modifying the configmap once 05/09/23 12:43:40.297
    STEP: changing the label value of the configmap 05/09/23 12:43:40.303
    STEP: Expecting to observe a delete notification for the watched object 05/09/23 12:43:40.31
    May  9 12:43:40.310: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  a8e7cb17-7a25-4478-adda-969358e889af 15962 0 2023-05-09 12:43:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 12:43:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 12:43:40.310: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  a8e7cb17-7a25-4478-adda-969358e889af 15963 0 2023-05-09 12:43:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 12:43:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 12:43:40.310: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  a8e7cb17-7a25-4478-adda-969358e889af 15964 0 2023-05-09 12:43:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 12:43:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 05/09/23 12:43:40.31
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/09/23 12:43:40.316
    STEP: changing the label value of the configmap back 05/09/23 12:43:50.317
    STEP: modifying the configmap a third time 05/09/23 12:43:50.324
    STEP: deleting the configmap 05/09/23 12:43:50.331
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/09/23 12:43:50.335
    May  9 12:43:50.335: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  a8e7cb17-7a25-4478-adda-969358e889af 16021 0 2023-05-09 12:43:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 12:43:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 12:43:50.335: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  a8e7cb17-7a25-4478-adda-969358e889af 16022 0 2023-05-09 12:43:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 12:43:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 12:43:50.335: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  a8e7cb17-7a25-4478-adda-969358e889af 16023 0 2023-05-09 12:43:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 12:43:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  9 12:43:50.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9983" for this suite. 05/09/23 12:43:50.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:43:50.344
May  9 12:43:50.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename cronjob 05/09/23 12:43:50.344
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:43:50.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:43:50.356
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 05/09/23 12:43:50.358
STEP: Ensuring a job is scheduled 05/09/23 12:43:50.362
STEP: Ensuring exactly one is scheduled 05/09/23 12:44:00.364
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/09/23 12:44:00.367
STEP: Ensuring no more jobs are scheduled 05/09/23 12:44:00.369
STEP: Removing cronjob 05/09/23 12:49:00.374
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  9 12:49:00.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1432" for this suite. 05/09/23 12:49:00.381
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":69,"skipped":1336,"failed":0}
------------------------------
• [SLOW TEST] [310.045 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:43:50.344
    May  9 12:43:50.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename cronjob 05/09/23 12:43:50.344
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:43:50.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:43:50.356
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 05/09/23 12:43:50.358
    STEP: Ensuring a job is scheduled 05/09/23 12:43:50.362
    STEP: Ensuring exactly one is scheduled 05/09/23 12:44:00.364
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/09/23 12:44:00.367
    STEP: Ensuring no more jobs are scheduled 05/09/23 12:44:00.369
    STEP: Removing cronjob 05/09/23 12:49:00.374
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  9 12:49:00.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1432" for this suite. 05/09/23 12:49:00.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:49:00.389
May  9 12:49:00.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 12:49:00.39
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:00.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:00.417
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 05/09/23 12:49:00.419
May  9 12:49:00.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6335 api-versions'
May  9 12:49:00.466: INFO: stderr: ""
May  9 12:49:00.466: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 12:49:00.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6335" for this suite. 05/09/23 12:49:00.469
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":70,"skipped":1361,"failed":0}
------------------------------
• [0.085 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:49:00.389
    May  9 12:49:00.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 12:49:00.39
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:00.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:00.417
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 05/09/23 12:49:00.419
    May  9 12:49:00.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6335 api-versions'
    May  9 12:49:00.466: INFO: stderr: ""
    May  9 12:49:00.466: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 12:49:00.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6335" for this suite. 05/09/23 12:49:00.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:49:00.475
May  9 12:49:00.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-probe 05/09/23 12:49:00.476
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:00.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:00.487
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
May  9 12:49:00.495: INFO: Waiting up to 5m0s for pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530" in namespace "container-probe-3636" to be "running and ready"
May  9 12:49:00.498: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Pending", Reason="", readiness=false. Elapsed: 3.086448ms
May  9 12:49:00.498: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Pending, waiting for it to be Running (with Ready = true)
May  9 12:49:02.501: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 2.006025908s
May  9 12:49:02.501: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
May  9 12:49:04.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 4.00689689s
May  9 12:49:04.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
May  9 12:49:06.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 6.007246671s
May  9 12:49:06.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
May  9 12:49:08.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 8.00727893s
May  9 12:49:08.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
May  9 12:49:10.501: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 10.006390756s
May  9 12:49:10.501: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
May  9 12:49:12.501: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 12.006328618s
May  9 12:49:12.501: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
May  9 12:49:14.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 14.007507275s
May  9 12:49:14.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
May  9 12:49:16.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 16.006989682s
May  9 12:49:16.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
May  9 12:49:18.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 18.006851784s
May  9 12:49:18.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
May  9 12:49:20.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 20.007327486s
May  9 12:49:20.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
May  9 12:49:22.501: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=true. Elapsed: 22.006256474s
May  9 12:49:22.501: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = true)
May  9 12:49:22.501: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530" satisfied condition "running and ready"
May  9 12:49:22.503: INFO: Container started at 2023-05-09 12:49:01 +0000 UTC, pod became ready at 2023-05-09 12:49:20 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  9 12:49:22.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3636" for this suite. 05/09/23 12:49:22.506
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":71,"skipped":1371,"failed":0}
------------------------------
• [SLOW TEST] [22.035 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:49:00.475
    May  9 12:49:00.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-probe 05/09/23 12:49:00.476
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:00.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:00.487
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    May  9 12:49:00.495: INFO: Waiting up to 5m0s for pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530" in namespace "container-probe-3636" to be "running and ready"
    May  9 12:49:00.498: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Pending", Reason="", readiness=false. Elapsed: 3.086448ms
    May  9 12:49:00.498: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:49:02.501: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 2.006025908s
    May  9 12:49:02.501: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
    May  9 12:49:04.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 4.00689689s
    May  9 12:49:04.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
    May  9 12:49:06.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 6.007246671s
    May  9 12:49:06.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
    May  9 12:49:08.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 8.00727893s
    May  9 12:49:08.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
    May  9 12:49:10.501: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 10.006390756s
    May  9 12:49:10.501: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
    May  9 12:49:12.501: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 12.006328618s
    May  9 12:49:12.501: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
    May  9 12:49:14.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 14.007507275s
    May  9 12:49:14.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
    May  9 12:49:16.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 16.006989682s
    May  9 12:49:16.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
    May  9 12:49:18.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 18.006851784s
    May  9 12:49:18.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
    May  9 12:49:20.502: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=false. Elapsed: 20.007327486s
    May  9 12:49:20.502: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = false)
    May  9 12:49:22.501: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530": Phase="Running", Reason="", readiness=true. Elapsed: 22.006256474s
    May  9 12:49:22.501: INFO: The phase of Pod test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530 is Running (Ready = true)
    May  9 12:49:22.501: INFO: Pod "test-webserver-efe23db5-2866-4f50-ae00-8ad95831c530" satisfied condition "running and ready"
    May  9 12:49:22.503: INFO: Container started at 2023-05-09 12:49:01 +0000 UTC, pod became ready at 2023-05-09 12:49:20 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  9 12:49:22.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3636" for this suite. 05/09/23 12:49:22.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:49:22.511
May  9 12:49:22.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 12:49:22.511
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:22.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:22.524
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 05/09/23 12:49:22.526
May  9 12:49:22.531: INFO: Waiting up to 5m0s for pod "annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7" in namespace "downward-api-2776" to be "running and ready"
May  9 12:49:22.538: INFO: Pod "annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.057331ms
May  9 12:49:22.538: INFO: The phase of Pod annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7 is Pending, waiting for it to be Running (with Ready = true)
May  9 12:49:24.541: INFO: Pod "annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010544886s
May  9 12:49:24.541: INFO: The phase of Pod annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7 is Running (Ready = true)
May  9 12:49:24.541: INFO: Pod "annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7" satisfied condition "running and ready"
May  9 12:49:25.066: INFO: Successfully updated pod "annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  9 12:49:29.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2776" for this suite. 05/09/23 12:49:29.086
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":72,"skipped":1393,"failed":0}
------------------------------
• [SLOW TEST] [6.581 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:49:22.511
    May  9 12:49:22.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 12:49:22.511
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:22.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:22.524
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 05/09/23 12:49:22.526
    May  9 12:49:22.531: INFO: Waiting up to 5m0s for pod "annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7" in namespace "downward-api-2776" to be "running and ready"
    May  9 12:49:22.538: INFO: Pod "annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.057331ms
    May  9 12:49:22.538: INFO: The phase of Pod annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7 is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:49:24.541: INFO: Pod "annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010544886s
    May  9 12:49:24.541: INFO: The phase of Pod annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7 is Running (Ready = true)
    May  9 12:49:24.541: INFO: Pod "annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7" satisfied condition "running and ready"
    May  9 12:49:25.066: INFO: Successfully updated pod "annotationupdate6bd781fb-bb0d-4234-90a1-1dd5e9d53fb7"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  9 12:49:29.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2776" for this suite. 05/09/23 12:49:29.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:49:29.092
May  9 12:49:29.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pods 05/09/23 12:49:29.093
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:29.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:29.158
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 05/09/23 12:49:29.16
May  9 12:49:29.165: INFO: Waiting up to 5m0s for pod "pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07" in namespace "pods-6172" to be "running and ready"
May  9 12:49:29.167: INFO: Pod "pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009384ms
May  9 12:49:29.167: INFO: The phase of Pod pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07 is Pending, waiting for it to be Running (with Ready = true)
May  9 12:49:31.170: INFO: Pod "pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07": Phase="Running", Reason="", readiness=true. Elapsed: 2.004845154s
May  9 12:49:31.170: INFO: The phase of Pod pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07 is Running (Ready = true)
May  9 12:49:31.170: INFO: Pod "pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07" satisfied condition "running and ready"
May  9 12:49:31.174: INFO: Pod pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07 has hostIP: 192.168.1.64
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  9 12:49:31.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6172" for this suite. 05/09/23 12:49:31.177
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":73,"skipped":1415,"failed":0}
------------------------------
• [2.090 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:49:29.092
    May  9 12:49:29.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pods 05/09/23 12:49:29.093
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:29.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:29.158
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 05/09/23 12:49:29.16
    May  9 12:49:29.165: INFO: Waiting up to 5m0s for pod "pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07" in namespace "pods-6172" to be "running and ready"
    May  9 12:49:29.167: INFO: Pod "pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009384ms
    May  9 12:49:29.167: INFO: The phase of Pod pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07 is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:49:31.170: INFO: Pod "pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07": Phase="Running", Reason="", readiness=true. Elapsed: 2.004845154s
    May  9 12:49:31.170: INFO: The phase of Pod pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07 is Running (Ready = true)
    May  9 12:49:31.170: INFO: Pod "pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07" satisfied condition "running and ready"
    May  9 12:49:31.174: INFO: Pod pod-hostip-5c2b0671-7d5c-4611-ab62-930a06e24b07 has hostIP: 192.168.1.64
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  9 12:49:31.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6172" for this suite. 05/09/23 12:49:31.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:49:31.183
May  9 12:49:31.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename init-container 05/09/23 12:49:31.184
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:31.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:31.198
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 05/09/23 12:49:31.199
May  9 12:49:31.200: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May  9 12:49:34.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1274" for this suite. 05/09/23 12:49:34.822
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":74,"skipped":1426,"failed":0}
------------------------------
• [3.643 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:49:31.183
    May  9 12:49:31.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename init-container 05/09/23 12:49:31.184
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:31.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:31.198
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 05/09/23 12:49:31.199
    May  9 12:49:31.200: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May  9 12:49:34.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1274" for this suite. 05/09/23 12:49:34.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:49:34.827
May  9 12:49:34.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename resourcequota 05/09/23 12:49:34.828
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:34.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:34.842
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 05/09/23 12:49:34.843
STEP: Creating a ResourceQuota 05/09/23 12:49:39.846
STEP: Ensuring resource quota status is calculated 05/09/23 12:49:39.852
STEP: Creating a ReplicationController 05/09/23 12:49:41.855
STEP: Ensuring resource quota status captures replication controller creation 05/09/23 12:49:41.864
STEP: Deleting a ReplicationController 05/09/23 12:49:43.868
STEP: Ensuring resource quota status released usage 05/09/23 12:49:43.877
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  9 12:49:45.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8596" for this suite. 05/09/23 12:49:45.884
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":75,"skipped":1464,"failed":0}
------------------------------
• [SLOW TEST] [11.062 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:49:34.827
    May  9 12:49:34.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename resourcequota 05/09/23 12:49:34.828
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:34.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:34.842
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 05/09/23 12:49:34.843
    STEP: Creating a ResourceQuota 05/09/23 12:49:39.846
    STEP: Ensuring resource quota status is calculated 05/09/23 12:49:39.852
    STEP: Creating a ReplicationController 05/09/23 12:49:41.855
    STEP: Ensuring resource quota status captures replication controller creation 05/09/23 12:49:41.864
    STEP: Deleting a ReplicationController 05/09/23 12:49:43.868
    STEP: Ensuring resource quota status released usage 05/09/23 12:49:43.877
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  9 12:49:45.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8596" for this suite. 05/09/23 12:49:45.884
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:49:45.889
May  9 12:49:45.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename proxy 05/09/23 12:49:45.89
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:45.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:45.906
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
May  9 12:49:45.907: INFO: Creating pod...
May  9 12:49:45.913: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2330" to be "running"
May  9 12:49:45.914: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.79736ms
May  9 12:49:47.918: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005174094s
May  9 12:49:47.918: INFO: Pod "agnhost" satisfied condition "running"
May  9 12:49:47.918: INFO: Creating service...
May  9 12:49:47.929: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=DELETE
May  9 12:49:47.933: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  9 12:49:47.933: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=OPTIONS
May  9 12:49:47.936: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  9 12:49:47.936: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=PATCH
May  9 12:49:47.938: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  9 12:49:47.938: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=POST
May  9 12:49:47.942: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  9 12:49:47.942: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=PUT
May  9 12:49:47.945: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  9 12:49:47.945: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=DELETE
May  9 12:49:47.949: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  9 12:49:47.949: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=OPTIONS
May  9 12:49:47.952: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  9 12:49:47.952: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=PATCH
May  9 12:49:47.957: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  9 12:49:47.957: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=POST
May  9 12:49:47.960: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  9 12:49:47.960: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=PUT
May  9 12:49:47.964: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  9 12:49:47.964: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=GET
May  9 12:49:47.966: INFO: http.Client request:GET StatusCode:301
May  9 12:49:47.966: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=GET
May  9 12:49:47.969: INFO: http.Client request:GET StatusCode:301
May  9 12:49:47.969: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=HEAD
May  9 12:49:47.971: INFO: http.Client request:HEAD StatusCode:301
May  9 12:49:47.971: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=HEAD
May  9 12:49:47.975: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
May  9 12:49:47.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2330" for this suite. 05/09/23 12:49:47.983
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":76,"skipped":1467,"failed":0}
------------------------------
• [2.100 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:49:45.889
    May  9 12:49:45.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename proxy 05/09/23 12:49:45.89
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:45.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:45.906
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    May  9 12:49:45.907: INFO: Creating pod...
    May  9 12:49:45.913: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2330" to be "running"
    May  9 12:49:45.914: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.79736ms
    May  9 12:49:47.918: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005174094s
    May  9 12:49:47.918: INFO: Pod "agnhost" satisfied condition "running"
    May  9 12:49:47.918: INFO: Creating service...
    May  9 12:49:47.929: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=DELETE
    May  9 12:49:47.933: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  9 12:49:47.933: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=OPTIONS
    May  9 12:49:47.936: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  9 12:49:47.936: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=PATCH
    May  9 12:49:47.938: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  9 12:49:47.938: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=POST
    May  9 12:49:47.942: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  9 12:49:47.942: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=PUT
    May  9 12:49:47.945: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  9 12:49:47.945: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=DELETE
    May  9 12:49:47.949: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  9 12:49:47.949: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=OPTIONS
    May  9 12:49:47.952: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  9 12:49:47.952: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=PATCH
    May  9 12:49:47.957: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  9 12:49:47.957: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=POST
    May  9 12:49:47.960: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  9 12:49:47.960: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=PUT
    May  9 12:49:47.964: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  9 12:49:47.964: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=GET
    May  9 12:49:47.966: INFO: http.Client request:GET StatusCode:301
    May  9 12:49:47.966: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=GET
    May  9 12:49:47.969: INFO: http.Client request:GET StatusCode:301
    May  9 12:49:47.969: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/pods/agnhost/proxy?method=HEAD
    May  9 12:49:47.971: INFO: http.Client request:HEAD StatusCode:301
    May  9 12:49:47.971: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2330/services/e2e-proxy-test-service/proxy?method=HEAD
    May  9 12:49:47.975: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    May  9 12:49:47.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2330" for this suite. 05/09/23 12:49:47.983
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:49:47.989
May  9 12:49:47.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename subpath 05/09/23 12:49:47.99
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:48.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:48.008
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/09/23 12:49:48.009
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-85cs 05/09/23 12:49:48.018
STEP: Creating a pod to test atomic-volume-subpath 05/09/23 12:49:48.018
May  9 12:49:48.024: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-85cs" in namespace "subpath-8564" to be "Succeeded or Failed"
May  9 12:49:48.026: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047948ms
May  9 12:49:50.030: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 2.00632285s
May  9 12:49:52.032: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 4.008758396s
May  9 12:49:54.030: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 6.006472029s
May  9 12:49:56.029: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 8.005286908s
May  9 12:49:58.029: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 10.005445325s
May  9 12:50:00.030: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 12.006325073s
May  9 12:50:02.029: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 14.005058295s
May  9 12:50:04.029: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 16.005299107s
May  9 12:50:06.031: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 18.007157687s
May  9 12:50:08.030: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 20.005797679s
May  9 12:50:10.029: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=false. Elapsed: 22.005522865s
May  9 12:50:12.029: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005757138s
STEP: Saw pod success 05/09/23 12:50:12.029
May  9 12:50:12.030: INFO: Pod "pod-subpath-test-configmap-85cs" satisfied condition "Succeeded or Failed"
May  9 12:50:12.032: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-subpath-test-configmap-85cs container test-container-subpath-configmap-85cs: <nil>
STEP: delete the pod 05/09/23 12:50:12.037
May  9 12:50:12.049: INFO: Waiting for pod pod-subpath-test-configmap-85cs to disappear
May  9 12:50:12.051: INFO: Pod pod-subpath-test-configmap-85cs no longer exists
STEP: Deleting pod pod-subpath-test-configmap-85cs 05/09/23 12:50:12.051
May  9 12:50:12.051: INFO: Deleting pod "pod-subpath-test-configmap-85cs" in namespace "subpath-8564"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  9 12:50:12.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8564" for this suite. 05/09/23 12:50:12.056
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":77,"skipped":1471,"failed":0}
------------------------------
• [SLOW TEST] [24.073 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:49:47.989
    May  9 12:49:47.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename subpath 05/09/23 12:49:47.99
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:49:48.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:49:48.008
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/09/23 12:49:48.009
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-85cs 05/09/23 12:49:48.018
    STEP: Creating a pod to test atomic-volume-subpath 05/09/23 12:49:48.018
    May  9 12:49:48.024: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-85cs" in namespace "subpath-8564" to be "Succeeded or Failed"
    May  9 12:49:48.026: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047948ms
    May  9 12:49:50.030: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 2.00632285s
    May  9 12:49:52.032: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 4.008758396s
    May  9 12:49:54.030: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 6.006472029s
    May  9 12:49:56.029: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 8.005286908s
    May  9 12:49:58.029: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 10.005445325s
    May  9 12:50:00.030: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 12.006325073s
    May  9 12:50:02.029: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 14.005058295s
    May  9 12:50:04.029: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 16.005299107s
    May  9 12:50:06.031: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 18.007157687s
    May  9 12:50:08.030: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=true. Elapsed: 20.005797679s
    May  9 12:50:10.029: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Running", Reason="", readiness=false. Elapsed: 22.005522865s
    May  9 12:50:12.029: INFO: Pod "pod-subpath-test-configmap-85cs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005757138s
    STEP: Saw pod success 05/09/23 12:50:12.029
    May  9 12:50:12.030: INFO: Pod "pod-subpath-test-configmap-85cs" satisfied condition "Succeeded or Failed"
    May  9 12:50:12.032: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-subpath-test-configmap-85cs container test-container-subpath-configmap-85cs: <nil>
    STEP: delete the pod 05/09/23 12:50:12.037
    May  9 12:50:12.049: INFO: Waiting for pod pod-subpath-test-configmap-85cs to disappear
    May  9 12:50:12.051: INFO: Pod pod-subpath-test-configmap-85cs no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-85cs 05/09/23 12:50:12.051
    May  9 12:50:12.051: INFO: Deleting pod "pod-subpath-test-configmap-85cs" in namespace "subpath-8564"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  9 12:50:12.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8564" for this suite. 05/09/23 12:50:12.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:50:12.063
May  9 12:50:12.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 12:50:12.063
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:50:12.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:50:12.076
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 05/09/23 12:50:12.078
May  9 12:50:12.084: INFO: Waiting up to 5m0s for pod "downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c" in namespace "downward-api-571" to be "Succeeded or Failed"
May  9 12:50:12.088: INFO: Pod "downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.928395ms
May  9 12:50:14.092: INFO: Pod "downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007517142s
May  9 12:50:16.093: INFO: Pod "downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009343975s
STEP: Saw pod success 05/09/23 12:50:16.093
May  9 12:50:16.093: INFO: Pod "downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c" satisfied condition "Succeeded or Failed"
May  9 12:50:16.096: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c container dapi-container: <nil>
STEP: delete the pod 05/09/23 12:50:16.108
May  9 12:50:16.119: INFO: Waiting for pod downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c to disappear
May  9 12:50:16.121: INFO: Pod downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  9 12:50:16.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-571" for this suite. 05/09/23 12:50:16.124
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":78,"skipped":1486,"failed":0}
------------------------------
• [4.065 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:50:12.063
    May  9 12:50:12.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 12:50:12.063
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:50:12.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:50:12.076
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 05/09/23 12:50:12.078
    May  9 12:50:12.084: INFO: Waiting up to 5m0s for pod "downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c" in namespace "downward-api-571" to be "Succeeded or Failed"
    May  9 12:50:12.088: INFO: Pod "downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.928395ms
    May  9 12:50:14.092: INFO: Pod "downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007517142s
    May  9 12:50:16.093: INFO: Pod "downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009343975s
    STEP: Saw pod success 05/09/23 12:50:16.093
    May  9 12:50:16.093: INFO: Pod "downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c" satisfied condition "Succeeded or Failed"
    May  9 12:50:16.096: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c container dapi-container: <nil>
    STEP: delete the pod 05/09/23 12:50:16.108
    May  9 12:50:16.119: INFO: Waiting for pod downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c to disappear
    May  9 12:50:16.121: INFO: Pod downward-api-ea5c43a6-26d8-41a2-88ef-92d55adf405c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  9 12:50:16.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-571" for this suite. 05/09/23 12:50:16.124
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:50:16.128
May  9 12:50:16.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 12:50:16.128
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:50:16.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:50:16.141
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/09/23 12:50:16.142
May  9 12:50:16.148: INFO: Waiting up to 5m0s for pod "pod-b8ced223-9a70-4448-adb7-a2dff32012f5" in namespace "emptydir-2086" to be "Succeeded or Failed"
May  9 12:50:16.156: INFO: Pod "pod-b8ced223-9a70-4448-adb7-a2dff32012f5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.837375ms
May  9 12:50:18.160: INFO: Pod "pod-b8ced223-9a70-4448-adb7-a2dff32012f5": Phase="Running", Reason="", readiness=false. Elapsed: 2.01190791s
May  9 12:50:20.160: INFO: Pod "pod-b8ced223-9a70-4448-adb7-a2dff32012f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012264246s
STEP: Saw pod success 05/09/23 12:50:20.16
May  9 12:50:20.160: INFO: Pod "pod-b8ced223-9a70-4448-adb7-a2dff32012f5" satisfied condition "Succeeded or Failed"
May  9 12:50:20.162: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-b8ced223-9a70-4448-adb7-a2dff32012f5 container test-container: <nil>
STEP: delete the pod 05/09/23 12:50:20.168
May  9 12:50:20.179: INFO: Waiting for pod pod-b8ced223-9a70-4448-adb7-a2dff32012f5 to disappear
May  9 12:50:20.181: INFO: Pod pod-b8ced223-9a70-4448-adb7-a2dff32012f5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 12:50:20.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2086" for this suite. 05/09/23 12:50:20.184
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":79,"skipped":1490,"failed":0}
------------------------------
• [4.061 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:50:16.128
    May  9 12:50:16.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 12:50:16.128
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:50:16.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:50:16.141
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/09/23 12:50:16.142
    May  9 12:50:16.148: INFO: Waiting up to 5m0s for pod "pod-b8ced223-9a70-4448-adb7-a2dff32012f5" in namespace "emptydir-2086" to be "Succeeded or Failed"
    May  9 12:50:16.156: INFO: Pod "pod-b8ced223-9a70-4448-adb7-a2dff32012f5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.837375ms
    May  9 12:50:18.160: INFO: Pod "pod-b8ced223-9a70-4448-adb7-a2dff32012f5": Phase="Running", Reason="", readiness=false. Elapsed: 2.01190791s
    May  9 12:50:20.160: INFO: Pod "pod-b8ced223-9a70-4448-adb7-a2dff32012f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012264246s
    STEP: Saw pod success 05/09/23 12:50:20.16
    May  9 12:50:20.160: INFO: Pod "pod-b8ced223-9a70-4448-adb7-a2dff32012f5" satisfied condition "Succeeded or Failed"
    May  9 12:50:20.162: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-b8ced223-9a70-4448-adb7-a2dff32012f5 container test-container: <nil>
    STEP: delete the pod 05/09/23 12:50:20.168
    May  9 12:50:20.179: INFO: Waiting for pod pod-b8ced223-9a70-4448-adb7-a2dff32012f5 to disappear
    May  9 12:50:20.181: INFO: Pod pod-b8ced223-9a70-4448-adb7-a2dff32012f5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 12:50:20.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2086" for this suite. 05/09/23 12:50:20.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:50:20.19
May  9 12:50:20.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename replication-controller 05/09/23 12:50:20.19
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:50:20.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:50:20.203
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 05/09/23 12:50:20.205
STEP: When the matched label of one of its pods change 05/09/23 12:50:20.208
May  9 12:50:20.210: INFO: Pod name pod-release: Found 0 pods out of 1
May  9 12:50:25.215: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 05/09/23 12:50:25.225
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  9 12:50:26.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1026" for this suite. 05/09/23 12:50:26.233
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":80,"skipped":1501,"failed":0}
------------------------------
• [SLOW TEST] [6.050 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:50:20.19
    May  9 12:50:20.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename replication-controller 05/09/23 12:50:20.19
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:50:20.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:50:20.203
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 05/09/23 12:50:20.205
    STEP: When the matched label of one of its pods change 05/09/23 12:50:20.208
    May  9 12:50:20.210: INFO: Pod name pod-release: Found 0 pods out of 1
    May  9 12:50:25.215: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/09/23 12:50:25.225
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  9 12:50:26.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1026" for this suite. 05/09/23 12:50:26.233
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:50:26.24
May  9 12:50:26.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename controllerrevisions 05/09/23 12:50:26.24
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:50:26.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:50:26.256
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-82g4w-daemon-set" 05/09/23 12:50:26.274
STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 12:50:26.279
May  9 12:50:26.282: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:50:26.282: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:50:26.282: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:50:26.284: INFO: Number of nodes with available pods controlled by daemonset e2e-82g4w-daemon-set: 0
May  9 12:50:26.284: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
May  9 12:50:27.288: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:50:27.288: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:50:27.288: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 12:50:27.293: INFO: Number of nodes with available pods controlled by daemonset e2e-82g4w-daemon-set: 3
May  9 12:50:27.293: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-82g4w-daemon-set
STEP: Confirm DaemonSet "e2e-82g4w-daemon-set" successfully created with "daemonset-name=e2e-82g4w-daemon-set" label 05/09/23 12:50:27.295
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-82g4w-daemon-set" 05/09/23 12:50:27.299
May  9 12:50:27.301: INFO: Located ControllerRevision: "e2e-82g4w-daemon-set-9d949597"
STEP: Patching ControllerRevision "e2e-82g4w-daemon-set-9d949597" 05/09/23 12:50:27.303
May  9 12:50:27.308: INFO: e2e-82g4w-daemon-set-9d949597 has been patched
STEP: Create a new ControllerRevision 05/09/23 12:50:27.308
May  9 12:50:27.313: INFO: Created ControllerRevision: e2e-82g4w-daemon-set-677d97c565
STEP: Confirm that there are two ControllerRevisions 05/09/23 12:50:27.313
May  9 12:50:27.314: INFO: Requesting list of ControllerRevisions to confirm quantity
May  9 12:50:27.316: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-82g4w-daemon-set-9d949597" 05/09/23 12:50:27.316
STEP: Confirm that there is only one ControllerRevision 05/09/23 12:50:27.32
May  9 12:50:27.320: INFO: Requesting list of ControllerRevisions to confirm quantity
May  9 12:50:27.322: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-82g4w-daemon-set-677d97c565" 05/09/23 12:50:27.323
May  9 12:50:27.329: INFO: e2e-82g4w-daemon-set-677d97c565 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 05/09/23 12:50:27.329
W0509 12:50:27.337100      24 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 05/09/23 12:50:27.337
May  9 12:50:27.337: INFO: Requesting list of ControllerRevisions to confirm quantity
May  9 12:50:28.340: INFO: Requesting list of ControllerRevisions to confirm quantity
May  9 12:50:28.342: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-82g4w-daemon-set-677d97c565=updated" 05/09/23 12:50:28.342
STEP: Confirm that there is only one ControllerRevision 05/09/23 12:50:28.347
May  9 12:50:28.348: INFO: Requesting list of ControllerRevisions to confirm quantity
May  9 12:50:28.349: INFO: Found 1 ControllerRevisions
May  9 12:50:28.351: INFO: ControllerRevision "e2e-82g4w-daemon-set-6cfb49c68d" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-82g4w-daemon-set" 05/09/23 12:50:28.352
STEP: deleting DaemonSet.extensions e2e-82g4w-daemon-set in namespace controllerrevisions-7399, will wait for the garbage collector to delete the pods 05/09/23 12:50:28.352
May  9 12:50:28.415: INFO: Deleting DaemonSet.extensions e2e-82g4w-daemon-set took: 10.502148ms
May  9 12:50:28.515: INFO: Terminating DaemonSet.extensions e2e-82g4w-daemon-set pods took: 100.396562ms
May  9 12:50:30.219: INFO: Number of nodes with available pods controlled by daemonset e2e-82g4w-daemon-set: 0
May  9 12:50:30.219: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-82g4w-daemon-set
May  9 12:50:30.220: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17871"},"items":null}

May  9 12:50:30.222: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17871"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
May  9 12:50:30.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-7399" for this suite. 05/09/23 12:50:30.232
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":81,"skipped":1504,"failed":0}
------------------------------
• [3.998 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:50:26.24
    May  9 12:50:26.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename controllerrevisions 05/09/23 12:50:26.24
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:50:26.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:50:26.256
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-82g4w-daemon-set" 05/09/23 12:50:26.274
    STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 12:50:26.279
    May  9 12:50:26.282: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:50:26.282: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:50:26.282: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:50:26.284: INFO: Number of nodes with available pods controlled by daemonset e2e-82g4w-daemon-set: 0
    May  9 12:50:26.284: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
    May  9 12:50:27.288: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:50:27.288: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:50:27.288: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 12:50:27.293: INFO: Number of nodes with available pods controlled by daemonset e2e-82g4w-daemon-set: 3
    May  9 12:50:27.293: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-82g4w-daemon-set
    STEP: Confirm DaemonSet "e2e-82g4w-daemon-set" successfully created with "daemonset-name=e2e-82g4w-daemon-set" label 05/09/23 12:50:27.295
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-82g4w-daemon-set" 05/09/23 12:50:27.299
    May  9 12:50:27.301: INFO: Located ControllerRevision: "e2e-82g4w-daemon-set-9d949597"
    STEP: Patching ControllerRevision "e2e-82g4w-daemon-set-9d949597" 05/09/23 12:50:27.303
    May  9 12:50:27.308: INFO: e2e-82g4w-daemon-set-9d949597 has been patched
    STEP: Create a new ControllerRevision 05/09/23 12:50:27.308
    May  9 12:50:27.313: INFO: Created ControllerRevision: e2e-82g4w-daemon-set-677d97c565
    STEP: Confirm that there are two ControllerRevisions 05/09/23 12:50:27.313
    May  9 12:50:27.314: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  9 12:50:27.316: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-82g4w-daemon-set-9d949597" 05/09/23 12:50:27.316
    STEP: Confirm that there is only one ControllerRevision 05/09/23 12:50:27.32
    May  9 12:50:27.320: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  9 12:50:27.322: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-82g4w-daemon-set-677d97c565" 05/09/23 12:50:27.323
    May  9 12:50:27.329: INFO: e2e-82g4w-daemon-set-677d97c565 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 05/09/23 12:50:27.329
    W0509 12:50:27.337100      24 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 05/09/23 12:50:27.337
    May  9 12:50:27.337: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  9 12:50:28.340: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  9 12:50:28.342: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-82g4w-daemon-set-677d97c565=updated" 05/09/23 12:50:28.342
    STEP: Confirm that there is only one ControllerRevision 05/09/23 12:50:28.347
    May  9 12:50:28.348: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  9 12:50:28.349: INFO: Found 1 ControllerRevisions
    May  9 12:50:28.351: INFO: ControllerRevision "e2e-82g4w-daemon-set-6cfb49c68d" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-82g4w-daemon-set" 05/09/23 12:50:28.352
    STEP: deleting DaemonSet.extensions e2e-82g4w-daemon-set in namespace controllerrevisions-7399, will wait for the garbage collector to delete the pods 05/09/23 12:50:28.352
    May  9 12:50:28.415: INFO: Deleting DaemonSet.extensions e2e-82g4w-daemon-set took: 10.502148ms
    May  9 12:50:28.515: INFO: Terminating DaemonSet.extensions e2e-82g4w-daemon-set pods took: 100.396562ms
    May  9 12:50:30.219: INFO: Number of nodes with available pods controlled by daemonset e2e-82g4w-daemon-set: 0
    May  9 12:50:30.219: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-82g4w-daemon-set
    May  9 12:50:30.220: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17871"},"items":null}

    May  9 12:50:30.222: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17871"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    May  9 12:50:30.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-7399" for this suite. 05/09/23 12:50:30.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:50:30.238
May  9 12:50:30.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 12:50:30.239
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:50:30.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:50:30.25
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 05/09/23 12:50:30.252
May  9 12:50:30.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2787 create -f -'
May  9 12:50:30.877: INFO: stderr: ""
May  9 12:50:30.877: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/09/23 12:50:30.877
May  9 12:50:31.881: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 12:50:31.881: INFO: Found 0 / 1
May  9 12:50:32.881: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 12:50:32.881: INFO: Found 1 / 1
May  9 12:50:32.881: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 05/09/23 12:50:32.881
May  9 12:50:32.883: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 12:50:32.883: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  9 12:50:32.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2787 patch pod agnhost-primary-xk7l6 -p {"metadata":{"annotations":{"x":"y"}}}'
May  9 12:50:32.939: INFO: stderr: ""
May  9 12:50:32.939: INFO: stdout: "pod/agnhost-primary-xk7l6 patched\n"
STEP: checking annotations 05/09/23 12:50:32.939
May  9 12:50:32.942: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 12:50:32.942: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 12:50:32.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2787" for this suite. 05/09/23 12:50:32.944
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":82,"skipped":1509,"failed":0}
------------------------------
• [2.711 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:50:30.238
    May  9 12:50:30.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 12:50:30.239
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:50:30.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:50:30.25
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 05/09/23 12:50:30.252
    May  9 12:50:30.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2787 create -f -'
    May  9 12:50:30.877: INFO: stderr: ""
    May  9 12:50:30.877: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/09/23 12:50:30.877
    May  9 12:50:31.881: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 12:50:31.881: INFO: Found 0 / 1
    May  9 12:50:32.881: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 12:50:32.881: INFO: Found 1 / 1
    May  9 12:50:32.881: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 05/09/23 12:50:32.881
    May  9 12:50:32.883: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 12:50:32.883: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  9 12:50:32.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2787 patch pod agnhost-primary-xk7l6 -p {"metadata":{"annotations":{"x":"y"}}}'
    May  9 12:50:32.939: INFO: stderr: ""
    May  9 12:50:32.939: INFO: stdout: "pod/agnhost-primary-xk7l6 patched\n"
    STEP: checking annotations 05/09/23 12:50:32.939
    May  9 12:50:32.942: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 12:50:32.942: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 12:50:32.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2787" for this suite. 05/09/23 12:50:32.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:50:32.951
May  9 12:50:32.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename sched-preemption 05/09/23 12:50:32.951
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:50:32.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:50:32.966
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May  9 12:50:32.980: INFO: Waiting up to 1m0s for all nodes to be ready
May  9 12:51:33.014: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:51:33.017
May  9 12:51:33.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename sched-preemption-path 05/09/23 12:51:33.018
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:51:33.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:51:33.032
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
May  9 12:51:33.044: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
May  9 12:51:33.046: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
May  9 12:51:33.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9133" for this suite. 05/09/23 12:51:33.064
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May  9 12:51:33.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7949" for this suite. 05/09/23 12:51:33.08
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":83,"skipped":1568,"failed":0}
------------------------------
• [SLOW TEST] [60.169 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:50:32.951
    May  9 12:50:32.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename sched-preemption 05/09/23 12:50:32.951
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:50:32.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:50:32.966
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May  9 12:50:32.980: INFO: Waiting up to 1m0s for all nodes to be ready
    May  9 12:51:33.014: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:51:33.017
    May  9 12:51:33.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename sched-preemption-path 05/09/23 12:51:33.018
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:51:33.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:51:33.032
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    May  9 12:51:33.044: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    May  9 12:51:33.046: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    May  9 12:51:33.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-9133" for this suite. 05/09/23 12:51:33.064
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May  9 12:51:33.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7949" for this suite. 05/09/23 12:51:33.08
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:51:33.12
May  9 12:51:33.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 12:51:33.12
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:51:33.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:51:33.133
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/09/23 12:51:33.137
May  9 12:51:33.142: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9027" to be "running and ready"
May  9 12:51:33.147: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363466ms
May  9 12:51:33.147: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  9 12:51:35.149: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007068982s
May  9 12:51:35.150: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  9 12:51:35.150: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 05/09/23 12:51:35.151
May  9 12:51:35.157: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9027" to be "running and ready"
May  9 12:51:35.160: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.35676ms
May  9 12:51:35.160: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  9 12:51:37.163: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006357576s
May  9 12:51:37.163: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
May  9 12:51:37.163: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/09/23 12:51:37.165
STEP: delete the pod with lifecycle hook 05/09/23 12:51:37.169
May  9 12:51:37.175: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  9 12:51:37.176: INFO: Pod pod-with-poststart-exec-hook still exists
May  9 12:51:39.178: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  9 12:51:39.181: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May  9 12:51:39.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9027" for this suite. 05/09/23 12:51:39.184
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":84,"skipped":1575,"failed":0}
------------------------------
• [SLOW TEST] [6.076 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:51:33.12
    May  9 12:51:33.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 12:51:33.12
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:51:33.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:51:33.133
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/09/23 12:51:33.137
    May  9 12:51:33.142: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9027" to be "running and ready"
    May  9 12:51:33.147: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363466ms
    May  9 12:51:33.147: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:51:35.149: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007068982s
    May  9 12:51:35.150: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  9 12:51:35.150: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 05/09/23 12:51:35.151
    May  9 12:51:35.157: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9027" to be "running and ready"
    May  9 12:51:35.160: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.35676ms
    May  9 12:51:35.160: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:51:37.163: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006357576s
    May  9 12:51:37.163: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    May  9 12:51:37.163: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/09/23 12:51:37.165
    STEP: delete the pod with lifecycle hook 05/09/23 12:51:37.169
    May  9 12:51:37.175: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  9 12:51:37.176: INFO: Pod pod-with-poststart-exec-hook still exists
    May  9 12:51:39.178: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  9 12:51:39.181: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May  9 12:51:39.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9027" for this suite. 05/09/23 12:51:39.184
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:51:39.196
May  9 12:51:39.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename events 05/09/23 12:51:39.197
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:51:39.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:51:39.208
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 05/09/23 12:51:39.213
STEP: get a list of Events with a label in the current namespace 05/09/23 12:51:39.224
STEP: delete a list of events 05/09/23 12:51:39.226
May  9 12:51:39.226: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/09/23 12:51:39.247
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
May  9 12:51:39.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3866" for this suite. 05/09/23 12:51:39.251
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":85,"skipped":1577,"failed":0}
------------------------------
• [0.060 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:51:39.196
    May  9 12:51:39.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename events 05/09/23 12:51:39.197
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:51:39.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:51:39.208
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 05/09/23 12:51:39.213
    STEP: get a list of Events with a label in the current namespace 05/09/23 12:51:39.224
    STEP: delete a list of events 05/09/23 12:51:39.226
    May  9 12:51:39.226: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/09/23 12:51:39.247
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    May  9 12:51:39.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3866" for this suite. 05/09/23 12:51:39.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:51:39.257
May  9 12:51:39.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-probe 05/09/23 12:51:39.258
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:51:39.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:51:39.269
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-1abcd526-7668-463b-8375-d182a894351e in namespace container-probe-4107 05/09/23 12:51:39.27
May  9 12:51:39.279: INFO: Waiting up to 5m0s for pod "liveness-1abcd526-7668-463b-8375-d182a894351e" in namespace "container-probe-4107" to be "not pending"
May  9 12:51:39.281: INFO: Pod "liveness-1abcd526-7668-463b-8375-d182a894351e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.486192ms
May  9 12:51:41.285: INFO: Pod "liveness-1abcd526-7668-463b-8375-d182a894351e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006220107s
May  9 12:51:41.285: INFO: Pod "liveness-1abcd526-7668-463b-8375-d182a894351e" satisfied condition "not pending"
May  9 12:51:41.285: INFO: Started pod liveness-1abcd526-7668-463b-8375-d182a894351e in namespace container-probe-4107
STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 12:51:41.285
May  9 12:51:41.287: INFO: Initial restart count of pod liveness-1abcd526-7668-463b-8375-d182a894351e is 0
STEP: deleting the pod 05/09/23 12:55:41.738
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  9 12:55:41.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4107" for this suite. 05/09/23 12:55:41.754
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":86,"skipped":1643,"failed":0}
------------------------------
• [SLOW TEST] [242.501 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:51:39.257
    May  9 12:51:39.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-probe 05/09/23 12:51:39.258
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:51:39.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:51:39.269
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-1abcd526-7668-463b-8375-d182a894351e in namespace container-probe-4107 05/09/23 12:51:39.27
    May  9 12:51:39.279: INFO: Waiting up to 5m0s for pod "liveness-1abcd526-7668-463b-8375-d182a894351e" in namespace "container-probe-4107" to be "not pending"
    May  9 12:51:39.281: INFO: Pod "liveness-1abcd526-7668-463b-8375-d182a894351e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.486192ms
    May  9 12:51:41.285: INFO: Pod "liveness-1abcd526-7668-463b-8375-d182a894351e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006220107s
    May  9 12:51:41.285: INFO: Pod "liveness-1abcd526-7668-463b-8375-d182a894351e" satisfied condition "not pending"
    May  9 12:51:41.285: INFO: Started pod liveness-1abcd526-7668-463b-8375-d182a894351e in namespace container-probe-4107
    STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 12:51:41.285
    May  9 12:51:41.287: INFO: Initial restart count of pod liveness-1abcd526-7668-463b-8375-d182a894351e is 0
    STEP: deleting the pod 05/09/23 12:55:41.738
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  9 12:55:41.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4107" for this suite. 05/09/23 12:55:41.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:55:41.759
May  9 12:55:41.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pods 05/09/23 12:55:41.76
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:55:41.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:55:41.781
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 05/09/23 12:55:41.787
STEP: watching for Pod to be ready 05/09/23 12:55:41.794
May  9 12:55:41.795: INFO: observed Pod pod-test in namespace pods-7843 in phase Pending with labels: map[test-pod-static:true] & conditions []
May  9 12:55:41.798: INFO: observed Pod pod-test in namespace pods-7843 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  }]
May  9 12:55:41.808: INFO: observed Pod pod-test in namespace pods-7843 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  }]
May  9 12:55:42.248: INFO: observed Pod pod-test in namespace pods-7843 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  }]
May  9 12:55:43.404: INFO: Found Pod pod-test in namespace pods-7843 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 05/09/23 12:55:43.409
STEP: getting the Pod and ensuring that it's patched 05/09/23 12:55:43.419
STEP: replacing the Pod's status Ready condition to False 05/09/23 12:55:43.422
STEP: check the Pod again to ensure its Ready conditions are False 05/09/23 12:55:43.437
STEP: deleting the Pod via a Collection with a LabelSelector 05/09/23 12:55:43.437
STEP: watching for the Pod to be deleted 05/09/23 12:55:43.452
May  9 12:55:43.453: INFO: observed event type MODIFIED
May  9 12:55:45.410: INFO: observed event type MODIFIED
May  9 12:55:45.748: INFO: observed event type MODIFIED
May  9 12:55:46.410: INFO: observed event type MODIFIED
May  9 12:55:46.416: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  9 12:55:46.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7843" for this suite. 05/09/23 12:55:46.423
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":87,"skipped":1655,"failed":0}
------------------------------
• [4.669 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:55:41.759
    May  9 12:55:41.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pods 05/09/23 12:55:41.76
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:55:41.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:55:41.781
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 05/09/23 12:55:41.787
    STEP: watching for Pod to be ready 05/09/23 12:55:41.794
    May  9 12:55:41.795: INFO: observed Pod pod-test in namespace pods-7843 in phase Pending with labels: map[test-pod-static:true] & conditions []
    May  9 12:55:41.798: INFO: observed Pod pod-test in namespace pods-7843 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  }]
    May  9 12:55:41.808: INFO: observed Pod pod-test in namespace pods-7843 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  }]
    May  9 12:55:42.248: INFO: observed Pod pod-test in namespace pods-7843 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  }]
    May  9 12:55:43.404: INFO: Found Pod pod-test in namespace pods-7843 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 12:55:41 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 05/09/23 12:55:43.409
    STEP: getting the Pod and ensuring that it's patched 05/09/23 12:55:43.419
    STEP: replacing the Pod's status Ready condition to False 05/09/23 12:55:43.422
    STEP: check the Pod again to ensure its Ready conditions are False 05/09/23 12:55:43.437
    STEP: deleting the Pod via a Collection with a LabelSelector 05/09/23 12:55:43.437
    STEP: watching for the Pod to be deleted 05/09/23 12:55:43.452
    May  9 12:55:43.453: INFO: observed event type MODIFIED
    May  9 12:55:45.410: INFO: observed event type MODIFIED
    May  9 12:55:45.748: INFO: observed event type MODIFIED
    May  9 12:55:46.410: INFO: observed event type MODIFIED
    May  9 12:55:46.416: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  9 12:55:46.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7843" for this suite. 05/09/23 12:55:46.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:55:46.428
May  9 12:55:46.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename disruption 05/09/23 12:55:46.429
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:55:46.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:55:46.441
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 05/09/23 12:55:46.443
STEP: Waiting for the pdb to be processed 05/09/23 12:55:46.446
STEP: updating the pdb 05/09/23 12:55:48.45
STEP: Waiting for the pdb to be processed 05/09/23 12:55:48.457
STEP: patching the pdb 05/09/23 12:55:50.461
STEP: Waiting for the pdb to be processed 05/09/23 12:55:50.469
STEP: Waiting for the pdb to be deleted 05/09/23 12:55:52.48
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  9 12:55:52.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9296" for this suite. 05/09/23 12:55:52.484
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":88,"skipped":1666,"failed":0}
------------------------------
• [SLOW TEST] [6.061 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:55:46.428
    May  9 12:55:46.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename disruption 05/09/23 12:55:46.429
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:55:46.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:55:46.441
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 05/09/23 12:55:46.443
    STEP: Waiting for the pdb to be processed 05/09/23 12:55:46.446
    STEP: updating the pdb 05/09/23 12:55:48.45
    STEP: Waiting for the pdb to be processed 05/09/23 12:55:48.457
    STEP: patching the pdb 05/09/23 12:55:50.461
    STEP: Waiting for the pdb to be processed 05/09/23 12:55:50.469
    STEP: Waiting for the pdb to be deleted 05/09/23 12:55:52.48
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  9 12:55:52.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9296" for this suite. 05/09/23 12:55:52.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:55:52.49
May  9 12:55:52.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 12:55:52.49
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:55:52.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:55:52.502
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 05/09/23 12:55:52.503
May  9 12:55:52.510: INFO: Waiting up to 5m0s for pod "labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c" in namespace "projected-4446" to be "running and ready"
May  9 12:55:52.511: INFO: Pod "labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.604195ms
May  9 12:55:52.511: INFO: The phase of Pod labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c is Pending, waiting for it to be Running (with Ready = true)
May  9 12:55:54.515: INFO: Pod "labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c": Phase="Running", Reason="", readiness=true. Elapsed: 2.005501099s
May  9 12:55:54.515: INFO: The phase of Pod labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c is Running (Ready = true)
May  9 12:55:54.515: INFO: Pod "labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c" satisfied condition "running and ready"
May  9 12:55:55.040: INFO: Successfully updated pod "labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  9 12:55:59.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4446" for this suite. 05/09/23 12:55:59.059
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":89,"skipped":1691,"failed":0}
------------------------------
• [SLOW TEST] [6.574 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:55:52.49
    May  9 12:55:52.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 12:55:52.49
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:55:52.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:55:52.502
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 05/09/23 12:55:52.503
    May  9 12:55:52.510: INFO: Waiting up to 5m0s for pod "labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c" in namespace "projected-4446" to be "running and ready"
    May  9 12:55:52.511: INFO: Pod "labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.604195ms
    May  9 12:55:52.511: INFO: The phase of Pod labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:55:54.515: INFO: Pod "labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c": Phase="Running", Reason="", readiness=true. Elapsed: 2.005501099s
    May  9 12:55:54.515: INFO: The phase of Pod labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c is Running (Ready = true)
    May  9 12:55:54.515: INFO: Pod "labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c" satisfied condition "running and ready"
    May  9 12:55:55.040: INFO: Successfully updated pod "labelsupdate7aef73fc-b9f9-4533-833d-6c48502aa41c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  9 12:55:59.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4446" for this suite. 05/09/23 12:55:59.059
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:55:59.064
May  9 12:55:59.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 12:55:59.064
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:55:59.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:55:59.077
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 05/09/23 12:55:59.079
STEP: fetching the ConfigMap 05/09/23 12:55:59.082
STEP: patching the ConfigMap 05/09/23 12:55:59.084
STEP: listing all ConfigMaps in all namespaces with a label selector 05/09/23 12:55:59.089
STEP: deleting the ConfigMap by collection with a label selector 05/09/23 12:55:59.091
STEP: listing all ConfigMaps in test namespace 05/09/23 12:55:59.096
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May  9 12:55:59.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7748" for this suite. 05/09/23 12:55:59.1
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":90,"skipped":1691,"failed":0}
------------------------------
• [0.041 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:55:59.064
    May  9 12:55:59.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 12:55:59.064
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:55:59.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:55:59.077
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 05/09/23 12:55:59.079
    STEP: fetching the ConfigMap 05/09/23 12:55:59.082
    STEP: patching the ConfigMap 05/09/23 12:55:59.084
    STEP: listing all ConfigMaps in all namespaces with a label selector 05/09/23 12:55:59.089
    STEP: deleting the ConfigMap by collection with a label selector 05/09/23 12:55:59.091
    STEP: listing all ConfigMaps in test namespace 05/09/23 12:55:59.096
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 12:55:59.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7748" for this suite. 05/09/23 12:55:59.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:55:59.107
May  9 12:55:59.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 12:55:59.107
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:55:59.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:55:59.123
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 05/09/23 12:55:59.124
May  9 12:55:59.129: INFO: Waiting up to 5m0s for pod "pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87" in namespace "emptydir-8738" to be "Succeeded or Failed"
May  9 12:55:59.131: INFO: Pod "pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87": Phase="Pending", Reason="", readiness=false. Elapsed: 1.694407ms
May  9 12:56:01.135: INFO: Pod "pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005869752s
May  9 12:56:03.134: INFO: Pod "pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005014728s
STEP: Saw pod success 05/09/23 12:56:03.134
May  9 12:56:03.134: INFO: Pod "pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87" satisfied condition "Succeeded or Failed"
May  9 12:56:03.136: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87 container test-container: <nil>
STEP: delete the pod 05/09/23 12:56:03.14
May  9 12:56:03.151: INFO: Waiting for pod pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87 to disappear
May  9 12:56:03.153: INFO: Pod pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 12:56:03.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8738" for this suite. 05/09/23 12:56:03.155
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":91,"skipped":1751,"failed":0}
------------------------------
• [4.052 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:55:59.107
    May  9 12:55:59.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 12:55:59.107
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:55:59.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:55:59.123
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 05/09/23 12:55:59.124
    May  9 12:55:59.129: INFO: Waiting up to 5m0s for pod "pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87" in namespace "emptydir-8738" to be "Succeeded or Failed"
    May  9 12:55:59.131: INFO: Pod "pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87": Phase="Pending", Reason="", readiness=false. Elapsed: 1.694407ms
    May  9 12:56:01.135: INFO: Pod "pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005869752s
    May  9 12:56:03.134: INFO: Pod "pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005014728s
    STEP: Saw pod success 05/09/23 12:56:03.134
    May  9 12:56:03.134: INFO: Pod "pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87" satisfied condition "Succeeded or Failed"
    May  9 12:56:03.136: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87 container test-container: <nil>
    STEP: delete the pod 05/09/23 12:56:03.14
    May  9 12:56:03.151: INFO: Waiting for pod pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87 to disappear
    May  9 12:56:03.153: INFO: Pod pod-9de69b29-a4d4-4936-a0fe-ca6b38919d87 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 12:56:03.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8738" for this suite. 05/09/23 12:56:03.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:56:03.161
May  9 12:56:03.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename security-context-test 05/09/23 12:56:03.162
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:56:03.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:56:03.174
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
May  9 12:56:03.180: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f0f25ffc-a38e-42bb-bf12-903b3e4608c4" in namespace "security-context-test-309" to be "Succeeded or Failed"
May  9 12:56:03.182: INFO: Pod "busybox-readonly-false-f0f25ffc-a38e-42bb-bf12-903b3e4608c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.509133ms
May  9 12:56:05.185: INFO: Pod "busybox-readonly-false-f0f25ffc-a38e-42bb-bf12-903b3e4608c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004713502s
May  9 12:56:07.185: INFO: Pod "busybox-readonly-false-f0f25ffc-a38e-42bb-bf12-903b3e4608c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005177114s
May  9 12:56:07.185: INFO: Pod "busybox-readonly-false-f0f25ffc-a38e-42bb-bf12-903b3e4608c4" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  9 12:56:07.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-309" for this suite. 05/09/23 12:56:07.189
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":92,"skipped":1783,"failed":0}
------------------------------
• [4.033 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:56:03.161
    May  9 12:56:03.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename security-context-test 05/09/23 12:56:03.162
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:56:03.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:56:03.174
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    May  9 12:56:03.180: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f0f25ffc-a38e-42bb-bf12-903b3e4608c4" in namespace "security-context-test-309" to be "Succeeded or Failed"
    May  9 12:56:03.182: INFO: Pod "busybox-readonly-false-f0f25ffc-a38e-42bb-bf12-903b3e4608c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.509133ms
    May  9 12:56:05.185: INFO: Pod "busybox-readonly-false-f0f25ffc-a38e-42bb-bf12-903b3e4608c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004713502s
    May  9 12:56:07.185: INFO: Pod "busybox-readonly-false-f0f25ffc-a38e-42bb-bf12-903b3e4608c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005177114s
    May  9 12:56:07.185: INFO: Pod "busybox-readonly-false-f0f25ffc-a38e-42bb-bf12-903b3e4608c4" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  9 12:56:07.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-309" for this suite. 05/09/23 12:56:07.189
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:56:07.194
May  9 12:56:07.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 12:56:07.195
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:56:07.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:56:07.209
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 05/09/23 12:56:07.211
May  9 12:56:07.211: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May  9 12:56:07.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 create -f -'
May  9 12:56:07.835: INFO: stderr: ""
May  9 12:56:07.835: INFO: stdout: "service/agnhost-replica created\n"
May  9 12:56:07.835: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May  9 12:56:07.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 create -f -'
May  9 12:56:07.970: INFO: stderr: ""
May  9 12:56:07.970: INFO: stdout: "service/agnhost-primary created\n"
May  9 12:56:07.970: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May  9 12:56:07.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 create -f -'
May  9 12:56:08.584: INFO: stderr: ""
May  9 12:56:08.584: INFO: stdout: "service/frontend created\n"
May  9 12:56:08.584: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May  9 12:56:08.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 create -f -'
May  9 12:56:08.711: INFO: stderr: ""
May  9 12:56:08.711: INFO: stdout: "deployment.apps/frontend created\n"
May  9 12:56:08.711: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  9 12:56:08.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 create -f -'
May  9 12:56:08.837: INFO: stderr: ""
May  9 12:56:08.837: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May  9 12:56:08.838: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  9 12:56:08.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 create -f -'
May  9 12:56:08.970: INFO: stderr: ""
May  9 12:56:08.970: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 05/09/23 12:56:08.97
May  9 12:56:08.970: INFO: Waiting for all frontend pods to be Running.
May  9 12:56:14.022: INFO: Waiting for frontend to serve content.
May  9 12:56:14.029: INFO: Trying to add a new entry to the guestbook.
May  9 12:56:14.036: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 05/09/23 12:56:14.042
May  9 12:56:14.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
May  9 12:56:14.113: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 12:56:14.114: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 05/09/23 12:56:14.114
May  9 12:56:14.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
May  9 12:56:14.180: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 12:56:14.180: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/09/23 12:56:14.18
May  9 12:56:14.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
May  9 12:56:14.251: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 12:56:14.251: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/09/23 12:56:14.251
May  9 12:56:14.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
May  9 12:56:14.309: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 12:56:14.309: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/09/23 12:56:14.309
May  9 12:56:14.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
May  9 12:56:14.368: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 12:56:14.368: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/09/23 12:56:14.368
May  9 12:56:14.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
May  9 12:56:14.428: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 12:56:14.428: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 12:56:14.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-492" for this suite. 05/09/23 12:56:14.432
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":93,"skipped":1784,"failed":0}
------------------------------
• [SLOW TEST] [7.253 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:56:07.194
    May  9 12:56:07.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 12:56:07.195
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:56:07.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:56:07.209
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 05/09/23 12:56:07.211
    May  9 12:56:07.211: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    May  9 12:56:07.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 create -f -'
    May  9 12:56:07.835: INFO: stderr: ""
    May  9 12:56:07.835: INFO: stdout: "service/agnhost-replica created\n"
    May  9 12:56:07.835: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    May  9 12:56:07.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 create -f -'
    May  9 12:56:07.970: INFO: stderr: ""
    May  9 12:56:07.970: INFO: stdout: "service/agnhost-primary created\n"
    May  9 12:56:07.970: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    May  9 12:56:07.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 create -f -'
    May  9 12:56:08.584: INFO: stderr: ""
    May  9 12:56:08.584: INFO: stdout: "service/frontend created\n"
    May  9 12:56:08.584: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    May  9 12:56:08.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 create -f -'
    May  9 12:56:08.711: INFO: stderr: ""
    May  9 12:56:08.711: INFO: stdout: "deployment.apps/frontend created\n"
    May  9 12:56:08.711: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May  9 12:56:08.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 create -f -'
    May  9 12:56:08.837: INFO: stderr: ""
    May  9 12:56:08.837: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    May  9 12:56:08.838: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May  9 12:56:08.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 create -f -'
    May  9 12:56:08.970: INFO: stderr: ""
    May  9 12:56:08.970: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 05/09/23 12:56:08.97
    May  9 12:56:08.970: INFO: Waiting for all frontend pods to be Running.
    May  9 12:56:14.022: INFO: Waiting for frontend to serve content.
    May  9 12:56:14.029: INFO: Trying to add a new entry to the guestbook.
    May  9 12:56:14.036: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 05/09/23 12:56:14.042
    May  9 12:56:14.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
    May  9 12:56:14.113: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 12:56:14.114: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 05/09/23 12:56:14.114
    May  9 12:56:14.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
    May  9 12:56:14.180: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 12:56:14.180: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/09/23 12:56:14.18
    May  9 12:56:14.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
    May  9 12:56:14.251: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 12:56:14.251: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/09/23 12:56:14.251
    May  9 12:56:14.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
    May  9 12:56:14.309: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 12:56:14.309: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/09/23 12:56:14.309
    May  9 12:56:14.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
    May  9 12:56:14.368: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 12:56:14.368: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/09/23 12:56:14.368
    May  9 12:56:14.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-492 delete --grace-period=0 --force -f -'
    May  9 12:56:14.428: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 12:56:14.428: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 12:56:14.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-492" for this suite. 05/09/23 12:56:14.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:56:14.448
May  9 12:56:14.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename disruption 05/09/23 12:56:14.448
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:56:14.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:56:14.467
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 05/09/23 12:56:14.468
STEP: Waiting for the pdb to be processed 05/09/23 12:56:14.475
STEP: First trying to evict a pod which shouldn't be evictable 05/09/23 12:56:16.49
STEP: Waiting for all pods to be running 05/09/23 12:56:16.49
May  9 12:56:16.492: INFO: pods: 0 < 3
STEP: locating a running pod 05/09/23 12:56:18.497
STEP: Updating the pdb to allow a pod to be evicted 05/09/23 12:56:18.503
STEP: Waiting for the pdb to be processed 05/09/23 12:56:18.511
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/09/23 12:56:20.519
STEP: Waiting for all pods to be running 05/09/23 12:56:20.519
STEP: Waiting for the pdb to observed all healthy pods 05/09/23 12:56:20.522
STEP: Patching the pdb to disallow a pod to be evicted 05/09/23 12:56:20.539
STEP: Waiting for the pdb to be processed 05/09/23 12:56:20.572
STEP: Waiting for all pods to be running 05/09/23 12:56:20.575
May  9 12:56:20.577: INFO: running pods: 2 < 3
STEP: locating a running pod 05/09/23 12:56:22.58
STEP: Deleting the pdb to allow a pod to be evicted 05/09/23 12:56:22.587
STEP: Waiting for the pdb to be deleted 05/09/23 12:56:22.595
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/09/23 12:56:22.597
STEP: Waiting for all pods to be running 05/09/23 12:56:22.597
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  9 12:56:22.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6664" for this suite. 05/09/23 12:56:22.609
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":94,"skipped":1818,"failed":0}
------------------------------
• [SLOW TEST] [8.175 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:56:14.448
    May  9 12:56:14.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename disruption 05/09/23 12:56:14.448
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:56:14.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:56:14.467
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 05/09/23 12:56:14.468
    STEP: Waiting for the pdb to be processed 05/09/23 12:56:14.475
    STEP: First trying to evict a pod which shouldn't be evictable 05/09/23 12:56:16.49
    STEP: Waiting for all pods to be running 05/09/23 12:56:16.49
    May  9 12:56:16.492: INFO: pods: 0 < 3
    STEP: locating a running pod 05/09/23 12:56:18.497
    STEP: Updating the pdb to allow a pod to be evicted 05/09/23 12:56:18.503
    STEP: Waiting for the pdb to be processed 05/09/23 12:56:18.511
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/09/23 12:56:20.519
    STEP: Waiting for all pods to be running 05/09/23 12:56:20.519
    STEP: Waiting for the pdb to observed all healthy pods 05/09/23 12:56:20.522
    STEP: Patching the pdb to disallow a pod to be evicted 05/09/23 12:56:20.539
    STEP: Waiting for the pdb to be processed 05/09/23 12:56:20.572
    STEP: Waiting for all pods to be running 05/09/23 12:56:20.575
    May  9 12:56:20.577: INFO: running pods: 2 < 3
    STEP: locating a running pod 05/09/23 12:56:22.58
    STEP: Deleting the pdb to allow a pod to be evicted 05/09/23 12:56:22.587
    STEP: Waiting for the pdb to be deleted 05/09/23 12:56:22.595
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/09/23 12:56:22.597
    STEP: Waiting for all pods to be running 05/09/23 12:56:22.597
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  9 12:56:22.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6664" for this suite. 05/09/23 12:56:22.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:56:22.623
May  9 12:56:22.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename cronjob 05/09/23 12:56:22.624
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:56:22.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:56:22.641
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 05/09/23 12:56:22.643
STEP: Ensuring more than one job is running at a time 05/09/23 12:56:22.648
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/09/23 12:58:00.651
STEP: Removing cronjob 05/09/23 12:58:00.654
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  9 12:58:00.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6079" for this suite. 05/09/23 12:58:00.668
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":95,"skipped":1840,"failed":0}
------------------------------
• [SLOW TEST] [98.051 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:56:22.623
    May  9 12:56:22.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename cronjob 05/09/23 12:56:22.624
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:56:22.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:56:22.641
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 05/09/23 12:56:22.643
    STEP: Ensuring more than one job is running at a time 05/09/23 12:56:22.648
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/09/23 12:58:00.651
    STEP: Removing cronjob 05/09/23 12:58:00.654
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  9 12:58:00.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6079" for this suite. 05/09/23 12:58:00.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:58:00.675
May  9 12:58:00.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 12:58:00.676
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:00.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:00.703
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1658 05/09/23 12:58:00.704
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/09/23 12:58:00.723
STEP: creating service externalsvc in namespace services-1658 05/09/23 12:58:00.723
STEP: creating replication controller externalsvc in namespace services-1658 05/09/23 12:58:00.74
I0509 12:58:00.748151      24 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1658, replica count: 2
I0509 12:58:03.800227      24 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 05/09/23 12:58:03.803
May  9 12:58:03.820: INFO: Creating new exec pod
May  9 12:58:03.829: INFO: Waiting up to 5m0s for pod "execpod6zvjv" in namespace "services-1658" to be "running"
May  9 12:58:03.832: INFO: Pod "execpod6zvjv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.264273ms
May  9 12:58:05.835: INFO: Pod "execpod6zvjv": Phase="Running", Reason="", readiness=true. Elapsed: 2.006080028s
May  9 12:58:05.835: INFO: Pod "execpod6zvjv" satisfied condition "running"
May  9 12:58:05.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1658 exec execpod6zvjv -- /bin/sh -x -c nslookup clusterip-service.services-1658.svc.cluster.local'
May  9 12:58:05.959: INFO: stderr: "+ nslookup clusterip-service.services-1658.svc.cluster.local\n"
May  9 12:58:05.959: INFO: stdout: "Server:\t\t169.254.53.53\nAddress:\t169.254.53.53#53\n\nclusterip-service.services-1658.svc.cluster.local\tcanonical name = externalsvc.services-1658.svc.cluster.local.\nName:\texternalsvc.services-1658.svc.cluster.local\nAddress: 10.103.237.185\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1658, will wait for the garbage collector to delete the pods 05/09/23 12:58:05.959
May  9 12:58:06.019: INFO: Deleting ReplicationController externalsvc took: 5.30563ms
May  9 12:58:06.120: INFO: Terminating ReplicationController externalsvc pods took: 100.56668ms
May  9 12:58:07.738: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 12:58:07.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1658" for this suite. 05/09/23 12:58:07.753
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":96,"skipped":1862,"failed":0}
------------------------------
• [SLOW TEST] [7.084 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:58:00.675
    May  9 12:58:00.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 12:58:00.676
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:00.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:00.703
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1658 05/09/23 12:58:00.704
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/09/23 12:58:00.723
    STEP: creating service externalsvc in namespace services-1658 05/09/23 12:58:00.723
    STEP: creating replication controller externalsvc in namespace services-1658 05/09/23 12:58:00.74
    I0509 12:58:00.748151      24 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1658, replica count: 2
    I0509 12:58:03.800227      24 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 05/09/23 12:58:03.803
    May  9 12:58:03.820: INFO: Creating new exec pod
    May  9 12:58:03.829: INFO: Waiting up to 5m0s for pod "execpod6zvjv" in namespace "services-1658" to be "running"
    May  9 12:58:03.832: INFO: Pod "execpod6zvjv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.264273ms
    May  9 12:58:05.835: INFO: Pod "execpod6zvjv": Phase="Running", Reason="", readiness=true. Elapsed: 2.006080028s
    May  9 12:58:05.835: INFO: Pod "execpod6zvjv" satisfied condition "running"
    May  9 12:58:05.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1658 exec execpod6zvjv -- /bin/sh -x -c nslookup clusterip-service.services-1658.svc.cluster.local'
    May  9 12:58:05.959: INFO: stderr: "+ nslookup clusterip-service.services-1658.svc.cluster.local\n"
    May  9 12:58:05.959: INFO: stdout: "Server:\t\t169.254.53.53\nAddress:\t169.254.53.53#53\n\nclusterip-service.services-1658.svc.cluster.local\tcanonical name = externalsvc.services-1658.svc.cluster.local.\nName:\texternalsvc.services-1658.svc.cluster.local\nAddress: 10.103.237.185\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1658, will wait for the garbage collector to delete the pods 05/09/23 12:58:05.959
    May  9 12:58:06.019: INFO: Deleting ReplicationController externalsvc took: 5.30563ms
    May  9 12:58:06.120: INFO: Terminating ReplicationController externalsvc pods took: 100.56668ms
    May  9 12:58:07.738: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 12:58:07.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1658" for this suite. 05/09/23 12:58:07.753
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:58:07.759
May  9 12:58:07.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 12:58:07.76
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:07.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:07.774
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 05/09/23 12:58:07.775
May  9 12:58:07.783: INFO: Waiting up to 5m0s for pod "pod-3b39a665-60de-4cb7-ba12-1c318bbe456a" in namespace "emptydir-9792" to be "Succeeded or Failed"
May  9 12:58:07.785: INFO: Pod "pod-3b39a665-60de-4cb7-ba12-1c318bbe456a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029593ms
May  9 12:58:09.789: INFO: Pod "pod-3b39a665-60de-4cb7-ba12-1c318bbe456a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005758946s
May  9 12:58:11.788: INFO: Pod "pod-3b39a665-60de-4cb7-ba12-1c318bbe456a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005216608s
STEP: Saw pod success 05/09/23 12:58:11.788
May  9 12:58:11.788: INFO: Pod "pod-3b39a665-60de-4cb7-ba12-1c318bbe456a" satisfied condition "Succeeded or Failed"
May  9 12:58:11.790: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-3b39a665-60de-4cb7-ba12-1c318bbe456a container test-container: <nil>
STEP: delete the pod 05/09/23 12:58:11.8
May  9 12:58:11.811: INFO: Waiting for pod pod-3b39a665-60de-4cb7-ba12-1c318bbe456a to disappear
May  9 12:58:11.813: INFO: Pod pod-3b39a665-60de-4cb7-ba12-1c318bbe456a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 12:58:11.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9792" for this suite. 05/09/23 12:58:11.817
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":97,"skipped":1863,"failed":0}
------------------------------
• [4.063 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:58:07.759
    May  9 12:58:07.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 12:58:07.76
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:07.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:07.774
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/09/23 12:58:07.775
    May  9 12:58:07.783: INFO: Waiting up to 5m0s for pod "pod-3b39a665-60de-4cb7-ba12-1c318bbe456a" in namespace "emptydir-9792" to be "Succeeded or Failed"
    May  9 12:58:07.785: INFO: Pod "pod-3b39a665-60de-4cb7-ba12-1c318bbe456a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029593ms
    May  9 12:58:09.789: INFO: Pod "pod-3b39a665-60de-4cb7-ba12-1c318bbe456a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005758946s
    May  9 12:58:11.788: INFO: Pod "pod-3b39a665-60de-4cb7-ba12-1c318bbe456a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005216608s
    STEP: Saw pod success 05/09/23 12:58:11.788
    May  9 12:58:11.788: INFO: Pod "pod-3b39a665-60de-4cb7-ba12-1c318bbe456a" satisfied condition "Succeeded or Failed"
    May  9 12:58:11.790: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-3b39a665-60de-4cb7-ba12-1c318bbe456a container test-container: <nil>
    STEP: delete the pod 05/09/23 12:58:11.8
    May  9 12:58:11.811: INFO: Waiting for pod pod-3b39a665-60de-4cb7-ba12-1c318bbe456a to disappear
    May  9 12:58:11.813: INFO: Pod pod-3b39a665-60de-4cb7-ba12-1c318bbe456a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 12:58:11.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9792" for this suite. 05/09/23 12:58:11.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:58:11.822
May  9 12:58:11.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename events 05/09/23 12:58:11.823
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:11.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:11.837
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 05/09/23 12:58:11.838
May  9 12:58:11.845: INFO: created test-event-1
May  9 12:58:11.848: INFO: created test-event-2
May  9 12:58:11.851: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 05/09/23 12:58:11.851
STEP: delete collection of events 05/09/23 12:58:11.854
May  9 12:58:11.854: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/09/23 12:58:11.871
May  9 12:58:11.871: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
May  9 12:58:11.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2087" for this suite. 05/09/23 12:58:11.876
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":98,"skipped":1887,"failed":0}
------------------------------
• [0.061 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:58:11.822
    May  9 12:58:11.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename events 05/09/23 12:58:11.823
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:11.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:11.837
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 05/09/23 12:58:11.838
    May  9 12:58:11.845: INFO: created test-event-1
    May  9 12:58:11.848: INFO: created test-event-2
    May  9 12:58:11.851: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 05/09/23 12:58:11.851
    STEP: delete collection of events 05/09/23 12:58:11.854
    May  9 12:58:11.854: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/09/23 12:58:11.871
    May  9 12:58:11.871: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    May  9 12:58:11.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-2087" for this suite. 05/09/23 12:58:11.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:58:11.883
May  9 12:58:11.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 12:58:11.884
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:11.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:11.897
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 05/09/23 12:58:11.898
May  9 12:58:11.905: INFO: Waiting up to 5m0s for pod "labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5" in namespace "downward-api-867" to be "running and ready"
May  9 12:58:11.907: INFO: Pod "labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.904495ms
May  9 12:58:11.907: INFO: The phase of Pod labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5 is Pending, waiting for it to be Running (with Ready = true)
May  9 12:58:13.911: INFO: Pod "labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005790663s
May  9 12:58:13.911: INFO: The phase of Pod labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5 is Running (Ready = true)
May  9 12:58:13.911: INFO: Pod "labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5" satisfied condition "running and ready"
May  9 12:58:14.426: INFO: Successfully updated pod "labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  9 12:58:18.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-867" for this suite. 05/09/23 12:58:18.45
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":99,"skipped":1905,"failed":0}
------------------------------
• [SLOW TEST] [6.572 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:58:11.883
    May  9 12:58:11.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 12:58:11.884
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:11.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:11.897
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 05/09/23 12:58:11.898
    May  9 12:58:11.905: INFO: Waiting up to 5m0s for pod "labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5" in namespace "downward-api-867" to be "running and ready"
    May  9 12:58:11.907: INFO: Pod "labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.904495ms
    May  9 12:58:11.907: INFO: The phase of Pod labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5 is Pending, waiting for it to be Running (with Ready = true)
    May  9 12:58:13.911: INFO: Pod "labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005790663s
    May  9 12:58:13.911: INFO: The phase of Pod labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5 is Running (Ready = true)
    May  9 12:58:13.911: INFO: Pod "labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5" satisfied condition "running and ready"
    May  9 12:58:14.426: INFO: Successfully updated pod "labelsupdatee63caadd-ae55-4e22-8131-ee0400bf16f5"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  9 12:58:18.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-867" for this suite. 05/09/23 12:58:18.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:58:18.456
May  9 12:58:18.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 12:58:18.457
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:18.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:18.471
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-e2fe4c0d-7d13-42e2-93e8-de190291e137 05/09/23 12:58:18.472
STEP: Creating a pod to test consume configMaps 05/09/23 12:58:18.475
May  9 12:58:18.481: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab" in namespace "projected-2902" to be "Succeeded or Failed"
May  9 12:58:18.482: INFO: Pod "pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab": Phase="Pending", Reason="", readiness=false. Elapsed: 1.706869ms
May  9 12:58:20.488: INFO: Pod "pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006714415s
May  9 12:58:22.486: INFO: Pod "pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005081662s
STEP: Saw pod success 05/09/23 12:58:22.486
May  9 12:58:22.486: INFO: Pod "pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab" satisfied condition "Succeeded or Failed"
May  9 12:58:22.488: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab container agnhost-container: <nil>
STEP: delete the pod 05/09/23 12:58:22.5
May  9 12:58:22.515: INFO: Waiting for pod pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab to disappear
May  9 12:58:22.516: INFO: Pod pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  9 12:58:22.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2902" for this suite. 05/09/23 12:58:22.52
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":100,"skipped":1920,"failed":0}
------------------------------
• [4.068 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:58:18.456
    May  9 12:58:18.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 12:58:18.457
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:18.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:18.471
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-e2fe4c0d-7d13-42e2-93e8-de190291e137 05/09/23 12:58:18.472
    STEP: Creating a pod to test consume configMaps 05/09/23 12:58:18.475
    May  9 12:58:18.481: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab" in namespace "projected-2902" to be "Succeeded or Failed"
    May  9 12:58:18.482: INFO: Pod "pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab": Phase="Pending", Reason="", readiness=false. Elapsed: 1.706869ms
    May  9 12:58:20.488: INFO: Pod "pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006714415s
    May  9 12:58:22.486: INFO: Pod "pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005081662s
    STEP: Saw pod success 05/09/23 12:58:22.486
    May  9 12:58:22.486: INFO: Pod "pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab" satisfied condition "Succeeded or Failed"
    May  9 12:58:22.488: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 12:58:22.5
    May  9 12:58:22.515: INFO: Waiting for pod pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab to disappear
    May  9 12:58:22.516: INFO: Pod pod-projected-configmaps-09fb8e2c-f50d-44b9-9948-45d647553fab no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  9 12:58:22.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2902" for this suite. 05/09/23 12:58:22.52
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:58:22.524
May  9 12:58:22.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 12:58:22.525
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:22.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:22.537
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-2560e882-e336-483d-b4b4-8ce68e79f1ce 05/09/23 12:58:22.539
STEP: Creating a pod to test consume secrets 05/09/23 12:58:22.543
May  9 12:58:22.549: INFO: Waiting up to 5m0s for pod "pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac" in namespace "secrets-8771" to be "Succeeded or Failed"
May  9 12:58:22.550: INFO: Pod "pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 1.604805ms
May  9 12:58:24.554: INFO: Pod "pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004687792s
May  9 12:58:26.553: INFO: Pod "pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004546492s
STEP: Saw pod success 05/09/23 12:58:26.553
May  9 12:58:26.554: INFO: Pod "pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac" satisfied condition "Succeeded or Failed"
May  9 12:58:26.555: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 12:58:26.56
May  9 12:58:26.574: INFO: Waiting for pod pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac to disappear
May  9 12:58:26.576: INFO: Pod pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  9 12:58:26.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8771" for this suite. 05/09/23 12:58:26.579
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":101,"skipped":1922,"failed":0}
------------------------------
• [4.060 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:58:22.524
    May  9 12:58:22.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 12:58:22.525
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:22.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:22.537
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-2560e882-e336-483d-b4b4-8ce68e79f1ce 05/09/23 12:58:22.539
    STEP: Creating a pod to test consume secrets 05/09/23 12:58:22.543
    May  9 12:58:22.549: INFO: Waiting up to 5m0s for pod "pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac" in namespace "secrets-8771" to be "Succeeded or Failed"
    May  9 12:58:22.550: INFO: Pod "pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 1.604805ms
    May  9 12:58:24.554: INFO: Pod "pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004687792s
    May  9 12:58:26.553: INFO: Pod "pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004546492s
    STEP: Saw pod success 05/09/23 12:58:26.553
    May  9 12:58:26.554: INFO: Pod "pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac" satisfied condition "Succeeded or Failed"
    May  9 12:58:26.555: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 12:58:26.56
    May  9 12:58:26.574: INFO: Waiting for pod pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac to disappear
    May  9 12:58:26.576: INFO: Pod pod-secrets-8fe065a7-09b0-4674-a83e-56fc7e18d7ac no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  9 12:58:26.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8771" for this suite. 05/09/23 12:58:26.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:58:26.585
May  9 12:58:26.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 12:58:26.585
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:26.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:26.598
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
May  9 12:58:26.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 12:58:29.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7274" for this suite. 05/09/23 12:58:29.7
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":102,"skipped":1936,"failed":0}
------------------------------
• [3.121 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:58:26.585
    May  9 12:58:26.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 12:58:26.585
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:26.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:26.598
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    May  9 12:58:26.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 12:58:29.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7274" for this suite. 05/09/23 12:58:29.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:58:29.706
May  9 12:58:29.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 12:58:29.707
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:29.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:29.722
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 05/09/23 12:58:29.723
May  9 12:58:29.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904" in namespace "downward-api-5162" to be "Succeeded or Failed"
May  9 12:58:29.732: INFO: Pod "downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904": Phase="Pending", Reason="", readiness=false. Elapsed: 1.825765ms
May  9 12:58:31.735: INFO: Pod "downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005229494s
May  9 12:58:33.736: INFO: Pod "downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00592615s
STEP: Saw pod success 05/09/23 12:58:33.736
May  9 12:58:33.736: INFO: Pod "downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904" satisfied condition "Succeeded or Failed"
May  9 12:58:33.738: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904 container client-container: <nil>
STEP: delete the pod 05/09/23 12:58:33.743
May  9 12:58:33.754: INFO: Waiting for pod downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904 to disappear
May  9 12:58:33.756: INFO: Pod downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  9 12:58:33.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5162" for this suite. 05/09/23 12:58:33.76
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":103,"skipped":1948,"failed":0}
------------------------------
• [4.058 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:58:29.706
    May  9 12:58:29.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 12:58:29.707
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:29.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:29.722
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 05/09/23 12:58:29.723
    May  9 12:58:29.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904" in namespace "downward-api-5162" to be "Succeeded or Failed"
    May  9 12:58:29.732: INFO: Pod "downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904": Phase="Pending", Reason="", readiness=false. Elapsed: 1.825765ms
    May  9 12:58:31.735: INFO: Pod "downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005229494s
    May  9 12:58:33.736: INFO: Pod "downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00592615s
    STEP: Saw pod success 05/09/23 12:58:33.736
    May  9 12:58:33.736: INFO: Pod "downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904" satisfied condition "Succeeded or Failed"
    May  9 12:58:33.738: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904 container client-container: <nil>
    STEP: delete the pod 05/09/23 12:58:33.743
    May  9 12:58:33.754: INFO: Waiting for pod downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904 to disappear
    May  9 12:58:33.756: INFO: Pod downwardapi-volume-7ec747fa-def0-42a8-ae69-f2323ac3b904 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  9 12:58:33.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5162" for this suite. 05/09/23 12:58:33.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:58:33.766
May  9 12:58:33.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-runtime 05/09/23 12:58:33.767
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:33.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:33.779
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 05/09/23 12:58:33.781
STEP: wait for the container to reach Succeeded 05/09/23 12:58:33.787
STEP: get the container status 05/09/23 12:58:36.803
STEP: the container should be terminated 05/09/23 12:58:36.808
STEP: the termination message should be set 05/09/23 12:58:36.809
May  9 12:58:36.809: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 05/09/23 12:58:36.809
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  9 12:58:36.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3213" for this suite. 05/09/23 12:58:36.838
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":104,"skipped":1995,"failed":0}
------------------------------
• [3.082 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:58:33.766
    May  9 12:58:33.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-runtime 05/09/23 12:58:33.767
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:33.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:33.779
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 05/09/23 12:58:33.781
    STEP: wait for the container to reach Succeeded 05/09/23 12:58:33.787
    STEP: get the container status 05/09/23 12:58:36.803
    STEP: the container should be terminated 05/09/23 12:58:36.808
    STEP: the termination message should be set 05/09/23 12:58:36.809
    May  9 12:58:36.809: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 05/09/23 12:58:36.809
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  9 12:58:36.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3213" for this suite. 05/09/23 12:58:36.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:58:36.848
May  9 12:58:36.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 12:58:36.849
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:36.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:36.873
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-3d4942ae-49bf-4d35-bf17-b5e289b6702d 05/09/23 12:58:36.875
STEP: Creating a pod to test consume configMaps 05/09/23 12:58:36.882
May  9 12:58:36.889: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6" in namespace "projected-4716" to be "Succeeded or Failed"
May  9 12:58:36.890: INFO: Pod "pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.746615ms
May  9 12:58:38.893: INFO: Pod "pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004270257s
May  9 12:58:40.894: INFO: Pod "pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005204648s
STEP: Saw pod success 05/09/23 12:58:40.894
May  9 12:58:40.894: INFO: Pod "pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6" satisfied condition "Succeeded or Failed"
May  9 12:58:40.896: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 12:58:40.901
May  9 12:58:40.912: INFO: Waiting for pod pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6 to disappear
May  9 12:58:40.914: INFO: Pod pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  9 12:58:40.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4716" for this suite. 05/09/23 12:58:40.917
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":105,"skipped":2000,"failed":0}
------------------------------
• [4.073 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:58:36.848
    May  9 12:58:36.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 12:58:36.849
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:36.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:36.873
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-3d4942ae-49bf-4d35-bf17-b5e289b6702d 05/09/23 12:58:36.875
    STEP: Creating a pod to test consume configMaps 05/09/23 12:58:36.882
    May  9 12:58:36.889: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6" in namespace "projected-4716" to be "Succeeded or Failed"
    May  9 12:58:36.890: INFO: Pod "pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.746615ms
    May  9 12:58:38.893: INFO: Pod "pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004270257s
    May  9 12:58:40.894: INFO: Pod "pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005204648s
    STEP: Saw pod success 05/09/23 12:58:40.894
    May  9 12:58:40.894: INFO: Pod "pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6" satisfied condition "Succeeded or Failed"
    May  9 12:58:40.896: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 12:58:40.901
    May  9 12:58:40.912: INFO: Waiting for pod pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6 to disappear
    May  9 12:58:40.914: INFO: Pod pod-projected-configmaps-22c9d627-8410-4d0b-8538-5339da5efad6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  9 12:58:40.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4716" for this suite. 05/09/23 12:58:40.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 12:58:40.922
May  9 12:58:40.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename cronjob 05/09/23 12:58:40.923
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:40.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:40.936
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 05/09/23 12:58:40.938
STEP: Ensuring a job is scheduled 05/09/23 12:58:40.943
STEP: Ensuring exactly one is scheduled 05/09/23 12:59:00.948
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/09/23 12:59:00.95
STEP: Ensuring the job is replaced with a new one 05/09/23 12:59:00.952
STEP: Removing cronjob 05/09/23 13:00:00.956
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  9 13:00:00.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8594" for this suite. 05/09/23 13:00:00.964
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":106,"skipped":2029,"failed":0}
------------------------------
• [SLOW TEST] [80.049 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 12:58:40.922
    May  9 12:58:40.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename cronjob 05/09/23 12:58:40.923
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 12:58:40.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 12:58:40.936
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 05/09/23 12:58:40.938
    STEP: Ensuring a job is scheduled 05/09/23 12:58:40.943
    STEP: Ensuring exactly one is scheduled 05/09/23 12:59:00.948
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/09/23 12:59:00.95
    STEP: Ensuring the job is replaced with a new one 05/09/23 12:59:00.952
    STEP: Removing cronjob 05/09/23 13:00:00.956
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  9 13:00:00.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8594" for this suite. 05/09/23 13:00:00.964
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:00:00.971
May  9 13:00:00.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename gc 05/09/23 13:00:00.972
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:00:00.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:00:00.994
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
May  9 13:00:01.015: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8c3ca945-5695-4228-ba7e-dc9468377260", Controller:(*bool)(0xc003ad33b6), BlockOwnerDeletion:(*bool)(0xc003ad33b7)}}
May  9 13:00:01.027: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"aacba175-5b26-4f90-9b69-2d3f1a9952be", Controller:(*bool)(0xc0034c38ba), BlockOwnerDeletion:(*bool)(0xc0034c38bb)}}
May  9 13:00:01.037: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"323f88fb-af87-469b-8cb6-54e1fc384612", Controller:(*bool)(0xc003ad35ca), BlockOwnerDeletion:(*bool)(0xc003ad35cb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  9 13:00:06.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9789" for this suite. 05/09/23 13:00:06.055
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":107,"skipped":2029,"failed":0}
------------------------------
• [SLOW TEST] [5.090 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:00:00.971
    May  9 13:00:00.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename gc 05/09/23 13:00:00.972
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:00:00.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:00:00.994
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    May  9 13:00:01.015: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8c3ca945-5695-4228-ba7e-dc9468377260", Controller:(*bool)(0xc003ad33b6), BlockOwnerDeletion:(*bool)(0xc003ad33b7)}}
    May  9 13:00:01.027: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"aacba175-5b26-4f90-9b69-2d3f1a9952be", Controller:(*bool)(0xc0034c38ba), BlockOwnerDeletion:(*bool)(0xc0034c38bb)}}
    May  9 13:00:01.037: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"323f88fb-af87-469b-8cb6-54e1fc384612", Controller:(*bool)(0xc003ad35ca), BlockOwnerDeletion:(*bool)(0xc003ad35cb)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  9 13:00:06.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9789" for this suite. 05/09/23 13:00:06.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:00:06.061
May  9 13:00:06.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename taint-single-pod 05/09/23 13:00:06.062
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:00:06.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:00:06.081
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
May  9 13:00:06.082: INFO: Waiting up to 1m0s for all nodes to be ready
May  9 13:01:06.114: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
May  9 13:01:06.116: INFO: Starting informer...
STEP: Starting pod... 05/09/23 13:01:06.116
May  9 13:01:06.328: INFO: Pod is running on cl-gks-cncf-ix1-md-0-48ljh. Tainting Node
STEP: Trying to apply a taint on the Node 05/09/23 13:01:06.329
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 13:01:06.338
STEP: Waiting short time to make sure Pod is queued for deletion 05/09/23 13:01:06.341
May  9 13:01:06.341: INFO: Pod wasn't evicted. Proceeding
May  9 13:01:06.341: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 13:01:06.354
STEP: Waiting some time to make sure that toleration time passed. 05/09/23 13:01:06.356
May  9 13:02:21.360: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
May  9 13:02:21.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1386" for this suite. 05/09/23 13:02:21.364
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":108,"skipped":2046,"failed":0}
------------------------------
• [SLOW TEST] [135.308 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:00:06.061
    May  9 13:00:06.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename taint-single-pod 05/09/23 13:00:06.062
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:00:06.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:00:06.081
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    May  9 13:00:06.082: INFO: Waiting up to 1m0s for all nodes to be ready
    May  9 13:01:06.114: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    May  9 13:01:06.116: INFO: Starting informer...
    STEP: Starting pod... 05/09/23 13:01:06.116
    May  9 13:01:06.328: INFO: Pod is running on cl-gks-cncf-ix1-md-0-48ljh. Tainting Node
    STEP: Trying to apply a taint on the Node 05/09/23 13:01:06.329
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 13:01:06.338
    STEP: Waiting short time to make sure Pod is queued for deletion 05/09/23 13:01:06.341
    May  9 13:01:06.341: INFO: Pod wasn't evicted. Proceeding
    May  9 13:01:06.341: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 13:01:06.354
    STEP: Waiting some time to make sure that toleration time passed. 05/09/23 13:01:06.356
    May  9 13:02:21.360: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:02:21.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-1386" for this suite. 05/09/23 13:02:21.364
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:02:21.369
May  9 13:02:21.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 13:02:21.37
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:21.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:21.386
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 05/09/23 13:02:21.387
May  9 13:02:21.388: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2292 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 05/09/23 13:02:21.424
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 13:02:21.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2292" for this suite. 05/09/23 13:02:21.433
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":109,"skipped":2050,"failed":0}
------------------------------
• [0.069 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:02:21.369
    May  9 13:02:21.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 13:02:21.37
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:21.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:21.386
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 05/09/23 13:02:21.387
    May  9 13:02:21.388: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2292 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 05/09/23 13:02:21.424
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 13:02:21.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2292" for this suite. 05/09/23 13:02:21.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:02:21.438
May  9 13:02:21.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename daemonsets 05/09/23 13:02:21.439
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:21.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:21.457
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 05/09/23 13:02:21.472
STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 13:02:21.477
May  9 13:02:21.482: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:02:21.482: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:02:21.482: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:02:21.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:02:21.484: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
May  9 13:02:22.488: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:02:22.488: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:02:22.488: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:02:22.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 13:02:22.491: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
May  9 13:02:23.487: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:02:23.487: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:02:23.487: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:02:23.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 13:02:23.491: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 05/09/23 13:02:23.493
STEP: DeleteCollection of the DaemonSets 05/09/23 13:02:23.495
STEP: Verify that ReplicaSets have been deleted 05/09/23 13:02:23.5
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
May  9 13:02:23.512: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21759"},"items":null}

May  9 13:02:23.518: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21759"},"items":[{"metadata":{"name":"daemon-set-55xkz","generateName":"daemon-set-","namespace":"daemonsets-2079","uid":"91e76efd-e3c3-4bc5-85a0-664afbef63d6","resourceVersion":"21748","creationTimestamp":"2023-05-09T13:02:21Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"8b79bb74f96bd28d5139ffffb323bfa06d0721ba4037be620a762baa776f1739","cni.projectcalico.org/podIP":"172.25.53.172/32","cni.projectcalico.org/podIPs":"172.25.53.172/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"706c370b-308c-4e59-8802-20f3f6ebc4a3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"706c370b-308c-4e59-8802-20f3f6ebc4a3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jd9wh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jd9wh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cl-gks-cncf-ix1-md-0-879bk","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cl-gks-cncf-ix1-md-0-879bk"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:21Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:22Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:22Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:21Z"}],"hostIP":"192.168.1.73","podIP":"172.25.53.172","podIPs":[{"ip":"172.25.53.172"}],"startTime":"2023-05-09T13:02:21Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-09T13:02:22Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://53f41e1e501b03ef560e805130eb717ad3351d9c24aabcf2afc813cef6c9d723","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ccf67","generateName":"daemon-set-","namespace":"daemonsets-2079","uid":"7d2b04c2-3c61-4bb2-86ae-8cf4a4cdf836","resourceVersion":"21757","creationTimestamp":"2023-05-09T13:02:21Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"fef1c9d87b066cd07cc2908620965944550934df96dfe122a8b88f0bfd43ad27","cni.projectcalico.org/podIP":"172.25.124.212/32","cni.projectcalico.org/podIPs":"172.25.124.212/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"706c370b-308c-4e59-8802-20f3f6ebc4a3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"706c370b-308c-4e59-8802-20f3f6ebc4a3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-snd8z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-snd8z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cl-gks-cncf-ix1-md-0-48ljh","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cl-gks-cncf-ix1-md-0-48ljh"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:21Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:23Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:23Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:21Z"}],"hostIP":"192.168.1.64","podIP":"172.25.124.212","podIPs":[{"ip":"172.25.124.212"}],"startTime":"2023-05-09T13:02:21Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-09T13:02:22Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://d4012133eee93effda82562ab5ae9ab3867b49f0fbc1d8cb5a019eab6991535f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-pxzkc","generateName":"daemon-set-","namespace":"daemonsets-2079","uid":"0cd40544-1b9b-4604-9d1c-5cbf52094b87","resourceVersion":"21752","creationTimestamp":"2023-05-09T13:02:21Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c37a0e9b3d74618a12b73329fb711b975e7c0972d75a731d5ce76340d5cdab80","cni.projectcalico.org/podIP":"172.25.72.209/32","cni.projectcalico.org/podIPs":"172.25.72.209/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"706c370b-308c-4e59-8802-20f3f6ebc4a3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"706c370b-308c-4e59-8802-20f3f6ebc4a3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.72.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qqn7h","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qqn7h","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cl-gks-cncf-ix1-md-0-skqqr","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cl-gks-cncf-ix1-md-0-skqqr"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:21Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:22Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:22Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:21Z"}],"hostIP":"192.168.1.89","podIP":"172.25.72.209","podIPs":[{"ip":"172.25.72.209"}],"startTime":"2023-05-09T13:02:21Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-09T13:02:22Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://aaa2fd73eed4919d38fdb94c075f689d08fc3ca857f405fe2caa2a995ced3b33","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  9 13:02:23.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2079" for this suite. 05/09/23 13:02:23.531
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":110,"skipped":2060,"failed":0}
------------------------------
• [2.097 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:02:21.438
    May  9 13:02:21.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename daemonsets 05/09/23 13:02:21.439
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:21.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:21.457
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 05/09/23 13:02:21.472
    STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 13:02:21.477
    May  9 13:02:21.482: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:02:21.482: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:02:21.482: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:02:21.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:02:21.484: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
    May  9 13:02:22.488: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:02:22.488: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:02:22.488: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:02:22.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 13:02:22.491: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
    May  9 13:02:23.487: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:02:23.487: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:02:23.487: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:02:23.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 13:02:23.491: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 05/09/23 13:02:23.493
    STEP: DeleteCollection of the DaemonSets 05/09/23 13:02:23.495
    STEP: Verify that ReplicaSets have been deleted 05/09/23 13:02:23.5
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    May  9 13:02:23.512: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21759"},"items":null}

    May  9 13:02:23.518: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21759"},"items":[{"metadata":{"name":"daemon-set-55xkz","generateName":"daemon-set-","namespace":"daemonsets-2079","uid":"91e76efd-e3c3-4bc5-85a0-664afbef63d6","resourceVersion":"21748","creationTimestamp":"2023-05-09T13:02:21Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"8b79bb74f96bd28d5139ffffb323bfa06d0721ba4037be620a762baa776f1739","cni.projectcalico.org/podIP":"172.25.53.172/32","cni.projectcalico.org/podIPs":"172.25.53.172/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"706c370b-308c-4e59-8802-20f3f6ebc4a3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"706c370b-308c-4e59-8802-20f3f6ebc4a3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jd9wh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jd9wh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cl-gks-cncf-ix1-md-0-879bk","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cl-gks-cncf-ix1-md-0-879bk"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:21Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:22Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:22Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:21Z"}],"hostIP":"192.168.1.73","podIP":"172.25.53.172","podIPs":[{"ip":"172.25.53.172"}],"startTime":"2023-05-09T13:02:21Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-09T13:02:22Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://53f41e1e501b03ef560e805130eb717ad3351d9c24aabcf2afc813cef6c9d723","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ccf67","generateName":"daemon-set-","namespace":"daemonsets-2079","uid":"7d2b04c2-3c61-4bb2-86ae-8cf4a4cdf836","resourceVersion":"21757","creationTimestamp":"2023-05-09T13:02:21Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"fef1c9d87b066cd07cc2908620965944550934df96dfe122a8b88f0bfd43ad27","cni.projectcalico.org/podIP":"172.25.124.212/32","cni.projectcalico.org/podIPs":"172.25.124.212/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"706c370b-308c-4e59-8802-20f3f6ebc4a3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"706c370b-308c-4e59-8802-20f3f6ebc4a3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-snd8z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-snd8z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cl-gks-cncf-ix1-md-0-48ljh","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cl-gks-cncf-ix1-md-0-48ljh"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:21Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:23Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:23Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:21Z"}],"hostIP":"192.168.1.64","podIP":"172.25.124.212","podIPs":[{"ip":"172.25.124.212"}],"startTime":"2023-05-09T13:02:21Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-09T13:02:22Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://d4012133eee93effda82562ab5ae9ab3867b49f0fbc1d8cb5a019eab6991535f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-pxzkc","generateName":"daemon-set-","namespace":"daemonsets-2079","uid":"0cd40544-1b9b-4604-9d1c-5cbf52094b87","resourceVersion":"21752","creationTimestamp":"2023-05-09T13:02:21Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c37a0e9b3d74618a12b73329fb711b975e7c0972d75a731d5ce76340d5cdab80","cni.projectcalico.org/podIP":"172.25.72.209/32","cni.projectcalico.org/podIPs":"172.25.72.209/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"706c370b-308c-4e59-8802-20f3f6ebc4a3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"706c370b-308c-4e59-8802-20f3f6ebc4a3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-09T13:02:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.72.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qqn7h","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qqn7h","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cl-gks-cncf-ix1-md-0-skqqr","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cl-gks-cncf-ix1-md-0-skqqr"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:21Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:22Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:22Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T13:02:21Z"}],"hostIP":"192.168.1.89","podIP":"172.25.72.209","podIPs":[{"ip":"172.25.72.209"}],"startTime":"2023-05-09T13:02:21Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-09T13:02:22Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://aaa2fd73eed4919d38fdb94c075f689d08fc3ca857f405fe2caa2a995ced3b33","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:02:23.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2079" for this suite. 05/09/23 13:02:23.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:02:23.536
May  9 13:02:23.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 13:02:23.537
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:23.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:23.55
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 05/09/23 13:02:23.551
May  9 13:02:23.552: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-678 proxy --unix-socket=/tmp/kubectl-proxy-unix1472100678/test'
STEP: retrieving proxy /api/ output 05/09/23 13:02:23.587
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 13:02:23.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-678" for this suite. 05/09/23 13:02:23.59
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":111,"skipped":2119,"failed":0}
------------------------------
• [0.058 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:02:23.536
    May  9 13:02:23.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 13:02:23.537
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:23.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:23.55
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 05/09/23 13:02:23.551
    May  9 13:02:23.552: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-678 proxy --unix-socket=/tmp/kubectl-proxy-unix1472100678/test'
    STEP: retrieving proxy /api/ output 05/09/23 13:02:23.587
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 13:02:23.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-678" for this suite. 05/09/23 13:02:23.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:02:23.595
May  9 13:02:23.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename var-expansion 05/09/23 13:02:23.595
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:23.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:23.611
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
May  9 13:02:23.620: INFO: Waiting up to 2m0s for pod "var-expansion-813d1bfb-5d54-4d94-88e9-b0ed8e86b254" in namespace "var-expansion-8755" to be "container 0 failed with reason CreateContainerConfigError"
May  9 13:02:23.625: INFO: Pod "var-expansion-813d1bfb-5d54-4d94-88e9-b0ed8e86b254": Phase="Pending", Reason="", readiness=false. Elapsed: 4.881172ms
May  9 13:02:25.629: INFO: Pod "var-expansion-813d1bfb-5d54-4d94-88e9-b0ed8e86b254": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0086893s
May  9 13:02:25.629: INFO: Pod "var-expansion-813d1bfb-5d54-4d94-88e9-b0ed8e86b254" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May  9 13:02:25.629: INFO: Deleting pod "var-expansion-813d1bfb-5d54-4d94-88e9-b0ed8e86b254" in namespace "var-expansion-8755"
May  9 13:02:25.635: INFO: Wait up to 5m0s for pod "var-expansion-813d1bfb-5d54-4d94-88e9-b0ed8e86b254" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  9 13:02:29.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8755" for this suite. 05/09/23 13:02:29.645
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":112,"skipped":2133,"failed":0}
------------------------------
• [SLOW TEST] [6.055 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:02:23.595
    May  9 13:02:23.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename var-expansion 05/09/23 13:02:23.595
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:23.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:23.611
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    May  9 13:02:23.620: INFO: Waiting up to 2m0s for pod "var-expansion-813d1bfb-5d54-4d94-88e9-b0ed8e86b254" in namespace "var-expansion-8755" to be "container 0 failed with reason CreateContainerConfigError"
    May  9 13:02:23.625: INFO: Pod "var-expansion-813d1bfb-5d54-4d94-88e9-b0ed8e86b254": Phase="Pending", Reason="", readiness=false. Elapsed: 4.881172ms
    May  9 13:02:25.629: INFO: Pod "var-expansion-813d1bfb-5d54-4d94-88e9-b0ed8e86b254": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0086893s
    May  9 13:02:25.629: INFO: Pod "var-expansion-813d1bfb-5d54-4d94-88e9-b0ed8e86b254" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May  9 13:02:25.629: INFO: Deleting pod "var-expansion-813d1bfb-5d54-4d94-88e9-b0ed8e86b254" in namespace "var-expansion-8755"
    May  9 13:02:25.635: INFO: Wait up to 5m0s for pod "var-expansion-813d1bfb-5d54-4d94-88e9-b0ed8e86b254" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  9 13:02:29.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8755" for this suite. 05/09/23 13:02:29.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:02:29.65
May  9 13:02:29.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pods 05/09/23 13:02:29.651
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:29.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:29.664
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 05/09/23 13:02:29.665
STEP: submitting the pod to kubernetes 05/09/23 13:02:29.666
May  9 13:02:29.672: INFO: Waiting up to 5m0s for pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea" in namespace "pods-4914" to be "running and ready"
May  9 13:02:29.674: INFO: Pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.710657ms
May  9 13:02:29.674: INFO: The phase of Pod pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea is Pending, waiting for it to be Running (with Ready = true)
May  9 13:02:31.677: INFO: Pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.004602344s
May  9 13:02:31.677: INFO: The phase of Pod pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea is Running (Ready = true)
May  9 13:02:31.677: INFO: Pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/09/23 13:02:31.679
STEP: updating the pod 05/09/23 13:02:31.681
May  9 13:02:32.200: INFO: Successfully updated pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea"
May  9 13:02:32.200: INFO: Waiting up to 5m0s for pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea" in namespace "pods-4914" to be "running"
May  9 13:02:32.202: INFO: Pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea": Phase="Running", Reason="", readiness=true. Elapsed: 1.941666ms
May  9 13:02:32.202: INFO: Pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 05/09/23 13:02:32.202
May  9 13:02:32.204: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  9 13:02:32.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4914" for this suite. 05/09/23 13:02:32.207
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":113,"skipped":2139,"failed":0}
------------------------------
• [2.562 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:02:29.65
    May  9 13:02:29.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pods 05/09/23 13:02:29.651
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:29.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:29.664
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 05/09/23 13:02:29.665
    STEP: submitting the pod to kubernetes 05/09/23 13:02:29.666
    May  9 13:02:29.672: INFO: Waiting up to 5m0s for pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea" in namespace "pods-4914" to be "running and ready"
    May  9 13:02:29.674: INFO: Pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.710657ms
    May  9 13:02:29.674: INFO: The phase of Pod pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:02:31.677: INFO: Pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.004602344s
    May  9 13:02:31.677: INFO: The phase of Pod pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea is Running (Ready = true)
    May  9 13:02:31.677: INFO: Pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/09/23 13:02:31.679
    STEP: updating the pod 05/09/23 13:02:31.681
    May  9 13:02:32.200: INFO: Successfully updated pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea"
    May  9 13:02:32.200: INFO: Waiting up to 5m0s for pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea" in namespace "pods-4914" to be "running"
    May  9 13:02:32.202: INFO: Pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea": Phase="Running", Reason="", readiness=true. Elapsed: 1.941666ms
    May  9 13:02:32.202: INFO: Pod "pod-update-153ac187-b4d4-4cac-8366-0efe4499e6ea" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 05/09/23 13:02:32.202
    May  9 13:02:32.204: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  9 13:02:32.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4914" for this suite. 05/09/23 13:02:32.207
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:02:32.213
May  9 13:02:32.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename endpointslice 05/09/23 13:02:32.213
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:32.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:32.227
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May  9 13:02:34.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9311" for this suite. 05/09/23 13:02:34.277
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":114,"skipped":2140,"failed":0}
------------------------------
• [2.069 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:02:32.213
    May  9 13:02:32.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename endpointslice 05/09/23 13:02:32.213
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:32.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:32.227
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May  9 13:02:34.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9311" for this suite. 05/09/23 13:02:34.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:02:34.286
May  9 13:02:34.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 13:02:34.286
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:34.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:34.303
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:02:34.304
May  9 13:02:34.312: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b" in namespace "downward-api-3266" to be "Succeeded or Failed"
May  9 13:02:34.315: INFO: Pod "downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.331503ms
May  9 13:02:36.319: INFO: Pod "downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b": Phase="Running", Reason="", readiness=false. Elapsed: 2.007270867s
May  9 13:02:38.318: INFO: Pod "downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006688155s
STEP: Saw pod success 05/09/23 13:02:38.318
May  9 13:02:38.319: INFO: Pod "downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b" satisfied condition "Succeeded or Failed"
May  9 13:02:38.321: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b container client-container: <nil>
STEP: delete the pod 05/09/23 13:02:38.333
May  9 13:02:38.341: INFO: Waiting for pod downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b to disappear
May  9 13:02:38.343: INFO: Pod downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  9 13:02:38.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3266" for this suite. 05/09/23 13:02:38.346
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":115,"skipped":2208,"failed":0}
------------------------------
• [4.065 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:02:34.286
    May  9 13:02:34.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 13:02:34.286
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:34.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:34.303
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:02:34.304
    May  9 13:02:34.312: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b" in namespace "downward-api-3266" to be "Succeeded or Failed"
    May  9 13:02:34.315: INFO: Pod "downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.331503ms
    May  9 13:02:36.319: INFO: Pod "downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b": Phase="Running", Reason="", readiness=false. Elapsed: 2.007270867s
    May  9 13:02:38.318: INFO: Pod "downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006688155s
    STEP: Saw pod success 05/09/23 13:02:38.318
    May  9 13:02:38.319: INFO: Pod "downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b" satisfied condition "Succeeded or Failed"
    May  9 13:02:38.321: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b container client-container: <nil>
    STEP: delete the pod 05/09/23 13:02:38.333
    May  9 13:02:38.341: INFO: Waiting for pod downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b to disappear
    May  9 13:02:38.343: INFO: Pod downwardapi-volume-0f7539b5-bbeb-4583-8471-152a7f60b74b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  9 13:02:38.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3266" for this suite. 05/09/23 13:02:38.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:02:38.352
May  9 13:02:38.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 13:02:38.353
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:38.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:38.366
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:02:38.368
May  9 13:02:38.375: INFO: Waiting up to 5m0s for pod "downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f" in namespace "downward-api-430" to be "Succeeded or Failed"
May  9 13:02:38.379: INFO: Pod "downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.845521ms
May  9 13:02:40.381: INFO: Pod "downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f": Phase="Running", Reason="", readiness=false. Elapsed: 2.006656785s
May  9 13:02:42.383: INFO: Pod "downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008662271s
STEP: Saw pod success 05/09/23 13:02:42.383
May  9 13:02:42.384: INFO: Pod "downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f" satisfied condition "Succeeded or Failed"
May  9 13:02:42.386: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f container client-container: <nil>
STEP: delete the pod 05/09/23 13:02:42.393
May  9 13:02:42.403: INFO: Waiting for pod downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f to disappear
May  9 13:02:42.405: INFO: Pod downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  9 13:02:42.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-430" for this suite. 05/09/23 13:02:42.407
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":116,"skipped":2249,"failed":0}
------------------------------
• [4.060 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:02:38.352
    May  9 13:02:38.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 13:02:38.353
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:38.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:38.366
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:02:38.368
    May  9 13:02:38.375: INFO: Waiting up to 5m0s for pod "downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f" in namespace "downward-api-430" to be "Succeeded or Failed"
    May  9 13:02:38.379: INFO: Pod "downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.845521ms
    May  9 13:02:40.381: INFO: Pod "downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f": Phase="Running", Reason="", readiness=false. Elapsed: 2.006656785s
    May  9 13:02:42.383: INFO: Pod "downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008662271s
    STEP: Saw pod success 05/09/23 13:02:42.383
    May  9 13:02:42.384: INFO: Pod "downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f" satisfied condition "Succeeded or Failed"
    May  9 13:02:42.386: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f container client-container: <nil>
    STEP: delete the pod 05/09/23 13:02:42.393
    May  9 13:02:42.403: INFO: Waiting for pod downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f to disappear
    May  9 13:02:42.405: INFO: Pod downwardapi-volume-495ab3b8-319e-4565-8909-7332d0648e1f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  9 13:02:42.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-430" for this suite. 05/09/23 13:02:42.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:02:42.414
May  9 13:02:42.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename dns 05/09/23 13:02:42.414
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:42.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:42.434
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/09/23 13:02:42.435
May  9 13:02:42.441: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9211  a7f7f92e-3fc4-4bd2-84d7-4c163ce5b1eb 22031 0 2023-05-09 13:02:42 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-09 13:02:42 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-75572,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-75572,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:02:42.441: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9211" to be "running and ready"
May  9 13:02:42.445: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.189055ms
May  9 13:02:42.445: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May  9 13:02:44.449: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.007243832s
May  9 13:02:44.449: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
May  9 13:02:44.449: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 05/09/23 13:02:44.449
May  9 13:02:44.449: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9211 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:02:44.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:02:44.449: INFO: ExecWithOptions: Clientset creation
May  9 13:02:44.449: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9211/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 05/09/23 13:02:44.506
May  9 13:02:44.506: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9211 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:02:44.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:02:44.507: INFO: ExecWithOptions: Clientset creation
May  9 13:02:44.507: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9211/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 13:02:44.563: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  9 13:02:44.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9211" for this suite. 05/09/23 13:02:44.576
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":117,"skipped":2277,"failed":0}
------------------------------
• [2.167 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:02:42.414
    May  9 13:02:42.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename dns 05/09/23 13:02:42.414
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:42.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:42.434
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/09/23 13:02:42.435
    May  9 13:02:42.441: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9211  a7f7f92e-3fc4-4bd2-84d7-4c163ce5b1eb 22031 0 2023-05-09 13:02:42 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-09 13:02:42 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-75572,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-75572,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:02:42.441: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9211" to be "running and ready"
    May  9 13:02:42.445: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.189055ms
    May  9 13:02:42.445: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:02:44.449: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.007243832s
    May  9 13:02:44.449: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    May  9 13:02:44.449: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 05/09/23 13:02:44.449
    May  9 13:02:44.449: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9211 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:02:44.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:02:44.449: INFO: ExecWithOptions: Clientset creation
    May  9 13:02:44.449: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9211/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 05/09/23 13:02:44.506
    May  9 13:02:44.506: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9211 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:02:44.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:02:44.507: INFO: ExecWithOptions: Clientset creation
    May  9 13:02:44.507: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9211/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 13:02:44.563: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  9 13:02:44.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9211" for this suite. 05/09/23 13:02:44.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:02:44.582
May  9 13:02:44.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:02:44.582
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:44.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:44.598
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-7277 05/09/23 13:02:44.6
STEP: creating service affinity-nodeport in namespace services-7277 05/09/23 13:02:44.6
STEP: creating replication controller affinity-nodeport in namespace services-7277 05/09/23 13:02:44.613
I0509 13:02:44.622184      24 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7277, replica count: 3
I0509 13:02:47.674037      24 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 13:02:47.682: INFO: Creating new exec pod
May  9 13:02:47.688: INFO: Waiting up to 5m0s for pod "execpod-affinityv54mz" in namespace "services-7277" to be "running"
May  9 13:02:47.690: INFO: Pod "execpod-affinityv54mz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.737127ms
May  9 13:02:49.693: INFO: Pod "execpod-affinityv54mz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005162126s
May  9 13:02:49.693: INFO: Pod "execpod-affinityv54mz" satisfied condition "running"
May  9 13:02:50.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7277 exec execpod-affinityv54mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
May  9 13:02:51.793: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May  9 13:02:51.793: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:02:51.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7277 exec execpod-affinityv54mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.77.0 80'
May  9 13:02:51.896: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.77.0 80\nConnection to 10.111.77.0 80 port [tcp/http] succeeded!\n"
May  9 13:02:51.896: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:02:51.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7277 exec execpod-affinityv54mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.64 32662'
May  9 13:02:51.993: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.64 32662\nConnection to 192.168.1.64 32662 port [tcp/*] succeeded!\n"
May  9 13:02:51.993: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:02:51.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7277 exec execpod-affinityv54mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.89 32662'
May  9 13:02:52.089: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.89 32662\nConnection to 192.168.1.89 32662 port [tcp/*] succeeded!\n"
May  9 13:02:52.089: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:02:52.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7277 exec execpod-affinityv54mz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.64:32662/ ; done'
May  9 13:02:52.233: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n"
May  9 13:02:52.233: INFO: stdout: "\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25"
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
May  9 13:02:52.233: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7277, will wait for the garbage collector to delete the pods 05/09/23 13:02:52.249
May  9 13:02:52.307: INFO: Deleting ReplicationController affinity-nodeport took: 4.857918ms
May  9 13:02:52.408: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.880569ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:02:54.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7277" for this suite. 05/09/23 13:02:54.434
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":118,"skipped":2283,"failed":0}
------------------------------
• [SLOW TEST] [9.858 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:02:44.582
    May  9 13:02:44.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:02:44.582
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:44.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:44.598
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-7277 05/09/23 13:02:44.6
    STEP: creating service affinity-nodeport in namespace services-7277 05/09/23 13:02:44.6
    STEP: creating replication controller affinity-nodeport in namespace services-7277 05/09/23 13:02:44.613
    I0509 13:02:44.622184      24 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7277, replica count: 3
    I0509 13:02:47.674037      24 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 13:02:47.682: INFO: Creating new exec pod
    May  9 13:02:47.688: INFO: Waiting up to 5m0s for pod "execpod-affinityv54mz" in namespace "services-7277" to be "running"
    May  9 13:02:47.690: INFO: Pod "execpod-affinityv54mz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.737127ms
    May  9 13:02:49.693: INFO: Pod "execpod-affinityv54mz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005162126s
    May  9 13:02:49.693: INFO: Pod "execpod-affinityv54mz" satisfied condition "running"
    May  9 13:02:50.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7277 exec execpod-affinityv54mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    May  9 13:02:51.793: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    May  9 13:02:51.793: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:02:51.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7277 exec execpod-affinityv54mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.77.0 80'
    May  9 13:02:51.896: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.77.0 80\nConnection to 10.111.77.0 80 port [tcp/http] succeeded!\n"
    May  9 13:02:51.896: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:02:51.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7277 exec execpod-affinityv54mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.64 32662'
    May  9 13:02:51.993: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.64 32662\nConnection to 192.168.1.64 32662 port [tcp/*] succeeded!\n"
    May  9 13:02:51.993: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:02:51.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7277 exec execpod-affinityv54mz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.89 32662'
    May  9 13:02:52.089: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.89 32662\nConnection to 192.168.1.89 32662 port [tcp/*] succeeded!\n"
    May  9 13:02:52.089: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:02:52.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7277 exec execpod-affinityv54mz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.64:32662/ ; done'
    May  9 13:02:52.233: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:32662/\n"
    May  9 13:02:52.233: INFO: stdout: "\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25\naffinity-nodeport-vbz25"
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Received response from host: affinity-nodeport-vbz25
    May  9 13:02:52.233: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-7277, will wait for the garbage collector to delete the pods 05/09/23 13:02:52.249
    May  9 13:02:52.307: INFO: Deleting ReplicationController affinity-nodeport took: 4.857918ms
    May  9 13:02:52.408: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.880569ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:02:54.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7277" for this suite. 05/09/23 13:02:54.434
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:02:54.439
May  9 13:02:54.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-probe 05/09/23 13:02:54.44
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:54.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:54.456
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-8232648a-534a-4fd5-9be7-0957c058e31d in namespace container-probe-7898 05/09/23 13:02:54.457
May  9 13:02:54.464: INFO: Waiting up to 5m0s for pod "liveness-8232648a-534a-4fd5-9be7-0957c058e31d" in namespace "container-probe-7898" to be "not pending"
May  9 13:02:54.466: INFO: Pod "liveness-8232648a-534a-4fd5-9be7-0957c058e31d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.660435ms
May  9 13:02:56.471: INFO: Pod "liveness-8232648a-534a-4fd5-9be7-0957c058e31d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007024257s
May  9 13:02:56.471: INFO: Pod "liveness-8232648a-534a-4fd5-9be7-0957c058e31d" satisfied condition "not pending"
May  9 13:02:56.471: INFO: Started pod liveness-8232648a-534a-4fd5-9be7-0957c058e31d in namespace container-probe-7898
STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 13:02:56.471
May  9 13:02:56.475: INFO: Initial restart count of pod liveness-8232648a-534a-4fd5-9be7-0957c058e31d is 0
May  9 13:03:16.514: INFO: Restart count of pod container-probe-7898/liveness-8232648a-534a-4fd5-9be7-0957c058e31d is now 1 (20.038893132s elapsed)
STEP: deleting the pod 05/09/23 13:03:16.514
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  9 13:03:16.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7898" for this suite. 05/09/23 13:03:16.528
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":119,"skipped":2292,"failed":0}
------------------------------
• [SLOW TEST] [22.093 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:02:54.439
    May  9 13:02:54.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-probe 05/09/23 13:02:54.44
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:02:54.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:02:54.456
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-8232648a-534a-4fd5-9be7-0957c058e31d in namespace container-probe-7898 05/09/23 13:02:54.457
    May  9 13:02:54.464: INFO: Waiting up to 5m0s for pod "liveness-8232648a-534a-4fd5-9be7-0957c058e31d" in namespace "container-probe-7898" to be "not pending"
    May  9 13:02:54.466: INFO: Pod "liveness-8232648a-534a-4fd5-9be7-0957c058e31d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.660435ms
    May  9 13:02:56.471: INFO: Pod "liveness-8232648a-534a-4fd5-9be7-0957c058e31d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007024257s
    May  9 13:02:56.471: INFO: Pod "liveness-8232648a-534a-4fd5-9be7-0957c058e31d" satisfied condition "not pending"
    May  9 13:02:56.471: INFO: Started pod liveness-8232648a-534a-4fd5-9be7-0957c058e31d in namespace container-probe-7898
    STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 13:02:56.471
    May  9 13:02:56.475: INFO: Initial restart count of pod liveness-8232648a-534a-4fd5-9be7-0957c058e31d is 0
    May  9 13:03:16.514: INFO: Restart count of pod container-probe-7898/liveness-8232648a-534a-4fd5-9be7-0957c058e31d is now 1 (20.038893132s elapsed)
    STEP: deleting the pod 05/09/23 13:03:16.514
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  9 13:03:16.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7898" for this suite. 05/09/23 13:03:16.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:03:16.533
May  9 13:03:16.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 13:03:16.533
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:16.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:16.566
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-4108f6b2-0d27-4361-9027-15e638956216 05/09/23 13:03:16.567
STEP: Creating a pod to test consume configMaps 05/09/23 13:03:16.573
May  9 13:03:16.581: INFO: Waiting up to 5m0s for pod "pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98" in namespace "configmap-2031" to be "Succeeded or Failed"
May  9 13:03:16.584: INFO: Pod "pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98": Phase="Pending", Reason="", readiness=false. Elapsed: 3.116363ms
May  9 13:03:18.588: INFO: Pod "pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98": Phase="Running", Reason="", readiness=false. Elapsed: 2.006815176s
May  9 13:03:20.589: INFO: Pod "pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007695356s
STEP: Saw pod success 05/09/23 13:03:20.589
May  9 13:03:20.589: INFO: Pod "pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98" satisfied condition "Succeeded or Failed"
May  9 13:03:20.591: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 13:03:20.595
May  9 13:03:20.606: INFO: Waiting for pod pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98 to disappear
May  9 13:03:20.608: INFO: Pod pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  9 13:03:20.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2031" for this suite. 05/09/23 13:03:20.611
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":120,"skipped":2307,"failed":0}
------------------------------
• [4.084 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:03:16.533
    May  9 13:03:16.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 13:03:16.533
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:16.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:16.566
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-4108f6b2-0d27-4361-9027-15e638956216 05/09/23 13:03:16.567
    STEP: Creating a pod to test consume configMaps 05/09/23 13:03:16.573
    May  9 13:03:16.581: INFO: Waiting up to 5m0s for pod "pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98" in namespace "configmap-2031" to be "Succeeded or Failed"
    May  9 13:03:16.584: INFO: Pod "pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98": Phase="Pending", Reason="", readiness=false. Elapsed: 3.116363ms
    May  9 13:03:18.588: INFO: Pod "pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98": Phase="Running", Reason="", readiness=false. Elapsed: 2.006815176s
    May  9 13:03:20.589: INFO: Pod "pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007695356s
    STEP: Saw pod success 05/09/23 13:03:20.589
    May  9 13:03:20.589: INFO: Pod "pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98" satisfied condition "Succeeded or Failed"
    May  9 13:03:20.591: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 13:03:20.595
    May  9 13:03:20.606: INFO: Waiting for pod pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98 to disappear
    May  9 13:03:20.608: INFO: Pod pod-configmaps-d44cf75a-de37-4bf6-8fb1-bc0ae282fa98 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 13:03:20.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2031" for this suite. 05/09/23 13:03:20.611
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:03:20.617
May  9 13:03:20.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:03:20.617
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:20.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:20.633
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-dad58e78-1c8f-45e7-9da8-8c70b38abfdb 05/09/23 13:03:20.634
STEP: Creating a pod to test consume configMaps 05/09/23 13:03:20.638
May  9 13:03:20.644: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45" in namespace "projected-8882" to be "Succeeded or Failed"
May  9 13:03:20.648: INFO: Pod "pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45": Phase="Pending", Reason="", readiness=false. Elapsed: 3.797119ms
May  9 13:03:22.651: INFO: Pod "pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006668163s
May  9 13:03:24.651: INFO: Pod "pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007135575s
STEP: Saw pod success 05/09/23 13:03:24.651
May  9 13:03:24.651: INFO: Pod "pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45" satisfied condition "Succeeded or Failed"
May  9 13:03:24.653: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 13:03:24.658
May  9 13:03:24.669: INFO: Waiting for pod pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45 to disappear
May  9 13:03:24.671: INFO: Pod pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  9 13:03:24.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8882" for this suite. 05/09/23 13:03:24.674
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":121,"skipped":2311,"failed":0}
------------------------------
• [4.071 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:03:20.617
    May  9 13:03:20.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:03:20.617
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:20.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:20.633
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-dad58e78-1c8f-45e7-9da8-8c70b38abfdb 05/09/23 13:03:20.634
    STEP: Creating a pod to test consume configMaps 05/09/23 13:03:20.638
    May  9 13:03:20.644: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45" in namespace "projected-8882" to be "Succeeded or Failed"
    May  9 13:03:20.648: INFO: Pod "pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45": Phase="Pending", Reason="", readiness=false. Elapsed: 3.797119ms
    May  9 13:03:22.651: INFO: Pod "pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006668163s
    May  9 13:03:24.651: INFO: Pod "pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007135575s
    STEP: Saw pod success 05/09/23 13:03:24.651
    May  9 13:03:24.651: INFO: Pod "pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45" satisfied condition "Succeeded or Failed"
    May  9 13:03:24.653: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 13:03:24.658
    May  9 13:03:24.669: INFO: Waiting for pod pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45 to disappear
    May  9 13:03:24.671: INFO: Pod pod-projected-configmaps-2fd65a2e-0cb5-4982-860a-d9229c0fcc45 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  9 13:03:24.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8882" for this suite. 05/09/23 13:03:24.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:03:24.689
May  9 13:03:24.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename dns 05/09/23 13:03:24.69
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:24.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:24.703
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/09/23 13:03:24.704
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/09/23 13:03:24.704
STEP: creating a pod to probe DNS 05/09/23 13:03:24.704
STEP: submitting the pod to kubernetes 05/09/23 13:03:24.704
May  9 13:03:24.711: INFO: Waiting up to 15m0s for pod "dns-test-56644754-ca69-4af2-ac20-a368f9f847a3" in namespace "dns-1974" to be "running"
May  9 13:03:24.717: INFO: Pod "dns-test-56644754-ca69-4af2-ac20-a368f9f847a3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.285987ms
May  9 13:03:26.720: INFO: Pod "dns-test-56644754-ca69-4af2-ac20-a368f9f847a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.009701545s
May  9 13:03:26.720: INFO: Pod "dns-test-56644754-ca69-4af2-ac20-a368f9f847a3" satisfied condition "running"
STEP: retrieving the pod 05/09/23 13:03:26.72
STEP: looking for the results for each expected name from probers 05/09/23 13:03:26.723
May  9 13:03:26.733: INFO: DNS probes using dns-1974/dns-test-56644754-ca69-4af2-ac20-a368f9f847a3 succeeded

STEP: deleting the pod 05/09/23 13:03:26.733
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  9 13:03:26.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1974" for this suite. 05/09/23 13:03:26.751
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":122,"skipped":2323,"failed":0}
------------------------------
• [2.068 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:03:24.689
    May  9 13:03:24.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename dns 05/09/23 13:03:24.69
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:24.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:24.703
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/09/23 13:03:24.704
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/09/23 13:03:24.704
    STEP: creating a pod to probe DNS 05/09/23 13:03:24.704
    STEP: submitting the pod to kubernetes 05/09/23 13:03:24.704
    May  9 13:03:24.711: INFO: Waiting up to 15m0s for pod "dns-test-56644754-ca69-4af2-ac20-a368f9f847a3" in namespace "dns-1974" to be "running"
    May  9 13:03:24.717: INFO: Pod "dns-test-56644754-ca69-4af2-ac20-a368f9f847a3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.285987ms
    May  9 13:03:26.720: INFO: Pod "dns-test-56644754-ca69-4af2-ac20-a368f9f847a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.009701545s
    May  9 13:03:26.720: INFO: Pod "dns-test-56644754-ca69-4af2-ac20-a368f9f847a3" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 13:03:26.72
    STEP: looking for the results for each expected name from probers 05/09/23 13:03:26.723
    May  9 13:03:26.733: INFO: DNS probes using dns-1974/dns-test-56644754-ca69-4af2-ac20-a368f9f847a3 succeeded

    STEP: deleting the pod 05/09/23 13:03:26.733
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  9 13:03:26.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1974" for this suite. 05/09/23 13:03:26.751
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:03:26.757
May  9 13:03:26.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename runtimeclass 05/09/23 13:03:26.758
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:26.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:26.779
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 05/09/23 13:03:26.78
STEP: getting /apis/node.k8s.io 05/09/23 13:03:26.781
STEP: getting /apis/node.k8s.io/v1 05/09/23 13:03:26.782
STEP: creating 05/09/23 13:03:26.783
STEP: watching 05/09/23 13:03:26.795
May  9 13:03:26.795: INFO: starting watch
STEP: getting 05/09/23 13:03:26.8
STEP: listing 05/09/23 13:03:26.802
STEP: patching 05/09/23 13:03:26.804
STEP: updating 05/09/23 13:03:26.809
May  9 13:03:26.812: INFO: waiting for watch events with expected annotations
STEP: deleting 05/09/23 13:03:26.812
STEP: deleting a collection 05/09/23 13:03:26.82
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  9 13:03:26.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9961" for this suite. 05/09/23 13:03:26.837
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":123,"skipped":2338,"failed":0}
------------------------------
• [0.099 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:03:26.757
    May  9 13:03:26.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename runtimeclass 05/09/23 13:03:26.758
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:26.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:26.779
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 05/09/23 13:03:26.78
    STEP: getting /apis/node.k8s.io 05/09/23 13:03:26.781
    STEP: getting /apis/node.k8s.io/v1 05/09/23 13:03:26.782
    STEP: creating 05/09/23 13:03:26.783
    STEP: watching 05/09/23 13:03:26.795
    May  9 13:03:26.795: INFO: starting watch
    STEP: getting 05/09/23 13:03:26.8
    STEP: listing 05/09/23 13:03:26.802
    STEP: patching 05/09/23 13:03:26.804
    STEP: updating 05/09/23 13:03:26.809
    May  9 13:03:26.812: INFO: waiting for watch events with expected annotations
    STEP: deleting 05/09/23 13:03:26.812
    STEP: deleting a collection 05/09/23 13:03:26.82
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  9 13:03:26.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9961" for this suite. 05/09/23 13:03:26.837
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:03:26.857
May  9 13:03:26.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 13:03:26.857
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:26.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:26.885
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 05/09/23 13:03:26.887
May  9 13:03:26.897: INFO: Waiting up to 5m0s for pod "pod-4b83b520-3b6d-4497-86b9-b0592b1fc822" in namespace "emptydir-6270" to be "Succeeded or Failed"
May  9 13:03:26.899: INFO: Pod "pod-4b83b520-3b6d-4497-86b9-b0592b1fc822": Phase="Pending", Reason="", readiness=false. Elapsed: 1.84394ms
May  9 13:03:28.902: INFO: Pod "pod-4b83b520-3b6d-4497-86b9-b0592b1fc822": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00487479s
May  9 13:03:30.902: INFO: Pod "pod-4b83b520-3b6d-4497-86b9-b0592b1fc822": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005287186s
STEP: Saw pod success 05/09/23 13:03:30.902
May  9 13:03:30.902: INFO: Pod "pod-4b83b520-3b6d-4497-86b9-b0592b1fc822" satisfied condition "Succeeded or Failed"
May  9 13:03:30.905: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-4b83b520-3b6d-4497-86b9-b0592b1fc822 container test-container: <nil>
STEP: delete the pod 05/09/23 13:03:30.909
May  9 13:03:30.918: INFO: Waiting for pod pod-4b83b520-3b6d-4497-86b9-b0592b1fc822 to disappear
May  9 13:03:30.919: INFO: Pod pod-4b83b520-3b6d-4497-86b9-b0592b1fc822 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 13:03:30.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6270" for this suite. 05/09/23 13:03:30.922
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":124,"skipped":2338,"failed":0}
------------------------------
• [4.074 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:03:26.857
    May  9 13:03:26.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 13:03:26.857
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:26.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:26.885
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/09/23 13:03:26.887
    May  9 13:03:26.897: INFO: Waiting up to 5m0s for pod "pod-4b83b520-3b6d-4497-86b9-b0592b1fc822" in namespace "emptydir-6270" to be "Succeeded or Failed"
    May  9 13:03:26.899: INFO: Pod "pod-4b83b520-3b6d-4497-86b9-b0592b1fc822": Phase="Pending", Reason="", readiness=false. Elapsed: 1.84394ms
    May  9 13:03:28.902: INFO: Pod "pod-4b83b520-3b6d-4497-86b9-b0592b1fc822": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00487479s
    May  9 13:03:30.902: INFO: Pod "pod-4b83b520-3b6d-4497-86b9-b0592b1fc822": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005287186s
    STEP: Saw pod success 05/09/23 13:03:30.902
    May  9 13:03:30.902: INFO: Pod "pod-4b83b520-3b6d-4497-86b9-b0592b1fc822" satisfied condition "Succeeded or Failed"
    May  9 13:03:30.905: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-4b83b520-3b6d-4497-86b9-b0592b1fc822 container test-container: <nil>
    STEP: delete the pod 05/09/23 13:03:30.909
    May  9 13:03:30.918: INFO: Waiting for pod pod-4b83b520-3b6d-4497-86b9-b0592b1fc822 to disappear
    May  9 13:03:30.919: INFO: Pod pod-4b83b520-3b6d-4497-86b9-b0592b1fc822 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 13:03:30.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6270" for this suite. 05/09/23 13:03:30.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:03:30.932
May  9 13:03:30.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:03:30.932
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:30.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:30.948
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-d8c5e2ff-f8be-487a-aa5b-994f7c04fc2e 05/09/23 13:03:30.949
STEP: Creating secret with name secret-projected-all-test-volume-4d85e46c-7c20-4b3b-a95a-bdcc9adaa8e0 05/09/23 13:03:30.952
STEP: Creating a pod to test Check all projections for projected volume plugin 05/09/23 13:03:30.956
May  9 13:03:30.970: INFO: Waiting up to 5m0s for pod "projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9" in namespace "projected-5265" to be "Succeeded or Failed"
May  9 13:03:30.972: INFO: Pod "projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.174009ms
May  9 13:03:32.975: INFO: Pod "projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004947937s
May  9 13:03:34.976: INFO: Pod "projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006281707s
STEP: Saw pod success 05/09/23 13:03:34.976
May  9 13:03:34.976: INFO: Pod "projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9" satisfied condition "Succeeded or Failed"
May  9 13:03:34.978: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9 container projected-all-volume-test: <nil>
STEP: delete the pod 05/09/23 13:03:34.982
May  9 13:03:34.993: INFO: Waiting for pod projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9 to disappear
May  9 13:03:34.994: INFO: Pod projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
May  9 13:03:34.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5265" for this suite. 05/09/23 13:03:34.997
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":125,"skipped":2348,"failed":0}
------------------------------
• [4.071 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:03:30.932
    May  9 13:03:30.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:03:30.932
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:30.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:30.948
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-d8c5e2ff-f8be-487a-aa5b-994f7c04fc2e 05/09/23 13:03:30.949
    STEP: Creating secret with name secret-projected-all-test-volume-4d85e46c-7c20-4b3b-a95a-bdcc9adaa8e0 05/09/23 13:03:30.952
    STEP: Creating a pod to test Check all projections for projected volume plugin 05/09/23 13:03:30.956
    May  9 13:03:30.970: INFO: Waiting up to 5m0s for pod "projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9" in namespace "projected-5265" to be "Succeeded or Failed"
    May  9 13:03:30.972: INFO: Pod "projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.174009ms
    May  9 13:03:32.975: INFO: Pod "projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004947937s
    May  9 13:03:34.976: INFO: Pod "projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006281707s
    STEP: Saw pod success 05/09/23 13:03:34.976
    May  9 13:03:34.976: INFO: Pod "projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9" satisfied condition "Succeeded or Failed"
    May  9 13:03:34.978: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9 container projected-all-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:03:34.982
    May  9 13:03:34.993: INFO: Waiting for pod projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9 to disappear
    May  9 13:03:34.994: INFO: Pod projected-volume-bd3fbd9b-c9cc-4e71-9804-3b8a1da69eb9 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    May  9 13:03:34.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5265" for this suite. 05/09/23 13:03:34.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:03:35.003
May  9 13:03:35.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename runtimeclass 05/09/23 13:03:35.004
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:35.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:35.016
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-8071-delete-me 05/09/23 13:03:35.021
STEP: Waiting for the RuntimeClass to disappear 05/09/23 13:03:35.026
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  9 13:03:35.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8071" for this suite. 05/09/23 13:03:35.033
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":126,"skipped":2355,"failed":0}
------------------------------
• [0.035 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:03:35.003
    May  9 13:03:35.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename runtimeclass 05/09/23 13:03:35.004
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:35.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:35.016
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-8071-delete-me 05/09/23 13:03:35.021
    STEP: Waiting for the RuntimeClass to disappear 05/09/23 13:03:35.026
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  9 13:03:35.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8071" for this suite. 05/09/23 13:03:35.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:03:35.039
May  9 13:03:35.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 13:03:35.04
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:35.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:35.052
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:03:35.053
May  9 13:03:35.060: INFO: Waiting up to 5m0s for pod "downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a" in namespace "downward-api-8252" to be "Succeeded or Failed"
May  9 13:03:35.063: INFO: Pod "downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.015471ms
May  9 13:03:37.066: INFO: Pod "downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006401892s
May  9 13:03:39.067: INFO: Pod "downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007410039s
STEP: Saw pod success 05/09/23 13:03:39.067
May  9 13:03:39.067: INFO: Pod "downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a" satisfied condition "Succeeded or Failed"
May  9 13:03:39.069: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a container client-container: <nil>
STEP: delete the pod 05/09/23 13:03:39.073
May  9 13:03:39.081: INFO: Waiting for pod downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a to disappear
May  9 13:03:39.083: INFO: Pod downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  9 13:03:39.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8252" for this suite. 05/09/23 13:03:39.086
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":127,"skipped":2400,"failed":0}
------------------------------
• [4.051 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:03:35.039
    May  9 13:03:35.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 13:03:35.04
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:35.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:35.052
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:03:35.053
    May  9 13:03:35.060: INFO: Waiting up to 5m0s for pod "downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a" in namespace "downward-api-8252" to be "Succeeded or Failed"
    May  9 13:03:35.063: INFO: Pod "downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.015471ms
    May  9 13:03:37.066: INFO: Pod "downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006401892s
    May  9 13:03:39.067: INFO: Pod "downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007410039s
    STEP: Saw pod success 05/09/23 13:03:39.067
    May  9 13:03:39.067: INFO: Pod "downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a" satisfied condition "Succeeded or Failed"
    May  9 13:03:39.069: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a container client-container: <nil>
    STEP: delete the pod 05/09/23 13:03:39.073
    May  9 13:03:39.081: INFO: Waiting for pod downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a to disappear
    May  9 13:03:39.083: INFO: Pod downwardapi-volume-336b5a3b-98c2-4e30-80d6-357f1585601a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  9 13:03:39.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8252" for this suite. 05/09/23 13:03:39.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:03:39.091
May  9 13:03:39.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:03:39.092
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:39.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:39.105
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-8802 05/09/23 13:03:39.106
May  9 13:03:39.111: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-8802" to be "running and ready"
May  9 13:03:39.114: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.167972ms
May  9 13:03:39.114: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May  9 13:03:41.117: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.005888643s
May  9 13:03:41.117: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
May  9 13:03:41.117: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
May  9 13:03:41.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May  9 13:03:41.218: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May  9 13:03:41.218: INFO: stdout: "iptables"
May  9 13:03:41.218: INFO: proxyMode: iptables
May  9 13:03:41.230: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May  9 13:03:41.232: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-8802 05/09/23 13:03:41.232
STEP: creating replication controller affinity-nodeport-timeout in namespace services-8802 05/09/23 13:03:41.247
I0509 13:03:41.254218      24 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8802, replica count: 3
I0509 13:03:44.305113      24 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 13:03:44.312: INFO: Creating new exec pod
May  9 13:03:44.316: INFO: Waiting up to 5m0s for pod "execpod-affinitymxl5m" in namespace "services-8802" to be "running"
May  9 13:03:44.320: INFO: Pod "execpod-affinitymxl5m": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487198ms
May  9 13:03:46.323: INFO: Pod "execpod-affinitymxl5m": Phase="Running", Reason="", readiness=true. Elapsed: 2.006298163s
May  9 13:03:46.323: INFO: Pod "execpod-affinitymxl5m" satisfied condition "running"
May  9 13:03:47.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
May  9 13:03:47.426: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May  9 13:03:47.426: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:03:47.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.7.167 80'
May  9 13:03:47.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.7.167 80\nConnection to 10.99.7.167 80 port [tcp/http] succeeded!\n"
May  9 13:03:47.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:03:47.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.64 30763'
May  9 13:03:47.620: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.64 30763\nConnection to 192.168.1.64 30763 port [tcp/*] succeeded!\n"
May  9 13:03:47.620: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:03:47.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.89 30763'
May  9 13:03:47.714: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.89 30763\nConnection to 192.168.1.89 30763 port [tcp/*] succeeded!\n"
May  9 13:03:47.714: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:03:47.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.64:30763/ ; done'
May  9 13:03:47.850: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n"
May  9 13:03:47.850: INFO: stdout: "\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn"
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
May  9 13:03:47.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.64:30763/'
May  9 13:03:47.955: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n"
May  9 13:03:47.955: INFO: stdout: "affinity-nodeport-timeout-j6dzn"
May  9 13:04:07.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.64:30763/'
May  9 13:04:08.057: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n"
May  9 13:04:08.057: INFO: stdout: "affinity-nodeport-timeout-58ldt"
May  9 13:04:08.057: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8802, will wait for the garbage collector to delete the pods 05/09/23 13:04:08.067
May  9 13:04:08.126: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.014077ms
May  9 13:04:08.227: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.086042ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:04:10.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8802" for this suite. 05/09/23 13:04:10.352
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":128,"skipped":2429,"failed":0}
------------------------------
• [SLOW TEST] [31.266 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:03:39.091
    May  9 13:03:39.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:03:39.092
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:03:39.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:03:39.105
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-8802 05/09/23 13:03:39.106
    May  9 13:03:39.111: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-8802" to be "running and ready"
    May  9 13:03:39.114: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.167972ms
    May  9 13:03:39.114: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:03:41.117: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.005888643s
    May  9 13:03:41.117: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    May  9 13:03:41.117: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    May  9 13:03:41.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    May  9 13:03:41.218: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    May  9 13:03:41.218: INFO: stdout: "iptables"
    May  9 13:03:41.218: INFO: proxyMode: iptables
    May  9 13:03:41.230: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    May  9 13:03:41.232: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-8802 05/09/23 13:03:41.232
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-8802 05/09/23 13:03:41.247
    I0509 13:03:41.254218      24 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8802, replica count: 3
    I0509 13:03:44.305113      24 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 13:03:44.312: INFO: Creating new exec pod
    May  9 13:03:44.316: INFO: Waiting up to 5m0s for pod "execpod-affinitymxl5m" in namespace "services-8802" to be "running"
    May  9 13:03:44.320: INFO: Pod "execpod-affinitymxl5m": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487198ms
    May  9 13:03:46.323: INFO: Pod "execpod-affinitymxl5m": Phase="Running", Reason="", readiness=true. Elapsed: 2.006298163s
    May  9 13:03:46.323: INFO: Pod "execpod-affinitymxl5m" satisfied condition "running"
    May  9 13:03:47.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    May  9 13:03:47.426: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    May  9 13:03:47.426: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:03:47.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.7.167 80'
    May  9 13:03:47.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.7.167 80\nConnection to 10.99.7.167 80 port [tcp/http] succeeded!\n"
    May  9 13:03:47.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:03:47.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.64 30763'
    May  9 13:03:47.620: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.64 30763\nConnection to 192.168.1.64 30763 port [tcp/*] succeeded!\n"
    May  9 13:03:47.620: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:03:47.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.89 30763'
    May  9 13:03:47.714: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.89 30763\nConnection to 192.168.1.89 30763 port [tcp/*] succeeded!\n"
    May  9 13:03:47.714: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:03:47.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.64:30763/ ; done'
    May  9 13:03:47.850: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n"
    May  9 13:03:47.850: INFO: stdout: "\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn\naffinity-nodeport-timeout-j6dzn"
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Received response from host: affinity-nodeport-timeout-j6dzn
    May  9 13:03:47.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.64:30763/'
    May  9 13:03:47.955: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n"
    May  9 13:03:47.955: INFO: stdout: "affinity-nodeport-timeout-j6dzn"
    May  9 13:04:07.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8802 exec execpod-affinitymxl5m -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.64:30763/'
    May  9 13:04:08.057: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.64:30763/\n"
    May  9 13:04:08.057: INFO: stdout: "affinity-nodeport-timeout-58ldt"
    May  9 13:04:08.057: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8802, will wait for the garbage collector to delete the pods 05/09/23 13:04:08.067
    May  9 13:04:08.126: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.014077ms
    May  9 13:04:08.227: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.086042ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:04:10.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8802" for this suite. 05/09/23 13:04:10.352
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:04:10.359
May  9 13:04:10.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename dns 05/09/23 13:04:10.359
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:04:10.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:04:10.375
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 05/09/23 13:04:10.376
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8223.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local; sleep 1; done
 05/09/23 13:04:10.379
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8223.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local; sleep 1; done
 05/09/23 13:04:10.379
STEP: creating a pod to probe DNS 05/09/23 13:04:10.379
STEP: submitting the pod to kubernetes 05/09/23 13:04:10.379
May  9 13:04:10.386: INFO: Waiting up to 15m0s for pod "dns-test-e1ad641c-a506-403a-badb-0a2060f9768f" in namespace "dns-8223" to be "running"
May  9 13:04:10.388: INFO: Pod "dns-test-e1ad641c-a506-403a-badb-0a2060f9768f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.729623ms
May  9 13:04:12.390: INFO: Pod "dns-test-e1ad641c-a506-403a-badb-0a2060f9768f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004419194s
May  9 13:04:12.390: INFO: Pod "dns-test-e1ad641c-a506-403a-badb-0a2060f9768f" satisfied condition "running"
STEP: retrieving the pod 05/09/23 13:04:12.39
STEP: looking for the results for each expected name from probers 05/09/23 13:04:12.392
May  9 13:04:12.398: INFO: DNS probes using dns-test-e1ad641c-a506-403a-badb-0a2060f9768f succeeded

STEP: deleting the pod 05/09/23 13:04:12.398
STEP: changing the externalName to bar.example.com 05/09/23 13:04:12.419
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8223.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local; sleep 1; done
 05/09/23 13:04:12.427
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8223.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local; sleep 1; done
 05/09/23 13:04:12.427
STEP: creating a second pod to probe DNS 05/09/23 13:04:12.427
STEP: submitting the pod to kubernetes 05/09/23 13:04:12.427
May  9 13:04:12.435: INFO: Waiting up to 15m0s for pod "dns-test-e435da56-0f39-47e5-a582-5475c3541cf6" in namespace "dns-8223" to be "running"
May  9 13:04:12.444: INFO: Pod "dns-test-e435da56-0f39-47e5-a582-5475c3541cf6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.624579ms
May  9 13:04:14.448: INFO: Pod "dns-test-e435da56-0f39-47e5-a582-5475c3541cf6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012580747s
May  9 13:04:14.448: INFO: Pod "dns-test-e435da56-0f39-47e5-a582-5475c3541cf6" satisfied condition "running"
STEP: retrieving the pod 05/09/23 13:04:14.448
STEP: looking for the results for each expected name from probers 05/09/23 13:04:14.45
May  9 13:04:14.453: INFO: File wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 13:04:14.455: INFO: File jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 13:04:14.455: INFO: Lookups using dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 failed for: [wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local]

May  9 13:04:19.459: INFO: File wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 13:04:19.461: INFO: File jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 13:04:19.461: INFO: Lookups using dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 failed for: [wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local]

May  9 13:04:24.459: INFO: File wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 13:04:24.461: INFO: File jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 13:04:24.461: INFO: Lookups using dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 failed for: [wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local]

May  9 13:04:29.461: INFO: File wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 13:04:29.463: INFO: File jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 13:04:29.463: INFO: Lookups using dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 failed for: [wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local]

May  9 13:04:34.461: INFO: File wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 13:04:34.463: INFO: File jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 13:04:34.463: INFO: Lookups using dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 failed for: [wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local]

May  9 13:04:39.461: INFO: File wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 13:04:39.463: INFO: File jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 13:04:39.463: INFO: Lookups using dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 failed for: [wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local]

May  9 13:04:44.463: INFO: DNS probes using dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 succeeded

STEP: deleting the pod 05/09/23 13:04:44.463
STEP: changing the service to type=ClusterIP 05/09/23 13:04:44.474
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8223.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local; sleep 1; done
 05/09/23 13:04:44.488
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8223.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local; sleep 1; done
 05/09/23 13:04:44.488
STEP: creating a third pod to probe DNS 05/09/23 13:04:44.488
STEP: submitting the pod to kubernetes 05/09/23 13:04:44.49
May  9 13:04:44.518: INFO: Waiting up to 15m0s for pod "dns-test-438115f9-1c73-4921-91d0-62189704ecfb" in namespace "dns-8223" to be "running"
May  9 13:04:44.520: INFO: Pod "dns-test-438115f9-1c73-4921-91d0-62189704ecfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041957ms
May  9 13:04:46.524: INFO: Pod "dns-test-438115f9-1c73-4921-91d0-62189704ecfb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005556332s
May  9 13:04:46.524: INFO: Pod "dns-test-438115f9-1c73-4921-91d0-62189704ecfb" satisfied condition "running"
STEP: retrieving the pod 05/09/23 13:04:46.524
STEP: looking for the results for each expected name from probers 05/09/23 13:04:46.526
May  9 13:04:46.531: INFO: DNS probes using dns-test-438115f9-1c73-4921-91d0-62189704ecfb succeeded

STEP: deleting the pod 05/09/23 13:04:46.531
STEP: deleting the test externalName service 05/09/23 13:04:46.54
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  9 13:04:46.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8223" for this suite. 05/09/23 13:04:46.557
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":129,"skipped":2499,"failed":0}
------------------------------
• [SLOW TEST] [36.203 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:04:10.359
    May  9 13:04:10.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename dns 05/09/23 13:04:10.359
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:04:10.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:04:10.375
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 05/09/23 13:04:10.376
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8223.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local; sleep 1; done
     05/09/23 13:04:10.379
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8223.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local; sleep 1; done
     05/09/23 13:04:10.379
    STEP: creating a pod to probe DNS 05/09/23 13:04:10.379
    STEP: submitting the pod to kubernetes 05/09/23 13:04:10.379
    May  9 13:04:10.386: INFO: Waiting up to 15m0s for pod "dns-test-e1ad641c-a506-403a-badb-0a2060f9768f" in namespace "dns-8223" to be "running"
    May  9 13:04:10.388: INFO: Pod "dns-test-e1ad641c-a506-403a-badb-0a2060f9768f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.729623ms
    May  9 13:04:12.390: INFO: Pod "dns-test-e1ad641c-a506-403a-badb-0a2060f9768f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004419194s
    May  9 13:04:12.390: INFO: Pod "dns-test-e1ad641c-a506-403a-badb-0a2060f9768f" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 13:04:12.39
    STEP: looking for the results for each expected name from probers 05/09/23 13:04:12.392
    May  9 13:04:12.398: INFO: DNS probes using dns-test-e1ad641c-a506-403a-badb-0a2060f9768f succeeded

    STEP: deleting the pod 05/09/23 13:04:12.398
    STEP: changing the externalName to bar.example.com 05/09/23 13:04:12.419
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8223.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local; sleep 1; done
     05/09/23 13:04:12.427
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8223.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local; sleep 1; done
     05/09/23 13:04:12.427
    STEP: creating a second pod to probe DNS 05/09/23 13:04:12.427
    STEP: submitting the pod to kubernetes 05/09/23 13:04:12.427
    May  9 13:04:12.435: INFO: Waiting up to 15m0s for pod "dns-test-e435da56-0f39-47e5-a582-5475c3541cf6" in namespace "dns-8223" to be "running"
    May  9 13:04:12.444: INFO: Pod "dns-test-e435da56-0f39-47e5-a582-5475c3541cf6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.624579ms
    May  9 13:04:14.448: INFO: Pod "dns-test-e435da56-0f39-47e5-a582-5475c3541cf6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012580747s
    May  9 13:04:14.448: INFO: Pod "dns-test-e435da56-0f39-47e5-a582-5475c3541cf6" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 13:04:14.448
    STEP: looking for the results for each expected name from probers 05/09/23 13:04:14.45
    May  9 13:04:14.453: INFO: File wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 13:04:14.455: INFO: File jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 13:04:14.455: INFO: Lookups using dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 failed for: [wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local]

    May  9 13:04:19.459: INFO: File wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 13:04:19.461: INFO: File jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 13:04:19.461: INFO: Lookups using dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 failed for: [wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local]

    May  9 13:04:24.459: INFO: File wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 13:04:24.461: INFO: File jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 13:04:24.461: INFO: Lookups using dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 failed for: [wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local]

    May  9 13:04:29.461: INFO: File wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 13:04:29.463: INFO: File jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 13:04:29.463: INFO: Lookups using dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 failed for: [wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local]

    May  9 13:04:34.461: INFO: File wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 13:04:34.463: INFO: File jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 13:04:34.463: INFO: Lookups using dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 failed for: [wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local]

    May  9 13:04:39.461: INFO: File wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 13:04:39.463: INFO: File jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local from pod  dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 13:04:39.463: INFO: Lookups using dns-8223/dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 failed for: [wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local]

    May  9 13:04:44.463: INFO: DNS probes using dns-test-e435da56-0f39-47e5-a582-5475c3541cf6 succeeded

    STEP: deleting the pod 05/09/23 13:04:44.463
    STEP: changing the service to type=ClusterIP 05/09/23 13:04:44.474
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8223.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8223.svc.cluster.local; sleep 1; done
     05/09/23 13:04:44.488
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8223.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8223.svc.cluster.local; sleep 1; done
     05/09/23 13:04:44.488
    STEP: creating a third pod to probe DNS 05/09/23 13:04:44.488
    STEP: submitting the pod to kubernetes 05/09/23 13:04:44.49
    May  9 13:04:44.518: INFO: Waiting up to 15m0s for pod "dns-test-438115f9-1c73-4921-91d0-62189704ecfb" in namespace "dns-8223" to be "running"
    May  9 13:04:44.520: INFO: Pod "dns-test-438115f9-1c73-4921-91d0-62189704ecfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041957ms
    May  9 13:04:46.524: INFO: Pod "dns-test-438115f9-1c73-4921-91d0-62189704ecfb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005556332s
    May  9 13:04:46.524: INFO: Pod "dns-test-438115f9-1c73-4921-91d0-62189704ecfb" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 13:04:46.524
    STEP: looking for the results for each expected name from probers 05/09/23 13:04:46.526
    May  9 13:04:46.531: INFO: DNS probes using dns-test-438115f9-1c73-4921-91d0-62189704ecfb succeeded

    STEP: deleting the pod 05/09/23 13:04:46.531
    STEP: deleting the test externalName service 05/09/23 13:04:46.54
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  9 13:04:46.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8223" for this suite. 05/09/23 13:04:46.557
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:04:46.562
May  9 13:04:46.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:04:46.563
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:04:46.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:04:46.589
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
May  9 13:04:46.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/09/23 13:04:49.037
May  9 13:04:49.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-6556 --namespace=crd-publish-openapi-6556 create -f -'
May  9 13:04:49.512: INFO: stderr: ""
May  9 13:04:49.512: INFO: stdout: "e2e-test-crd-publish-openapi-3747-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  9 13:04:49.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-6556 --namespace=crd-publish-openapi-6556 delete e2e-test-crd-publish-openapi-3747-crds test-cr'
May  9 13:04:49.585: INFO: stderr: ""
May  9 13:04:49.585: INFO: stdout: "e2e-test-crd-publish-openapi-3747-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May  9 13:04:49.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-6556 --namespace=crd-publish-openapi-6556 apply -f -'
May  9 13:04:50.007: INFO: stderr: ""
May  9 13:04:50.007: INFO: stdout: "e2e-test-crd-publish-openapi-3747-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  9 13:04:50.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-6556 --namespace=crd-publish-openapi-6556 delete e2e-test-crd-publish-openapi-3747-crds test-cr'
May  9 13:04:50.060: INFO: stderr: ""
May  9 13:04:50.060: INFO: stdout: "e2e-test-crd-publish-openapi-3747-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 05/09/23 13:04:50.06
May  9 13:04:50.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-6556 explain e2e-test-crd-publish-openapi-3747-crds'
May  9 13:04:50.185: INFO: stderr: ""
May  9 13:04:50.185: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3747-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:04:52.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6556" for this suite. 05/09/23 13:04:52.636
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":130,"skipped":2500,"failed":0}
------------------------------
• [SLOW TEST] [6.078 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:04:46.562
    May  9 13:04:46.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:04:46.563
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:04:46.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:04:46.589
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    May  9 13:04:46.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/09/23 13:04:49.037
    May  9 13:04:49.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-6556 --namespace=crd-publish-openapi-6556 create -f -'
    May  9 13:04:49.512: INFO: stderr: ""
    May  9 13:04:49.512: INFO: stdout: "e2e-test-crd-publish-openapi-3747-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May  9 13:04:49.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-6556 --namespace=crd-publish-openapi-6556 delete e2e-test-crd-publish-openapi-3747-crds test-cr'
    May  9 13:04:49.585: INFO: stderr: ""
    May  9 13:04:49.585: INFO: stdout: "e2e-test-crd-publish-openapi-3747-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    May  9 13:04:49.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-6556 --namespace=crd-publish-openapi-6556 apply -f -'
    May  9 13:04:50.007: INFO: stderr: ""
    May  9 13:04:50.007: INFO: stdout: "e2e-test-crd-publish-openapi-3747-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May  9 13:04:50.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-6556 --namespace=crd-publish-openapi-6556 delete e2e-test-crd-publish-openapi-3747-crds test-cr'
    May  9 13:04:50.060: INFO: stderr: ""
    May  9 13:04:50.060: INFO: stdout: "e2e-test-crd-publish-openapi-3747-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 05/09/23 13:04:50.06
    May  9 13:04:50.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-6556 explain e2e-test-crd-publish-openapi-3747-crds'
    May  9 13:04:50.185: INFO: stderr: ""
    May  9 13:04:50.185: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3747-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:04:52.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6556" for this suite. 05/09/23 13:04:52.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:04:52.641
May  9 13:04:52.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:04:52.641
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:04:52.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:04:52.653
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-dca15282-8a56-4207-b454-e6b82a7fc9be 05/09/23 13:04:52.657
STEP: Creating configMap with name cm-test-opt-upd-2c317e6e-42c1-48be-9437-ca9c05898f58 05/09/23 13:04:52.66
STEP: Creating the pod 05/09/23 13:04:52.663
May  9 13:04:52.669: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-260d97e6-9c7b-499f-bac2-1796bfb3f16e" in namespace "projected-4238" to be "running and ready"
May  9 13:04:52.671: INFO: Pod "pod-projected-configmaps-260d97e6-9c7b-499f-bac2-1796bfb3f16e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.728ms
May  9 13:04:52.671: INFO: The phase of Pod pod-projected-configmaps-260d97e6-9c7b-499f-bac2-1796bfb3f16e is Pending, waiting for it to be Running (with Ready = true)
May  9 13:04:54.675: INFO: Pod "pod-projected-configmaps-260d97e6-9c7b-499f-bac2-1796bfb3f16e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005617912s
May  9 13:04:54.675: INFO: The phase of Pod pod-projected-configmaps-260d97e6-9c7b-499f-bac2-1796bfb3f16e is Running (Ready = true)
May  9 13:04:54.675: INFO: Pod "pod-projected-configmaps-260d97e6-9c7b-499f-bac2-1796bfb3f16e" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-dca15282-8a56-4207-b454-e6b82a7fc9be 05/09/23 13:04:54.693
STEP: Updating configmap cm-test-opt-upd-2c317e6e-42c1-48be-9437-ca9c05898f58 05/09/23 13:04:54.697
STEP: Creating configMap with name cm-test-opt-create-63ecca1c-3d60-4a31-9fc5-716147e7b442 05/09/23 13:04:54.7
STEP: waiting to observe update in volume 05/09/23 13:04:54.703
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  9 13:04:58.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4238" for this suite. 05/09/23 13:04:58.726
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":131,"skipped":2519,"failed":0}
------------------------------
• [SLOW TEST] [6.090 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:04:52.641
    May  9 13:04:52.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:04:52.641
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:04:52.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:04:52.653
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-dca15282-8a56-4207-b454-e6b82a7fc9be 05/09/23 13:04:52.657
    STEP: Creating configMap with name cm-test-opt-upd-2c317e6e-42c1-48be-9437-ca9c05898f58 05/09/23 13:04:52.66
    STEP: Creating the pod 05/09/23 13:04:52.663
    May  9 13:04:52.669: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-260d97e6-9c7b-499f-bac2-1796bfb3f16e" in namespace "projected-4238" to be "running and ready"
    May  9 13:04:52.671: INFO: Pod "pod-projected-configmaps-260d97e6-9c7b-499f-bac2-1796bfb3f16e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.728ms
    May  9 13:04:52.671: INFO: The phase of Pod pod-projected-configmaps-260d97e6-9c7b-499f-bac2-1796bfb3f16e is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:04:54.675: INFO: Pod "pod-projected-configmaps-260d97e6-9c7b-499f-bac2-1796bfb3f16e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005617912s
    May  9 13:04:54.675: INFO: The phase of Pod pod-projected-configmaps-260d97e6-9c7b-499f-bac2-1796bfb3f16e is Running (Ready = true)
    May  9 13:04:54.675: INFO: Pod "pod-projected-configmaps-260d97e6-9c7b-499f-bac2-1796bfb3f16e" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-dca15282-8a56-4207-b454-e6b82a7fc9be 05/09/23 13:04:54.693
    STEP: Updating configmap cm-test-opt-upd-2c317e6e-42c1-48be-9437-ca9c05898f58 05/09/23 13:04:54.697
    STEP: Creating configMap with name cm-test-opt-create-63ecca1c-3d60-4a31-9fc5-716147e7b442 05/09/23 13:04:54.7
    STEP: waiting to observe update in volume 05/09/23 13:04:54.703
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  9 13:04:58.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4238" for this suite. 05/09/23 13:04:58.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:04:58.732
May  9 13:04:58.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename replication-controller 05/09/23 13:04:58.732
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:04:58.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:04:58.746
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e 05/09/23 13:04:58.747
May  9 13:04:58.752: INFO: Pod name my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e: Found 0 pods out of 1
May  9 13:05:03.756: INFO: Pod name my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e: Found 1 pods out of 1
May  9 13:05:03.756: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e" are running
May  9 13:05:03.756: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e-5kjzc" in namespace "replication-controller-9743" to be "running"
May  9 13:05:03.760: INFO: Pod "my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e-5kjzc": Phase="Running", Reason="", readiness=true. Elapsed: 4.249842ms
May  9 13:05:03.760: INFO: Pod "my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e-5kjzc" satisfied condition "running"
May  9 13:05:03.760: INFO: Pod "my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e-5kjzc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 13:04:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 13:04:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 13:04:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 13:04:58 +0000 UTC Reason: Message:}])
May  9 13:05:03.760: INFO: Trying to dial the pod
May  9 13:05:08.768: INFO: Controller my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e: Got expected result from replica 1 [my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e-5kjzc]: "my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e-5kjzc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  9 13:05:08.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9743" for this suite. 05/09/23 13:05:08.771
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":132,"skipped":2530,"failed":0}
------------------------------
• [SLOW TEST] [10.043 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:04:58.732
    May  9 13:04:58.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename replication-controller 05/09/23 13:04:58.732
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:04:58.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:04:58.746
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e 05/09/23 13:04:58.747
    May  9 13:04:58.752: INFO: Pod name my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e: Found 0 pods out of 1
    May  9 13:05:03.756: INFO: Pod name my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e: Found 1 pods out of 1
    May  9 13:05:03.756: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e" are running
    May  9 13:05:03.756: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e-5kjzc" in namespace "replication-controller-9743" to be "running"
    May  9 13:05:03.760: INFO: Pod "my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e-5kjzc": Phase="Running", Reason="", readiness=true. Elapsed: 4.249842ms
    May  9 13:05:03.760: INFO: Pod "my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e-5kjzc" satisfied condition "running"
    May  9 13:05:03.760: INFO: Pod "my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e-5kjzc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 13:04:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 13:04:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 13:04:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 13:04:58 +0000 UTC Reason: Message:}])
    May  9 13:05:03.760: INFO: Trying to dial the pod
    May  9 13:05:08.768: INFO: Controller my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e: Got expected result from replica 1 [my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e-5kjzc]: "my-hostname-basic-3bd83fa4-92f8-4cc5-a332-c4b53057059e-5kjzc", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  9 13:05:08.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9743" for this suite. 05/09/23 13:05:08.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:05:08.776
May  9 13:05:08.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename security-context-test 05/09/23 13:05:08.776
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:08.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:08.788
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
May  9 13:05:08.794: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-ba0f0878-164b-4c09-a382-fba692b7ee31" in namespace "security-context-test-8939" to be "Succeeded or Failed"
May  9 13:05:08.795: INFO: Pod "alpine-nnp-false-ba0f0878-164b-4c09-a382-fba692b7ee31": Phase="Pending", Reason="", readiness=false. Elapsed: 1.581751ms
May  9 13:05:10.799: INFO: Pod "alpine-nnp-false-ba0f0878-164b-4c09-a382-fba692b7ee31": Phase="Running", Reason="", readiness=true. Elapsed: 2.005089067s
May  9 13:05:12.798: INFO: Pod "alpine-nnp-false-ba0f0878-164b-4c09-a382-fba692b7ee31": Phase="Running", Reason="", readiness=false. Elapsed: 4.004218901s
May  9 13:05:14.800: INFO: Pod "alpine-nnp-false-ba0f0878-164b-4c09-a382-fba692b7ee31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005889442s
May  9 13:05:14.800: INFO: Pod "alpine-nnp-false-ba0f0878-164b-4c09-a382-fba692b7ee31" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  9 13:05:14.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8939" for this suite. 05/09/23 13:05:14.807
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":133,"skipped":2576,"failed":0}
------------------------------
• [SLOW TEST] [6.036 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:05:08.776
    May  9 13:05:08.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename security-context-test 05/09/23 13:05:08.776
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:08.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:08.788
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    May  9 13:05:08.794: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-ba0f0878-164b-4c09-a382-fba692b7ee31" in namespace "security-context-test-8939" to be "Succeeded or Failed"
    May  9 13:05:08.795: INFO: Pod "alpine-nnp-false-ba0f0878-164b-4c09-a382-fba692b7ee31": Phase="Pending", Reason="", readiness=false. Elapsed: 1.581751ms
    May  9 13:05:10.799: INFO: Pod "alpine-nnp-false-ba0f0878-164b-4c09-a382-fba692b7ee31": Phase="Running", Reason="", readiness=true. Elapsed: 2.005089067s
    May  9 13:05:12.798: INFO: Pod "alpine-nnp-false-ba0f0878-164b-4c09-a382-fba692b7ee31": Phase="Running", Reason="", readiness=false. Elapsed: 4.004218901s
    May  9 13:05:14.800: INFO: Pod "alpine-nnp-false-ba0f0878-164b-4c09-a382-fba692b7ee31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005889442s
    May  9 13:05:14.800: INFO: Pod "alpine-nnp-false-ba0f0878-164b-4c09-a382-fba692b7ee31" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  9 13:05:14.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-8939" for this suite. 05/09/23 13:05:14.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:05:14.813
May  9 13:05:14.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pods 05/09/23 13:05:14.814
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:14.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:14.823
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 05/09/23 13:05:14.825
May  9 13:05:14.830: INFO: created test-pod-1
May  9 13:05:14.834: INFO: created test-pod-2
May  9 13:05:14.844: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 05/09/23 13:05:14.844
May  9 13:05:14.844: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1003' to be running and ready
May  9 13:05:14.853: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  9 13:05:14.853: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  9 13:05:14.853: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  9 13:05:14.853: INFO: 0 / 3 pods in namespace 'pods-1003' are running and ready (0 seconds elapsed)
May  9 13:05:14.853: INFO: expected 0 pod replicas in namespace 'pods-1003', 0 are Running and Ready.
May  9 13:05:14.853: INFO: POD         NODE                        PHASE    GRACE  CONDITIONS
May  9 13:05:14.853: INFO: test-pod-1  cl-gks-cncf-ix1-md-0-48ljh  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:05:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:05:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:05:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:05:14 +0000 UTC  }]
May  9 13:05:14.853: INFO: test-pod-2  cl-gks-cncf-ix1-md-0-48ljh  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:05:14 +0000 UTC  }]
May  9 13:05:14.853: INFO: test-pod-3  cl-gks-cncf-ix1-md-0-879bk  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:05:14 +0000 UTC  }]
May  9 13:05:14.853: INFO: 
May  9 13:05:16.861: INFO: 3 / 3 pods in namespace 'pods-1003' are running and ready (2 seconds elapsed)
May  9 13:05:16.861: INFO: expected 0 pod replicas in namespace 'pods-1003', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 05/09/23 13:05:16.876
May  9 13:05:16.878: INFO: Pod quantity 3 is different from expected quantity 0
May  9 13:05:17.881: INFO: Pod quantity 3 is different from expected quantity 0
May  9 13:05:18.884: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  9 13:05:19.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1003" for this suite. 05/09/23 13:05:19.883
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":134,"skipped":2586,"failed":0}
------------------------------
• [SLOW TEST] [5.074 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:05:14.813
    May  9 13:05:14.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pods 05/09/23 13:05:14.814
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:14.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:14.823
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 05/09/23 13:05:14.825
    May  9 13:05:14.830: INFO: created test-pod-1
    May  9 13:05:14.834: INFO: created test-pod-2
    May  9 13:05:14.844: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 05/09/23 13:05:14.844
    May  9 13:05:14.844: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1003' to be running and ready
    May  9 13:05:14.853: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  9 13:05:14.853: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  9 13:05:14.853: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  9 13:05:14.853: INFO: 0 / 3 pods in namespace 'pods-1003' are running and ready (0 seconds elapsed)
    May  9 13:05:14.853: INFO: expected 0 pod replicas in namespace 'pods-1003', 0 are Running and Ready.
    May  9 13:05:14.853: INFO: POD         NODE                        PHASE    GRACE  CONDITIONS
    May  9 13:05:14.853: INFO: test-pod-1  cl-gks-cncf-ix1-md-0-48ljh  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:05:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:05:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:05:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:05:14 +0000 UTC  }]
    May  9 13:05:14.853: INFO: test-pod-2  cl-gks-cncf-ix1-md-0-48ljh  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:05:14 +0000 UTC  }]
    May  9 13:05:14.853: INFO: test-pod-3  cl-gks-cncf-ix1-md-0-879bk  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:05:14 +0000 UTC  }]
    May  9 13:05:14.853: INFO: 
    May  9 13:05:16.861: INFO: 3 / 3 pods in namespace 'pods-1003' are running and ready (2 seconds elapsed)
    May  9 13:05:16.861: INFO: expected 0 pod replicas in namespace 'pods-1003', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 05/09/23 13:05:16.876
    May  9 13:05:16.878: INFO: Pod quantity 3 is different from expected quantity 0
    May  9 13:05:17.881: INFO: Pod quantity 3 is different from expected quantity 0
    May  9 13:05:18.884: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  9 13:05:19.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1003" for this suite. 05/09/23 13:05:19.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:05:19.888
May  9 13:05:19.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pod-network-test 05/09/23 13:05:19.889
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:19.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:19.901
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-5840 05/09/23 13:05:19.902
STEP: creating a selector 05/09/23 13:05:19.902
STEP: Creating the service pods in kubernetes 05/09/23 13:05:19.902
May  9 13:05:19.902: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  9 13:05:19.926: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5840" to be "running and ready"
May  9 13:05:19.932: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346863ms
May  9 13:05:19.932: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:05:21.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009174006s
May  9 13:05:21.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:05:23.936: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010820312s
May  9 13:05:23.936: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:05:25.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009650992s
May  9 13:05:25.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:05:27.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008891434s
May  9 13:05:27.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:05:29.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00969634s
May  9 13:05:29.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:05:31.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009720824s
May  9 13:05:31.935: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  9 13:05:31.935: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  9 13:05:31.938: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5840" to be "running and ready"
May  9 13:05:31.939: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.649721ms
May  9 13:05:31.939: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  9 13:05:31.939: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  9 13:05:31.941: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5840" to be "running and ready"
May  9 13:05:31.942: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.409593ms
May  9 13:05:31.942: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  9 13:05:31.942: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/09/23 13:05:31.944
May  9 13:05:31.949: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5840" to be "running"
May  9 13:05:31.951: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.568415ms
May  9 13:05:33.954: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005280011s
May  9 13:05:33.954: INFO: Pod "test-container-pod" satisfied condition "running"
May  9 13:05:33.956: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  9 13:05:33.956: INFO: Breadth first check of 172.25.124.235 on host 192.168.1.64...
May  9 13:05:33.957: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.124.236:9080/dial?request=hostname&protocol=udp&host=172.25.124.235&port=8081&tries=1'] Namespace:pod-network-test-5840 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:05:33.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:05:33.958: INFO: ExecWithOptions: Clientset creation
May  9 13:05:33.958: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5840/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.124.236%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.124.235%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  9 13:05:34.014: INFO: Waiting for responses: map[]
May  9 13:05:34.014: INFO: reached 172.25.124.235 after 0/1 tries
May  9 13:05:34.014: INFO: Breadth first check of 172.25.53.177 on host 192.168.1.73...
May  9 13:05:34.016: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.124.236:9080/dial?request=hostname&protocol=udp&host=172.25.53.177&port=8081&tries=1'] Namespace:pod-network-test-5840 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:05:34.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:05:34.016: INFO: ExecWithOptions: Clientset creation
May  9 13:05:34.016: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5840/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.124.236%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.53.177%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  9 13:05:34.069: INFO: Waiting for responses: map[]
May  9 13:05:34.069: INFO: reached 172.25.53.177 after 0/1 tries
May  9 13:05:34.069: INFO: Breadth first check of 172.25.72.212 on host 192.168.1.89...
May  9 13:05:34.071: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.124.236:9080/dial?request=hostname&protocol=udp&host=172.25.72.212&port=8081&tries=1'] Namespace:pod-network-test-5840 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:05:34.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:05:34.072: INFO: ExecWithOptions: Clientset creation
May  9 13:05:34.072: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5840/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.124.236%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.72.212%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  9 13:05:34.121: INFO: Waiting for responses: map[]
May  9 13:05:34.121: INFO: reached 172.25.72.212 after 0/1 tries
May  9 13:05:34.121: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May  9 13:05:34.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5840" for this suite. 05/09/23 13:05:34.124
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":135,"skipped":2618,"failed":0}
------------------------------
• [SLOW TEST] [14.240 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:05:19.888
    May  9 13:05:19.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pod-network-test 05/09/23 13:05:19.889
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:19.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:19.901
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-5840 05/09/23 13:05:19.902
    STEP: creating a selector 05/09/23 13:05:19.902
    STEP: Creating the service pods in kubernetes 05/09/23 13:05:19.902
    May  9 13:05:19.902: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  9 13:05:19.926: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5840" to be "running and ready"
    May  9 13:05:19.932: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346863ms
    May  9 13:05:19.932: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:05:21.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009174006s
    May  9 13:05:21.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:05:23.936: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010820312s
    May  9 13:05:23.936: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:05:25.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009650992s
    May  9 13:05:25.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:05:27.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008891434s
    May  9 13:05:27.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:05:29.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00969634s
    May  9 13:05:29.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:05:31.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009720824s
    May  9 13:05:31.935: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  9 13:05:31.935: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  9 13:05:31.938: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5840" to be "running and ready"
    May  9 13:05:31.939: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.649721ms
    May  9 13:05:31.939: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  9 13:05:31.939: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  9 13:05:31.941: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5840" to be "running and ready"
    May  9 13:05:31.942: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.409593ms
    May  9 13:05:31.942: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  9 13:05:31.942: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/09/23 13:05:31.944
    May  9 13:05:31.949: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5840" to be "running"
    May  9 13:05:31.951: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.568415ms
    May  9 13:05:33.954: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005280011s
    May  9 13:05:33.954: INFO: Pod "test-container-pod" satisfied condition "running"
    May  9 13:05:33.956: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May  9 13:05:33.956: INFO: Breadth first check of 172.25.124.235 on host 192.168.1.64...
    May  9 13:05:33.957: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.124.236:9080/dial?request=hostname&protocol=udp&host=172.25.124.235&port=8081&tries=1'] Namespace:pod-network-test-5840 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:05:33.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:05:33.958: INFO: ExecWithOptions: Clientset creation
    May  9 13:05:33.958: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5840/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.124.236%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.124.235%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  9 13:05:34.014: INFO: Waiting for responses: map[]
    May  9 13:05:34.014: INFO: reached 172.25.124.235 after 0/1 tries
    May  9 13:05:34.014: INFO: Breadth first check of 172.25.53.177 on host 192.168.1.73...
    May  9 13:05:34.016: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.124.236:9080/dial?request=hostname&protocol=udp&host=172.25.53.177&port=8081&tries=1'] Namespace:pod-network-test-5840 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:05:34.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:05:34.016: INFO: ExecWithOptions: Clientset creation
    May  9 13:05:34.016: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5840/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.124.236%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.53.177%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  9 13:05:34.069: INFO: Waiting for responses: map[]
    May  9 13:05:34.069: INFO: reached 172.25.53.177 after 0/1 tries
    May  9 13:05:34.069: INFO: Breadth first check of 172.25.72.212 on host 192.168.1.89...
    May  9 13:05:34.071: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.124.236:9080/dial?request=hostname&protocol=udp&host=172.25.72.212&port=8081&tries=1'] Namespace:pod-network-test-5840 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:05:34.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:05:34.072: INFO: ExecWithOptions: Clientset creation
    May  9 13:05:34.072: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5840/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.124.236%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.72.212%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  9 13:05:34.121: INFO: Waiting for responses: map[]
    May  9 13:05:34.121: INFO: reached 172.25.72.212 after 0/1 tries
    May  9 13:05:34.121: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May  9 13:05:34.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5840" for this suite. 05/09/23 13:05:34.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:05:34.129
May  9 13:05:34.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:05:34.129
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:34.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:34.141
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/09/23 13:05:34.142
May  9 13:05:34.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:05:36.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:05:46.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-924" for this suite. 05/09/23 13:05:46.506
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":136,"skipped":2632,"failed":0}
------------------------------
• [SLOW TEST] [12.382 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:05:34.129
    May  9 13:05:34.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:05:34.129
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:34.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:34.141
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/09/23 13:05:34.142
    May  9 13:05:34.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:05:36.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:05:46.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-924" for this suite. 05/09/23 13:05:46.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:05:46.512
May  9 13:05:46.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-runtime 05/09/23 13:05:46.513
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:46.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:46.528
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 05/09/23 13:05:46.53
STEP: wait for the container to reach Failed 05/09/23 13:05:46.536
STEP: get the container status 05/09/23 13:05:49.548
STEP: the container should be terminated 05/09/23 13:05:49.556
STEP: the termination message should be set 05/09/23 13:05:49.556
May  9 13:05:49.556: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/09/23 13:05:49.556
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  9 13:05:49.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4034" for this suite. 05/09/23 13:05:49.573
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":137,"skipped":2672,"failed":0}
------------------------------
• [3.065 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:05:46.512
    May  9 13:05:46.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-runtime 05/09/23 13:05:46.513
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:46.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:46.528
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 05/09/23 13:05:46.53
    STEP: wait for the container to reach Failed 05/09/23 13:05:46.536
    STEP: get the container status 05/09/23 13:05:49.548
    STEP: the container should be terminated 05/09/23 13:05:49.556
    STEP: the termination message should be set 05/09/23 13:05:49.556
    May  9 13:05:49.556: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/09/23 13:05:49.556
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  9 13:05:49.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4034" for this suite. 05/09/23 13:05:49.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:05:49.577
May  9 13:05:49.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 13:05:49.578
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:49.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:49.592
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-4208/secret-test-8f1d857a-d75d-48eb-80d2-265368defd3f 05/09/23 13:05:49.594
STEP: Creating a pod to test consume secrets 05/09/23 13:05:49.597
May  9 13:05:49.603: INFO: Waiting up to 5m0s for pod "pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc" in namespace "secrets-4208" to be "Succeeded or Failed"
May  9 13:05:49.605: INFO: Pod "pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.75493ms
May  9 13:05:51.608: INFO: Pod "pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005138215s
May  9 13:05:53.609: INFO: Pod "pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005990925s
STEP: Saw pod success 05/09/23 13:05:53.609
May  9 13:05:53.609: INFO: Pod "pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc" satisfied condition "Succeeded or Failed"
May  9 13:05:53.611: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc container env-test: <nil>
STEP: delete the pod 05/09/23 13:05:53.621
May  9 13:05:53.630: INFO: Waiting for pod pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc to disappear
May  9 13:05:53.632: INFO: Pod pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May  9 13:05:53.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4208" for this suite. 05/09/23 13:05:53.634
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":138,"skipped":2678,"failed":0}
------------------------------
• [4.066 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:05:49.577
    May  9 13:05:49.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 13:05:49.578
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:49.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:49.592
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-4208/secret-test-8f1d857a-d75d-48eb-80d2-265368defd3f 05/09/23 13:05:49.594
    STEP: Creating a pod to test consume secrets 05/09/23 13:05:49.597
    May  9 13:05:49.603: INFO: Waiting up to 5m0s for pod "pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc" in namespace "secrets-4208" to be "Succeeded or Failed"
    May  9 13:05:49.605: INFO: Pod "pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.75493ms
    May  9 13:05:51.608: INFO: Pod "pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005138215s
    May  9 13:05:53.609: INFO: Pod "pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005990925s
    STEP: Saw pod success 05/09/23 13:05:53.609
    May  9 13:05:53.609: INFO: Pod "pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc" satisfied condition "Succeeded or Failed"
    May  9 13:05:53.611: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc container env-test: <nil>
    STEP: delete the pod 05/09/23 13:05:53.621
    May  9 13:05:53.630: INFO: Waiting for pod pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc to disappear
    May  9 13:05:53.632: INFO: Pod pod-configmaps-946d105d-c5b9-47a4-a9b0-1c72ce2c89cc no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May  9 13:05:53.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4208" for this suite. 05/09/23 13:05:53.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:05:53.644
May  9 13:05:53.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename deployment 05/09/23 13:05:53.645
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:53.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:53.656
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
May  9 13:05:53.658: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May  9 13:05:53.664: INFO: Pod name sample-pod: Found 0 pods out of 1
May  9 13:05:58.668: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/09/23 13:05:58.668
May  9 13:05:58.668: INFO: Creating deployment "test-rolling-update-deployment"
May  9 13:05:58.680: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May  9 13:05:58.689: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May  9 13:06:00.695: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May  9 13:06:00.698: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 13:06:00.724: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3388  d93180c7-bf10-468f-8d50-3243ca34d587 23932 1 2023-05-09 13:05:58 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-09 13:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:05:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043456d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 13:05:58 +0000 UTC,LastTransitionTime:2023-05-09 13:05:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-05-09 13:05:59 +0000 UTC,LastTransitionTime:2023-05-09 13:05:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  9 13:06:00.732: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-3388  a06d6a32-35a4-4994-9c7c-4a014044789e 23922 1 2023-05-09 13:05:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d93180c7-bf10-468f-8d50-3243ca34d587 0xc0043bb5b7 0xc0043bb5b8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d93180c7-bf10-468f-8d50-3243ca34d587\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:05:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043bb6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  9 13:06:00.732: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May  9 13:06:00.732: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3388  0f3d3823-c333-4c3f-8c7f-c921d1fa55f8 23931 2 2023-05-09 13:05:53 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d93180c7-bf10-468f-8d50-3243ca34d587 0xc0043bb3b7 0xc0043bb3b8}] [] [{e2e.test Update apps/v1 2023-05-09 13:05:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:05:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d93180c7-bf10-468f-8d50-3243ca34d587\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:05:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0043bb518 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 13:06:00.738: INFO: Pod "test-rolling-update-deployment-78f575d8ff-hqwmd" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-hqwmd test-rolling-update-deployment-78f575d8ff- deployment-3388  3772b7c3-ba0f-4cfe-a529-299f215b5f3e 23921 0 2023-05-09 13:05:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:4cf8c9865d0f39037ef55e8a4e80178a93829f93be74eda8308bf8d691cc17fd cni.projectcalico.org/podIP:172.25.53.178/32 cni.projectcalico.org/podIPs:172.25.53.178/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff a06d6a32-35a4-4994-9c7c-4a014044789e 0xc0043e0bb7 0xc0043e0bb8}] [] [{kube-controller-manager Update v1 2023-05-09 13:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a06d6a32-35a4-4994-9c7c-4a014044789e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:05:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:05:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fj64p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fj64p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:05:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:05:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:05:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.178,StartTime:2023-05-09 13:05:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:05:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://062f93793d780130cdb75798f7de65a3f9df7f0c118c6a29dde14ed3f6b83ba7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  9 13:06:00.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3388" for this suite. 05/09/23 13:06:00.741
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":139,"skipped":2683,"failed":0}
------------------------------
• [SLOW TEST] [7.104 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:05:53.644
    May  9 13:05:53.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename deployment 05/09/23 13:05:53.645
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:05:53.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:05:53.656
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    May  9 13:05:53.658: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    May  9 13:05:53.664: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  9 13:05:58.668: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/09/23 13:05:58.668
    May  9 13:05:58.668: INFO: Creating deployment "test-rolling-update-deployment"
    May  9 13:05:58.680: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    May  9 13:05:58.689: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    May  9 13:06:00.695: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    May  9 13:06:00.698: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 13:06:00.724: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3388  d93180c7-bf10-468f-8d50-3243ca34d587 23932 1 2023-05-09 13:05:58 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-09 13:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:05:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043456d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 13:05:58 +0000 UTC,LastTransitionTime:2023-05-09 13:05:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-05-09 13:05:59 +0000 UTC,LastTransitionTime:2023-05-09 13:05:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  9 13:06:00.732: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-3388  a06d6a32-35a4-4994-9c7c-4a014044789e 23922 1 2023-05-09 13:05:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d93180c7-bf10-468f-8d50-3243ca34d587 0xc0043bb5b7 0xc0043bb5b8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d93180c7-bf10-468f-8d50-3243ca34d587\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:05:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043bb6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  9 13:06:00.732: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    May  9 13:06:00.732: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3388  0f3d3823-c333-4c3f-8c7f-c921d1fa55f8 23931 2 2023-05-09 13:05:53 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d93180c7-bf10-468f-8d50-3243ca34d587 0xc0043bb3b7 0xc0043bb3b8}] [] [{e2e.test Update apps/v1 2023-05-09 13:05:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:05:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d93180c7-bf10-468f-8d50-3243ca34d587\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:05:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0043bb518 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 13:06:00.738: INFO: Pod "test-rolling-update-deployment-78f575d8ff-hqwmd" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-hqwmd test-rolling-update-deployment-78f575d8ff- deployment-3388  3772b7c3-ba0f-4cfe-a529-299f215b5f3e 23921 0 2023-05-09 13:05:58 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:4cf8c9865d0f39037ef55e8a4e80178a93829f93be74eda8308bf8d691cc17fd cni.projectcalico.org/podIP:172.25.53.178/32 cni.projectcalico.org/podIPs:172.25.53.178/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff a06d6a32-35a4-4994-9c7c-4a014044789e 0xc0043e0bb7 0xc0043e0bb8}] [] [{kube-controller-manager Update v1 2023-05-09 13:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a06d6a32-35a4-4994-9c7c-4a014044789e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:05:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:05:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fj64p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fj64p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:05:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:05:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:05:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.178,StartTime:2023-05-09 13:05:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:05:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://062f93793d780130cdb75798f7de65a3f9df7f0c118c6a29dde14ed3f6b83ba7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  9 13:06:00.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3388" for this suite. 05/09/23 13:06:00.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:06:00.749
May  9 13:06:00.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename job 05/09/23 13:06:00.75
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:06:00.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:06:00.767
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 05/09/23 13:06:00.768
STEP: Ensure pods equal to paralellism count is attached to the job 05/09/23 13:06:00.775
STEP: patching /status 05/09/23 13:06:02.778
STEP: updating /status 05/09/23 13:06:02.785
STEP: get /status 05/09/23 13:06:02.79
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  9 13:06:02.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2704" for this suite. 05/09/23 13:06:02.794
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":140,"skipped":2711,"failed":0}
------------------------------
• [2.050 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:06:00.749
    May  9 13:06:00.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename job 05/09/23 13:06:00.75
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:06:00.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:06:00.767
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 05/09/23 13:06:00.768
    STEP: Ensure pods equal to paralellism count is attached to the job 05/09/23 13:06:00.775
    STEP: patching /status 05/09/23 13:06:02.778
    STEP: updating /status 05/09/23 13:06:02.785
    STEP: get /status 05/09/23 13:06:02.79
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  9 13:06:02.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2704" for this suite. 05/09/23 13:06:02.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:06:02.799
May  9 13:06:02.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:06:02.8
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:06:02.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:06:02.812
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:06:02.827
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:06:03.126
STEP: Deploying the webhook pod 05/09/23 13:06:03.132
STEP: Wait for the deployment to be ready 05/09/23 13:06:03.148
May  9 13:06:03.157: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 13:06:05.165
STEP: Verifying the service has paired with the endpoint 05/09/23 13:06:05.176
May  9 13:06:06.177: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/09/23 13:06:06.179
STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 13:06:06.179
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/09/23 13:06:06.189
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/09/23 13:06:07.196
STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 13:06:07.196
STEP: Having no error when timeout is longer than webhook latency 05/09/23 13:06:08.219
STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 13:06:08.219
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/09/23 13:06:13.248
STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 13:06:13.248
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:06:18.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7190" for this suite. 05/09/23 13:06:18.274
STEP: Destroying namespace "webhook-7190-markers" for this suite. 05/09/23 13:06:18.279
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":141,"skipped":2717,"failed":0}
------------------------------
• [SLOW TEST] [15.522 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:06:02.799
    May  9 13:06:02.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:06:02.8
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:06:02.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:06:02.812
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:06:02.827
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:06:03.126
    STEP: Deploying the webhook pod 05/09/23 13:06:03.132
    STEP: Wait for the deployment to be ready 05/09/23 13:06:03.148
    May  9 13:06:03.157: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 13:06:05.165
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:06:05.176
    May  9 13:06:06.177: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/09/23 13:06:06.179
    STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 13:06:06.179
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/09/23 13:06:06.189
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/09/23 13:06:07.196
    STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 13:06:07.196
    STEP: Having no error when timeout is longer than webhook latency 05/09/23 13:06:08.219
    STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 13:06:08.219
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/09/23 13:06:13.248
    STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 13:06:13.248
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:06:18.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7190" for this suite. 05/09/23 13:06:18.274
    STEP: Destroying namespace "webhook-7190-markers" for this suite. 05/09/23 13:06:18.279
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:06:18.323
May  9 13:06:18.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:06:18.324
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:06:18.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:06:18.34
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-5324 05/09/23 13:06:18.342
STEP: creating service affinity-clusterip in namespace services-5324 05/09/23 13:06:18.342
STEP: creating replication controller affinity-clusterip in namespace services-5324 05/09/23 13:06:18.351
I0509 13:06:18.358149      24 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5324, replica count: 3
I0509 13:06:21.409882      24 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 13:06:21.414: INFO: Creating new exec pod
May  9 13:06:21.419: INFO: Waiting up to 5m0s for pod "execpod-affinity644fz" in namespace "services-5324" to be "running"
May  9 13:06:21.421: INFO: Pod "execpod-affinity644fz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.904897ms
May  9 13:06:23.425: INFO: Pod "execpod-affinity644fz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005801098s
May  9 13:06:23.425: INFO: Pod "execpod-affinity644fz" satisfied condition "running"
May  9 13:06:24.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-5324 exec execpod-affinity644fz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
May  9 13:06:24.532: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May  9 13:06:24.532: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:06:24.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-5324 exec execpod-affinity644fz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.21.132 80'
May  9 13:06:24.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.21.132 80\nConnection to 10.106.21.132 80 port [tcp/http] succeeded!\n"
May  9 13:06:24.639: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:06:24.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-5324 exec execpod-affinity644fz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.21.132:80/ ; done'
May  9 13:06:24.801: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n"
May  9 13:06:24.801: INFO: stdout: "\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj"
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
May  9 13:06:24.801: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5324, will wait for the garbage collector to delete the pods 05/09/23 13:06:24.811
May  9 13:06:24.873: INFO: Deleting ReplicationController affinity-clusterip took: 7.361555ms
May  9 13:06:24.974: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.707811ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:06:26.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5324" for this suite. 05/09/23 13:06:26.799
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":142,"skipped":2742,"failed":0}
------------------------------
• [SLOW TEST] [8.482 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:06:18.323
    May  9 13:06:18.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:06:18.324
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:06:18.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:06:18.34
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-5324 05/09/23 13:06:18.342
    STEP: creating service affinity-clusterip in namespace services-5324 05/09/23 13:06:18.342
    STEP: creating replication controller affinity-clusterip in namespace services-5324 05/09/23 13:06:18.351
    I0509 13:06:18.358149      24 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5324, replica count: 3
    I0509 13:06:21.409882      24 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 13:06:21.414: INFO: Creating new exec pod
    May  9 13:06:21.419: INFO: Waiting up to 5m0s for pod "execpod-affinity644fz" in namespace "services-5324" to be "running"
    May  9 13:06:21.421: INFO: Pod "execpod-affinity644fz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.904897ms
    May  9 13:06:23.425: INFO: Pod "execpod-affinity644fz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005801098s
    May  9 13:06:23.425: INFO: Pod "execpod-affinity644fz" satisfied condition "running"
    May  9 13:06:24.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-5324 exec execpod-affinity644fz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    May  9 13:06:24.532: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    May  9 13:06:24.532: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:06:24.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-5324 exec execpod-affinity644fz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.21.132 80'
    May  9 13:06:24.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.21.132 80\nConnection to 10.106.21.132 80 port [tcp/http] succeeded!\n"
    May  9 13:06:24.639: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:06:24.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-5324 exec execpod-affinity644fz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.21.132:80/ ; done'
    May  9 13:06:24.801: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.21.132:80/\n"
    May  9 13:06:24.801: INFO: stdout: "\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj\naffinity-clusterip-p6vjj"
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Received response from host: affinity-clusterip-p6vjj
    May  9 13:06:24.801: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-5324, will wait for the garbage collector to delete the pods 05/09/23 13:06:24.811
    May  9 13:06:24.873: INFO: Deleting ReplicationController affinity-clusterip took: 7.361555ms
    May  9 13:06:24.974: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.707811ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:06:26.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5324" for this suite. 05/09/23 13:06:26.799
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:06:26.807
May  9 13:06:26.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:06:26.807
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:06:26.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:06:26.834
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 05/09/23 13:06:26.836
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:06:26.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6905" for this suite. 05/09/23 13:06:26.844
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":143,"skipped":2777,"failed":0}
------------------------------
• [0.043 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:06:26.807
    May  9 13:06:26.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:06:26.807
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:06:26.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:06:26.834
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 05/09/23 13:06:26.836
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:06:26.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6905" for this suite. 05/09/23 13:06:26.844
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:06:26.85
May  9 13:06:26.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-probe 05/09/23 13:06:26.85
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:06:26.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:06:26.863
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 in namespace container-probe-7151 05/09/23 13:06:26.864
May  9 13:06:26.872: INFO: Waiting up to 5m0s for pod "liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9" in namespace "container-probe-7151" to be "not pending"
May  9 13:06:26.876: INFO: Pod "liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.397041ms
May  9 13:06:28.879: INFO: Pod "liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007233624s
May  9 13:06:28.879: INFO: Pod "liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9" satisfied condition "not pending"
May  9 13:06:28.879: INFO: Started pod liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 in namespace container-probe-7151
STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 13:06:28.879
May  9 13:06:28.881: INFO: Initial restart count of pod liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 is 0
May  9 13:06:48.920: INFO: Restart count of pod container-probe-7151/liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 is now 1 (20.038804128s elapsed)
May  9 13:07:08.959: INFO: Restart count of pod container-probe-7151/liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 is now 2 (40.077364833s elapsed)
May  9 13:07:28.994: INFO: Restart count of pod container-probe-7151/liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 is now 3 (1m0.112880379s elapsed)
May  9 13:07:49.031: INFO: Restart count of pod container-probe-7151/liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 is now 4 (1m20.14996665s elapsed)
May  9 13:08:49.142: INFO: Restart count of pod container-probe-7151/liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 is now 5 (2m20.260585388s elapsed)
STEP: deleting the pod 05/09/23 13:08:49.142
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  9 13:08:49.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7151" for this suite. 05/09/23 13:08:49.155
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":144,"skipped":2777,"failed":0}
------------------------------
• [SLOW TEST] [142.311 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:06:26.85
    May  9 13:06:26.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-probe 05/09/23 13:06:26.85
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:06:26.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:06:26.863
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 in namespace container-probe-7151 05/09/23 13:06:26.864
    May  9 13:06:26.872: INFO: Waiting up to 5m0s for pod "liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9" in namespace "container-probe-7151" to be "not pending"
    May  9 13:06:26.876: INFO: Pod "liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.397041ms
    May  9 13:06:28.879: INFO: Pod "liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007233624s
    May  9 13:06:28.879: INFO: Pod "liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9" satisfied condition "not pending"
    May  9 13:06:28.879: INFO: Started pod liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 in namespace container-probe-7151
    STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 13:06:28.879
    May  9 13:06:28.881: INFO: Initial restart count of pod liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 is 0
    May  9 13:06:48.920: INFO: Restart count of pod container-probe-7151/liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 is now 1 (20.038804128s elapsed)
    May  9 13:07:08.959: INFO: Restart count of pod container-probe-7151/liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 is now 2 (40.077364833s elapsed)
    May  9 13:07:28.994: INFO: Restart count of pod container-probe-7151/liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 is now 3 (1m0.112880379s elapsed)
    May  9 13:07:49.031: INFO: Restart count of pod container-probe-7151/liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 is now 4 (1m20.14996665s elapsed)
    May  9 13:08:49.142: INFO: Restart count of pod container-probe-7151/liveness-e5ca068d-58bd-4789-9fa3-00c060aa8fc9 is now 5 (2m20.260585388s elapsed)
    STEP: deleting the pod 05/09/23 13:08:49.142
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  9 13:08:49.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7151" for this suite. 05/09/23 13:08:49.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:08:49.161
May  9 13:08:49.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename podtemplate 05/09/23 13:08:49.162
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:08:49.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:08:49.178
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 05/09/23 13:08:49.179
May  9 13:08:49.186: INFO: created test-podtemplate-1
May  9 13:08:49.191: INFO: created test-podtemplate-2
May  9 13:08:49.196: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 05/09/23 13:08:49.196
STEP: delete collection of pod templates 05/09/23 13:08:49.198
May  9 13:08:49.198: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 05/09/23 13:08:49.21
May  9 13:08:49.210: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
May  9 13:08:49.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4333" for this suite. 05/09/23 13:08:49.214
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":145,"skipped":2797,"failed":0}
------------------------------
• [0.058 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:08:49.161
    May  9 13:08:49.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename podtemplate 05/09/23 13:08:49.162
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:08:49.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:08:49.178
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 05/09/23 13:08:49.179
    May  9 13:08:49.186: INFO: created test-podtemplate-1
    May  9 13:08:49.191: INFO: created test-podtemplate-2
    May  9 13:08:49.196: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 05/09/23 13:08:49.196
    STEP: delete collection of pod templates 05/09/23 13:08:49.198
    May  9 13:08:49.198: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 05/09/23 13:08:49.21
    May  9 13:08:49.210: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    May  9 13:08:49.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-4333" for this suite. 05/09/23 13:08:49.214
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:08:49.219
May  9 13:08:49.219: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename statefulset 05/09/23 13:08:49.22
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:08:49.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:08:49.234
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7060 05/09/23 13:08:49.235
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 05/09/23 13:08:49.239
STEP: Creating pod with conflicting port in namespace statefulset-7060 05/09/23 13:08:49.241
STEP: Waiting until pod test-pod will start running in namespace statefulset-7060 05/09/23 13:08:49.249
May  9 13:08:49.249: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7060" to be "running"
May  9 13:08:49.251: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.742929ms
May  9 13:08:51.254: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00498395s
May  9 13:08:51.254: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-7060 05/09/23 13:08:51.254
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7060 05/09/23 13:08:51.26
May  9 13:08:51.274: INFO: Observed stateful pod in namespace: statefulset-7060, name: ss-0, uid: a071ed8b-2ec0-4523-95a1-35e461e4f042, status phase: Pending. Waiting for statefulset controller to delete.
May  9 13:08:51.286: INFO: Observed stateful pod in namespace: statefulset-7060, name: ss-0, uid: a071ed8b-2ec0-4523-95a1-35e461e4f042, status phase: Failed. Waiting for statefulset controller to delete.
May  9 13:08:51.291: INFO: Observed stateful pod in namespace: statefulset-7060, name: ss-0, uid: a071ed8b-2ec0-4523-95a1-35e461e4f042, status phase: Failed. Waiting for statefulset controller to delete.
May  9 13:08:51.296: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7060
STEP: Removing pod with conflicting port in namespace statefulset-7060 05/09/23 13:08:51.296
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7060 and will be in running state 05/09/23 13:08:51.307
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  9 13:08:53.314: INFO: Deleting all statefulset in ns statefulset-7060
May  9 13:08:53.316: INFO: Scaling statefulset ss to 0
May  9 13:09:03.328: INFO: Waiting for statefulset status.replicas updated to 0
May  9 13:09:03.331: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  9 13:09:03.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7060" for this suite. 05/09/23 13:09:03.344
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":146,"skipped":2799,"failed":0}
------------------------------
• [SLOW TEST] [14.131 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:08:49.219
    May  9 13:08:49.219: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename statefulset 05/09/23 13:08:49.22
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:08:49.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:08:49.234
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7060 05/09/23 13:08:49.235
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 05/09/23 13:08:49.239
    STEP: Creating pod with conflicting port in namespace statefulset-7060 05/09/23 13:08:49.241
    STEP: Waiting until pod test-pod will start running in namespace statefulset-7060 05/09/23 13:08:49.249
    May  9 13:08:49.249: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7060" to be "running"
    May  9 13:08:49.251: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.742929ms
    May  9 13:08:51.254: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00498395s
    May  9 13:08:51.254: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-7060 05/09/23 13:08:51.254
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7060 05/09/23 13:08:51.26
    May  9 13:08:51.274: INFO: Observed stateful pod in namespace: statefulset-7060, name: ss-0, uid: a071ed8b-2ec0-4523-95a1-35e461e4f042, status phase: Pending. Waiting for statefulset controller to delete.
    May  9 13:08:51.286: INFO: Observed stateful pod in namespace: statefulset-7060, name: ss-0, uid: a071ed8b-2ec0-4523-95a1-35e461e4f042, status phase: Failed. Waiting for statefulset controller to delete.
    May  9 13:08:51.291: INFO: Observed stateful pod in namespace: statefulset-7060, name: ss-0, uid: a071ed8b-2ec0-4523-95a1-35e461e4f042, status phase: Failed. Waiting for statefulset controller to delete.
    May  9 13:08:51.296: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7060
    STEP: Removing pod with conflicting port in namespace statefulset-7060 05/09/23 13:08:51.296
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7060 and will be in running state 05/09/23 13:08:51.307
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  9 13:08:53.314: INFO: Deleting all statefulset in ns statefulset-7060
    May  9 13:08:53.316: INFO: Scaling statefulset ss to 0
    May  9 13:09:03.328: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 13:09:03.331: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  9 13:09:03.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7060" for this suite. 05/09/23 13:09:03.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:09:03.351
May  9 13:09:03.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 13:09:03.352
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:03.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:03.366
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 05/09/23 13:09:03.367
May  9 13:09:03.373: INFO: Waiting up to 5m0s for pod "pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e" in namespace "emptydir-6474" to be "Succeeded or Failed"
May  9 13:09:03.377: INFO: Pod "pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.90837ms
May  9 13:09:05.380: INFO: Pod "pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007018306s
May  9 13:09:07.380: INFO: Pod "pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006576576s
STEP: Saw pod success 05/09/23 13:09:07.38
May  9 13:09:07.380: INFO: Pod "pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e" satisfied condition "Succeeded or Failed"
May  9 13:09:07.385: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e container test-container: <nil>
STEP: delete the pod 05/09/23 13:09:07.397
May  9 13:09:07.415: INFO: Waiting for pod pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e to disappear
May  9 13:09:07.417: INFO: Pod pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 13:09:07.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6474" for this suite. 05/09/23 13:09:07.42
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":147,"skipped":2859,"failed":0}
------------------------------
• [4.073 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:09:03.351
    May  9 13:09:03.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 13:09:03.352
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:03.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:03.366
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 05/09/23 13:09:03.367
    May  9 13:09:03.373: INFO: Waiting up to 5m0s for pod "pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e" in namespace "emptydir-6474" to be "Succeeded or Failed"
    May  9 13:09:03.377: INFO: Pod "pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.90837ms
    May  9 13:09:05.380: INFO: Pod "pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007018306s
    May  9 13:09:07.380: INFO: Pod "pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006576576s
    STEP: Saw pod success 05/09/23 13:09:07.38
    May  9 13:09:07.380: INFO: Pod "pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e" satisfied condition "Succeeded or Failed"
    May  9 13:09:07.385: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e container test-container: <nil>
    STEP: delete the pod 05/09/23 13:09:07.397
    May  9 13:09:07.415: INFO: Waiting for pod pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e to disappear
    May  9 13:09:07.417: INFO: Pod pod-e56ea8c7-5d6f-41d0-aa69-fe5bda64c69e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 13:09:07.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6474" for this suite. 05/09/23 13:09:07.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:09:07.424
May  9 13:09:07.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 13:09:07.425
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:07.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:07.439
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  9 13:09:07.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2887" for this suite. 05/09/23 13:09:07.468
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":148,"skipped":2874,"failed":0}
------------------------------
• [0.048 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:09:07.424
    May  9 13:09:07.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 13:09:07.425
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:07.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:07.439
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  9 13:09:07.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2887" for this suite. 05/09/23 13:09:07.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:09:07.474
May  9 13:09:07.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename deployment 05/09/23 13:09:07.474
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:07.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:07.489
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
May  9 13:09:07.497: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May  9 13:09:12.500: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/09/23 13:09:12.5
May  9 13:09:12.500: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/09/23 13:09:12.507
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 13:09:14.523: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4329  9c1f2ede-a473-4cc2-a153-1c77bdf2c149 25190 1 2023-05-09 13:09:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-09 13:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:09:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00380ecd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 13:09:12 +0000 UTC,LastTransitionTime:2023-05-09 13:09:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-05-09 13:09:13 +0000 UTC,LastTransitionTime:2023-05-09 13:09:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  9 13:09:14.525: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-4329  5ac37567-7cfa-4bb9-9778-b8d95a9ab91b 25180 1 2023-05-09 13:09:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 9c1f2ede-a473-4cc2-a153-1c77bdf2c149 0xc0034c2097 0xc0034c2098}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c1f2ede-a473-4cc2-a153-1c77bdf2c149\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:09:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034c2148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  9 13:09:14.527: INFO: Pod "test-cleanup-deployment-69cb9c5497-xffm2" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-xffm2 test-cleanup-deployment-69cb9c5497- deployment-4329  63bb725d-6c3f-4c6e-86a4-7e5f93f582c1 25179 0 2023-05-09 13:09:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:19f0971c6f5d722703e87e945bda364334f6ec7d07a168f1ef24c6e8708e8129 cni.projectcalico.org/podIP:172.25.53.182/32 cni.projectcalico.org/podIPs:172.25.53.182/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 5ac37567-7cfa-4bb9-9778-b8d95a9ab91b 0xc0044e6b07 0xc0044e6b08}] [] [{calico Update v1 2023-05-09 13:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 13:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ac37567-7cfa-4bb9-9778-b8d95a9ab91b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:09:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bwls8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bwls8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:09:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:09:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:09:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:09:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.182,StartTime:2023-05-09 13:09:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:09:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://c834c9099471ced46810de79960aa9c7d4518ed7e8f6d90ab97b2f646825657a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  9 13:09:14.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4329" for this suite. 05/09/23 13:09:14.531
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":149,"skipped":2899,"failed":0}
------------------------------
• [SLOW TEST] [7.062 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:09:07.474
    May  9 13:09:07.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename deployment 05/09/23 13:09:07.474
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:07.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:07.489
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    May  9 13:09:07.497: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    May  9 13:09:12.500: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/09/23 13:09:12.5
    May  9 13:09:12.500: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/09/23 13:09:12.507
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 13:09:14.523: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4329  9c1f2ede-a473-4cc2-a153-1c77bdf2c149 25190 1 2023-05-09 13:09:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-09 13:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:09:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00380ecd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 13:09:12 +0000 UTC,LastTransitionTime:2023-05-09 13:09:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-05-09 13:09:13 +0000 UTC,LastTransitionTime:2023-05-09 13:09:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  9 13:09:14.525: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-4329  5ac37567-7cfa-4bb9-9778-b8d95a9ab91b 25180 1 2023-05-09 13:09:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 9c1f2ede-a473-4cc2-a153-1c77bdf2c149 0xc0034c2097 0xc0034c2098}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c1f2ede-a473-4cc2-a153-1c77bdf2c149\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:09:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034c2148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  9 13:09:14.527: INFO: Pod "test-cleanup-deployment-69cb9c5497-xffm2" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-xffm2 test-cleanup-deployment-69cb9c5497- deployment-4329  63bb725d-6c3f-4c6e-86a4-7e5f93f582c1 25179 0 2023-05-09 13:09:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:19f0971c6f5d722703e87e945bda364334f6ec7d07a168f1ef24c6e8708e8129 cni.projectcalico.org/podIP:172.25.53.182/32 cni.projectcalico.org/podIPs:172.25.53.182/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 5ac37567-7cfa-4bb9-9778-b8d95a9ab91b 0xc0044e6b07 0xc0044e6b08}] [] [{calico Update v1 2023-05-09 13:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 13:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ac37567-7cfa-4bb9-9778-b8d95a9ab91b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:09:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bwls8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bwls8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:09:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:09:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:09:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:09:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.182,StartTime:2023-05-09 13:09:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:09:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://c834c9099471ced46810de79960aa9c7d4518ed7e8f6d90ab97b2f646825657a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  9 13:09:14.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4329" for this suite. 05/09/23 13:09:14.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:09:14.536
May  9 13:09:14.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:09:14.537
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:14.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:14.552
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:09:14.566
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:09:15.263
STEP: Deploying the webhook pod 05/09/23 13:09:15.271
STEP: Wait for the deployment to be ready 05/09/23 13:09:15.283
May  9 13:09:15.287: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/09/23 13:09:17.294
STEP: Verifying the service has paired with the endpoint 05/09/23 13:09:17.312
May  9 13:09:18.312: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 05/09/23 13:09:18.316
STEP: create a pod that should be denied by the webhook 05/09/23 13:09:18.328
STEP: create a pod that causes the webhook to hang 05/09/23 13:09:18.336
STEP: create a configmap that should be denied by the webhook 05/09/23 13:09:28.342
STEP: create a configmap that should be admitted by the webhook 05/09/23 13:09:28.348
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/09/23 13:09:28.356
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/09/23 13:09:28.361
STEP: create a namespace that bypass the webhook 05/09/23 13:09:28.364
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/09/23 13:09:28.369
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:09:28.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9620" for this suite. 05/09/23 13:09:28.393
STEP: Destroying namespace "webhook-9620-markers" for this suite. 05/09/23 13:09:28.398
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":150,"skipped":2915,"failed":0}
------------------------------
• [SLOW TEST] [13.904 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:09:14.536
    May  9 13:09:14.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:09:14.537
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:14.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:14.552
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:09:14.566
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:09:15.263
    STEP: Deploying the webhook pod 05/09/23 13:09:15.271
    STEP: Wait for the deployment to be ready 05/09/23 13:09:15.283
    May  9 13:09:15.287: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/09/23 13:09:17.294
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:09:17.312
    May  9 13:09:18.312: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 05/09/23 13:09:18.316
    STEP: create a pod that should be denied by the webhook 05/09/23 13:09:18.328
    STEP: create a pod that causes the webhook to hang 05/09/23 13:09:18.336
    STEP: create a configmap that should be denied by the webhook 05/09/23 13:09:28.342
    STEP: create a configmap that should be admitted by the webhook 05/09/23 13:09:28.348
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/09/23 13:09:28.356
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/09/23 13:09:28.361
    STEP: create a namespace that bypass the webhook 05/09/23 13:09:28.364
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/09/23 13:09:28.369
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:09:28.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9620" for this suite. 05/09/23 13:09:28.393
    STEP: Destroying namespace "webhook-9620-markers" for this suite. 05/09/23 13:09:28.398
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:09:28.44
May  9 13:09:28.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename events 05/09/23 13:09:28.441
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:28.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:28.456
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 05/09/23 13:09:28.457
STEP: listing all events in all namespaces 05/09/23 13:09:28.465
STEP: patching the test event 05/09/23 13:09:28.468
STEP: fetching the test event 05/09/23 13:09:28.476
STEP: updating the test event 05/09/23 13:09:28.478
STEP: getting the test event 05/09/23 13:09:28.486
STEP: deleting the test event 05/09/23 13:09:28.488
STEP: listing all events in all namespaces 05/09/23 13:09:28.493
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
May  9 13:09:28.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4015" for this suite. 05/09/23 13:09:28.499
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":151,"skipped":2917,"failed":0}
------------------------------
• [0.063 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:09:28.44
    May  9 13:09:28.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename events 05/09/23 13:09:28.441
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:28.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:28.456
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 05/09/23 13:09:28.457
    STEP: listing all events in all namespaces 05/09/23 13:09:28.465
    STEP: patching the test event 05/09/23 13:09:28.468
    STEP: fetching the test event 05/09/23 13:09:28.476
    STEP: updating the test event 05/09/23 13:09:28.478
    STEP: getting the test event 05/09/23 13:09:28.486
    STEP: deleting the test event 05/09/23 13:09:28.488
    STEP: listing all events in all namespaces 05/09/23 13:09:28.493
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    May  9 13:09:28.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4015" for this suite. 05/09/23 13:09:28.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:09:28.504
May  9 13:09:28.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename podtemplate 05/09/23 13:09:28.505
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:28.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:28.522
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
May  9 13:09:28.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-107" for this suite. 05/09/23 13:09:28.547
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":152,"skipped":2939,"failed":0}
------------------------------
• [0.047 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:09:28.504
    May  9 13:09:28.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename podtemplate 05/09/23 13:09:28.505
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:28.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:28.522
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    May  9 13:09:28.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-107" for this suite. 05/09/23 13:09:28.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:09:28.552
May  9 13:09:28.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:09:28.553
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:28.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:28.568
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/09/23 13:09:28.569
May  9 13:09:28.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:09:30.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:09:41.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2807" for this suite. 05/09/23 13:09:41.811
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":153,"skipped":2953,"failed":0}
------------------------------
• [SLOW TEST] [13.264 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:09:28.552
    May  9 13:09:28.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:09:28.553
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:28.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:28.568
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/09/23 13:09:28.569
    May  9 13:09:28.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:09:30.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:09:41.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2807" for this suite. 05/09/23 13:09:41.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:09:41.817
May  9 13:09:41.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-runtime 05/09/23 13:09:41.818
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:41.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:41.83
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 05/09/23 13:09:41.832
STEP: wait for the container to reach Succeeded 05/09/23 13:09:41.84
STEP: get the container status 05/09/23 13:09:45.858
STEP: the container should be terminated 05/09/23 13:09:45.86
STEP: the termination message should be set 05/09/23 13:09:45.86
May  9 13:09:45.860: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 05/09/23 13:09:45.86
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  9 13:09:45.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5938" for this suite. 05/09/23 13:09:45.875
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":154,"skipped":2972,"failed":0}
------------------------------
• [4.064 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:09:41.817
    May  9 13:09:41.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-runtime 05/09/23 13:09:41.818
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:41.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:41.83
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 05/09/23 13:09:41.832
    STEP: wait for the container to reach Succeeded 05/09/23 13:09:41.84
    STEP: get the container status 05/09/23 13:09:45.858
    STEP: the container should be terminated 05/09/23 13:09:45.86
    STEP: the termination message should be set 05/09/23 13:09:45.86
    May  9 13:09:45.860: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 05/09/23 13:09:45.86
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  9 13:09:45.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5938" for this suite. 05/09/23 13:09:45.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:09:45.881
May  9 13:09:45.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename gc 05/09/23 13:09:45.882
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:45.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:45.893
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 05/09/23 13:09:45.895
STEP: Wait for the Deployment to create new ReplicaSet 05/09/23 13:09:45.899
STEP: delete the deployment 05/09/23 13:09:46.008
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/09/23 13:09:46.013
STEP: Gathering metrics 05/09/23 13:09:46.528
May  9 13:09:46.549: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" in namespace "kube-system" to be "running and ready"
May  9 13:09:46.551: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv": Phase="Running", Reason="", readiness=true. Elapsed: 1.925116ms
May  9 13:09:46.551: INFO: The phase of Pod kube-controller-manager-cl-gks-cncf-control-plane-gv9sv is Running (Ready = true)
May  9 13:09:46.551: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" satisfied condition "running and ready"
May  9 13:09:46.589: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  9 13:09:46.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6273" for this suite. 05/09/23 13:09:46.592
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":155,"skipped":2990,"failed":0}
------------------------------
• [0.716 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:09:45.881
    May  9 13:09:45.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename gc 05/09/23 13:09:45.882
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:45.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:45.893
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 05/09/23 13:09:45.895
    STEP: Wait for the Deployment to create new ReplicaSet 05/09/23 13:09:45.899
    STEP: delete the deployment 05/09/23 13:09:46.008
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/09/23 13:09:46.013
    STEP: Gathering metrics 05/09/23 13:09:46.528
    May  9 13:09:46.549: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" in namespace "kube-system" to be "running and ready"
    May  9 13:09:46.551: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv": Phase="Running", Reason="", readiness=true. Elapsed: 1.925116ms
    May  9 13:09:46.551: INFO: The phase of Pod kube-controller-manager-cl-gks-cncf-control-plane-gv9sv is Running (Ready = true)
    May  9 13:09:46.551: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" satisfied condition "running and ready"
    May  9 13:09:46.589: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  9 13:09:46.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6273" for this suite. 05/09/23 13:09:46.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:09:46.597
May  9 13:09:46.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename svcaccounts 05/09/23 13:09:46.598
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:46.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:46.612
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
May  9 13:09:46.623: INFO: Waiting up to 5m0s for pod "pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a" in namespace "svcaccounts-2242" to be "running"
May  9 13:09:46.625: INFO: Pod "pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.71245ms
May  9 13:09:48.628: INFO: Pod "pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004588286s
May  9 13:09:48.628: INFO: Pod "pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a" satisfied condition "running"
STEP: reading a file in the container 05/09/23 13:09:48.628
May  9 13:09:48.628: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2242 pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 05/09/23 13:09:48.725
May  9 13:09:48.725: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2242 pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 05/09/23 13:09:48.821
May  9 13:09:48.821: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2242 pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
May  9 13:09:48.916: INFO: Got root ca configmap in namespace "svcaccounts-2242"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  9 13:09:48.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2242" for this suite. 05/09/23 13:09:48.921
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":156,"skipped":2996,"failed":0}
------------------------------
• [2.329 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:09:46.597
    May  9 13:09:46.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 13:09:46.598
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:46.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:46.612
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    May  9 13:09:46.623: INFO: Waiting up to 5m0s for pod "pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a" in namespace "svcaccounts-2242" to be "running"
    May  9 13:09:46.625: INFO: Pod "pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.71245ms
    May  9 13:09:48.628: INFO: Pod "pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004588286s
    May  9 13:09:48.628: INFO: Pod "pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a" satisfied condition "running"
    STEP: reading a file in the container 05/09/23 13:09:48.628
    May  9 13:09:48.628: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2242 pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 05/09/23 13:09:48.725
    May  9 13:09:48.725: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2242 pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 05/09/23 13:09:48.821
    May  9 13:09:48.821: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2242 pod-service-account-d0394c98-3261-4ec3-adcd-38f7bf87030a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    May  9 13:09:48.916: INFO: Got root ca configmap in namespace "svcaccounts-2242"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  9 13:09:48.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2242" for this suite. 05/09/23 13:09:48.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:09:48.927
May  9 13:09:48.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename gc 05/09/23 13:09:48.928
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:48.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:48.943
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 05/09/23 13:09:48.947
STEP: create the rc2 05/09/23 13:09:48.953
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/09/23 13:09:53.962
STEP: delete the rc simpletest-rc-to-be-deleted 05/09/23 13:09:54.311
STEP: wait for the rc to be deleted 05/09/23 13:09:54.316
May  9 13:09:59.326: INFO: 72 pods remaining
May  9 13:09:59.326: INFO: 72 pods has nil DeletionTimestamp
May  9 13:09:59.326: INFO: 
STEP: Gathering metrics 05/09/23 13:10:04.327
May  9 13:10:04.341: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" in namespace "kube-system" to be "running and ready"
May  9 13:10:04.344: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv": Phase="Running", Reason="", readiness=true. Elapsed: 2.340527ms
May  9 13:10:04.344: INFO: The phase of Pod kube-controller-manager-cl-gks-cncf-control-plane-gv9sv is Running (Ready = true)
May  9 13:10:04.344: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" satisfied condition "running and ready"
May  9 13:10:04.381: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May  9 13:10:04.381: INFO: Deleting pod "simpletest-rc-to-be-deleted-25ns4" in namespace "gc-590"
May  9 13:10:04.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-2l7tv" in namespace "gc-590"
May  9 13:10:04.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-2pj2r" in namespace "gc-590"
May  9 13:10:04.415: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ddbt" in namespace "gc-590"
May  9 13:10:04.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kpcf" in namespace "gc-590"
May  9 13:10:04.433: INFO: Deleting pod "simpletest-rc-to-be-deleted-4m826" in namespace "gc-590"
May  9 13:10:04.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-4n4fn" in namespace "gc-590"
May  9 13:10:04.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rfh2" in namespace "gc-590"
May  9 13:10:04.469: INFO: Deleting pod "simpletest-rc-to-be-deleted-557td" in namespace "gc-590"
May  9 13:10:04.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mtkb" in namespace "gc-590"
May  9 13:10:04.493: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xnh8" in namespace "gc-590"
May  9 13:10:04.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zkxm" in namespace "gc-590"
May  9 13:10:04.513: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bfcn" in namespace "gc-590"
May  9 13:10:04.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dgj4" in namespace "gc-590"
May  9 13:10:04.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-6v6tp" in namespace "gc-590"
May  9 13:10:04.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vpv8" in namespace "gc-590"
May  9 13:10:04.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hj55" in namespace "gc-590"
May  9 13:10:04.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vlwb" in namespace "gc-590"
May  9 13:10:04.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-96zgh" in namespace "gc-590"
May  9 13:10:04.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-97zfc" in namespace "gc-590"
May  9 13:10:04.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nl26" in namespace "gc-590"
May  9 13:10:04.597: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sfmp" in namespace "gc-590"
May  9 13:10:04.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xh8x" in namespace "gc-590"
May  9 13:10:04.615: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xxnp" in namespace "gc-590"
May  9 13:10:04.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-b62q7" in namespace "gc-590"
May  9 13:10:04.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8qkb" in namespace "gc-590"
May  9 13:10:04.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-b94r7" in namespace "gc-590"
May  9 13:10:04.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-bs827" in namespace "gc-590"
May  9 13:10:04.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7sp9" in namespace "gc-590"
May  9 13:10:04.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb79q" in namespace "gc-590"
May  9 13:10:04.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbplk" in namespace "gc-590"
May  9 13:10:04.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdcqm" in namespace "gc-590"
May  9 13:10:04.691: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5fd5" in namespace "gc-590"
May  9 13:10:04.699: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6rk9" in namespace "gc-590"
May  9 13:10:04.707: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcbg9" in namespace "gc-590"
May  9 13:10:04.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcckx" in namespace "gc-590"
May  9 13:10:04.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcqkl" in namespace "gc-590"
May  9 13:10:04.734: INFO: Deleting pod "simpletest-rc-to-be-deleted-dm6sr" in namespace "gc-590"
May  9 13:10:04.745: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnczb" in namespace "gc-590"
May  9 13:10:04.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-dr5pm" in namespace "gc-590"
May  9 13:10:04.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7njx" in namespace "gc-590"
May  9 13:10:04.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8bwr" in namespace "gc-590"
May  9 13:10:04.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl8qg" in namespace "gc-590"
May  9 13:10:04.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-flrd4" in namespace "gc-590"
May  9 13:10:04.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsvbs" in namespace "gc-590"
May  9 13:10:04.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8bn8" in namespace "gc-590"
May  9 13:10:04.813: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkq7n" in namespace "gc-590"
May  9 13:10:04.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkvwd" in namespace "gc-590"
May  9 13:10:04.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxwmx" in namespace "gc-590"
May  9 13:10:04.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-h62ff" in namespace "gc-590"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  9 13:10:04.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-590" for this suite. 05/09/23 13:10:04.852
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":157,"skipped":3049,"failed":0}
------------------------------
• [SLOW TEST] [15.930 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:09:48.927
    May  9 13:09:48.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename gc 05/09/23 13:09:48.928
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:09:48.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:09:48.943
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 05/09/23 13:09:48.947
    STEP: create the rc2 05/09/23 13:09:48.953
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/09/23 13:09:53.962
    STEP: delete the rc simpletest-rc-to-be-deleted 05/09/23 13:09:54.311
    STEP: wait for the rc to be deleted 05/09/23 13:09:54.316
    May  9 13:09:59.326: INFO: 72 pods remaining
    May  9 13:09:59.326: INFO: 72 pods has nil DeletionTimestamp
    May  9 13:09:59.326: INFO: 
    STEP: Gathering metrics 05/09/23 13:10:04.327
    May  9 13:10:04.341: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" in namespace "kube-system" to be "running and ready"
    May  9 13:10:04.344: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv": Phase="Running", Reason="", readiness=true. Elapsed: 2.340527ms
    May  9 13:10:04.344: INFO: The phase of Pod kube-controller-manager-cl-gks-cncf-control-plane-gv9sv is Running (Ready = true)
    May  9 13:10:04.344: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" satisfied condition "running and ready"
    May  9 13:10:04.381: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May  9 13:10:04.381: INFO: Deleting pod "simpletest-rc-to-be-deleted-25ns4" in namespace "gc-590"
    May  9 13:10:04.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-2l7tv" in namespace "gc-590"
    May  9 13:10:04.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-2pj2r" in namespace "gc-590"
    May  9 13:10:04.415: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ddbt" in namespace "gc-590"
    May  9 13:10:04.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kpcf" in namespace "gc-590"
    May  9 13:10:04.433: INFO: Deleting pod "simpletest-rc-to-be-deleted-4m826" in namespace "gc-590"
    May  9 13:10:04.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-4n4fn" in namespace "gc-590"
    May  9 13:10:04.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rfh2" in namespace "gc-590"
    May  9 13:10:04.469: INFO: Deleting pod "simpletest-rc-to-be-deleted-557td" in namespace "gc-590"
    May  9 13:10:04.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mtkb" in namespace "gc-590"
    May  9 13:10:04.493: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xnh8" in namespace "gc-590"
    May  9 13:10:04.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zkxm" in namespace "gc-590"
    May  9 13:10:04.513: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bfcn" in namespace "gc-590"
    May  9 13:10:04.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dgj4" in namespace "gc-590"
    May  9 13:10:04.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-6v6tp" in namespace "gc-590"
    May  9 13:10:04.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vpv8" in namespace "gc-590"
    May  9 13:10:04.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hj55" in namespace "gc-590"
    May  9 13:10:04.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vlwb" in namespace "gc-590"
    May  9 13:10:04.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-96zgh" in namespace "gc-590"
    May  9 13:10:04.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-97zfc" in namespace "gc-590"
    May  9 13:10:04.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nl26" in namespace "gc-590"
    May  9 13:10:04.597: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sfmp" in namespace "gc-590"
    May  9 13:10:04.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xh8x" in namespace "gc-590"
    May  9 13:10:04.615: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xxnp" in namespace "gc-590"
    May  9 13:10:04.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-b62q7" in namespace "gc-590"
    May  9 13:10:04.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8qkb" in namespace "gc-590"
    May  9 13:10:04.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-b94r7" in namespace "gc-590"
    May  9 13:10:04.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-bs827" in namespace "gc-590"
    May  9 13:10:04.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7sp9" in namespace "gc-590"
    May  9 13:10:04.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb79q" in namespace "gc-590"
    May  9 13:10:04.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbplk" in namespace "gc-590"
    May  9 13:10:04.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdcqm" in namespace "gc-590"
    May  9 13:10:04.691: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5fd5" in namespace "gc-590"
    May  9 13:10:04.699: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6rk9" in namespace "gc-590"
    May  9 13:10:04.707: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcbg9" in namespace "gc-590"
    May  9 13:10:04.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcckx" in namespace "gc-590"
    May  9 13:10:04.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcqkl" in namespace "gc-590"
    May  9 13:10:04.734: INFO: Deleting pod "simpletest-rc-to-be-deleted-dm6sr" in namespace "gc-590"
    May  9 13:10:04.745: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnczb" in namespace "gc-590"
    May  9 13:10:04.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-dr5pm" in namespace "gc-590"
    May  9 13:10:04.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7njx" in namespace "gc-590"
    May  9 13:10:04.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8bwr" in namespace "gc-590"
    May  9 13:10:04.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl8qg" in namespace "gc-590"
    May  9 13:10:04.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-flrd4" in namespace "gc-590"
    May  9 13:10:04.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsvbs" in namespace "gc-590"
    May  9 13:10:04.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8bn8" in namespace "gc-590"
    May  9 13:10:04.813: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkq7n" in namespace "gc-590"
    May  9 13:10:04.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkvwd" in namespace "gc-590"
    May  9 13:10:04.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxwmx" in namespace "gc-590"
    May  9 13:10:04.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-h62ff" in namespace "gc-590"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  9 13:10:04.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-590" for this suite. 05/09/23 13:10:04.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:10:04.858
May  9 13:10:04.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename namespaces 05/09/23 13:10:04.858
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:10:04.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:10:04.871
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 05/09/23 13:10:04.872
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:10:04.882
STEP: Creating a pod in the namespace 05/09/23 13:10:04.884
STEP: Waiting for the pod to have running status 05/09/23 13:10:04.89
May  9 13:10:04.890: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3735" to be "running"
May  9 13:10:04.892: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.763588ms
May  9 13:10:06.895: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005121538s
May  9 13:10:08.894: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004325728s
May  9 13:10:08.894: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 05/09/23 13:10:08.894
STEP: Waiting for the namespace to be removed. 05/09/23 13:10:08.901
STEP: Recreating the namespace 05/09/23 13:10:19.904
STEP: Verifying there are no pods in the namespace 05/09/23 13:10:19.917
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May  9 13:10:19.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-494" for this suite. 05/09/23 13:10:19.923
STEP: Destroying namespace "nsdeletetest-3735" for this suite. 05/09/23 13:10:19.927
May  9 13:10:19.929: INFO: Namespace nsdeletetest-3735 was already deleted
STEP: Destroying namespace "nsdeletetest-6653" for this suite. 05/09/23 13:10:19.929
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":158,"skipped":3085,"failed":0}
------------------------------
• [SLOW TEST] [15.076 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:10:04.858
    May  9 13:10:04.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename namespaces 05/09/23 13:10:04.858
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:10:04.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:10:04.871
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 05/09/23 13:10:04.872
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:10:04.882
    STEP: Creating a pod in the namespace 05/09/23 13:10:04.884
    STEP: Waiting for the pod to have running status 05/09/23 13:10:04.89
    May  9 13:10:04.890: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3735" to be "running"
    May  9 13:10:04.892: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.763588ms
    May  9 13:10:06.895: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005121538s
    May  9 13:10:08.894: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004325728s
    May  9 13:10:08.894: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 05/09/23 13:10:08.894
    STEP: Waiting for the namespace to be removed. 05/09/23 13:10:08.901
    STEP: Recreating the namespace 05/09/23 13:10:19.904
    STEP: Verifying there are no pods in the namespace 05/09/23 13:10:19.917
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:10:19.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-494" for this suite. 05/09/23 13:10:19.923
    STEP: Destroying namespace "nsdeletetest-3735" for this suite. 05/09/23 13:10:19.927
    May  9 13:10:19.929: INFO: Namespace nsdeletetest-3735 was already deleted
    STEP: Destroying namespace "nsdeletetest-6653" for this suite. 05/09/23 13:10:19.929
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:10:19.934
May  9 13:10:19.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 13:10:19.934
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:10:19.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:10:19.948
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/09/23 13:10:19.952
May  9 13:10:19.964: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3826" to be "running and ready"
May  9 13:10:19.974: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.781492ms
May  9 13:10:19.974: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  9 13:10:21.977: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013002867s
May  9 13:10:21.977: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  9 13:10:21.977: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 05/09/23 13:10:21.979
May  9 13:10:21.983: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-3826" to be "running and ready"
May  9 13:10:21.989: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.208531ms
May  9 13:10:21.989: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  9 13:10:23.993: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01011962s
May  9 13:10:23.993: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
May  9 13:10:23.993: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/09/23 13:10:23.996
STEP: delete the pod with lifecycle hook 05/09/23 13:10:24.001
May  9 13:10:24.007: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  9 13:10:24.010: INFO: Pod pod-with-poststart-http-hook still exists
May  9 13:10:26.011: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  9 13:10:26.018: INFO: Pod pod-with-poststart-http-hook still exists
May  9 13:10:28.010: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  9 13:10:28.014: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May  9 13:10:28.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3826" for this suite. 05/09/23 13:10:28.018
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":159,"skipped":3086,"failed":0}
------------------------------
• [SLOW TEST] [8.090 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:10:19.934
    May  9 13:10:19.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 13:10:19.934
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:10:19.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:10:19.948
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/09/23 13:10:19.952
    May  9 13:10:19.964: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3826" to be "running and ready"
    May  9 13:10:19.974: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.781492ms
    May  9 13:10:19.974: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:10:21.977: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013002867s
    May  9 13:10:21.977: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  9 13:10:21.977: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 05/09/23 13:10:21.979
    May  9 13:10:21.983: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-3826" to be "running and ready"
    May  9 13:10:21.989: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.208531ms
    May  9 13:10:21.989: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:10:23.993: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01011962s
    May  9 13:10:23.993: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    May  9 13:10:23.993: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/09/23 13:10:23.996
    STEP: delete the pod with lifecycle hook 05/09/23 13:10:24.001
    May  9 13:10:24.007: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  9 13:10:24.010: INFO: Pod pod-with-poststart-http-hook still exists
    May  9 13:10:26.011: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  9 13:10:26.018: INFO: Pod pod-with-poststart-http-hook still exists
    May  9 13:10:28.010: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  9 13:10:28.014: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May  9 13:10:28.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-3826" for this suite. 05/09/23 13:10:28.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:10:28.024
May  9 13:10:28.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename deployment 05/09/23 13:10:28.025
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:10:28.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:10:28.043
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
May  9 13:10:28.045: INFO: Creating simple deployment test-new-deployment
May  9 13:10:28.055: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 05/09/23 13:10:30.063
STEP: updating a scale subresource 05/09/23 13:10:30.065
STEP: verifying the deployment Spec.Replicas was modified 05/09/23 13:10:30.07
STEP: Patch a scale subresource 05/09/23 13:10:30.072
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 13:10:30.108: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-4929  fea2cf54-dd34-4b09-933c-93af907a48bb 27862 3 2023-05-09 13:10:28 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-09 13:10:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a4fb58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-05-09 13:10:29 +0000 UTC,LastTransitionTime:2023-05-09 13:10:28 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-09 13:10:30 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  9 13:10:30.118: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-4929  6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38 27864 3 2023-05-09 13:10:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment fea2cf54-dd34-4b09-933c-93af907a48bb 0xc004a4ff97 0xc004a4ff98}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fea2cf54-dd34-4b09-933c-93af907a48bb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a44028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  9 13:10:30.127: INFO: Pod "test-new-deployment-845c8977d9-289gh" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-289gh test-new-deployment-845c8977d9- deployment-4929  11da951a-0661-44c2-8800-77c8b11046b7 27863 0 2023-05-09 13:10:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38 0xc0034aafd7 0xc0034aafd8}] [] [{kube-controller-manager Update v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlxf5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlxf5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:,StartTime:2023-05-09 13:10:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:10:30.127: INFO: Pod "test-new-deployment-845c8977d9-ckmjg" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-ckmjg test-new-deployment-845c8977d9- deployment-4929  577e3fc9-62d7-40f5-9e97-4a9771bc050d 27871 0 2023-05-09 13:10:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38 0xc0034ab197 0xc0034ab198}] [] [{kube-controller-manager Update v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rr9k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rr9k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:10:30.127: INFO: Pod "test-new-deployment-845c8977d9-rh47m" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-rh47m test-new-deployment-845c8977d9- deployment-4929  695f1979-08c2-4c4d-a1f9-e4d4ef042cdc 27845 0 2023-05-09 13:10:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:87d533b5283c0ee9d65a1e90ae326c22c56f0fc897f3fbc741c7892f6acd1f3b cni.projectcalico.org/podIP:172.25.124.224/32 cni.projectcalico.org/podIPs:172.25.124.224/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38 0xc0034ab2f0 0xc0034ab2f1}] [] [{calico Update v1 2023-05-09 13:10:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 13:10:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:10:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w9swb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w9swb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.224,StartTime:2023-05-09 13:10:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:10:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f2e6c77a7b47c375b110db2ef3354f27e5734ca719ab7c967f1c14462f84aa2f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:10:30.127: INFO: Pod "test-new-deployment-845c8977d9-zrrqj" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-zrrqj test-new-deployment-845c8977d9- deployment-4929  e78c05de-68b0-467f-aac3-f583e5a16707 27870 0 2023-05-09 13:10:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38 0xc0034ab5c7 0xc0034ab5c8}] [] [{kube-controller-manager Update v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mk4xn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mk4xn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-skqqr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  9 13:10:30.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4929" for this suite. 05/09/23 13:10:30.136
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":160,"skipped":3096,"failed":0}
------------------------------
• [2.126 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:10:28.024
    May  9 13:10:28.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename deployment 05/09/23 13:10:28.025
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:10:28.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:10:28.043
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    May  9 13:10:28.045: INFO: Creating simple deployment test-new-deployment
    May  9 13:10:28.055: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 05/09/23 13:10:30.063
    STEP: updating a scale subresource 05/09/23 13:10:30.065
    STEP: verifying the deployment Spec.Replicas was modified 05/09/23 13:10:30.07
    STEP: Patch a scale subresource 05/09/23 13:10:30.072
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 13:10:30.108: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-4929  fea2cf54-dd34-4b09-933c-93af907a48bb 27862 3 2023-05-09 13:10:28 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-09 13:10:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a4fb58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-05-09 13:10:29 +0000 UTC,LastTransitionTime:2023-05-09 13:10:28 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-09 13:10:30 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  9 13:10:30.118: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-4929  6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38 27864 3 2023-05-09 13:10:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment fea2cf54-dd34-4b09-933c-93af907a48bb 0xc004a4ff97 0xc004a4ff98}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fea2cf54-dd34-4b09-933c-93af907a48bb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a44028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  9 13:10:30.127: INFO: Pod "test-new-deployment-845c8977d9-289gh" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-289gh test-new-deployment-845c8977d9- deployment-4929  11da951a-0661-44c2-8800-77c8b11046b7 27863 0 2023-05-09 13:10:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38 0xc0034aafd7 0xc0034aafd8}] [] [{kube-controller-manager Update v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlxf5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlxf5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:,StartTime:2023-05-09 13:10:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:10:30.127: INFO: Pod "test-new-deployment-845c8977d9-ckmjg" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-ckmjg test-new-deployment-845c8977d9- deployment-4929  577e3fc9-62d7-40f5-9e97-4a9771bc050d 27871 0 2023-05-09 13:10:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38 0xc0034ab197 0xc0034ab198}] [] [{kube-controller-manager Update v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rr9k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rr9k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:10:30.127: INFO: Pod "test-new-deployment-845c8977d9-rh47m" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-rh47m test-new-deployment-845c8977d9- deployment-4929  695f1979-08c2-4c4d-a1f9-e4d4ef042cdc 27845 0 2023-05-09 13:10:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:87d533b5283c0ee9d65a1e90ae326c22c56f0fc897f3fbc741c7892f6acd1f3b cni.projectcalico.org/podIP:172.25.124.224/32 cni.projectcalico.org/podIPs:172.25.124.224/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38 0xc0034ab2f0 0xc0034ab2f1}] [] [{calico Update v1 2023-05-09 13:10:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 13:10:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:10:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w9swb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w9swb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.224,StartTime:2023-05-09 13:10:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:10:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f2e6c77a7b47c375b110db2ef3354f27e5734ca719ab7c967f1c14462f84aa2f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:10:30.127: INFO: Pod "test-new-deployment-845c8977d9-zrrqj" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-zrrqj test-new-deployment-845c8977d9- deployment-4929  e78c05de-68b0-467f-aac3-f583e5a16707 27870 0 2023-05-09 13:10:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38 0xc0034ab5c7 0xc0034ab5c8}] [] [{kube-controller-manager Update v1 2023-05-09 13:10:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6dbe8adc-1a71-4ffd-8b4f-d15b3d2bba38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mk4xn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mk4xn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-skqqr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:10:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  9 13:10:30.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4929" for this suite. 05/09/23 13:10:30.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:10:30.151
May  9 13:10:30.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:10:30.151
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:10:30.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:10:30.164
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:10:30.166
May  9 13:10:30.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758" in namespace "projected-6803" to be "Succeeded or Failed"
May  9 13:10:30.181: INFO: Pod "downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758": Phase="Pending", Reason="", readiness=false. Elapsed: 8.216984ms
May  9 13:10:32.184: INFO: Pod "downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01162318s
May  9 13:10:34.185: INFO: Pod "downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012508146s
STEP: Saw pod success 05/09/23 13:10:34.185
May  9 13:10:34.185: INFO: Pod "downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758" satisfied condition "Succeeded or Failed"
May  9 13:10:34.187: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758 container client-container: <nil>
STEP: delete the pod 05/09/23 13:10:34.2
May  9 13:10:34.214: INFO: Waiting for pod downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758 to disappear
May  9 13:10:34.216: INFO: Pod downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  9 13:10:34.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6803" for this suite. 05/09/23 13:10:34.22
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":161,"skipped":3127,"failed":0}
------------------------------
• [4.074 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:10:30.151
    May  9 13:10:30.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:10:30.151
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:10:30.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:10:30.164
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:10:30.166
    May  9 13:10:30.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758" in namespace "projected-6803" to be "Succeeded or Failed"
    May  9 13:10:30.181: INFO: Pod "downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758": Phase="Pending", Reason="", readiness=false. Elapsed: 8.216984ms
    May  9 13:10:32.184: INFO: Pod "downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01162318s
    May  9 13:10:34.185: INFO: Pod "downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012508146s
    STEP: Saw pod success 05/09/23 13:10:34.185
    May  9 13:10:34.185: INFO: Pod "downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758" satisfied condition "Succeeded or Failed"
    May  9 13:10:34.187: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758 container client-container: <nil>
    STEP: delete the pod 05/09/23 13:10:34.2
    May  9 13:10:34.214: INFO: Waiting for pod downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758 to disappear
    May  9 13:10:34.216: INFO: Pod downwardapi-volume-d9c235d2-9d2e-42c8-8cb3-bf17cb02e758 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  9 13:10:34.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6803" for this suite. 05/09/23 13:10:34.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:10:34.227
May  9 13:10:34.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-probe 05/09/23 13:10:34.227
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:10:34.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:10:34.243
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9 in namespace container-probe-8386 05/09/23 13:10:34.245
May  9 13:10:34.257: INFO: Waiting up to 5m0s for pod "busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9" in namespace "container-probe-8386" to be "not pending"
May  9 13:10:34.260: INFO: Pod "busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.079093ms
May  9 13:10:36.265: INFO: Pod "busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007910628s
May  9 13:10:36.265: INFO: Pod "busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9" satisfied condition "not pending"
May  9 13:10:36.265: INFO: Started pod busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9 in namespace container-probe-8386
STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 13:10:36.265
May  9 13:10:36.267: INFO: Initial restart count of pod busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9 is 0
May  9 13:11:26.354: INFO: Restart count of pod container-probe-8386/busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9 is now 1 (50.087070242s elapsed)
STEP: deleting the pod 05/09/23 13:11:26.354
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  9 13:11:26.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8386" for this suite. 05/09/23 13:11:26.374
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":162,"skipped":3158,"failed":0}
------------------------------
• [SLOW TEST] [52.154 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:10:34.227
    May  9 13:10:34.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-probe 05/09/23 13:10:34.227
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:10:34.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:10:34.243
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9 in namespace container-probe-8386 05/09/23 13:10:34.245
    May  9 13:10:34.257: INFO: Waiting up to 5m0s for pod "busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9" in namespace "container-probe-8386" to be "not pending"
    May  9 13:10:34.260: INFO: Pod "busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.079093ms
    May  9 13:10:36.265: INFO: Pod "busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007910628s
    May  9 13:10:36.265: INFO: Pod "busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9" satisfied condition "not pending"
    May  9 13:10:36.265: INFO: Started pod busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9 in namespace container-probe-8386
    STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 13:10:36.265
    May  9 13:10:36.267: INFO: Initial restart count of pod busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9 is 0
    May  9 13:11:26.354: INFO: Restart count of pod container-probe-8386/busybox-feb92170-b39c-42ca-bde0-177afcd7a9e9 is now 1 (50.087070242s elapsed)
    STEP: deleting the pod 05/09/23 13:11:26.354
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  9 13:11:26.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8386" for this suite. 05/09/23 13:11:26.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:11:26.381
May  9 13:11:26.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 13:11:26.382
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:11:26.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:11:26.395
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 05/09/23 13:11:26.396
May  9 13:11:26.405: INFO: Waiting up to 5m0s for pod "downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37" in namespace "downward-api-4493" to be "Succeeded or Failed"
May  9 13:11:26.408: INFO: Pod "downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.950768ms
May  9 13:11:28.412: INFO: Pod "downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006522764s
May  9 13:11:30.411: INFO: Pod "downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005915503s
STEP: Saw pod success 05/09/23 13:11:30.411
May  9 13:11:30.411: INFO: Pod "downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37" satisfied condition "Succeeded or Failed"
May  9 13:11:30.413: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37 container dapi-container: <nil>
STEP: delete the pod 05/09/23 13:11:30.419
May  9 13:11:30.427: INFO: Waiting for pod downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37 to disappear
May  9 13:11:30.429: INFO: Pod downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  9 13:11:30.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4493" for this suite. 05/09/23 13:11:30.432
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":163,"skipped":3181,"failed":0}
------------------------------
• [4.055 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:11:26.381
    May  9 13:11:26.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 13:11:26.382
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:11:26.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:11:26.395
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 05/09/23 13:11:26.396
    May  9 13:11:26.405: INFO: Waiting up to 5m0s for pod "downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37" in namespace "downward-api-4493" to be "Succeeded or Failed"
    May  9 13:11:26.408: INFO: Pod "downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.950768ms
    May  9 13:11:28.412: INFO: Pod "downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006522764s
    May  9 13:11:30.411: INFO: Pod "downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005915503s
    STEP: Saw pod success 05/09/23 13:11:30.411
    May  9 13:11:30.411: INFO: Pod "downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37" satisfied condition "Succeeded or Failed"
    May  9 13:11:30.413: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37 container dapi-container: <nil>
    STEP: delete the pod 05/09/23 13:11:30.419
    May  9 13:11:30.427: INFO: Waiting for pod downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37 to disappear
    May  9 13:11:30.429: INFO: Pod downward-api-961f9d4d-56b1-4fb5-b469-2ea88895bc37 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  9 13:11:30.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4493" for this suite. 05/09/23 13:11:30.432
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:11:30.436
May  9 13:11:30.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:11:30.437
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:11:30.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:11:30.453
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:11:30.454
May  9 13:11:30.460: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf" in namespace "projected-521" to be "Succeeded or Failed"
May  9 13:11:30.463: INFO: Pod "downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.578434ms
May  9 13:11:32.466: INFO: Pod "downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf": Phase="Running", Reason="", readiness=false. Elapsed: 2.00655469s
May  9 13:11:34.466: INFO: Pod "downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006434843s
STEP: Saw pod success 05/09/23 13:11:34.466
May  9 13:11:34.466: INFO: Pod "downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf" satisfied condition "Succeeded or Failed"
May  9 13:11:34.468: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf container client-container: <nil>
STEP: delete the pod 05/09/23 13:11:34.472
May  9 13:11:34.481: INFO: Waiting for pod downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf to disappear
May  9 13:11:34.483: INFO: Pod downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  9 13:11:34.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-521" for this suite. 05/09/23 13:11:34.486
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":164,"skipped":3182,"failed":0}
------------------------------
• [4.054 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:11:30.436
    May  9 13:11:30.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:11:30.437
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:11:30.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:11:30.453
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:11:30.454
    May  9 13:11:30.460: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf" in namespace "projected-521" to be "Succeeded or Failed"
    May  9 13:11:30.463: INFO: Pod "downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.578434ms
    May  9 13:11:32.466: INFO: Pod "downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf": Phase="Running", Reason="", readiness=false. Elapsed: 2.00655469s
    May  9 13:11:34.466: INFO: Pod "downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006434843s
    STEP: Saw pod success 05/09/23 13:11:34.466
    May  9 13:11:34.466: INFO: Pod "downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf" satisfied condition "Succeeded or Failed"
    May  9 13:11:34.468: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf container client-container: <nil>
    STEP: delete the pod 05/09/23 13:11:34.472
    May  9 13:11:34.481: INFO: Waiting for pod downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf to disappear
    May  9 13:11:34.483: INFO: Pod downwardapi-volume-6f041b48-c8e6-4faa-aa97-9c8a9a7eb1bf no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  9 13:11:34.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-521" for this suite. 05/09/23 13:11:34.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:11:34.492
May  9 13:11:34.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 13:11:34.492
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:11:34.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:11:34.509
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 05/09/23 13:11:34.51
May  9 13:11:34.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 create -f -'
May  9 13:11:34.998: INFO: stderr: ""
May  9 13:11:34.998: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 13:11:34.998
May  9 13:11:34.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 13:11:35.048: INFO: stderr: ""
May  9 13:11:35.048: INFO: stdout: "update-demo-nautilus-cxjtr update-demo-nautilus-nkdr2 "
May  9 13:11:35.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods update-demo-nautilus-cxjtr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 13:11:35.101: INFO: stderr: ""
May  9 13:11:35.101: INFO: stdout: ""
May  9 13:11:35.101: INFO: update-demo-nautilus-cxjtr is created but not running
May  9 13:11:40.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 13:11:40.151: INFO: stderr: ""
May  9 13:11:40.151: INFO: stdout: "update-demo-nautilus-cxjtr update-demo-nautilus-nkdr2 "
May  9 13:11:40.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods update-demo-nautilus-cxjtr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 13:11:40.201: INFO: stderr: ""
May  9 13:11:40.201: INFO: stdout: "true"
May  9 13:11:40.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods update-demo-nautilus-cxjtr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 13:11:40.254: INFO: stderr: ""
May  9 13:11:40.254: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  9 13:11:40.254: INFO: validating pod update-demo-nautilus-cxjtr
May  9 13:11:40.257: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 13:11:40.257: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 13:11:40.257: INFO: update-demo-nautilus-cxjtr is verified up and running
May  9 13:11:40.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods update-demo-nautilus-nkdr2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 13:11:40.307: INFO: stderr: ""
May  9 13:11:40.307: INFO: stdout: "true"
May  9 13:11:40.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods update-demo-nautilus-nkdr2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 13:11:40.359: INFO: stderr: ""
May  9 13:11:40.359: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  9 13:11:40.359: INFO: validating pod update-demo-nautilus-nkdr2
May  9 13:11:40.362: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 13:11:40.362: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 13:11:40.362: INFO: update-demo-nautilus-nkdr2 is verified up and running
STEP: using delete to clean up resources 05/09/23 13:11:40.362
May  9 13:11:40.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 delete --grace-period=0 --force -f -'
May  9 13:11:40.419: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 13:11:40.419: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  9 13:11:40.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get rc,svc -l name=update-demo --no-headers'
May  9 13:11:40.480: INFO: stderr: "No resources found in kubectl-8222 namespace.\n"
May  9 13:11:40.480: INFO: stdout: ""
May  9 13:11:40.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  9 13:11:40.533: INFO: stderr: ""
May  9 13:11:40.533: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 13:11:40.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8222" for this suite. 05/09/23 13:11:40.537
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":165,"skipped":3212,"failed":0}
------------------------------
• [SLOW TEST] [6.050 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:11:34.492
    May  9 13:11:34.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 13:11:34.492
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:11:34.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:11:34.509
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 05/09/23 13:11:34.51
    May  9 13:11:34.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 create -f -'
    May  9 13:11:34.998: INFO: stderr: ""
    May  9 13:11:34.998: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 13:11:34.998
    May  9 13:11:34.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 13:11:35.048: INFO: stderr: ""
    May  9 13:11:35.048: INFO: stdout: "update-demo-nautilus-cxjtr update-demo-nautilus-nkdr2 "
    May  9 13:11:35.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods update-demo-nautilus-cxjtr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 13:11:35.101: INFO: stderr: ""
    May  9 13:11:35.101: INFO: stdout: ""
    May  9 13:11:35.101: INFO: update-demo-nautilus-cxjtr is created but not running
    May  9 13:11:40.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 13:11:40.151: INFO: stderr: ""
    May  9 13:11:40.151: INFO: stdout: "update-demo-nautilus-cxjtr update-demo-nautilus-nkdr2 "
    May  9 13:11:40.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods update-demo-nautilus-cxjtr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 13:11:40.201: INFO: stderr: ""
    May  9 13:11:40.201: INFO: stdout: "true"
    May  9 13:11:40.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods update-demo-nautilus-cxjtr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 13:11:40.254: INFO: stderr: ""
    May  9 13:11:40.254: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  9 13:11:40.254: INFO: validating pod update-demo-nautilus-cxjtr
    May  9 13:11:40.257: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 13:11:40.257: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 13:11:40.257: INFO: update-demo-nautilus-cxjtr is verified up and running
    May  9 13:11:40.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods update-demo-nautilus-nkdr2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 13:11:40.307: INFO: stderr: ""
    May  9 13:11:40.307: INFO: stdout: "true"
    May  9 13:11:40.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods update-demo-nautilus-nkdr2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 13:11:40.359: INFO: stderr: ""
    May  9 13:11:40.359: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  9 13:11:40.359: INFO: validating pod update-demo-nautilus-nkdr2
    May  9 13:11:40.362: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 13:11:40.362: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 13:11:40.362: INFO: update-demo-nautilus-nkdr2 is verified up and running
    STEP: using delete to clean up resources 05/09/23 13:11:40.362
    May  9 13:11:40.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 delete --grace-period=0 --force -f -'
    May  9 13:11:40.419: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 13:11:40.419: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May  9 13:11:40.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get rc,svc -l name=update-demo --no-headers'
    May  9 13:11:40.480: INFO: stderr: "No resources found in kubectl-8222 namespace.\n"
    May  9 13:11:40.480: INFO: stdout: ""
    May  9 13:11:40.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-8222 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  9 13:11:40.533: INFO: stderr: ""
    May  9 13:11:40.533: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 13:11:40.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8222" for this suite. 05/09/23 13:11:40.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:11:40.542
May  9 13:11:40.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename watch 05/09/23 13:11:40.543
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:11:40.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:11:40.557
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 05/09/23 13:11:40.558
STEP: creating a new configmap 05/09/23 13:11:40.559
STEP: modifying the configmap once 05/09/23 13:11:40.562
STEP: closing the watch once it receives two notifications 05/09/23 13:11:40.568
May  9 13:11:40.568: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7220  f0e3c175-84a0-485b-a8ce-2c417e4a1823 28349 0 2023-05-09 13:11:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 13:11:40.568: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7220  f0e3c175-84a0-485b-a8ce-2c417e4a1823 28350 0 2023-05-09 13:11:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 13:11:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 05/09/23 13:11:40.568
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/09/23 13:11:40.574
STEP: deleting the configmap 05/09/23 13:11:40.575
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/09/23 13:11:40.579
May  9 13:11:40.579: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7220  f0e3c175-84a0-485b-a8ce-2c417e4a1823 28351 0 2023-05-09 13:11:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 13:11:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 13:11:40.579: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7220  f0e3c175-84a0-485b-a8ce-2c417e4a1823 28352 0 2023-05-09 13:11:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 13:11:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  9 13:11:40.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7220" for this suite. 05/09/23 13:11:40.582
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":166,"skipped":3220,"failed":0}
------------------------------
• [0.044 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:11:40.542
    May  9 13:11:40.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename watch 05/09/23 13:11:40.543
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:11:40.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:11:40.557
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 05/09/23 13:11:40.558
    STEP: creating a new configmap 05/09/23 13:11:40.559
    STEP: modifying the configmap once 05/09/23 13:11:40.562
    STEP: closing the watch once it receives two notifications 05/09/23 13:11:40.568
    May  9 13:11:40.568: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7220  f0e3c175-84a0-485b-a8ce-2c417e4a1823 28349 0 2023-05-09 13:11:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 13:11:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 13:11:40.568: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7220  f0e3c175-84a0-485b-a8ce-2c417e4a1823 28350 0 2023-05-09 13:11:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 13:11:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 05/09/23 13:11:40.568
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/09/23 13:11:40.574
    STEP: deleting the configmap 05/09/23 13:11:40.575
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/09/23 13:11:40.579
    May  9 13:11:40.579: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7220  f0e3c175-84a0-485b-a8ce-2c417e4a1823 28351 0 2023-05-09 13:11:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 13:11:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 13:11:40.579: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7220  f0e3c175-84a0-485b-a8ce-2c417e4a1823 28352 0 2023-05-09 13:11:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 13:11:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  9 13:11:40.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7220" for this suite. 05/09/23 13:11:40.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:11:40.587
May  9 13:11:40.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename statefulset 05/09/23 13:11:40.588
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:11:40.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:11:40.601
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9911 05/09/23 13:11:40.603
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-9911 05/09/23 13:11:40.606
May  9 13:11:40.618: INFO: Found 0 stateful pods, waiting for 1
May  9 13:11:50.622: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 05/09/23 13:11:50.626
STEP: updating a scale subresource 05/09/23 13:11:50.628
STEP: verifying the statefulset Spec.Replicas was modified 05/09/23 13:11:50.632
STEP: Patch a scale subresource 05/09/23 13:11:50.634
STEP: verifying the statefulset Spec.Replicas was modified 05/09/23 13:11:50.642
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  9 13:11:50.652: INFO: Deleting all statefulset in ns statefulset-9911
May  9 13:11:50.654: INFO: Scaling statefulset ss to 0
May  9 13:12:00.670: INFO: Waiting for statefulset status.replicas updated to 0
May  9 13:12:00.672: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  9 13:12:00.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9911" for this suite. 05/09/23 13:12:00.686
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":167,"skipped":3245,"failed":0}
------------------------------
• [SLOW TEST] [20.104 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:11:40.587
    May  9 13:11:40.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename statefulset 05/09/23 13:11:40.588
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:11:40.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:11:40.601
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9911 05/09/23 13:11:40.603
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-9911 05/09/23 13:11:40.606
    May  9 13:11:40.618: INFO: Found 0 stateful pods, waiting for 1
    May  9 13:11:50.622: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 05/09/23 13:11:50.626
    STEP: updating a scale subresource 05/09/23 13:11:50.628
    STEP: verifying the statefulset Spec.Replicas was modified 05/09/23 13:11:50.632
    STEP: Patch a scale subresource 05/09/23 13:11:50.634
    STEP: verifying the statefulset Spec.Replicas was modified 05/09/23 13:11:50.642
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  9 13:11:50.652: INFO: Deleting all statefulset in ns statefulset-9911
    May  9 13:11:50.654: INFO: Scaling statefulset ss to 0
    May  9 13:12:00.670: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 13:12:00.672: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  9 13:12:00.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9911" for this suite. 05/09/23 13:12:00.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:00.692
May  9 13:12:00.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:12:00.693
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:00.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:00.705
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-e7cdea10-67a2-454e-b934-0d7e66db2772 05/09/23 13:12:00.706
STEP: Creating a pod to test consume secrets 05/09/23 13:12:00.71
May  9 13:12:00.717: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff" in namespace "projected-6715" to be "Succeeded or Failed"
May  9 13:12:00.720: INFO: Pod "pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.474474ms
May  9 13:12:02.723: INFO: Pod "pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006055051s
May  9 13:12:04.724: INFO: Pod "pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007387294s
STEP: Saw pod success 05/09/23 13:12:04.724
May  9 13:12:04.724: INFO: Pod "pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff" satisfied condition "Succeeded or Failed"
May  9 13:12:04.726: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff container projected-secret-volume-test: <nil>
STEP: delete the pod 05/09/23 13:12:04.731
May  9 13:12:04.740: INFO: Waiting for pod pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff to disappear
May  9 13:12:04.742: INFO: Pod pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  9 13:12:04.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6715" for this suite. 05/09/23 13:12:04.745
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":168,"skipped":3260,"failed":0}
------------------------------
• [4.059 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:00.692
    May  9 13:12:00.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:12:00.693
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:00.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:00.705
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-e7cdea10-67a2-454e-b934-0d7e66db2772 05/09/23 13:12:00.706
    STEP: Creating a pod to test consume secrets 05/09/23 13:12:00.71
    May  9 13:12:00.717: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff" in namespace "projected-6715" to be "Succeeded or Failed"
    May  9 13:12:00.720: INFO: Pod "pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.474474ms
    May  9 13:12:02.723: INFO: Pod "pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006055051s
    May  9 13:12:04.724: INFO: Pod "pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007387294s
    STEP: Saw pod success 05/09/23 13:12:04.724
    May  9 13:12:04.724: INFO: Pod "pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff" satisfied condition "Succeeded or Failed"
    May  9 13:12:04.726: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:12:04.731
    May  9 13:12:04.740: INFO: Waiting for pod pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff to disappear
    May  9 13:12:04.742: INFO: Pod pod-projected-secrets-fa2b0cc6-83c5-4a72-a7cd-20b4b24535ff no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  9 13:12:04.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6715" for this suite. 05/09/23 13:12:04.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:04.752
May  9 13:12:04.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename podtemplate 05/09/23 13:12:04.753
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:04.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:04.765
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 05/09/23 13:12:04.766
STEP: Replace a pod template 05/09/23 13:12:04.771
May  9 13:12:04.776: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
May  9 13:12:04.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8971" for this suite. 05/09/23 13:12:04.779
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":169,"skipped":3288,"failed":0}
------------------------------
• [0.032 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:04.752
    May  9 13:12:04.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename podtemplate 05/09/23 13:12:04.753
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:04.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:04.765
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 05/09/23 13:12:04.766
    STEP: Replace a pod template 05/09/23 13:12:04.771
    May  9 13:12:04.776: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    May  9 13:12:04.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-8971" for this suite. 05/09/23 13:12:04.779
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:04.784
May  9 13:12:04.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename dns 05/09/23 13:12:04.784
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:04.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:04.798
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2755.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2755.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 05/09/23 13:12:04.8
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2755.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2755.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 05/09/23 13:12:04.8
STEP: creating a pod to probe /etc/hosts 05/09/23 13:12:04.8
STEP: submitting the pod to kubernetes 05/09/23 13:12:04.8
May  9 13:12:04.806: INFO: Waiting up to 15m0s for pod "dns-test-cdfea113-09d6-4029-be5b-5a19b5794cd4" in namespace "dns-2755" to be "running"
May  9 13:12:04.810: INFO: Pod "dns-test-cdfea113-09d6-4029-be5b-5a19b5794cd4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.723228ms
May  9 13:12:06.813: INFO: Pod "dns-test-cdfea113-09d6-4029-be5b-5a19b5794cd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006909816s
May  9 13:12:06.813: INFO: Pod "dns-test-cdfea113-09d6-4029-be5b-5a19b5794cd4" satisfied condition "running"
STEP: retrieving the pod 05/09/23 13:12:06.813
STEP: looking for the results for each expected name from probers 05/09/23 13:12:06.816
May  9 13:12:06.825: INFO: DNS probes using dns-2755/dns-test-cdfea113-09d6-4029-be5b-5a19b5794cd4 succeeded

STEP: deleting the pod 05/09/23 13:12:06.825
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  9 13:12:06.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2755" for this suite. 05/09/23 13:12:06.837
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":170,"skipped":3290,"failed":0}
------------------------------
• [2.057 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:04.784
    May  9 13:12:04.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename dns 05/09/23 13:12:04.784
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:04.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:04.798
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2755.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2755.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     05/09/23 13:12:04.8
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2755.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2755.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     05/09/23 13:12:04.8
    STEP: creating a pod to probe /etc/hosts 05/09/23 13:12:04.8
    STEP: submitting the pod to kubernetes 05/09/23 13:12:04.8
    May  9 13:12:04.806: INFO: Waiting up to 15m0s for pod "dns-test-cdfea113-09d6-4029-be5b-5a19b5794cd4" in namespace "dns-2755" to be "running"
    May  9 13:12:04.810: INFO: Pod "dns-test-cdfea113-09d6-4029-be5b-5a19b5794cd4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.723228ms
    May  9 13:12:06.813: INFO: Pod "dns-test-cdfea113-09d6-4029-be5b-5a19b5794cd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006909816s
    May  9 13:12:06.813: INFO: Pod "dns-test-cdfea113-09d6-4029-be5b-5a19b5794cd4" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 13:12:06.813
    STEP: looking for the results for each expected name from probers 05/09/23 13:12:06.816
    May  9 13:12:06.825: INFO: DNS probes using dns-2755/dns-test-cdfea113-09d6-4029-be5b-5a19b5794cd4 succeeded

    STEP: deleting the pod 05/09/23 13:12:06.825
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  9 13:12:06.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2755" for this suite. 05/09/23 13:12:06.837
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:06.842
May  9 13:12:06.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:12:06.843
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:06.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:06.863
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
May  9 13:12:06.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/09/23 13:12:09.3
May  9 13:12:09.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-5482 --namespace=crd-publish-openapi-5482 create -f -'
May  9 13:12:09.817: INFO: stderr: ""
May  9 13:12:09.817: INFO: stdout: "e2e-test-crd-publish-openapi-9995-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  9 13:12:09.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-5482 --namespace=crd-publish-openapi-5482 delete e2e-test-crd-publish-openapi-9995-crds test-cr'
May  9 13:12:09.872: INFO: stderr: ""
May  9 13:12:09.872: INFO: stdout: "e2e-test-crd-publish-openapi-9995-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May  9 13:12:09.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-5482 --namespace=crd-publish-openapi-5482 apply -f -'
May  9 13:12:10.006: INFO: stderr: ""
May  9 13:12:10.006: INFO: stdout: "e2e-test-crd-publish-openapi-9995-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  9 13:12:10.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-5482 --namespace=crd-publish-openapi-5482 delete e2e-test-crd-publish-openapi-9995-crds test-cr'
May  9 13:12:10.080: INFO: stderr: ""
May  9 13:12:10.080: INFO: stdout: "e2e-test-crd-publish-openapi-9995-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/09/23 13:12:10.08
May  9 13:12:10.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-5482 explain e2e-test-crd-publish-openapi-9995-crds'
May  9 13:12:10.199: INFO: stderr: ""
May  9 13:12:10.199: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9995-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:12:12.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5482" for this suite. 05/09/23 13:12:12.631
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":171,"skipped":3293,"failed":0}
------------------------------
• [SLOW TEST] [5.796 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:06.842
    May  9 13:12:06.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:12:06.843
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:06.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:06.863
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    May  9 13:12:06.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/09/23 13:12:09.3
    May  9 13:12:09.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-5482 --namespace=crd-publish-openapi-5482 create -f -'
    May  9 13:12:09.817: INFO: stderr: ""
    May  9 13:12:09.817: INFO: stdout: "e2e-test-crd-publish-openapi-9995-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May  9 13:12:09.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-5482 --namespace=crd-publish-openapi-5482 delete e2e-test-crd-publish-openapi-9995-crds test-cr'
    May  9 13:12:09.872: INFO: stderr: ""
    May  9 13:12:09.872: INFO: stdout: "e2e-test-crd-publish-openapi-9995-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    May  9 13:12:09.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-5482 --namespace=crd-publish-openapi-5482 apply -f -'
    May  9 13:12:10.006: INFO: stderr: ""
    May  9 13:12:10.006: INFO: stdout: "e2e-test-crd-publish-openapi-9995-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May  9 13:12:10.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-5482 --namespace=crd-publish-openapi-5482 delete e2e-test-crd-publish-openapi-9995-crds test-cr'
    May  9 13:12:10.080: INFO: stderr: ""
    May  9 13:12:10.080: INFO: stdout: "e2e-test-crd-publish-openapi-9995-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/09/23 13:12:10.08
    May  9 13:12:10.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-5482 explain e2e-test-crd-publish-openapi-9995-crds'
    May  9 13:12:10.199: INFO: stderr: ""
    May  9 13:12:10.199: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9995-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:12:12.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5482" for this suite. 05/09/23 13:12:12.631
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:12.638
May  9 13:12:12.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-runtime 05/09/23 13:12:12.639
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:12.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:12.654
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 05/09/23 13:12:12.656
STEP: wait for the container to reach Succeeded 05/09/23 13:12:12.662
STEP: get the container status 05/09/23 13:12:16.678
STEP: the container should be terminated 05/09/23 13:12:16.681
STEP: the termination message should be set 05/09/23 13:12:16.681
May  9 13:12:16.681: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/09/23 13:12:16.681
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  9 13:12:16.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8503" for this suite. 05/09/23 13:12:16.699
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":172,"skipped":3294,"failed":0}
------------------------------
• [4.066 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:12.638
    May  9 13:12:12.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-runtime 05/09/23 13:12:12.639
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:12.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:12.654
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 05/09/23 13:12:12.656
    STEP: wait for the container to reach Succeeded 05/09/23 13:12:12.662
    STEP: get the container status 05/09/23 13:12:16.678
    STEP: the container should be terminated 05/09/23 13:12:16.681
    STEP: the termination message should be set 05/09/23 13:12:16.681
    May  9 13:12:16.681: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/09/23 13:12:16.681
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  9 13:12:16.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8503" for this suite. 05/09/23 13:12:16.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:16.706
May  9 13:12:16.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:12:16.706
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:16.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:16.722
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:12:16.724
May  9 13:12:16.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19" in namespace "projected-42" to be "Succeeded or Failed"
May  9 13:12:16.736: INFO: Pod "downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19": Phase="Pending", Reason="", readiness=false. Elapsed: 5.782199ms
May  9 13:12:18.741: INFO: Pod "downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011310608s
May  9 13:12:20.741: INFO: Pod "downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010600984s
STEP: Saw pod success 05/09/23 13:12:20.741
May  9 13:12:20.741: INFO: Pod "downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19" satisfied condition "Succeeded or Failed"
May  9 13:12:20.743: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19 container client-container: <nil>
STEP: delete the pod 05/09/23 13:12:20.754
May  9 13:12:20.764: INFO: Waiting for pod downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19 to disappear
May  9 13:12:20.766: INFO: Pod downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  9 13:12:20.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-42" for this suite. 05/09/23 13:12:20.77
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":173,"skipped":3313,"failed":0}
------------------------------
• [4.069 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:16.706
    May  9 13:12:16.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:12:16.706
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:16.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:16.722
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:12:16.724
    May  9 13:12:16.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19" in namespace "projected-42" to be "Succeeded or Failed"
    May  9 13:12:16.736: INFO: Pod "downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19": Phase="Pending", Reason="", readiness=false. Elapsed: 5.782199ms
    May  9 13:12:18.741: INFO: Pod "downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011310608s
    May  9 13:12:20.741: INFO: Pod "downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010600984s
    STEP: Saw pod success 05/09/23 13:12:20.741
    May  9 13:12:20.741: INFO: Pod "downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19" satisfied condition "Succeeded or Failed"
    May  9 13:12:20.743: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19 container client-container: <nil>
    STEP: delete the pod 05/09/23 13:12:20.754
    May  9 13:12:20.764: INFO: Waiting for pod downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19 to disappear
    May  9 13:12:20.766: INFO: Pod downwardapi-volume-9ccc8675-3487-41fd-a63d-83ecf5f0da19 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  9 13:12:20.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-42" for this suite. 05/09/23 13:12:20.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:20.776
May  9 13:12:20.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 13:12:20.777
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:20.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:20.791
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/09/23 13:12:20.793
May  9 13:12:20.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-7803 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
May  9 13:12:20.849: INFO: stderr: ""
May  9 13:12:20.849: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 05/09/23 13:12:20.849
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
May  9 13:12:20.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-7803 delete pods e2e-test-httpd-pod'
May  9 13:12:23.352: INFO: stderr: ""
May  9 13:12:23.352: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 13:12:23.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7803" for this suite. 05/09/23 13:12:23.356
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":174,"skipped":3357,"failed":0}
------------------------------
• [2.588 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:20.776
    May  9 13:12:20.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 13:12:20.777
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:20.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:20.791
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/09/23 13:12:20.793
    May  9 13:12:20.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-7803 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    May  9 13:12:20.849: INFO: stderr: ""
    May  9 13:12:20.849: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 05/09/23 13:12:20.849
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    May  9 13:12:20.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-7803 delete pods e2e-test-httpd-pod'
    May  9 13:12:23.352: INFO: stderr: ""
    May  9 13:12:23.352: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 13:12:23.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7803" for this suite. 05/09/23 13:12:23.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:23.365
May  9 13:12:23.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 13:12:23.366
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:23.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:23.381
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-3508c671-bd0a-4abf-89c9-aeb8a8084251 05/09/23 13:12:23.383
STEP: Creating a pod to test consume secrets 05/09/23 13:12:23.386
May  9 13:12:23.394: INFO: Waiting up to 5m0s for pod "pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3" in namespace "secrets-6163" to be "Succeeded or Failed"
May  9 13:12:23.398: INFO: Pod "pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025483ms
May  9 13:12:25.402: INFO: Pod "pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008377744s
May  9 13:12:27.401: INFO: Pod "pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007743193s
STEP: Saw pod success 05/09/23 13:12:27.401
May  9 13:12:27.402: INFO: Pod "pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3" satisfied condition "Succeeded or Failed"
May  9 13:12:27.407: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3 container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 13:12:27.412
May  9 13:12:27.424: INFO: Waiting for pod pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3 to disappear
May  9 13:12:27.427: INFO: Pod pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  9 13:12:27.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6163" for this suite. 05/09/23 13:12:27.432
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":175,"skipped":3383,"failed":0}
------------------------------
• [4.072 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:23.365
    May  9 13:12:23.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 13:12:23.366
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:23.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:23.381
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-3508c671-bd0a-4abf-89c9-aeb8a8084251 05/09/23 13:12:23.383
    STEP: Creating a pod to test consume secrets 05/09/23 13:12:23.386
    May  9 13:12:23.394: INFO: Waiting up to 5m0s for pod "pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3" in namespace "secrets-6163" to be "Succeeded or Failed"
    May  9 13:12:23.398: INFO: Pod "pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025483ms
    May  9 13:12:25.402: INFO: Pod "pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008377744s
    May  9 13:12:27.401: INFO: Pod "pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007743193s
    STEP: Saw pod success 05/09/23 13:12:27.401
    May  9 13:12:27.402: INFO: Pod "pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3" satisfied condition "Succeeded or Failed"
    May  9 13:12:27.407: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3 container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:12:27.412
    May  9 13:12:27.424: INFO: Waiting for pod pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3 to disappear
    May  9 13:12:27.427: INFO: Pod pod-secrets-b62329b6-3b23-4164-b894-9cabf4bc2cd3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  9 13:12:27.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6163" for this suite. 05/09/23 13:12:27.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:27.439
May  9 13:12:27.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename prestop 05/09/23 13:12:27.439
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:27.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:27.454
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9237 05/09/23 13:12:27.456
STEP: Waiting for pods to come up. 05/09/23 13:12:27.463
May  9 13:12:27.463: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9237" to be "running"
May  9 13:12:27.466: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.074173ms
May  9 13:12:29.469: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.006405255s
May  9 13:12:29.469: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9237 05/09/23 13:12:29.472
May  9 13:12:29.476: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9237" to be "running"
May  9 13:12:29.481: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.686031ms
May  9 13:12:31.485: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.008807601s
May  9 13:12:31.485: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 05/09/23 13:12:31.485
May  9 13:12:36.496: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 05/09/23 13:12:36.496
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
May  9 13:12:36.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9237" for this suite. 05/09/23 13:12:36.511
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":176,"skipped":3427,"failed":0}
------------------------------
• [SLOW TEST] [9.077 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:27.439
    May  9 13:12:27.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename prestop 05/09/23 13:12:27.439
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:27.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:27.454
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9237 05/09/23 13:12:27.456
    STEP: Waiting for pods to come up. 05/09/23 13:12:27.463
    May  9 13:12:27.463: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9237" to be "running"
    May  9 13:12:27.466: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.074173ms
    May  9 13:12:29.469: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.006405255s
    May  9 13:12:29.469: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9237 05/09/23 13:12:29.472
    May  9 13:12:29.476: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9237" to be "running"
    May  9 13:12:29.481: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.686031ms
    May  9 13:12:31.485: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.008807601s
    May  9 13:12:31.485: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 05/09/23 13:12:31.485
    May  9 13:12:36.496: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 05/09/23 13:12:36.496
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    May  9 13:12:36.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-9237" for this suite. 05/09/23 13:12:36.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:36.517
May  9 13:12:36.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:12:36.518
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:36.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:36.534
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-d266c539-b703-4561-9dba-01f7a31ca9d1 05/09/23 13:12:36.536
STEP: Creating a pod to test consume configMaps 05/09/23 13:12:36.54
May  9 13:12:36.548: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0" in namespace "projected-9899" to be "Succeeded or Failed"
May  9 13:12:36.551: INFO: Pod "pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.481088ms
May  9 13:12:38.555: INFO: Pod "pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007044973s
May  9 13:12:40.554: INFO: Pod "pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006493448s
STEP: Saw pod success 05/09/23 13:12:40.554
May  9 13:12:40.554: INFO: Pod "pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0" satisfied condition "Succeeded or Failed"
May  9 13:12:40.557: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 13:12:40.562
May  9 13:12:40.573: INFO: Waiting for pod pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0 to disappear
May  9 13:12:40.575: INFO: Pod pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  9 13:12:40.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9899" for this suite. 05/09/23 13:12:40.58
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":177,"skipped":3446,"failed":0}
------------------------------
• [4.069 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:36.517
    May  9 13:12:36.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:12:36.518
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:36.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:36.534
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-d266c539-b703-4561-9dba-01f7a31ca9d1 05/09/23 13:12:36.536
    STEP: Creating a pod to test consume configMaps 05/09/23 13:12:36.54
    May  9 13:12:36.548: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0" in namespace "projected-9899" to be "Succeeded or Failed"
    May  9 13:12:36.551: INFO: Pod "pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.481088ms
    May  9 13:12:38.555: INFO: Pod "pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007044973s
    May  9 13:12:40.554: INFO: Pod "pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006493448s
    STEP: Saw pod success 05/09/23 13:12:40.554
    May  9 13:12:40.554: INFO: Pod "pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0" satisfied condition "Succeeded or Failed"
    May  9 13:12:40.557: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 13:12:40.562
    May  9 13:12:40.573: INFO: Waiting for pod pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0 to disappear
    May  9 13:12:40.575: INFO: Pod pod-projected-configmaps-97b91477-5208-4124-8dc5-79bc269dfda0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  9 13:12:40.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9899" for this suite. 05/09/23 13:12:40.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:40.587
May  9 13:12:40.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:12:40.588
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:40.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:40.603
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:12:40.616
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:12:40.736
STEP: Deploying the webhook pod 05/09/23 13:12:40.743
STEP: Wait for the deployment to be ready 05/09/23 13:12:40.755
May  9 13:12:40.764: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 13:12:42.776
STEP: Verifying the service has paired with the endpoint 05/09/23 13:12:42.791
May  9 13:12:43.791: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 05/09/23 13:12:43.849
STEP: Creating a configMap that should be mutated 05/09/23 13:12:43.861
STEP: Deleting the collection of validation webhooks 05/09/23 13:12:43.884
STEP: Creating a configMap that should not be mutated 05/09/23 13:12:43.93
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:12:43.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1370" for this suite. 05/09/23 13:12:43.944
STEP: Destroying namespace "webhook-1370-markers" for this suite. 05/09/23 13:12:43.951
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":178,"skipped":3456,"failed":0}
------------------------------
• [3.406 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:40.587
    May  9 13:12:40.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:12:40.588
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:40.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:40.603
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:12:40.616
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:12:40.736
    STEP: Deploying the webhook pod 05/09/23 13:12:40.743
    STEP: Wait for the deployment to be ready 05/09/23 13:12:40.755
    May  9 13:12:40.764: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 13:12:42.776
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:12:42.791
    May  9 13:12:43.791: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 05/09/23 13:12:43.849
    STEP: Creating a configMap that should be mutated 05/09/23 13:12:43.861
    STEP: Deleting the collection of validation webhooks 05/09/23 13:12:43.884
    STEP: Creating a configMap that should not be mutated 05/09/23 13:12:43.93
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:12:43.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1370" for this suite. 05/09/23 13:12:43.944
    STEP: Destroying namespace "webhook-1370-markers" for this suite. 05/09/23 13:12:43.951
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:43.994
May  9 13:12:43.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 13:12:43.995
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:44.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:44.012
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-b953f1df-43e0-46ec-8d50-b0826dc09324 05/09/23 13:12:44.014
STEP: Creating a pod to test consume configMaps 05/09/23 13:12:44.018
May  9 13:12:44.025: INFO: Waiting up to 5m0s for pod "pod-configmaps-db090379-e03b-415c-af1f-253871633916" in namespace "configmap-2413" to be "Succeeded or Failed"
May  9 13:12:44.028: INFO: Pod "pod-configmaps-db090379-e03b-415c-af1f-253871633916": Phase="Pending", Reason="", readiness=false. Elapsed: 3.340159ms
May  9 13:12:46.033: INFO: Pod "pod-configmaps-db090379-e03b-415c-af1f-253871633916": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007858208s
May  9 13:12:48.032: INFO: Pod "pod-configmaps-db090379-e03b-415c-af1f-253871633916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007182051s
STEP: Saw pod success 05/09/23 13:12:48.032
May  9 13:12:48.032: INFO: Pod "pod-configmaps-db090379-e03b-415c-af1f-253871633916" satisfied condition "Succeeded or Failed"
May  9 13:12:48.035: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-db090379-e03b-415c-af1f-253871633916 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 13:12:48.041
May  9 13:12:48.052: INFO: Waiting for pod pod-configmaps-db090379-e03b-415c-af1f-253871633916 to disappear
May  9 13:12:48.054: INFO: Pod pod-configmaps-db090379-e03b-415c-af1f-253871633916 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  9 13:12:48.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2413" for this suite. 05/09/23 13:12:48.058
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":179,"skipped":3468,"failed":0}
------------------------------
• [4.071 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:43.994
    May  9 13:12:43.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 13:12:43.995
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:44.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:44.012
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-b953f1df-43e0-46ec-8d50-b0826dc09324 05/09/23 13:12:44.014
    STEP: Creating a pod to test consume configMaps 05/09/23 13:12:44.018
    May  9 13:12:44.025: INFO: Waiting up to 5m0s for pod "pod-configmaps-db090379-e03b-415c-af1f-253871633916" in namespace "configmap-2413" to be "Succeeded or Failed"
    May  9 13:12:44.028: INFO: Pod "pod-configmaps-db090379-e03b-415c-af1f-253871633916": Phase="Pending", Reason="", readiness=false. Elapsed: 3.340159ms
    May  9 13:12:46.033: INFO: Pod "pod-configmaps-db090379-e03b-415c-af1f-253871633916": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007858208s
    May  9 13:12:48.032: INFO: Pod "pod-configmaps-db090379-e03b-415c-af1f-253871633916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007182051s
    STEP: Saw pod success 05/09/23 13:12:48.032
    May  9 13:12:48.032: INFO: Pod "pod-configmaps-db090379-e03b-415c-af1f-253871633916" satisfied condition "Succeeded or Failed"
    May  9 13:12:48.035: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-db090379-e03b-415c-af1f-253871633916 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 13:12:48.041
    May  9 13:12:48.052: INFO: Waiting for pod pod-configmaps-db090379-e03b-415c-af1f-253871633916 to disappear
    May  9 13:12:48.054: INFO: Pod pod-configmaps-db090379-e03b-415c-af1f-253871633916 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 13:12:48.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2413" for this suite. 05/09/23 13:12:48.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:48.066
May  9 13:12:48.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename resourcequota 05/09/23 13:12:48.066
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:48.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:48.083
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 05/09/23 13:12:48.085
STEP: Creating a ResourceQuota 05/09/23 13:12:53.089
STEP: Ensuring resource quota status is calculated 05/09/23 13:12:53.095
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  9 13:12:55.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-686" for this suite. 05/09/23 13:12:55.103
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":180,"skipped":3517,"failed":0}
------------------------------
• [SLOW TEST] [7.043 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:48.066
    May  9 13:12:48.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename resourcequota 05/09/23 13:12:48.066
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:48.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:48.083
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 05/09/23 13:12:48.085
    STEP: Creating a ResourceQuota 05/09/23 13:12:53.089
    STEP: Ensuring resource quota status is calculated 05/09/23 13:12:53.095
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  9 13:12:55.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-686" for this suite. 05/09/23 13:12:55.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:12:55.109
May  9 13:12:55.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename subpath 05/09/23 13:12:55.11
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:55.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:55.125
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/09/23 13:12:55.127
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-h225 05/09/23 13:12:55.134
STEP: Creating a pod to test atomic-volume-subpath 05/09/23 13:12:55.134
May  9 13:12:55.141: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-h225" in namespace "subpath-5666" to be "Succeeded or Failed"
May  9 13:12:55.144: INFO: Pod "pod-subpath-test-projected-h225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.827763ms
May  9 13:12:57.148: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 2.006282124s
May  9 13:12:59.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 4.005848875s
May  9 13:13:01.149: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 6.007867185s
May  9 13:13:03.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 8.006064286s
May  9 13:13:05.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 10.006137857s
May  9 13:13:07.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 12.006110628s
May  9 13:13:09.148: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 14.006430461s
May  9 13:13:11.149: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 16.007705285s
May  9 13:13:13.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 18.005564839s
May  9 13:13:15.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 20.006134346s
May  9 13:13:17.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=false. Elapsed: 22.006196696s
May  9 13:13:19.148: INFO: Pod "pod-subpath-test-projected-h225": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007121941s
STEP: Saw pod success 05/09/23 13:13:19.148
May  9 13:13:19.149: INFO: Pod "pod-subpath-test-projected-h225" satisfied condition "Succeeded or Failed"
May  9 13:13:19.151: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-subpath-test-projected-h225 container test-container-subpath-projected-h225: <nil>
STEP: delete the pod 05/09/23 13:13:19.157
May  9 13:13:19.166: INFO: Waiting for pod pod-subpath-test-projected-h225 to disappear
May  9 13:13:19.169: INFO: Pod pod-subpath-test-projected-h225 no longer exists
STEP: Deleting pod pod-subpath-test-projected-h225 05/09/23 13:13:19.169
May  9 13:13:19.169: INFO: Deleting pod "pod-subpath-test-projected-h225" in namespace "subpath-5666"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  9 13:13:19.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5666" for this suite. 05/09/23 13:13:19.175
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":181,"skipped":3527,"failed":0}
------------------------------
• [SLOW TEST] [24.072 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:12:55.109
    May  9 13:12:55.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename subpath 05/09/23 13:12:55.11
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:12:55.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:12:55.125
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/09/23 13:12:55.127
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-h225 05/09/23 13:12:55.134
    STEP: Creating a pod to test atomic-volume-subpath 05/09/23 13:12:55.134
    May  9 13:12:55.141: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-h225" in namespace "subpath-5666" to be "Succeeded or Failed"
    May  9 13:12:55.144: INFO: Pod "pod-subpath-test-projected-h225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.827763ms
    May  9 13:12:57.148: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 2.006282124s
    May  9 13:12:59.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 4.005848875s
    May  9 13:13:01.149: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 6.007867185s
    May  9 13:13:03.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 8.006064286s
    May  9 13:13:05.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 10.006137857s
    May  9 13:13:07.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 12.006110628s
    May  9 13:13:09.148: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 14.006430461s
    May  9 13:13:11.149: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 16.007705285s
    May  9 13:13:13.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 18.005564839s
    May  9 13:13:15.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=true. Elapsed: 20.006134346s
    May  9 13:13:17.147: INFO: Pod "pod-subpath-test-projected-h225": Phase="Running", Reason="", readiness=false. Elapsed: 22.006196696s
    May  9 13:13:19.148: INFO: Pod "pod-subpath-test-projected-h225": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007121941s
    STEP: Saw pod success 05/09/23 13:13:19.148
    May  9 13:13:19.149: INFO: Pod "pod-subpath-test-projected-h225" satisfied condition "Succeeded or Failed"
    May  9 13:13:19.151: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-subpath-test-projected-h225 container test-container-subpath-projected-h225: <nil>
    STEP: delete the pod 05/09/23 13:13:19.157
    May  9 13:13:19.166: INFO: Waiting for pod pod-subpath-test-projected-h225 to disappear
    May  9 13:13:19.169: INFO: Pod pod-subpath-test-projected-h225 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-h225 05/09/23 13:13:19.169
    May  9 13:13:19.169: INFO: Deleting pod "pod-subpath-test-projected-h225" in namespace "subpath-5666"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  9 13:13:19.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5666" for this suite. 05/09/23 13:13:19.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:13:19.182
May  9 13:13:19.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename sched-pred 05/09/23 13:13:19.183
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:19.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:19.197
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May  9 13:13:19.199: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  9 13:13:19.206: INFO: Waiting for terminating namespaces to be deleted...
May  9 13:13:19.208: INFO: 
Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-48ljh before test
May  9 13:13:19.214: INFO: calico-node-fdkdq from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.214: INFO: 	Container calico-node ready: true, restart count 0
May  9 13:13:19.214: INFO: csi-cinder-nodeplugin-f688q from kube-system started at 2023-05-09 11:59:12 +0000 UTC (3 container statuses recorded)
May  9 13:13:19.214: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May  9 13:13:19.214: INFO: 	Container liveness-probe ready: true, restart count 0
May  9 13:13:19.214: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  9 13:13:19.214: INFO: kube-proxy-lbgl4 from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.214: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 13:13:19.214: INFO: node-local-dns-vg5zf from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.214: INFO: 	Container node-cache ready: true, restart count 0
May  9 13:13:19.214: INFO: sonobuoy from sonobuoy started at 2023-05-09 12:27:44 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.214: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  9 13:13:19.214: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-nvk78 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:13:19.214: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:13:19.214: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 13:13:19.214: INFO: 
Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-879bk before test
May  9 13:13:19.220: INFO: calico-node-gdlh8 from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.220: INFO: 	Container calico-node ready: true, restart count 0
May  9 13:13:19.220: INFO: csi-cinder-nodeplugin-hzf44 from kube-system started at 2023-05-09 11:59:09 +0000 UTC (3 container statuses recorded)
May  9 13:13:19.220: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May  9 13:13:19.220: INFO: 	Container liveness-probe ready: true, restart count 0
May  9 13:13:19.220: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  9 13:13:19.220: INFO: kube-proxy-pcp2c from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.220: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 13:13:19.220: INFO: node-local-dns-hvfcl from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.220: INFO: 	Container node-cache ready: true, restart count 0
May  9 13:13:19.220: INFO: sonobuoy-e2e-job-588b06ca4a824bd1 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:13:19.220: INFO: 	Container e2e ready: true, restart count 0
May  9 13:13:19.220: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:13:19.220: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-6xh5z from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:13:19.220: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:13:19.220: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 13:13:19.220: INFO: 
Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-skqqr before test
May  9 13:13:19.226: INFO: calico-kube-controllers-578488968b-2nchl from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.226: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  9 13:13:19.226: INFO: calico-node-fm8kr from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.226: INFO: 	Container calico-node ready: true, restart count 0
May  9 13:13:19.226: INFO: coredns-677bf479b7-t4l9l from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.226: INFO: 	Container coredns ready: true, restart count 0
May  9 13:13:19.226: INFO: coredns-677bf479b7-t7hjx from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.226: INFO: 	Container coredns ready: true, restart count 0
May  9 13:13:19.226: INFO: csi-cinder-nodeplugin-f82jf from kube-system started at 2023-05-09 11:59:06 +0000 UTC (3 container statuses recorded)
May  9 13:13:19.226: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May  9 13:13:19.226: INFO: 	Container liveness-probe ready: true, restart count 0
May  9 13:13:19.226: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  9 13:13:19.226: INFO: kube-proxy-d9lqj from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.226: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 13:13:19.226: INFO: metrics-server-5974658d45-zx6c9 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.226: INFO: 	Container metrics-server ready: true, restart count 0
May  9 13:13:19.226: INFO: node-local-dns-hfgs9 from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.226: INFO: 	Container node-cache ready: true, restart count 0
May  9 13:13:19.226: INFO: snapshot-controller-677c67cc64-78dwg from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.226: INFO: 	Container snapshot-controller ready: true, restart count 0
May  9 13:13:19.226: INFO: snapshot-controller-677c67cc64-jrp67 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:13:19.226: INFO: 	Container snapshot-controller ready: true, restart count 0
May  9 13:13:19.226: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-67f2c from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:13:19.226: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:13:19.226: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/09/23 13:13:19.226
May  9 13:13:19.232: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1912" to be "running"
May  9 13:13:19.236: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.909995ms
May  9 13:13:21.240: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008062836s
May  9 13:13:21.240: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/09/23 13:13:21.243
STEP: Trying to apply a random label on the found node. 05/09/23 13:13:21.254
STEP: verifying the node has the label kubernetes.io/e2e-17390aea-0fdf-4e69-8efc-88e5c7598b24 42 05/09/23 13:13:21.263
STEP: Trying to relaunch the pod, now with labels. 05/09/23 13:13:21.266
May  9 13:13:21.271: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1912" to be "not pending"
May  9 13:13:21.275: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.69005ms
May  9 13:13:23.282: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.010990573s
May  9 13:13:23.282: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-17390aea-0fdf-4e69-8efc-88e5c7598b24 off the node cl-gks-cncf-ix1-md-0-48ljh 05/09/23 13:13:23.284
STEP: verifying the node doesn't have the label kubernetes.io/e2e-17390aea-0fdf-4e69-8efc-88e5c7598b24 05/09/23 13:13:23.296
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May  9 13:13:23.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1912" for this suite. 05/09/23 13:13:23.302
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":182,"skipped":3562,"failed":0}
------------------------------
• [4.125 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:13:19.182
    May  9 13:13:19.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename sched-pred 05/09/23 13:13:19.183
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:19.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:19.197
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May  9 13:13:19.199: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  9 13:13:19.206: INFO: Waiting for terminating namespaces to be deleted...
    May  9 13:13:19.208: INFO: 
    Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-48ljh before test
    May  9 13:13:19.214: INFO: calico-node-fdkdq from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.214: INFO: 	Container calico-node ready: true, restart count 0
    May  9 13:13:19.214: INFO: csi-cinder-nodeplugin-f688q from kube-system started at 2023-05-09 11:59:12 +0000 UTC (3 container statuses recorded)
    May  9 13:13:19.214: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    May  9 13:13:19.214: INFO: 	Container liveness-probe ready: true, restart count 0
    May  9 13:13:19.214: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May  9 13:13:19.214: INFO: kube-proxy-lbgl4 from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.214: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 13:13:19.214: INFO: node-local-dns-vg5zf from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.214: INFO: 	Container node-cache ready: true, restart count 0
    May  9 13:13:19.214: INFO: sonobuoy from sonobuoy started at 2023-05-09 12:27:44 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.214: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  9 13:13:19.214: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-nvk78 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:13:19.214: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:13:19.214: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 13:13:19.214: INFO: 
    Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-879bk before test
    May  9 13:13:19.220: INFO: calico-node-gdlh8 from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.220: INFO: 	Container calico-node ready: true, restart count 0
    May  9 13:13:19.220: INFO: csi-cinder-nodeplugin-hzf44 from kube-system started at 2023-05-09 11:59:09 +0000 UTC (3 container statuses recorded)
    May  9 13:13:19.220: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    May  9 13:13:19.220: INFO: 	Container liveness-probe ready: true, restart count 0
    May  9 13:13:19.220: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May  9 13:13:19.220: INFO: kube-proxy-pcp2c from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.220: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 13:13:19.220: INFO: node-local-dns-hvfcl from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.220: INFO: 	Container node-cache ready: true, restart count 0
    May  9 13:13:19.220: INFO: sonobuoy-e2e-job-588b06ca4a824bd1 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:13:19.220: INFO: 	Container e2e ready: true, restart count 0
    May  9 13:13:19.220: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:13:19.220: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-6xh5z from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:13:19.220: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:13:19.220: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 13:13:19.220: INFO: 
    Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-skqqr before test
    May  9 13:13:19.226: INFO: calico-kube-controllers-578488968b-2nchl from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.226: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  9 13:13:19.226: INFO: calico-node-fm8kr from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.226: INFO: 	Container calico-node ready: true, restart count 0
    May  9 13:13:19.226: INFO: coredns-677bf479b7-t4l9l from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.226: INFO: 	Container coredns ready: true, restart count 0
    May  9 13:13:19.226: INFO: coredns-677bf479b7-t7hjx from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.226: INFO: 	Container coredns ready: true, restart count 0
    May  9 13:13:19.226: INFO: csi-cinder-nodeplugin-f82jf from kube-system started at 2023-05-09 11:59:06 +0000 UTC (3 container statuses recorded)
    May  9 13:13:19.226: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    May  9 13:13:19.226: INFO: 	Container liveness-probe ready: true, restart count 0
    May  9 13:13:19.226: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May  9 13:13:19.226: INFO: kube-proxy-d9lqj from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.226: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 13:13:19.226: INFO: metrics-server-5974658d45-zx6c9 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.226: INFO: 	Container metrics-server ready: true, restart count 0
    May  9 13:13:19.226: INFO: node-local-dns-hfgs9 from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.226: INFO: 	Container node-cache ready: true, restart count 0
    May  9 13:13:19.226: INFO: snapshot-controller-677c67cc64-78dwg from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.226: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  9 13:13:19.226: INFO: snapshot-controller-677c67cc64-jrp67 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:13:19.226: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  9 13:13:19.226: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-67f2c from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:13:19.226: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:13:19.226: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/09/23 13:13:19.226
    May  9 13:13:19.232: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1912" to be "running"
    May  9 13:13:19.236: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.909995ms
    May  9 13:13:21.240: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008062836s
    May  9 13:13:21.240: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/09/23 13:13:21.243
    STEP: Trying to apply a random label on the found node. 05/09/23 13:13:21.254
    STEP: verifying the node has the label kubernetes.io/e2e-17390aea-0fdf-4e69-8efc-88e5c7598b24 42 05/09/23 13:13:21.263
    STEP: Trying to relaunch the pod, now with labels. 05/09/23 13:13:21.266
    May  9 13:13:21.271: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1912" to be "not pending"
    May  9 13:13:21.275: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.69005ms
    May  9 13:13:23.282: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.010990573s
    May  9 13:13:23.282: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-17390aea-0fdf-4e69-8efc-88e5c7598b24 off the node cl-gks-cncf-ix1-md-0-48ljh 05/09/23 13:13:23.284
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-17390aea-0fdf-4e69-8efc-88e5c7598b24 05/09/23 13:13:23.296
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:13:23.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1912" for this suite. 05/09/23 13:13:23.302
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:13:23.308
May  9 13:13:23.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 13:13:23.309
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:23.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:23.324
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  9 13:13:23.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3480" for this suite. 05/09/23 13:13:23.362
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":183,"skipped":3570,"failed":0}
------------------------------
• [0.059 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:13:23.308
    May  9 13:13:23.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 13:13:23.309
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:23.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:23.324
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 13:13:23.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3480" for this suite. 05/09/23 13:13:23.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:13:23.368
May  9 13:13:23.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:13:23.369
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:23.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:23.383
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-47de2388-aa3e-47ba-b293-8308582a0842 05/09/23 13:13:23.389
STEP: Creating the pod 05/09/23 13:13:23.393
May  9 13:13:23.399: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eb221e00-c17c-4716-a323-71b037856fb1" in namespace "projected-3983" to be "running and ready"
May  9 13:13:23.404: INFO: Pod "pod-projected-configmaps-eb221e00-c17c-4716-a323-71b037856fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.969101ms
May  9 13:13:23.404: INFO: The phase of Pod pod-projected-configmaps-eb221e00-c17c-4716-a323-71b037856fb1 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:13:25.409: INFO: Pod "pod-projected-configmaps-eb221e00-c17c-4716-a323-71b037856fb1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009407148s
May  9 13:13:25.409: INFO: The phase of Pod pod-projected-configmaps-eb221e00-c17c-4716-a323-71b037856fb1 is Running (Ready = true)
May  9 13:13:25.409: INFO: Pod "pod-projected-configmaps-eb221e00-c17c-4716-a323-71b037856fb1" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-47de2388-aa3e-47ba-b293-8308582a0842 05/09/23 13:13:25.422
STEP: waiting to observe update in volume 05/09/23 13:13:25.426
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  9 13:13:27.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3983" for this suite. 05/09/23 13:13:27.441
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":184,"skipped":3602,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:13:23.368
    May  9 13:13:23.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:13:23.369
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:23.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:23.383
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-47de2388-aa3e-47ba-b293-8308582a0842 05/09/23 13:13:23.389
    STEP: Creating the pod 05/09/23 13:13:23.393
    May  9 13:13:23.399: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eb221e00-c17c-4716-a323-71b037856fb1" in namespace "projected-3983" to be "running and ready"
    May  9 13:13:23.404: INFO: Pod "pod-projected-configmaps-eb221e00-c17c-4716-a323-71b037856fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.969101ms
    May  9 13:13:23.404: INFO: The phase of Pod pod-projected-configmaps-eb221e00-c17c-4716-a323-71b037856fb1 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:13:25.409: INFO: Pod "pod-projected-configmaps-eb221e00-c17c-4716-a323-71b037856fb1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009407148s
    May  9 13:13:25.409: INFO: The phase of Pod pod-projected-configmaps-eb221e00-c17c-4716-a323-71b037856fb1 is Running (Ready = true)
    May  9 13:13:25.409: INFO: Pod "pod-projected-configmaps-eb221e00-c17c-4716-a323-71b037856fb1" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-47de2388-aa3e-47ba-b293-8308582a0842 05/09/23 13:13:25.422
    STEP: waiting to observe update in volume 05/09/23 13:13:25.426
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  9 13:13:27.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3983" for this suite. 05/09/23 13:13:27.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:13:27.447
May  9 13:13:27.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubelet-test 05/09/23 13:13:27.448
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:27.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:27.463
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  9 13:13:27.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4023" for this suite. 05/09/23 13:13:27.491
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":185,"skipped":3609,"failed":0}
------------------------------
• [0.049 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:13:27.447
    May  9 13:13:27.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubelet-test 05/09/23 13:13:27.448
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:27.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:27.463
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  9 13:13:27.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4023" for this suite. 05/09/23 13:13:27.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:13:27.498
May  9 13:13:27.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename proxy 05/09/23 13:13:27.498
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:27.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:27.513
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 05/09/23 13:13:27.527
STEP: creating replication controller proxy-service-xmz2x in namespace proxy-8664 05/09/23 13:13:27.527
I0509 13:13:27.535498      24 runners.go:193] Created replication controller with name: proxy-service-xmz2x, namespace: proxy-8664, replica count: 1
I0509 13:13:28.587376      24 runners.go:193] proxy-service-xmz2x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0509 13:13:29.588078      24 runners.go:193] proxy-service-xmz2x Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 13:13:29.591: INFO: setup took 2.075742065s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/09/23 13:13:29.591
May  9 13:13:29.597: INFO: (0) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.502981ms)
May  9 13:13:29.597: INFO: (0) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.538949ms)
May  9 13:13:29.597: INFO: (0) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.738709ms)
May  9 13:13:29.598: INFO: (0) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 6.638058ms)
May  9 13:13:29.598: INFO: (0) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.863438ms)
May  9 13:13:29.598: INFO: (0) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 7.009866ms)
May  9 13:13:29.598: INFO: (0) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.970491ms)
May  9 13:13:29.599: INFO: (0) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.751718ms)
May  9 13:13:29.599: INFO: (0) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.163662ms)
May  9 13:13:29.600: INFO: (0) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 9.019352ms)
May  9 13:13:29.600: INFO: (0) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 9.23936ms)
May  9 13:13:29.602: INFO: (0) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 10.934277ms)
May  9 13:13:29.602: INFO: (0) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 11.387541ms)
May  9 13:13:29.602: INFO: (0) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 11.40272ms)
May  9 13:13:29.602: INFO: (0) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 11.574144ms)
May  9 13:13:29.602: INFO: (0) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 11.704645ms)
May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.162532ms)
May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.30852ms)
May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.353315ms)
May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 6.399753ms)
May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 6.56075ms)
May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.459278ms)
May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.470278ms)
May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.515213ms)
May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 6.488423ms)
May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.451352ms)
May  9 13:13:29.610: INFO: (1) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 6.985398ms)
May  9 13:13:29.610: INFO: (1) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.72658ms)
May  9 13:13:29.612: INFO: (1) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 9.002689ms)
May  9 13:13:29.612: INFO: (1) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 9.051392ms)
May  9 13:13:29.612: INFO: (1) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 9.1818ms)
May  9 13:13:29.612: INFO: (1) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 9.187271ms)
May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 4.960915ms)
May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.182888ms)
May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.287408ms)
May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.302144ms)
May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.35234ms)
May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.449154ms)
May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.386115ms)
May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.572308ms)
May  9 13:13:29.618: INFO: (2) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.581496ms)
May  9 13:13:29.618: INFO: (2) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.613869ms)
May  9 13:13:29.618: INFO: (2) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 6.433789ms)
May  9 13:13:29.619: INFO: (2) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 7.244913ms)
May  9 13:13:29.619: INFO: (2) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.371494ms)
May  9 13:13:29.619: INFO: (2) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.417312ms)
May  9 13:13:29.619: INFO: (2) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 7.528574ms)
May  9 13:13:29.619: INFO: (2) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 7.554413ms)
May  9 13:13:29.624: INFO: (3) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 4.987486ms)
May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.290924ms)
May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.397817ms)
May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.321812ms)
May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.51004ms)
May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.513045ms)
May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.48341ms)
May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.649226ms)
May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.707587ms)
May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.716473ms)
May  9 13:13:29.626: INFO: (3) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 6.154377ms)
May  9 13:13:29.627: INFO: (3) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 7.025255ms)
May  9 13:13:29.627: INFO: (3) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 7.034372ms)
May  9 13:13:29.627: INFO: (3) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.137008ms)
May  9 13:13:29.627: INFO: (3) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 7.123593ms)
May  9 13:13:29.627: INFO: (3) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.156515ms)
May  9 13:13:29.630: INFO: (4) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 3.528618ms)
May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 4.343069ms)
May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 4.417761ms)
May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 4.532409ms)
May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 4.571263ms)
May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 4.561854ms)
May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 4.563006ms)
May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 4.639733ms)
May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 4.585099ms)
May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 4.589427ms)
May  9 13:13:29.633: INFO: (4) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 5.960768ms)
May  9 13:13:29.634: INFO: (4) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 7.134413ms)
May  9 13:13:29.634: INFO: (4) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.312511ms)
May  9 13:13:29.634: INFO: (4) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.471504ms)
May  9 13:13:29.634: INFO: (4) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 7.486392ms)
May  9 13:13:29.634: INFO: (4) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 7.44229ms)
May  9 13:13:29.639: INFO: (5) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 4.962969ms)
May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.198387ms)
May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.255286ms)
May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.342671ms)
May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.330208ms)
May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.214447ms)
May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.472298ms)
May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.378039ms)
May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.42637ms)
May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.4149ms)
May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 5.958925ms)
May  9 13:13:29.642: INFO: (5) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 7.595922ms)
May  9 13:13:29.643: INFO: (5) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.307907ms)
May  9 13:13:29.643: INFO: (5) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.447302ms)
May  9 13:13:29.643: INFO: (5) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.425672ms)
May  9 13:13:29.643: INFO: (5) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 8.452712ms)
May  9 13:13:29.647: INFO: (6) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 4.319464ms)
May  9 13:13:29.649: INFO: (6) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.068773ms)
May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.656994ms)
May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.657534ms)
May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 6.657065ms)
May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 6.666302ms)
May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.75397ms)
May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.806469ms)
May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.72317ms)
May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.712529ms)
May  9 13:13:29.651: INFO: (6) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.142253ms)
May  9 13:13:29.652: INFO: (6) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 8.593822ms)
May  9 13:13:29.652: INFO: (6) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.896968ms)
May  9 13:13:29.652: INFO: (6) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 9.092982ms)
May  9 13:13:29.652: INFO: (6) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 9.143067ms)
May  9 13:13:29.652: INFO: (6) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 9.192031ms)
May  9 13:13:29.656: INFO: (7) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 3.588051ms)
May  9 13:13:29.657: INFO: (7) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 4.79522ms)
May  9 13:13:29.657: INFO: (7) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.017442ms)
May  9 13:13:29.657: INFO: (7) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.234357ms)
May  9 13:13:29.658: INFO: (7) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.288819ms)
May  9 13:13:29.658: INFO: (7) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.330519ms)
May  9 13:13:29.658: INFO: (7) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.407185ms)
May  9 13:13:29.658: INFO: (7) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.559764ms)
May  9 13:13:29.658: INFO: (7) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.658844ms)
May  9 13:13:29.658: INFO: (7) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.988061ms)
May  9 13:13:29.659: INFO: (7) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 6.998274ms)
May  9 13:13:29.661: INFO: (7) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.418068ms)
May  9 13:13:29.661: INFO: (7) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.511815ms)
May  9 13:13:29.661: INFO: (7) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.56224ms)
May  9 13:13:29.661: INFO: (7) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 9.33378ms)
May  9 13:13:29.662: INFO: (7) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 9.361343ms)
May  9 13:13:29.664: INFO: (8) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 2.766186ms)
May  9 13:13:29.666: INFO: (8) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 4.145051ms)
May  9 13:13:29.668: INFO: (8) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.054627ms)
May  9 13:13:29.668: INFO: (8) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 6.49257ms)
May  9 13:13:29.668: INFO: (8) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.190745ms)
May  9 13:13:29.668: INFO: (8) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.399412ms)
May  9 13:13:29.668: INFO: (8) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.22999ms)
May  9 13:13:29.669: INFO: (8) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.533719ms)
May  9 13:13:29.669: INFO: (8) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 6.649219ms)
May  9 13:13:29.669: INFO: (8) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 7.214615ms)
May  9 13:13:29.670: INFO: (8) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 7.45361ms)
May  9 13:13:29.671: INFO: (8) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.888072ms)
May  9 13:13:29.671: INFO: (8) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 8.632635ms)
May  9 13:13:29.671: INFO: (8) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.89246ms)
May  9 13:13:29.671: INFO: (8) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.908901ms)
May  9 13:13:29.671: INFO: (8) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 9.145101ms)
May  9 13:13:29.675: INFO: (9) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 4.170519ms)
May  9 13:13:29.677: INFO: (9) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.364544ms)
May  9 13:13:29.677: INFO: (9) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.972912ms)
May  9 13:13:29.677: INFO: (9) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.952382ms)
May  9 13:13:29.678: INFO: (9) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.385597ms)
May  9 13:13:29.678: INFO: (9) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 6.249438ms)
May  9 13:13:29.678: INFO: (9) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.200805ms)
May  9 13:13:29.678: INFO: (9) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.048986ms)
May  9 13:13:29.678: INFO: (9) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.41871ms)
May  9 13:13:29.678: INFO: (9) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.040971ms)
May  9 13:13:29.679: INFO: (9) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 7.402043ms)
May  9 13:13:29.680: INFO: (9) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.3831ms)
May  9 13:13:29.680: INFO: (9) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.59302ms)
May  9 13:13:29.680: INFO: (9) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.55641ms)
May  9 13:13:29.680: INFO: (9) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.965899ms)
May  9 13:13:29.680: INFO: (9) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.661871ms)
May  9 13:13:29.684: INFO: (10) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 3.5977ms)
May  9 13:13:29.685: INFO: (10) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 4.4176ms)
May  9 13:13:29.685: INFO: (10) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 4.546076ms)
May  9 13:13:29.687: INFO: (10) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 6.527768ms)
May  9 13:13:29.687: INFO: (10) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.57082ms)
May  9 13:13:29.687: INFO: (10) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 6.67051ms)
May  9 13:13:29.687: INFO: (10) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.76003ms)
May  9 13:13:29.688: INFO: (10) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.094868ms)
May  9 13:13:29.688: INFO: (10) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 7.035023ms)
May  9 13:13:29.688: INFO: (10) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 7.179459ms)
May  9 13:13:29.688: INFO: (10) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 7.349342ms)
May  9 13:13:29.688: INFO: (10) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 7.366715ms)
May  9 13:13:29.688: INFO: (10) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 7.420849ms)
May  9 13:13:29.689: INFO: (10) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.095703ms)
May  9 13:13:29.689: INFO: (10) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 8.743797ms)
May  9 13:13:29.689: INFO: (10) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.812648ms)
May  9 13:13:29.694: INFO: (11) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 4.268145ms)
May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.656313ms)
May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 6.652085ms)
May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 6.68675ms)
May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.681801ms)
May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.756142ms)
May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 6.855784ms)
May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.797531ms)
May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.823882ms)
May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 6.876101ms)
May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.839652ms)
May  9 13:13:29.697: INFO: (11) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 7.114776ms)
May  9 13:13:29.698: INFO: (11) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.430219ms)
May  9 13:13:29.698: INFO: (11) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.528387ms)
May  9 13:13:29.698: INFO: (11) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.612918ms)
May  9 13:13:29.698: INFO: (11) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.582509ms)
May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.59943ms)
May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.810944ms)
May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.869423ms)
May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.795744ms)
May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.850568ms)
May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.83018ms)
May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.905583ms)
May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.818988ms)
May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.834538ms)
May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.910954ms)
May  9 13:13:29.705: INFO: (12) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 6.489876ms)
May  9 13:13:29.705: INFO: (12) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.100178ms)
May  9 13:13:29.707: INFO: (12) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.473452ms)
May  9 13:13:29.707: INFO: (12) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 8.484083ms)
May  9 13:13:29.707: INFO: (12) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.693211ms)
May  9 13:13:29.707: INFO: (12) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.644598ms)
May  9 13:13:29.711: INFO: (13) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 4.357676ms)
May  9 13:13:29.711: INFO: (13) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 4.262205ms)
May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 3.889365ms)
May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 4.410967ms)
May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 3.969988ms)
May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 4.3962ms)
May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 4.651505ms)
May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 4.421738ms)
May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 4.941197ms)
May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.071515ms)
May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 5.515842ms)
May  9 13:13:29.713: INFO: (13) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 6.006786ms)
May  9 13:13:29.714: INFO: (13) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 6.802261ms)
May  9 13:13:29.714: INFO: (13) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 6.867566ms)
May  9 13:13:29.714: INFO: (13) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 6.96495ms)
May  9 13:13:29.714: INFO: (13) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.032538ms)
May  9 13:13:29.717: INFO: (14) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 2.944477ms)
May  9 13:13:29.719: INFO: (14) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 4.903105ms)
May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.193538ms)
May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.312014ms)
May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.232122ms)
May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.315871ms)
May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.379942ms)
May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.313606ms)
May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.303597ms)
May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.372189ms)
May  9 13:13:29.721: INFO: (14) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 6.53403ms)
May  9 13:13:29.722: INFO: (14) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.750206ms)
May  9 13:13:29.722: INFO: (14) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.851969ms)
May  9 13:13:29.722: INFO: (14) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 7.985794ms)
May  9 13:13:29.722: INFO: (14) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 7.998708ms)
May  9 13:13:29.723: INFO: (14) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.07322ms)
May  9 13:13:29.726: INFO: (15) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 3.120732ms)
May  9 13:13:29.726: INFO: (15) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 3.773535ms)
May  9 13:13:29.727: INFO: (15) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 3.871431ms)
May  9 13:13:29.728: INFO: (15) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.657039ms)
May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.369592ms)
May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.85139ms)
May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.678ms)
May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.802297ms)
May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.834477ms)
May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.914961ms)
May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 6.585448ms)
May  9 13:13:29.730: INFO: (15) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 7.140724ms)
May  9 13:13:29.731: INFO: (15) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 7.645005ms)
May  9 13:13:29.731: INFO: (15) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.194081ms)
May  9 13:13:29.731: INFO: (15) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.108006ms)
May  9 13:13:29.731: INFO: (15) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.304019ms)
May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 6.321986ms)
May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.930822ms)
May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.069284ms)
May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.616206ms)
May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 6.555661ms)
May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.520394ms)
May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.266071ms)
May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.589765ms)
May  9 13:13:29.739: INFO: (16) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 7.527039ms)
May  9 13:13:29.739: INFO: (16) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 7.745587ms)
May  9 13:13:29.740: INFO: (16) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.052751ms)
May  9 13:13:29.740: INFO: (16) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.29863ms)
May  9 13:13:29.740: INFO: (16) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.092898ms)
May  9 13:13:29.740: INFO: (16) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 8.471659ms)
May  9 13:13:29.740: INFO: (16) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.406485ms)
May  9 13:13:29.740: INFO: (16) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.329648ms)
May  9 13:13:29.745: INFO: (17) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 4.857167ms)
May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.665024ms)
May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.724679ms)
May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.723566ms)
May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.697857ms)
May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.673431ms)
May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.396168ms)
May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.387781ms)
May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.405856ms)
May  9 13:13:29.747: INFO: (17) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.459377ms)
May  9 13:13:29.747: INFO: (17) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 7.100357ms)
May  9 13:13:29.748: INFO: (17) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 8.057701ms)
May  9 13:13:29.748: INFO: (17) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.080825ms)
May  9 13:13:29.748: INFO: (17) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.105031ms)
May  9 13:13:29.748: INFO: (17) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.129949ms)
May  9 13:13:29.748: INFO: (17) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.155838ms)
May  9 13:13:29.751: INFO: (18) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 3.273502ms)
May  9 13:13:29.752: INFO: (18) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 3.720855ms)
May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.187917ms)
May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 4.887144ms)
May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.077497ms)
May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.373822ms)
May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.231581ms)
May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.520241ms)
May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.596125ms)
May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.329787ms)
May  9 13:13:29.755: INFO: (18) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 5.942163ms)
May  9 13:13:29.755: INFO: (18) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 6.259267ms)
May  9 13:13:29.755: INFO: (18) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 6.148086ms)
May  9 13:13:29.755: INFO: (18) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 6.524702ms)
May  9 13:13:29.755: INFO: (18) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 6.105445ms)
May  9 13:13:29.755: INFO: (18) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 6.545953ms)
May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.622243ms)
May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.606814ms)
May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.570305ms)
May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.719408ms)
May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.647421ms)
May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.757481ms)
May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.580314ms)
May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.64656ms)
May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.760618ms)
May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.665365ms)
May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 6.559188ms)
May  9 13:13:29.763: INFO: (19) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 7.529595ms)
May  9 13:13:29.763: INFO: (19) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.620678ms)
May  9 13:13:29.763: INFO: (19) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.682836ms)
May  9 13:13:29.763: INFO: (19) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 7.62129ms)
May  9 13:13:29.763: INFO: (19) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 7.798788ms)
STEP: deleting ReplicationController proxy-service-xmz2x in namespace proxy-8664, will wait for the garbage collector to delete the pods 05/09/23 13:13:29.763
May  9 13:13:29.822: INFO: Deleting ReplicationController proxy-service-xmz2x took: 5.836853ms
May  9 13:13:29.923: INFO: Terminating ReplicationController proxy-service-xmz2x pods took: 100.628352ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
May  9 13:13:32.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8664" for this suite. 05/09/23 13:13:32.532
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":186,"skipped":3697,"failed":0}
------------------------------
• [SLOW TEST] [5.041 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:13:27.498
    May  9 13:13:27.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename proxy 05/09/23 13:13:27.498
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:27.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:27.513
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 05/09/23 13:13:27.527
    STEP: creating replication controller proxy-service-xmz2x in namespace proxy-8664 05/09/23 13:13:27.527
    I0509 13:13:27.535498      24 runners.go:193] Created replication controller with name: proxy-service-xmz2x, namespace: proxy-8664, replica count: 1
    I0509 13:13:28.587376      24 runners.go:193] proxy-service-xmz2x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0509 13:13:29.588078      24 runners.go:193] proxy-service-xmz2x Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 13:13:29.591: INFO: setup took 2.075742065s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/09/23 13:13:29.591
    May  9 13:13:29.597: INFO: (0) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.502981ms)
    May  9 13:13:29.597: INFO: (0) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.538949ms)
    May  9 13:13:29.597: INFO: (0) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.738709ms)
    May  9 13:13:29.598: INFO: (0) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 6.638058ms)
    May  9 13:13:29.598: INFO: (0) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.863438ms)
    May  9 13:13:29.598: INFO: (0) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 7.009866ms)
    May  9 13:13:29.598: INFO: (0) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.970491ms)
    May  9 13:13:29.599: INFO: (0) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.751718ms)
    May  9 13:13:29.599: INFO: (0) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.163662ms)
    May  9 13:13:29.600: INFO: (0) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 9.019352ms)
    May  9 13:13:29.600: INFO: (0) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 9.23936ms)
    May  9 13:13:29.602: INFO: (0) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 10.934277ms)
    May  9 13:13:29.602: INFO: (0) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 11.387541ms)
    May  9 13:13:29.602: INFO: (0) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 11.40272ms)
    May  9 13:13:29.602: INFO: (0) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 11.574144ms)
    May  9 13:13:29.602: INFO: (0) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 11.704645ms)
    May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.162532ms)
    May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.30852ms)
    May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.353315ms)
    May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 6.399753ms)
    May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 6.56075ms)
    May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.459278ms)
    May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.470278ms)
    May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.515213ms)
    May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 6.488423ms)
    May  9 13:13:29.609: INFO: (1) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.451352ms)
    May  9 13:13:29.610: INFO: (1) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 6.985398ms)
    May  9 13:13:29.610: INFO: (1) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.72658ms)
    May  9 13:13:29.612: INFO: (1) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 9.002689ms)
    May  9 13:13:29.612: INFO: (1) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 9.051392ms)
    May  9 13:13:29.612: INFO: (1) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 9.1818ms)
    May  9 13:13:29.612: INFO: (1) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 9.187271ms)
    May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 4.960915ms)
    May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.182888ms)
    May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.287408ms)
    May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.302144ms)
    May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.35234ms)
    May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.449154ms)
    May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.386115ms)
    May  9 13:13:29.617: INFO: (2) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.572308ms)
    May  9 13:13:29.618: INFO: (2) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.581496ms)
    May  9 13:13:29.618: INFO: (2) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.613869ms)
    May  9 13:13:29.618: INFO: (2) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 6.433789ms)
    May  9 13:13:29.619: INFO: (2) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 7.244913ms)
    May  9 13:13:29.619: INFO: (2) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.371494ms)
    May  9 13:13:29.619: INFO: (2) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.417312ms)
    May  9 13:13:29.619: INFO: (2) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 7.528574ms)
    May  9 13:13:29.619: INFO: (2) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 7.554413ms)
    May  9 13:13:29.624: INFO: (3) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 4.987486ms)
    May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.290924ms)
    May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.397817ms)
    May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.321812ms)
    May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.51004ms)
    May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.513045ms)
    May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.48341ms)
    May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.649226ms)
    May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.707587ms)
    May  9 13:13:29.625: INFO: (3) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.716473ms)
    May  9 13:13:29.626: INFO: (3) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 6.154377ms)
    May  9 13:13:29.627: INFO: (3) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 7.025255ms)
    May  9 13:13:29.627: INFO: (3) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 7.034372ms)
    May  9 13:13:29.627: INFO: (3) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.137008ms)
    May  9 13:13:29.627: INFO: (3) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 7.123593ms)
    May  9 13:13:29.627: INFO: (3) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.156515ms)
    May  9 13:13:29.630: INFO: (4) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 3.528618ms)
    May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 4.343069ms)
    May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 4.417761ms)
    May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 4.532409ms)
    May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 4.571263ms)
    May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 4.561854ms)
    May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 4.563006ms)
    May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 4.639733ms)
    May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 4.585099ms)
    May  9 13:13:29.631: INFO: (4) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 4.589427ms)
    May  9 13:13:29.633: INFO: (4) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 5.960768ms)
    May  9 13:13:29.634: INFO: (4) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 7.134413ms)
    May  9 13:13:29.634: INFO: (4) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.312511ms)
    May  9 13:13:29.634: INFO: (4) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.471504ms)
    May  9 13:13:29.634: INFO: (4) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 7.486392ms)
    May  9 13:13:29.634: INFO: (4) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 7.44229ms)
    May  9 13:13:29.639: INFO: (5) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 4.962969ms)
    May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.198387ms)
    May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.255286ms)
    May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.342671ms)
    May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.330208ms)
    May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.214447ms)
    May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.472298ms)
    May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.378039ms)
    May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.42637ms)
    May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.4149ms)
    May  9 13:13:29.640: INFO: (5) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 5.958925ms)
    May  9 13:13:29.642: INFO: (5) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 7.595922ms)
    May  9 13:13:29.643: INFO: (5) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.307907ms)
    May  9 13:13:29.643: INFO: (5) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.447302ms)
    May  9 13:13:29.643: INFO: (5) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.425672ms)
    May  9 13:13:29.643: INFO: (5) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 8.452712ms)
    May  9 13:13:29.647: INFO: (6) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 4.319464ms)
    May  9 13:13:29.649: INFO: (6) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.068773ms)
    May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.656994ms)
    May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.657534ms)
    May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 6.657065ms)
    May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 6.666302ms)
    May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.75397ms)
    May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.806469ms)
    May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.72317ms)
    May  9 13:13:29.650: INFO: (6) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.712529ms)
    May  9 13:13:29.651: INFO: (6) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.142253ms)
    May  9 13:13:29.652: INFO: (6) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 8.593822ms)
    May  9 13:13:29.652: INFO: (6) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.896968ms)
    May  9 13:13:29.652: INFO: (6) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 9.092982ms)
    May  9 13:13:29.652: INFO: (6) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 9.143067ms)
    May  9 13:13:29.652: INFO: (6) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 9.192031ms)
    May  9 13:13:29.656: INFO: (7) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 3.588051ms)
    May  9 13:13:29.657: INFO: (7) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 4.79522ms)
    May  9 13:13:29.657: INFO: (7) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.017442ms)
    May  9 13:13:29.657: INFO: (7) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.234357ms)
    May  9 13:13:29.658: INFO: (7) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.288819ms)
    May  9 13:13:29.658: INFO: (7) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.330519ms)
    May  9 13:13:29.658: INFO: (7) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.407185ms)
    May  9 13:13:29.658: INFO: (7) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.559764ms)
    May  9 13:13:29.658: INFO: (7) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.658844ms)
    May  9 13:13:29.658: INFO: (7) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.988061ms)
    May  9 13:13:29.659: INFO: (7) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 6.998274ms)
    May  9 13:13:29.661: INFO: (7) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.418068ms)
    May  9 13:13:29.661: INFO: (7) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.511815ms)
    May  9 13:13:29.661: INFO: (7) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.56224ms)
    May  9 13:13:29.661: INFO: (7) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 9.33378ms)
    May  9 13:13:29.662: INFO: (7) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 9.361343ms)
    May  9 13:13:29.664: INFO: (8) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 2.766186ms)
    May  9 13:13:29.666: INFO: (8) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 4.145051ms)
    May  9 13:13:29.668: INFO: (8) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.054627ms)
    May  9 13:13:29.668: INFO: (8) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 6.49257ms)
    May  9 13:13:29.668: INFO: (8) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.190745ms)
    May  9 13:13:29.668: INFO: (8) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.399412ms)
    May  9 13:13:29.668: INFO: (8) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.22999ms)
    May  9 13:13:29.669: INFO: (8) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.533719ms)
    May  9 13:13:29.669: INFO: (8) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 6.649219ms)
    May  9 13:13:29.669: INFO: (8) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 7.214615ms)
    May  9 13:13:29.670: INFO: (8) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 7.45361ms)
    May  9 13:13:29.671: INFO: (8) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.888072ms)
    May  9 13:13:29.671: INFO: (8) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 8.632635ms)
    May  9 13:13:29.671: INFO: (8) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.89246ms)
    May  9 13:13:29.671: INFO: (8) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.908901ms)
    May  9 13:13:29.671: INFO: (8) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 9.145101ms)
    May  9 13:13:29.675: INFO: (9) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 4.170519ms)
    May  9 13:13:29.677: INFO: (9) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.364544ms)
    May  9 13:13:29.677: INFO: (9) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.972912ms)
    May  9 13:13:29.677: INFO: (9) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.952382ms)
    May  9 13:13:29.678: INFO: (9) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.385597ms)
    May  9 13:13:29.678: INFO: (9) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 6.249438ms)
    May  9 13:13:29.678: INFO: (9) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.200805ms)
    May  9 13:13:29.678: INFO: (9) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.048986ms)
    May  9 13:13:29.678: INFO: (9) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.41871ms)
    May  9 13:13:29.678: INFO: (9) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.040971ms)
    May  9 13:13:29.679: INFO: (9) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 7.402043ms)
    May  9 13:13:29.680: INFO: (9) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.3831ms)
    May  9 13:13:29.680: INFO: (9) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.59302ms)
    May  9 13:13:29.680: INFO: (9) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.55641ms)
    May  9 13:13:29.680: INFO: (9) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.965899ms)
    May  9 13:13:29.680: INFO: (9) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.661871ms)
    May  9 13:13:29.684: INFO: (10) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 3.5977ms)
    May  9 13:13:29.685: INFO: (10) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 4.4176ms)
    May  9 13:13:29.685: INFO: (10) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 4.546076ms)
    May  9 13:13:29.687: INFO: (10) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 6.527768ms)
    May  9 13:13:29.687: INFO: (10) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.57082ms)
    May  9 13:13:29.687: INFO: (10) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 6.67051ms)
    May  9 13:13:29.687: INFO: (10) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.76003ms)
    May  9 13:13:29.688: INFO: (10) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.094868ms)
    May  9 13:13:29.688: INFO: (10) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 7.035023ms)
    May  9 13:13:29.688: INFO: (10) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 7.179459ms)
    May  9 13:13:29.688: INFO: (10) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 7.349342ms)
    May  9 13:13:29.688: INFO: (10) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 7.366715ms)
    May  9 13:13:29.688: INFO: (10) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 7.420849ms)
    May  9 13:13:29.689: INFO: (10) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.095703ms)
    May  9 13:13:29.689: INFO: (10) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 8.743797ms)
    May  9 13:13:29.689: INFO: (10) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.812648ms)
    May  9 13:13:29.694: INFO: (11) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 4.268145ms)
    May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.656313ms)
    May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 6.652085ms)
    May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 6.68675ms)
    May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.681801ms)
    May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.756142ms)
    May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 6.855784ms)
    May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.797531ms)
    May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.823882ms)
    May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 6.876101ms)
    May  9 13:13:29.696: INFO: (11) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.839652ms)
    May  9 13:13:29.697: INFO: (11) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 7.114776ms)
    May  9 13:13:29.698: INFO: (11) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.430219ms)
    May  9 13:13:29.698: INFO: (11) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.528387ms)
    May  9 13:13:29.698: INFO: (11) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.612918ms)
    May  9 13:13:29.698: INFO: (11) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.582509ms)
    May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.59943ms)
    May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.810944ms)
    May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.869423ms)
    May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.795744ms)
    May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.850568ms)
    May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.83018ms)
    May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.905583ms)
    May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.818988ms)
    May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.834538ms)
    May  9 13:13:29.704: INFO: (12) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.910954ms)
    May  9 13:13:29.705: INFO: (12) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 6.489876ms)
    May  9 13:13:29.705: INFO: (12) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.100178ms)
    May  9 13:13:29.707: INFO: (12) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.473452ms)
    May  9 13:13:29.707: INFO: (12) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 8.484083ms)
    May  9 13:13:29.707: INFO: (12) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.693211ms)
    May  9 13:13:29.707: INFO: (12) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.644598ms)
    May  9 13:13:29.711: INFO: (13) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 4.357676ms)
    May  9 13:13:29.711: INFO: (13) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 4.262205ms)
    May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 3.889365ms)
    May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 4.410967ms)
    May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 3.969988ms)
    May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 4.3962ms)
    May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 4.651505ms)
    May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 4.421738ms)
    May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 4.941197ms)
    May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.071515ms)
    May  9 13:13:29.712: INFO: (13) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 5.515842ms)
    May  9 13:13:29.713: INFO: (13) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 6.006786ms)
    May  9 13:13:29.714: INFO: (13) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 6.802261ms)
    May  9 13:13:29.714: INFO: (13) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 6.867566ms)
    May  9 13:13:29.714: INFO: (13) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 6.96495ms)
    May  9 13:13:29.714: INFO: (13) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.032538ms)
    May  9 13:13:29.717: INFO: (14) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 2.944477ms)
    May  9 13:13:29.719: INFO: (14) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 4.903105ms)
    May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.193538ms)
    May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.312014ms)
    May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.232122ms)
    May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.315871ms)
    May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.379942ms)
    May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.313606ms)
    May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.303597ms)
    May  9 13:13:29.720: INFO: (14) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.372189ms)
    May  9 13:13:29.721: INFO: (14) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 6.53403ms)
    May  9 13:13:29.722: INFO: (14) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.750206ms)
    May  9 13:13:29.722: INFO: (14) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.851969ms)
    May  9 13:13:29.722: INFO: (14) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 7.985794ms)
    May  9 13:13:29.722: INFO: (14) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 7.998708ms)
    May  9 13:13:29.723: INFO: (14) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.07322ms)
    May  9 13:13:29.726: INFO: (15) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 3.120732ms)
    May  9 13:13:29.726: INFO: (15) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 3.773535ms)
    May  9 13:13:29.727: INFO: (15) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 3.871431ms)
    May  9 13:13:29.728: INFO: (15) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.657039ms)
    May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.369592ms)
    May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.85139ms)
    May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.678ms)
    May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.802297ms)
    May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.834477ms)
    May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.914961ms)
    May  9 13:13:29.729: INFO: (15) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 6.585448ms)
    May  9 13:13:29.730: INFO: (15) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 7.140724ms)
    May  9 13:13:29.731: INFO: (15) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 7.645005ms)
    May  9 13:13:29.731: INFO: (15) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.194081ms)
    May  9 13:13:29.731: INFO: (15) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.108006ms)
    May  9 13:13:29.731: INFO: (15) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.304019ms)
    May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 6.321986ms)
    May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.930822ms)
    May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.069284ms)
    May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 6.616206ms)
    May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 6.555661ms)
    May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.520394ms)
    May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.266071ms)
    May  9 13:13:29.738: INFO: (16) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.589765ms)
    May  9 13:13:29.739: INFO: (16) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 7.527039ms)
    May  9 13:13:29.739: INFO: (16) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 7.745587ms)
    May  9 13:13:29.740: INFO: (16) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.052751ms)
    May  9 13:13:29.740: INFO: (16) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.29863ms)
    May  9 13:13:29.740: INFO: (16) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.092898ms)
    May  9 13:13:29.740: INFO: (16) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 8.471659ms)
    May  9 13:13:29.740: INFO: (16) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.406485ms)
    May  9 13:13:29.740: INFO: (16) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 8.329648ms)
    May  9 13:13:29.745: INFO: (17) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 4.857167ms)
    May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.665024ms)
    May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.724679ms)
    May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.723566ms)
    May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.697857ms)
    May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.673431ms)
    May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 6.396168ms)
    May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 6.387781ms)
    May  9 13:13:29.746: INFO: (17) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 6.405856ms)
    May  9 13:13:29.747: INFO: (17) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 6.459377ms)
    May  9 13:13:29.747: INFO: (17) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 7.100357ms)
    May  9 13:13:29.748: INFO: (17) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 8.057701ms)
    May  9 13:13:29.748: INFO: (17) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 8.080825ms)
    May  9 13:13:29.748: INFO: (17) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 8.105031ms)
    May  9 13:13:29.748: INFO: (17) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 8.129949ms)
    May  9 13:13:29.748: INFO: (17) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 8.155838ms)
    May  9 13:13:29.751: INFO: (18) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 3.273502ms)
    May  9 13:13:29.752: INFO: (18) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 3.720855ms)
    May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.187917ms)
    May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 4.887144ms)
    May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.077497ms)
    May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.373822ms)
    May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.231581ms)
    May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.520241ms)
    May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.596125ms)
    May  9 13:13:29.754: INFO: (18) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.329787ms)
    May  9 13:13:29.755: INFO: (18) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 5.942163ms)
    May  9 13:13:29.755: INFO: (18) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 6.259267ms)
    May  9 13:13:29.755: INFO: (18) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 6.148086ms)
    May  9 13:13:29.755: INFO: (18) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 6.524702ms)
    May  9 13:13:29.755: INFO: (18) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 6.105445ms)
    May  9 13:13:29.755: INFO: (18) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 6.545953ms)
    May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.622243ms)
    May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.606814ms)
    May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:460/proxy/: tls baz (200; 5.570305ms)
    May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">test<... (200; 5.719408ms)
    May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:160/proxy/: foo (200; 5.647421ms)
    May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:462/proxy/: tls qux (200; 5.757481ms)
    May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7/proxy/rewriteme">test</a> (200; 5.580314ms)
    May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/http:proxy-service-xmz2x-ckpq7:1080/proxy/rewriteme">... (200; 5.64656ms)
    May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/: <a href="/api/v1/namespaces/proxy-8664/pods/https:proxy-service-xmz2x-ckpq7:443/proxy/tlsrewritem... (200; 5.760618ms)
    May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/pods/proxy-service-xmz2x-ckpq7:162/proxy/: bar (200; 5.665365ms)
    May  9 13:13:29.761: INFO: (19) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname1/proxy/: foo (200; 6.559188ms)
    May  9 13:13:29.763: INFO: (19) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname1/proxy/: tls baz (200; 7.529595ms)
    May  9 13:13:29.763: INFO: (19) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname2/proxy/: bar (200; 7.620678ms)
    May  9 13:13:29.763: INFO: (19) /api/v1/namespaces/proxy-8664/services/proxy-service-xmz2x:portname1/proxy/: foo (200; 7.682836ms)
    May  9 13:13:29.763: INFO: (19) /api/v1/namespaces/proxy-8664/services/https:proxy-service-xmz2x:tlsportname2/proxy/: tls qux (200; 7.62129ms)
    May  9 13:13:29.763: INFO: (19) /api/v1/namespaces/proxy-8664/services/http:proxy-service-xmz2x:portname2/proxy/: bar (200; 7.798788ms)
    STEP: deleting ReplicationController proxy-service-xmz2x in namespace proxy-8664, will wait for the garbage collector to delete the pods 05/09/23 13:13:29.763
    May  9 13:13:29.822: INFO: Deleting ReplicationController proxy-service-xmz2x took: 5.836853ms
    May  9 13:13:29.923: INFO: Terminating ReplicationController proxy-service-xmz2x pods took: 100.628352ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    May  9 13:13:32.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-8664" for this suite. 05/09/23 13:13:32.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:13:32.54
May  9 13:13:32.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pods 05/09/23 13:13:32.54
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:32.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:32.558
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 05/09/23 13:13:32.559
STEP: submitting the pod to kubernetes 05/09/23 13:13:32.559
STEP: verifying QOS class is set on the pod 05/09/23 13:13:32.566
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
May  9 13:13:32.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3694" for this suite. 05/09/23 13:13:32.573
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":187,"skipped":3720,"failed":0}
------------------------------
• [0.039 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:13:32.54
    May  9 13:13:32.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pods 05/09/23 13:13:32.54
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:32.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:32.558
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 05/09/23 13:13:32.559
    STEP: submitting the pod to kubernetes 05/09/23 13:13:32.559
    STEP: verifying QOS class is set on the pod 05/09/23 13:13:32.566
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    May  9 13:13:32.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3694" for this suite. 05/09/23 13:13:32.573
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:13:32.579
May  9 13:13:32.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:13:32.58
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:32.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:32.597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:13:32.611
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:13:33.059
STEP: Deploying the webhook pod 05/09/23 13:13:33.066
STEP: Wait for the deployment to be ready 05/09/23 13:13:33.076
May  9 13:13:33.084: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 13:13:35.094
STEP: Verifying the service has paired with the endpoint 05/09/23 13:13:35.107
May  9 13:13:36.107: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
May  9 13:13:36.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4201-crds.webhook.example.com via the AdmissionRegistration API 05/09/23 13:13:36.626
STEP: Creating a custom resource that should be mutated by the webhook 05/09/23 13:13:36.639
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:13:39.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-390" for this suite. 05/09/23 13:13:39.205
STEP: Destroying namespace "webhook-390-markers" for this suite. 05/09/23 13:13:39.211
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":188,"skipped":3723,"failed":0}
------------------------------
• [SLOW TEST] [6.707 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:13:32.579
    May  9 13:13:32.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:13:32.58
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:32.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:32.597
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:13:32.611
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:13:33.059
    STEP: Deploying the webhook pod 05/09/23 13:13:33.066
    STEP: Wait for the deployment to be ready 05/09/23 13:13:33.076
    May  9 13:13:33.084: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 13:13:35.094
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:13:35.107
    May  9 13:13:36.107: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    May  9 13:13:36.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4201-crds.webhook.example.com via the AdmissionRegistration API 05/09/23 13:13:36.626
    STEP: Creating a custom resource that should be mutated by the webhook 05/09/23 13:13:36.639
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:13:39.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-390" for this suite. 05/09/23 13:13:39.205
    STEP: Destroying namespace "webhook-390-markers" for this suite. 05/09/23 13:13:39.211
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:13:39.287
May  9 13:13:39.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename daemonsets 05/09/23 13:13:39.287
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:39.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:39.303
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
May  9 13:13:39.331: INFO: Create a RollingUpdate DaemonSet
May  9 13:13:39.336: INFO: Check that daemon pods launch on every node of the cluster
May  9 13:13:39.341: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:39.341: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:39.341: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:39.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:13:39.346: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
May  9 13:13:40.352: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:40.352: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:40.352: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:40.355: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:13:40.355: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
May  9 13:13:41.351: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:41.351: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:41.351: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:41.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 13:13:41.354: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
May  9 13:13:41.354: INFO: Update the DaemonSet to trigger a rollout
May  9 13:13:41.363: INFO: Updating DaemonSet daemon-set
May  9 13:13:44.378: INFO: Roll back the DaemonSet before rollout is complete
May  9 13:13:44.385: INFO: Updating DaemonSet daemon-set
May  9 13:13:44.385: INFO: Make sure DaemonSet rollback is complete
May  9 13:13:44.390: INFO: Wrong image for pod: daemon-set-28zw9. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
May  9 13:13:44.390: INFO: Pod daemon-set-28zw9 is not available
May  9 13:13:44.399: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:44.399: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:44.399: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:45.407: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:45.407: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:45.407: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:46.408: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:46.408: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:46.408: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:47.406: INFO: Pod daemon-set-vq659 is not available
May  9 13:13:47.410: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:47.410: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:13:47.410: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/09/23 13:13:47.415
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7578, will wait for the garbage collector to delete the pods 05/09/23 13:13:47.415
May  9 13:13:47.474: INFO: Deleting DaemonSet.extensions daemon-set took: 5.582908ms
May  9 13:13:47.575: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.529408ms
May  9 13:13:49.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:13:49.578: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  9 13:13:49.580: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29821"},"items":null}

May  9 13:13:49.583: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29821"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  9 13:13:49.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7578" for this suite. 05/09/23 13:13:49.597
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":189,"skipped":3744,"failed":0}
------------------------------
• [SLOW TEST] [10.315 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:13:39.287
    May  9 13:13:39.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename daemonsets 05/09/23 13:13:39.287
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:39.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:39.303
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    May  9 13:13:39.331: INFO: Create a RollingUpdate DaemonSet
    May  9 13:13:39.336: INFO: Check that daemon pods launch on every node of the cluster
    May  9 13:13:39.341: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:39.341: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:39.341: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:39.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:13:39.346: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
    May  9 13:13:40.352: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:40.352: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:40.352: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:40.355: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:13:40.355: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
    May  9 13:13:41.351: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:41.351: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:41.351: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:41.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 13:13:41.354: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    May  9 13:13:41.354: INFO: Update the DaemonSet to trigger a rollout
    May  9 13:13:41.363: INFO: Updating DaemonSet daemon-set
    May  9 13:13:44.378: INFO: Roll back the DaemonSet before rollout is complete
    May  9 13:13:44.385: INFO: Updating DaemonSet daemon-set
    May  9 13:13:44.385: INFO: Make sure DaemonSet rollback is complete
    May  9 13:13:44.390: INFO: Wrong image for pod: daemon-set-28zw9. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    May  9 13:13:44.390: INFO: Pod daemon-set-28zw9 is not available
    May  9 13:13:44.399: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:44.399: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:44.399: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:45.407: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:45.407: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:45.407: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:46.408: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:46.408: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:46.408: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:47.406: INFO: Pod daemon-set-vq659 is not available
    May  9 13:13:47.410: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:47.410: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:13:47.410: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/09/23 13:13:47.415
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7578, will wait for the garbage collector to delete the pods 05/09/23 13:13:47.415
    May  9 13:13:47.474: INFO: Deleting DaemonSet.extensions daemon-set took: 5.582908ms
    May  9 13:13:47.575: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.529408ms
    May  9 13:13:49.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:13:49.578: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  9 13:13:49.580: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29821"},"items":null}

    May  9 13:13:49.583: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29821"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:13:49.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7578" for this suite. 05/09/23 13:13:49.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:13:49.603
May  9 13:13:49.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 13:13:49.603
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:49.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:49.624
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 05/09/23 13:13:49.626
May  9 13:13:49.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-7244 cluster-info'
May  9 13:13:49.678: INFO: stderr: ""
May  9 13:13:49.678: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 13:13:49.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7244" for this suite. 05/09/23 13:13:49.682
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":190,"skipped":3766,"failed":0}
------------------------------
• [0.085 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:13:49.603
    May  9 13:13:49.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 13:13:49.603
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:49.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:49.624
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 05/09/23 13:13:49.626
    May  9 13:13:49.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-7244 cluster-info'
    May  9 13:13:49.678: INFO: stderr: ""
    May  9 13:13:49.678: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 13:13:49.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7244" for this suite. 05/09/23 13:13:49.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:13:49.689
May  9 13:13:49.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename crd-watch 05/09/23 13:13:49.69
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:49.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:49.704
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
May  9 13:13:49.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Creating first CR  05/09/23 13:13:52.245
May  9 13:13:52.251: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T13:13:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T13:13:52Z]] name:name1 resourceVersion:29846 uid:45ff1a77-7022-47a5-90e3-bde5e1c464ad] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 05/09/23 13:14:02.252
May  9 13:14:02.257: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T13:14:02Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T13:14:02Z]] name:name2 resourceVersion:29920 uid:a9004d32-68d3-470c-8422-1c6f6c3a4d94] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 05/09/23 13:14:12.258
May  9 13:14:12.264: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T13:13:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T13:14:12Z]] name:name1 resourceVersion:29954 uid:45ff1a77-7022-47a5-90e3-bde5e1c464ad] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 05/09/23 13:14:22.265
May  9 13:14:22.271: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T13:14:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T13:14:22Z]] name:name2 resourceVersion:29993 uid:a9004d32-68d3-470c-8422-1c6f6c3a4d94] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 05/09/23 13:14:32.271
May  9 13:14:32.278: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T13:13:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T13:14:12Z]] name:name1 resourceVersion:30028 uid:45ff1a77-7022-47a5-90e3-bde5e1c464ad] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 05/09/23 13:14:42.278
May  9 13:14:42.285: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T13:14:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T13:14:22Z]] name:name2 resourceVersion:30063 uid:a9004d32-68d3-470c-8422-1c6f6c3a4d94] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:14:52.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2527" for this suite. 05/09/23 13:14:52.802
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":191,"skipped":3788,"failed":0}
------------------------------
• [SLOW TEST] [63.119 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:13:49.689
    May  9 13:13:49.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename crd-watch 05/09/23 13:13:49.69
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:13:49.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:13:49.704
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    May  9 13:13:49.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Creating first CR  05/09/23 13:13:52.245
    May  9 13:13:52.251: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T13:13:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T13:13:52Z]] name:name1 resourceVersion:29846 uid:45ff1a77-7022-47a5-90e3-bde5e1c464ad] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 05/09/23 13:14:02.252
    May  9 13:14:02.257: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T13:14:02Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T13:14:02Z]] name:name2 resourceVersion:29920 uid:a9004d32-68d3-470c-8422-1c6f6c3a4d94] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 05/09/23 13:14:12.258
    May  9 13:14:12.264: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T13:13:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T13:14:12Z]] name:name1 resourceVersion:29954 uid:45ff1a77-7022-47a5-90e3-bde5e1c464ad] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 05/09/23 13:14:22.265
    May  9 13:14:22.271: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T13:14:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T13:14:22Z]] name:name2 resourceVersion:29993 uid:a9004d32-68d3-470c-8422-1c6f6c3a4d94] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 05/09/23 13:14:32.271
    May  9 13:14:32.278: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T13:13:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T13:14:12Z]] name:name1 resourceVersion:30028 uid:45ff1a77-7022-47a5-90e3-bde5e1c464ad] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 05/09/23 13:14:42.278
    May  9 13:14:42.285: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T13:14:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T13:14:22Z]] name:name2 resourceVersion:30063 uid:a9004d32-68d3-470c-8422-1c6f6c3a4d94] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:14:52.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-2527" for this suite. 05/09/23 13:14:52.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:14:52.809
May  9 13:14:52.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:14:52.81
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:14:52.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:14:52.825
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:14:52.842
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:14:53.413
STEP: Deploying the webhook pod 05/09/23 13:14:53.42
STEP: Wait for the deployment to be ready 05/09/23 13:14:53.434
May  9 13:14:53.443: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 13:14:55.452
STEP: Verifying the service has paired with the endpoint 05/09/23 13:14:55.466
May  9 13:14:56.466: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/09/23 13:14:56.47
STEP: create a pod that should be updated by the webhook 05/09/23 13:14:56.482
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:14:56.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9924" for this suite. 05/09/23 13:14:56.507
STEP: Destroying namespace "webhook-9924-markers" for this suite. 05/09/23 13:14:56.513
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":192,"skipped":3811,"failed":0}
------------------------------
• [3.756 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:14:52.809
    May  9 13:14:52.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:14:52.81
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:14:52.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:14:52.825
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:14:52.842
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:14:53.413
    STEP: Deploying the webhook pod 05/09/23 13:14:53.42
    STEP: Wait for the deployment to be ready 05/09/23 13:14:53.434
    May  9 13:14:53.443: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 13:14:55.452
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:14:55.466
    May  9 13:14:56.466: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/09/23 13:14:56.47
    STEP: create a pod that should be updated by the webhook 05/09/23 13:14:56.482
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:14:56.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9924" for this suite. 05/09/23 13:14:56.507
    STEP: Destroying namespace "webhook-9924-markers" for this suite. 05/09/23 13:14:56.513
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:14:56.566
May  9 13:14:56.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:14:56.566
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:14:56.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:14:56.581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:14:56.596
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:14:56.921
STEP: Deploying the webhook pod 05/09/23 13:14:56.926
STEP: Wait for the deployment to be ready 05/09/23 13:14:56.937
May  9 13:14:56.946: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 13:14:58.958
STEP: Verifying the service has paired with the endpoint 05/09/23 13:14:58.977
May  9 13:14:59.977: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 05/09/23 13:14:59.98
May  9 13:14:59.997: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook 05/09/23 13:15:00.105
May  9 13:15:00.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:15:00.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7415" for this suite. 05/09/23 13:15:00.121
STEP: Destroying namespace "webhook-7415-markers" for this suite. 05/09/23 13:15:00.127
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":193,"skipped":3811,"failed":0}
------------------------------
• [3.636 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:14:56.566
    May  9 13:14:56.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:14:56.566
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:14:56.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:14:56.581
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:14:56.596
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:14:56.921
    STEP: Deploying the webhook pod 05/09/23 13:14:56.926
    STEP: Wait for the deployment to be ready 05/09/23 13:14:56.937
    May  9 13:14:56.946: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 13:14:58.958
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:14:58.977
    May  9 13:14:59.977: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 05/09/23 13:14:59.98
    May  9 13:14:59.997: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a custom resource definition that should be denied by the webhook 05/09/23 13:15:00.105
    May  9 13:15:00.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:15:00.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7415" for this suite. 05/09/23 13:15:00.121
    STEP: Destroying namespace "webhook-7415-markers" for this suite. 05/09/23 13:15:00.127
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:15:00.201
May  9 13:15:00.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:15:00.202
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:00.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:00.217
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:15:00.229
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:15:00.551
STEP: Deploying the webhook pod 05/09/23 13:15:00.556
STEP: Wait for the deployment to be ready 05/09/23 13:15:00.566
May  9 13:15:00.572: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 13:15:02.581
STEP: Verifying the service has paired with the endpoint 05/09/23 13:15:02.595
May  9 13:15:03.596: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 05/09/23 13:15:03.6
STEP: create a pod 05/09/23 13:15:03.611
May  9 13:15:03.618: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7876" to be "running"
May  9 13:15:03.621: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.291482ms
May  9 13:15:05.625: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006799065s
May  9 13:15:05.625: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 05/09/23 13:15:05.625
May  9 13:15:05.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=webhook-7876 attach --namespace=webhook-7876 to-be-attached-pod -i -c=container1'
May  9 13:15:05.688: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:15:05.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7876" for this suite. 05/09/23 13:15:05.699
STEP: Destroying namespace "webhook-7876-markers" for this suite. 05/09/23 13:15:05.705
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":194,"skipped":3821,"failed":0}
------------------------------
• [SLOW TEST] [5.549 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:15:00.201
    May  9 13:15:00.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:15:00.202
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:00.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:00.217
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:15:00.229
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:15:00.551
    STEP: Deploying the webhook pod 05/09/23 13:15:00.556
    STEP: Wait for the deployment to be ready 05/09/23 13:15:00.566
    May  9 13:15:00.572: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 13:15:02.581
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:15:02.595
    May  9 13:15:03.596: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 05/09/23 13:15:03.6
    STEP: create a pod 05/09/23 13:15:03.611
    May  9 13:15:03.618: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7876" to be "running"
    May  9 13:15:03.621: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.291482ms
    May  9 13:15:05.625: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006799065s
    May  9 13:15:05.625: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 05/09/23 13:15:05.625
    May  9 13:15:05.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=webhook-7876 attach --namespace=webhook-7876 to-be-attached-pod -i -c=container1'
    May  9 13:15:05.688: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:15:05.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7876" for this suite. 05/09/23 13:15:05.699
    STEP: Destroying namespace "webhook-7876-markers" for this suite. 05/09/23 13:15:05.705
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:15:05.752
May  9 13:15:05.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:15:05.753
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:05.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:05.77
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:15:05.784
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:15:06.051
STEP: Deploying the webhook pod 05/09/23 13:15:06.055
STEP: Wait for the deployment to be ready 05/09/23 13:15:06.066
May  9 13:15:06.078: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 13:15:08.087
STEP: Verifying the service has paired with the endpoint 05/09/23 13:15:08.103
May  9 13:15:09.104: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/09/23 13:15:09.107
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/09/23 13:15:09.121
STEP: Creating a dummy validating-webhook-configuration object 05/09/23 13:15:09.133
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/09/23 13:15:09.141
STEP: Creating a dummy mutating-webhook-configuration object 05/09/23 13:15:09.148
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/09/23 13:15:09.155
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:15:09.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3299" for this suite. 05/09/23 13:15:09.177
STEP: Destroying namespace "webhook-3299-markers" for this suite. 05/09/23 13:15:09.182
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":195,"skipped":3846,"failed":0}
------------------------------
• [3.476 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:15:05.752
    May  9 13:15:05.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:15:05.753
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:05.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:05.77
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:15:05.784
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:15:06.051
    STEP: Deploying the webhook pod 05/09/23 13:15:06.055
    STEP: Wait for the deployment to be ready 05/09/23 13:15:06.066
    May  9 13:15:06.078: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 13:15:08.087
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:15:08.103
    May  9 13:15:09.104: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/09/23 13:15:09.107
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/09/23 13:15:09.121
    STEP: Creating a dummy validating-webhook-configuration object 05/09/23 13:15:09.133
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/09/23 13:15:09.141
    STEP: Creating a dummy mutating-webhook-configuration object 05/09/23 13:15:09.148
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/09/23 13:15:09.155
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:15:09.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3299" for this suite. 05/09/23 13:15:09.177
    STEP: Destroying namespace "webhook-3299-markers" for this suite. 05/09/23 13:15:09.182
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:15:09.23
May  9 13:15:09.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubelet-test 05/09/23 13:15:09.231
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:09.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:09.245
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
May  9 13:15:09.253: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs2eb4d495-668b-4792-8119-f4d1720699cf" in namespace "kubelet-test-9261" to be "running and ready"
May  9 13:15:09.257: INFO: Pod "busybox-readonly-fs2eb4d495-668b-4792-8119-f4d1720699cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.127158ms
May  9 13:15:09.257: INFO: The phase of Pod busybox-readonly-fs2eb4d495-668b-4792-8119-f4d1720699cf is Pending, waiting for it to be Running (with Ready = true)
May  9 13:15:11.261: INFO: Pod "busybox-readonly-fs2eb4d495-668b-4792-8119-f4d1720699cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.00868356s
May  9 13:15:11.261: INFO: The phase of Pod busybox-readonly-fs2eb4d495-668b-4792-8119-f4d1720699cf is Running (Ready = true)
May  9 13:15:11.261: INFO: Pod "busybox-readonly-fs2eb4d495-668b-4792-8119-f4d1720699cf" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  9 13:15:11.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9261" for this suite. 05/09/23 13:15:11.279
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":196,"skipped":3894,"failed":0}
------------------------------
• [2.062 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:15:09.23
    May  9 13:15:09.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubelet-test 05/09/23 13:15:09.231
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:09.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:09.245
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    May  9 13:15:09.253: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs2eb4d495-668b-4792-8119-f4d1720699cf" in namespace "kubelet-test-9261" to be "running and ready"
    May  9 13:15:09.257: INFO: Pod "busybox-readonly-fs2eb4d495-668b-4792-8119-f4d1720699cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.127158ms
    May  9 13:15:09.257: INFO: The phase of Pod busybox-readonly-fs2eb4d495-668b-4792-8119-f4d1720699cf is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:15:11.261: INFO: Pod "busybox-readonly-fs2eb4d495-668b-4792-8119-f4d1720699cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.00868356s
    May  9 13:15:11.261: INFO: The phase of Pod busybox-readonly-fs2eb4d495-668b-4792-8119-f4d1720699cf is Running (Ready = true)
    May  9 13:15:11.261: INFO: Pod "busybox-readonly-fs2eb4d495-668b-4792-8119-f4d1720699cf" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  9 13:15:11.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9261" for this suite. 05/09/23 13:15:11.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:15:11.294
May  9 13:15:11.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:15:11.294
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:11.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:11.309
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:15:11.321
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:15:11.59
STEP: Deploying the webhook pod 05/09/23 13:15:11.594
STEP: Wait for the deployment to be ready 05/09/23 13:15:11.604
May  9 13:15:11.613: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 13:15:13.621
STEP: Verifying the service has paired with the endpoint 05/09/23 13:15:13.635
May  9 13:15:14.635: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
May  9 13:15:14.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5828-crds.webhook.example.com via the AdmissionRegistration API 05/09/23 13:15:15.147
STEP: Creating a custom resource while v1 is storage version 05/09/23 13:15:15.16
STEP: Patching Custom Resource Definition to set v2 as storage 05/09/23 13:15:17.206
STEP: Patching the custom resource while v2 is storage version 05/09/23 13:15:17.229
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:15:17.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2257" for this suite. 05/09/23 13:15:17.799
STEP: Destroying namespace "webhook-2257-markers" for this suite. 05/09/23 13:15:17.805
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":197,"skipped":3974,"failed":0}
------------------------------
• [SLOW TEST] [6.569 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:15:11.294
    May  9 13:15:11.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:15:11.294
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:11.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:11.309
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:15:11.321
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:15:11.59
    STEP: Deploying the webhook pod 05/09/23 13:15:11.594
    STEP: Wait for the deployment to be ready 05/09/23 13:15:11.604
    May  9 13:15:11.613: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 13:15:13.621
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:15:13.635
    May  9 13:15:14.635: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    May  9 13:15:14.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5828-crds.webhook.example.com via the AdmissionRegistration API 05/09/23 13:15:15.147
    STEP: Creating a custom resource while v1 is storage version 05/09/23 13:15:15.16
    STEP: Patching Custom Resource Definition to set v2 as storage 05/09/23 13:15:17.206
    STEP: Patching the custom resource while v2 is storage version 05/09/23 13:15:17.229
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:15:17.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2257" for this suite. 05/09/23 13:15:17.799
    STEP: Destroying namespace "webhook-2257-markers" for this suite. 05/09/23 13:15:17.805
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:15:17.864
May  9 13:15:17.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir-wrapper 05/09/23 13:15:17.865
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:17.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:17.896
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
May  9 13:15:17.919: INFO: Waiting up to 5m0s for pod "pod-secrets-2bc98fff-7990-4855-adbd-083a9fcaee51" in namespace "emptydir-wrapper-6201" to be "running and ready"
May  9 13:15:17.928: INFO: Pod "pod-secrets-2bc98fff-7990-4855-adbd-083a9fcaee51": Phase="Pending", Reason="", readiness=false. Elapsed: 8.895976ms
May  9 13:15:17.928: INFO: The phase of Pod pod-secrets-2bc98fff-7990-4855-adbd-083a9fcaee51 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:15:19.932: INFO: Pod "pod-secrets-2bc98fff-7990-4855-adbd-083a9fcaee51": Phase="Running", Reason="", readiness=true. Elapsed: 2.013100491s
May  9 13:15:19.932: INFO: The phase of Pod pod-secrets-2bc98fff-7990-4855-adbd-083a9fcaee51 is Running (Ready = true)
May  9 13:15:19.932: INFO: Pod "pod-secrets-2bc98fff-7990-4855-adbd-083a9fcaee51" satisfied condition "running and ready"
STEP: Cleaning up the secret 05/09/23 13:15:19.936
STEP: Cleaning up the configmap 05/09/23 13:15:19.942
STEP: Cleaning up the pod 05/09/23 13:15:19.948
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
May  9 13:15:19.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6201" for this suite. 05/09/23 13:15:19.962
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":198,"skipped":4001,"failed":0}
------------------------------
• [2.102 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:15:17.864
    May  9 13:15:17.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir-wrapper 05/09/23 13:15:17.865
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:17.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:17.896
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    May  9 13:15:17.919: INFO: Waiting up to 5m0s for pod "pod-secrets-2bc98fff-7990-4855-adbd-083a9fcaee51" in namespace "emptydir-wrapper-6201" to be "running and ready"
    May  9 13:15:17.928: INFO: Pod "pod-secrets-2bc98fff-7990-4855-adbd-083a9fcaee51": Phase="Pending", Reason="", readiness=false. Elapsed: 8.895976ms
    May  9 13:15:17.928: INFO: The phase of Pod pod-secrets-2bc98fff-7990-4855-adbd-083a9fcaee51 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:15:19.932: INFO: Pod "pod-secrets-2bc98fff-7990-4855-adbd-083a9fcaee51": Phase="Running", Reason="", readiness=true. Elapsed: 2.013100491s
    May  9 13:15:19.932: INFO: The phase of Pod pod-secrets-2bc98fff-7990-4855-adbd-083a9fcaee51 is Running (Ready = true)
    May  9 13:15:19.932: INFO: Pod "pod-secrets-2bc98fff-7990-4855-adbd-083a9fcaee51" satisfied condition "running and ready"
    STEP: Cleaning up the secret 05/09/23 13:15:19.936
    STEP: Cleaning up the configmap 05/09/23 13:15:19.942
    STEP: Cleaning up the pod 05/09/23 13:15:19.948
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    May  9 13:15:19.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-6201" for this suite. 05/09/23 13:15:19.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:15:19.967
May  9 13:15:19.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename svcaccounts 05/09/23 13:15:19.968
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:19.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:19.994
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
May  9 13:15:19.998: INFO: Got root ca configmap in namespace "svcaccounts-6030"
May  9 13:15:20.003: INFO: Deleted root ca configmap in namespace "svcaccounts-6030"
STEP: waiting for a new root ca configmap created 05/09/23 13:15:20.503
May  9 13:15:20.506: INFO: Recreated root ca configmap in namespace "svcaccounts-6030"
May  9 13:15:20.511: INFO: Updated root ca configmap in namespace "svcaccounts-6030"
STEP: waiting for the root ca configmap reconciled 05/09/23 13:15:21.012
May  9 13:15:21.017: INFO: Reconciled root ca configmap in namespace "svcaccounts-6030"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  9 13:15:21.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6030" for this suite. 05/09/23 13:15:21.021
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":199,"skipped":4017,"failed":0}
------------------------------
• [1.061 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:15:19.967
    May  9 13:15:19.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 13:15:19.968
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:19.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:19.994
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    May  9 13:15:19.998: INFO: Got root ca configmap in namespace "svcaccounts-6030"
    May  9 13:15:20.003: INFO: Deleted root ca configmap in namespace "svcaccounts-6030"
    STEP: waiting for a new root ca configmap created 05/09/23 13:15:20.503
    May  9 13:15:20.506: INFO: Recreated root ca configmap in namespace "svcaccounts-6030"
    May  9 13:15:20.511: INFO: Updated root ca configmap in namespace "svcaccounts-6030"
    STEP: waiting for the root ca configmap reconciled 05/09/23 13:15:21.012
    May  9 13:15:21.017: INFO: Reconciled root ca configmap in namespace "svcaccounts-6030"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  9 13:15:21.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6030" for this suite. 05/09/23 13:15:21.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:15:21.029
May  9 13:15:21.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename cronjob 05/09/23 13:15:21.03
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:21.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:21.045
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 05/09/23 13:15:21.046
STEP: Ensuring no jobs are scheduled 05/09/23 13:15:21.051
STEP: Ensuring no job exists by listing jobs explicitly 05/09/23 13:20:21.058
STEP: Removing cronjob 05/09/23 13:20:21.061
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  9 13:20:21.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5673" for this suite. 05/09/23 13:20:21.07
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":200,"skipped":4027,"failed":0}
------------------------------
• [SLOW TEST] [300.046 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:15:21.029
    May  9 13:15:21.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename cronjob 05/09/23 13:15:21.03
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:15:21.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:15:21.045
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 05/09/23 13:15:21.046
    STEP: Ensuring no jobs are scheduled 05/09/23 13:15:21.051
    STEP: Ensuring no job exists by listing jobs explicitly 05/09/23 13:20:21.058
    STEP: Removing cronjob 05/09/23 13:20:21.061
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  9 13:20:21.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5673" for this suite. 05/09/23 13:20:21.07
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:20:21.076
May  9 13:20:21.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 13:20:21.076
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:20:21.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:20:21.091
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
May  9 13:20:21.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:20:21.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9786" for this suite. 05/09/23 13:20:21.635
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":201,"skipped":4027,"failed":0}
------------------------------
• [0.567 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:20:21.076
    May  9 13:20:21.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 13:20:21.076
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:20:21.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:20:21.091
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    May  9 13:20:21.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:20:21.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9786" for this suite. 05/09/23 13:20:21.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:20:21.644
May  9 13:20:21.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename subpath 05/09/23 13:20:21.644
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:20:21.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:20:21.659
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/09/23 13:20:21.661
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-6m67 05/09/23 13:20:21.668
STEP: Creating a pod to test atomic-volume-subpath 05/09/23 13:20:21.668
May  9 13:20:21.676: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6m67" in namespace "subpath-78" to be "Succeeded or Failed"
May  9 13:20:21.681: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.90897ms
May  9 13:20:23.684: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 2.008524933s
May  9 13:20:25.686: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 4.009697291s
May  9 13:20:27.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 6.009290938s
May  9 13:20:29.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 8.009158666s
May  9 13:20:31.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 10.008995322s
May  9 13:20:33.686: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 12.009656425s
May  9 13:20:35.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 14.008726806s
May  9 13:20:37.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 16.00923427s
May  9 13:20:39.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 18.009120569s
May  9 13:20:41.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 20.008607724s
May  9 13:20:43.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=false. Elapsed: 22.008620156s
May  9 13:20:45.686: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009887756s
STEP: Saw pod success 05/09/23 13:20:45.686
May  9 13:20:45.686: INFO: Pod "pod-subpath-test-secret-6m67" satisfied condition "Succeeded or Failed"
May  9 13:20:45.688: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-subpath-test-secret-6m67 container test-container-subpath-secret-6m67: <nil>
STEP: delete the pod 05/09/23 13:20:45.702
May  9 13:20:45.717: INFO: Waiting for pod pod-subpath-test-secret-6m67 to disappear
May  9 13:20:45.719: INFO: Pod pod-subpath-test-secret-6m67 no longer exists
STEP: Deleting pod pod-subpath-test-secret-6m67 05/09/23 13:20:45.719
May  9 13:20:45.719: INFO: Deleting pod "pod-subpath-test-secret-6m67" in namespace "subpath-78"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  9 13:20:45.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-78" for this suite. 05/09/23 13:20:45.726
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":202,"skipped":4044,"failed":0}
------------------------------
• [SLOW TEST] [24.087 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:20:21.644
    May  9 13:20:21.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename subpath 05/09/23 13:20:21.644
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:20:21.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:20:21.659
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/09/23 13:20:21.661
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-6m67 05/09/23 13:20:21.668
    STEP: Creating a pod to test atomic-volume-subpath 05/09/23 13:20:21.668
    May  9 13:20:21.676: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6m67" in namespace "subpath-78" to be "Succeeded or Failed"
    May  9 13:20:21.681: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.90897ms
    May  9 13:20:23.684: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 2.008524933s
    May  9 13:20:25.686: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 4.009697291s
    May  9 13:20:27.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 6.009290938s
    May  9 13:20:29.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 8.009158666s
    May  9 13:20:31.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 10.008995322s
    May  9 13:20:33.686: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 12.009656425s
    May  9 13:20:35.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 14.008726806s
    May  9 13:20:37.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 16.00923427s
    May  9 13:20:39.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 18.009120569s
    May  9 13:20:41.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=true. Elapsed: 20.008607724s
    May  9 13:20:43.685: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Running", Reason="", readiness=false. Elapsed: 22.008620156s
    May  9 13:20:45.686: INFO: Pod "pod-subpath-test-secret-6m67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009887756s
    STEP: Saw pod success 05/09/23 13:20:45.686
    May  9 13:20:45.686: INFO: Pod "pod-subpath-test-secret-6m67" satisfied condition "Succeeded or Failed"
    May  9 13:20:45.688: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-subpath-test-secret-6m67 container test-container-subpath-secret-6m67: <nil>
    STEP: delete the pod 05/09/23 13:20:45.702
    May  9 13:20:45.717: INFO: Waiting for pod pod-subpath-test-secret-6m67 to disappear
    May  9 13:20:45.719: INFO: Pod pod-subpath-test-secret-6m67 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-6m67 05/09/23 13:20:45.719
    May  9 13:20:45.719: INFO: Deleting pod "pod-subpath-test-secret-6m67" in namespace "subpath-78"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  9 13:20:45.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-78" for this suite. 05/09/23 13:20:45.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:20:45.732
May  9 13:20:45.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename endpointslicemirroring 05/09/23 13:20:45.732
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:20:45.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:20:45.746
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 05/09/23 13:20:45.764
May  9 13:20:45.770: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 05/09/23 13:20:47.774
May  9 13:20:47.782: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 05/09/23 13:20:49.787
May  9 13:20:49.795: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
May  9 13:20:51.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-7047" for this suite. 05/09/23 13:20:51.804
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":203,"skipped":4059,"failed":0}
------------------------------
• [SLOW TEST] [6.079 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:20:45.732
    May  9 13:20:45.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename endpointslicemirroring 05/09/23 13:20:45.732
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:20:45.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:20:45.746
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 05/09/23 13:20:45.764
    May  9 13:20:45.770: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 05/09/23 13:20:47.774
    May  9 13:20:47.782: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 05/09/23 13:20:49.787
    May  9 13:20:49.795: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    May  9 13:20:51.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-7047" for this suite. 05/09/23 13:20:51.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:20:51.811
May  9 13:20:51.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 13:20:51.812
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:20:51.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:20:51.826
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:20:51.828
May  9 13:20:51.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219" in namespace "downward-api-5593" to be "Succeeded or Failed"
May  9 13:20:51.846: INFO: Pod "downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219": Phase="Pending", Reason="", readiness=false. Elapsed: 7.455189ms
May  9 13:20:53.849: INFO: Pod "downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010446247s
May  9 13:20:55.850: INFO: Pod "downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011256501s
STEP: Saw pod success 05/09/23 13:20:55.85
May  9 13:20:55.850: INFO: Pod "downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219" satisfied condition "Succeeded or Failed"
May  9 13:20:55.853: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219 container client-container: <nil>
STEP: delete the pod 05/09/23 13:20:55.859
May  9 13:20:55.869: INFO: Waiting for pod downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219 to disappear
May  9 13:20:55.871: INFO: Pod downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  9 13:20:55.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5593" for this suite. 05/09/23 13:20:55.875
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":204,"skipped":4071,"failed":0}
------------------------------
• [4.071 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:20:51.811
    May  9 13:20:51.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 13:20:51.812
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:20:51.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:20:51.826
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:20:51.828
    May  9 13:20:51.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219" in namespace "downward-api-5593" to be "Succeeded or Failed"
    May  9 13:20:51.846: INFO: Pod "downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219": Phase="Pending", Reason="", readiness=false. Elapsed: 7.455189ms
    May  9 13:20:53.849: INFO: Pod "downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010446247s
    May  9 13:20:55.850: INFO: Pod "downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011256501s
    STEP: Saw pod success 05/09/23 13:20:55.85
    May  9 13:20:55.850: INFO: Pod "downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219" satisfied condition "Succeeded or Failed"
    May  9 13:20:55.853: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219 container client-container: <nil>
    STEP: delete the pod 05/09/23 13:20:55.859
    May  9 13:20:55.869: INFO: Waiting for pod downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219 to disappear
    May  9 13:20:55.871: INFO: Pod downwardapi-volume-d61fa2a4-d923-479f-944f-a0125ca37219 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  9 13:20:55.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5593" for this suite. 05/09/23 13:20:55.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:20:55.883
May  9 13:20:55.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename sched-preemption 05/09/23 13:20:55.884
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:20:55.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:20:55.899
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May  9 13:20:55.915: INFO: Waiting up to 1m0s for all nodes to be ready
May  9 13:21:55.950: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:21:55.953
May  9 13:21:55.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename sched-preemption-path 05/09/23 13:21:55.953
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:21:55.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:21:55.969
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 05/09/23 13:21:55.971
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/09/23 13:21:55.971
May  9 13:21:55.978: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2759" to be "running"
May  9 13:21:55.984: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.659639ms
May  9 13:21:57.987: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009590673s
May  9 13:21:57.987: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/09/23 13:21:57.99
May  9 13:21:58.005: INFO: found a healthy node: cl-gks-cncf-ix1-md-0-48ljh
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
May  9 13:22:06.084: INFO: pods created so far: [1 1 1]
May  9 13:22:06.084: INFO: length of pods created so far: 3
May  9 13:22:08.095: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
May  9 13:22:15.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2759" for this suite. 05/09/23 13:22:15.101
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May  9 13:22:15.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9877" for this suite. 05/09/23 13:22:15.138
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":205,"skipped":4093,"failed":0}
------------------------------
• [SLOW TEST] [79.295 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:20:55.883
    May  9 13:20:55.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename sched-preemption 05/09/23 13:20:55.884
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:20:55.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:20:55.899
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May  9 13:20:55.915: INFO: Waiting up to 1m0s for all nodes to be ready
    May  9 13:21:55.950: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:21:55.953
    May  9 13:21:55.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename sched-preemption-path 05/09/23 13:21:55.953
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:21:55.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:21:55.969
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 05/09/23 13:21:55.971
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/09/23 13:21:55.971
    May  9 13:21:55.978: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2759" to be "running"
    May  9 13:21:55.984: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.659639ms
    May  9 13:21:57.987: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009590673s
    May  9 13:21:57.987: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/09/23 13:21:57.99
    May  9 13:21:58.005: INFO: found a healthy node: cl-gks-cncf-ix1-md-0-48ljh
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    May  9 13:22:06.084: INFO: pods created so far: [1 1 1]
    May  9 13:22:06.084: INFO: length of pods created so far: 3
    May  9 13:22:08.095: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    May  9 13:22:15.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-2759" for this suite. 05/09/23 13:22:15.101
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:22:15.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9877" for this suite. 05/09/23 13:22:15.138
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:22:15.184
May  9 13:22:15.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 13:22:15.185
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:22:15.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:22:15.2
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 05/09/23 13:22:15.202
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/09/23 13:22:15.203
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/09/23 13:22:15.203
STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/09/23 13:22:15.203
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/09/23 13:22:15.203
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/09/23 13:22:15.203
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/09/23 13:22:15.204
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:22:15.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2264" for this suite. 05/09/23 13:22:15.208
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":206,"skipped":4153,"failed":0}
------------------------------
• [0.032 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:22:15.184
    May  9 13:22:15.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 13:22:15.185
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:22:15.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:22:15.2
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 05/09/23 13:22:15.202
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/09/23 13:22:15.203
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/09/23 13:22:15.203
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/09/23 13:22:15.203
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/09/23 13:22:15.203
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/09/23 13:22:15.203
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/09/23 13:22:15.204
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:22:15.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2264" for this suite. 05/09/23 13:22:15.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:22:15.217
May  9 13:22:15.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:22:15.218
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:22:15.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:22:15.234
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
May  9 13:22:15.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/09/23 13:22:17.71
May  9 13:22:17.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-7438 --namespace=crd-publish-openapi-7438 create -f -'
May  9 13:22:18.205: INFO: stderr: ""
May  9 13:22:18.205: INFO: stdout: "e2e-test-crd-publish-openapi-3748-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  9 13:22:18.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-7438 --namespace=crd-publish-openapi-7438 delete e2e-test-crd-publish-openapi-3748-crds test-cr'
May  9 13:22:18.279: INFO: stderr: ""
May  9 13:22:18.279: INFO: stdout: "e2e-test-crd-publish-openapi-3748-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May  9 13:22:18.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-7438 --namespace=crd-publish-openapi-7438 apply -f -'
May  9 13:22:18.410: INFO: stderr: ""
May  9 13:22:18.410: INFO: stdout: "e2e-test-crd-publish-openapi-3748-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  9 13:22:18.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-7438 --namespace=crd-publish-openapi-7438 delete e2e-test-crd-publish-openapi-3748-crds test-cr'
May  9 13:22:18.469: INFO: stderr: ""
May  9 13:22:18.469: INFO: stdout: "e2e-test-crd-publish-openapi-3748-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/09/23 13:22:18.469
May  9 13:22:18.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-7438 explain e2e-test-crd-publish-openapi-3748-crds'
May  9 13:22:18.593: INFO: stderr: ""
May  9 13:22:18.593: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3748-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:22:21.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7438" for this suite. 05/09/23 13:22:21.036
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":207,"skipped":4167,"failed":0}
------------------------------
• [SLOW TEST] [5.825 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:22:15.217
    May  9 13:22:15.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:22:15.218
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:22:15.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:22:15.234
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    May  9 13:22:15.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/09/23 13:22:17.71
    May  9 13:22:17.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-7438 --namespace=crd-publish-openapi-7438 create -f -'
    May  9 13:22:18.205: INFO: stderr: ""
    May  9 13:22:18.205: INFO: stdout: "e2e-test-crd-publish-openapi-3748-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May  9 13:22:18.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-7438 --namespace=crd-publish-openapi-7438 delete e2e-test-crd-publish-openapi-3748-crds test-cr'
    May  9 13:22:18.279: INFO: stderr: ""
    May  9 13:22:18.279: INFO: stdout: "e2e-test-crd-publish-openapi-3748-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    May  9 13:22:18.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-7438 --namespace=crd-publish-openapi-7438 apply -f -'
    May  9 13:22:18.410: INFO: stderr: ""
    May  9 13:22:18.410: INFO: stdout: "e2e-test-crd-publish-openapi-3748-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May  9 13:22:18.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-7438 --namespace=crd-publish-openapi-7438 delete e2e-test-crd-publish-openapi-3748-crds test-cr'
    May  9 13:22:18.469: INFO: stderr: ""
    May  9 13:22:18.469: INFO: stdout: "e2e-test-crd-publish-openapi-3748-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/09/23 13:22:18.469
    May  9 13:22:18.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-7438 explain e2e-test-crd-publish-openapi-3748-crds'
    May  9 13:22:18.593: INFO: stderr: ""
    May  9 13:22:18.593: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3748-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:22:21.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7438" for this suite. 05/09/23 13:22:21.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:22:21.043
May  9 13:22:21.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename gc 05/09/23 13:22:21.043
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:22:21.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:22:21.055
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 05/09/23 13:22:21.057
STEP: delete the rc 05/09/23 13:22:26.063
STEP: wait for all pods to be garbage collected 05/09/23 13:22:26.068
STEP: Gathering metrics 05/09/23 13:22:31.073
May  9 13:22:31.093: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" in namespace "kube-system" to be "running and ready"
May  9 13:22:31.095: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv": Phase="Running", Reason="", readiness=true. Elapsed: 1.760601ms
May  9 13:22:31.095: INFO: The phase of Pod kube-controller-manager-cl-gks-cncf-control-plane-gv9sv is Running (Ready = true)
May  9 13:22:31.095: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" satisfied condition "running and ready"
May  9 13:22:31.131: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  9 13:22:31.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5475" for this suite. 05/09/23 13:22:31.134
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":208,"skipped":4201,"failed":0}
------------------------------
• [SLOW TEST] [10.096 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:22:21.043
    May  9 13:22:21.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename gc 05/09/23 13:22:21.043
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:22:21.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:22:21.055
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 05/09/23 13:22:21.057
    STEP: delete the rc 05/09/23 13:22:26.063
    STEP: wait for all pods to be garbage collected 05/09/23 13:22:26.068
    STEP: Gathering metrics 05/09/23 13:22:31.073
    May  9 13:22:31.093: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" in namespace "kube-system" to be "running and ready"
    May  9 13:22:31.095: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv": Phase="Running", Reason="", readiness=true. Elapsed: 1.760601ms
    May  9 13:22:31.095: INFO: The phase of Pod kube-controller-manager-cl-gks-cncf-control-plane-gv9sv is Running (Ready = true)
    May  9 13:22:31.095: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" satisfied condition "running and ready"
    May  9 13:22:31.131: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  9 13:22:31.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5475" for this suite. 05/09/23 13:22:31.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:22:31.14
May  9 13:22:31.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:22:31.141
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:22:31.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:22:31.153
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-3019 05/09/23 13:22:31.154
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3019 to expose endpoints map[] 05/09/23 13:22:31.165
May  9 13:22:31.168: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May  9 13:22:32.172: INFO: successfully validated that service multi-endpoint-test in namespace services-3019 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3019 05/09/23 13:22:32.172
May  9 13:22:32.177: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3019" to be "running and ready"
May  9 13:22:32.179: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.571502ms
May  9 13:22:32.179: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:22:34.182: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004969097s
May  9 13:22:34.182: INFO: The phase of Pod pod1 is Running (Ready = true)
May  9 13:22:34.182: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3019 to expose endpoints map[pod1:[100]] 05/09/23 13:22:34.184
May  9 13:22:34.190: INFO: successfully validated that service multi-endpoint-test in namespace services-3019 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3019 05/09/23 13:22:34.19
May  9 13:22:34.197: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3019" to be "running and ready"
May  9 13:22:34.200: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.872377ms
May  9 13:22:34.200: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:22:36.202: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005514921s
May  9 13:22:36.202: INFO: The phase of Pod pod2 is Running (Ready = true)
May  9 13:22:36.202: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3019 to expose endpoints map[pod1:[100] pod2:[101]] 05/09/23 13:22:36.205
May  9 13:22:36.213: INFO: successfully validated that service multi-endpoint-test in namespace services-3019 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 05/09/23 13:22:36.213
May  9 13:22:36.213: INFO: Creating new exec pod
May  9 13:22:36.218: INFO: Waiting up to 5m0s for pod "execpodgkbpt" in namespace "services-3019" to be "running"
May  9 13:22:36.221: INFO: Pod "execpodgkbpt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.887638ms
May  9 13:22:38.223: INFO: Pod "execpodgkbpt": Phase="Running", Reason="", readiness=true. Elapsed: 2.005844813s
May  9 13:22:38.223: INFO: Pod "execpodgkbpt" satisfied condition "running"
May  9 13:22:39.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3019 exec execpodgkbpt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
May  9 13:22:39.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
May  9 13:22:39.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:22:39.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3019 exec execpodgkbpt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.67.124 80'
May  9 13:22:39.426: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.67.124 80\nConnection to 10.97.67.124 80 port [tcp/http] succeeded!\n"
May  9 13:22:39.426: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:22:39.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3019 exec execpodgkbpt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
May  9 13:22:39.520: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
May  9 13:22:39.520: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:22:39.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3019 exec execpodgkbpt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.67.124 81'
May  9 13:22:39.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.67.124 81\nConnection to 10.97.67.124 81 port [tcp/*] succeeded!\n"
May  9 13:22:39.619: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3019 05/09/23 13:22:39.619
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3019 to expose endpoints map[pod2:[101]] 05/09/23 13:22:39.635
May  9 13:22:39.641: INFO: successfully validated that service multi-endpoint-test in namespace services-3019 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3019 05/09/23 13:22:39.641
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3019 to expose endpoints map[] 05/09/23 13:22:39.657
May  9 13:22:39.665: INFO: successfully validated that service multi-endpoint-test in namespace services-3019 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:22:39.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3019" for this suite. 05/09/23 13:22:39.693
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":209,"skipped":4267,"failed":0}
------------------------------
• [SLOW TEST] [8.557 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:22:31.14
    May  9 13:22:31.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:22:31.141
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:22:31.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:22:31.153
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-3019 05/09/23 13:22:31.154
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3019 to expose endpoints map[] 05/09/23 13:22:31.165
    May  9 13:22:31.168: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    May  9 13:22:32.172: INFO: successfully validated that service multi-endpoint-test in namespace services-3019 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3019 05/09/23 13:22:32.172
    May  9 13:22:32.177: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3019" to be "running and ready"
    May  9 13:22:32.179: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.571502ms
    May  9 13:22:32.179: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:22:34.182: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004969097s
    May  9 13:22:34.182: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  9 13:22:34.182: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3019 to expose endpoints map[pod1:[100]] 05/09/23 13:22:34.184
    May  9 13:22:34.190: INFO: successfully validated that service multi-endpoint-test in namespace services-3019 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-3019 05/09/23 13:22:34.19
    May  9 13:22:34.197: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3019" to be "running and ready"
    May  9 13:22:34.200: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.872377ms
    May  9 13:22:34.200: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:22:36.202: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005514921s
    May  9 13:22:36.202: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  9 13:22:36.202: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3019 to expose endpoints map[pod1:[100] pod2:[101]] 05/09/23 13:22:36.205
    May  9 13:22:36.213: INFO: successfully validated that service multi-endpoint-test in namespace services-3019 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 05/09/23 13:22:36.213
    May  9 13:22:36.213: INFO: Creating new exec pod
    May  9 13:22:36.218: INFO: Waiting up to 5m0s for pod "execpodgkbpt" in namespace "services-3019" to be "running"
    May  9 13:22:36.221: INFO: Pod "execpodgkbpt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.887638ms
    May  9 13:22:38.223: INFO: Pod "execpodgkbpt": Phase="Running", Reason="", readiness=true. Elapsed: 2.005844813s
    May  9 13:22:38.223: INFO: Pod "execpodgkbpt" satisfied condition "running"
    May  9 13:22:39.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3019 exec execpodgkbpt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    May  9 13:22:39.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    May  9 13:22:39.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:22:39.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3019 exec execpodgkbpt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.67.124 80'
    May  9 13:22:39.426: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.67.124 80\nConnection to 10.97.67.124 80 port [tcp/http] succeeded!\n"
    May  9 13:22:39.426: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:22:39.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3019 exec execpodgkbpt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    May  9 13:22:39.520: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    May  9 13:22:39.520: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:22:39.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-3019 exec execpodgkbpt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.67.124 81'
    May  9 13:22:39.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.67.124 81\nConnection to 10.97.67.124 81 port [tcp/*] succeeded!\n"
    May  9 13:22:39.619: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-3019 05/09/23 13:22:39.619
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3019 to expose endpoints map[pod2:[101]] 05/09/23 13:22:39.635
    May  9 13:22:39.641: INFO: successfully validated that service multi-endpoint-test in namespace services-3019 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-3019 05/09/23 13:22:39.641
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3019 to expose endpoints map[] 05/09/23 13:22:39.657
    May  9 13:22:39.665: INFO: successfully validated that service multi-endpoint-test in namespace services-3019 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:22:39.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3019" for this suite. 05/09/23 13:22:39.693
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:22:39.698
May  9 13:22:39.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename gc 05/09/23 13:22:39.698
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:22:39.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:22:39.711
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 05/09/23 13:22:39.716
STEP: delete the rc 05/09/23 13:22:44.724
STEP: wait for the rc to be deleted 05/09/23 13:22:44.729
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/09/23 13:22:49.731
STEP: Gathering metrics 05/09/23 13:23:19.741
May  9 13:23:19.754: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" in namespace "kube-system" to be "running and ready"
May  9 13:23:19.755: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv": Phase="Running", Reason="", readiness=true. Elapsed: 1.50194ms
May  9 13:23:19.755: INFO: The phase of Pod kube-controller-manager-cl-gks-cncf-control-plane-gv9sv is Running (Ready = true)
May  9 13:23:19.755: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" satisfied condition "running and ready"
May  9 13:23:19.787: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May  9 13:23:19.787: INFO: Deleting pod "simpletest.rc-22dzs" in namespace "gc-1490"
May  9 13:23:19.801: INFO: Deleting pod "simpletest.rc-24vjv" in namespace "gc-1490"
May  9 13:23:19.811: INFO: Deleting pod "simpletest.rc-27cfj" in namespace "gc-1490"
May  9 13:23:19.824: INFO: Deleting pod "simpletest.rc-2qhw2" in namespace "gc-1490"
May  9 13:23:19.832: INFO: Deleting pod "simpletest.rc-2rfxz" in namespace "gc-1490"
May  9 13:23:19.845: INFO: Deleting pod "simpletest.rc-2scvx" in namespace "gc-1490"
May  9 13:23:19.857: INFO: Deleting pod "simpletest.rc-2sth5" in namespace "gc-1490"
May  9 13:23:19.872: INFO: Deleting pod "simpletest.rc-2vdw8" in namespace "gc-1490"
May  9 13:23:19.879: INFO: Deleting pod "simpletest.rc-464hk" in namespace "gc-1490"
May  9 13:23:19.893: INFO: Deleting pod "simpletest.rc-4g4h6" in namespace "gc-1490"
May  9 13:23:19.902: INFO: Deleting pod "simpletest.rc-4l56d" in namespace "gc-1490"
May  9 13:23:19.919: INFO: Deleting pod "simpletest.rc-4qz6c" in namespace "gc-1490"
May  9 13:23:19.930: INFO: Deleting pod "simpletest.rc-5lqmt" in namespace "gc-1490"
May  9 13:23:19.945: INFO: Deleting pod "simpletest.rc-6bgkp" in namespace "gc-1490"
May  9 13:23:19.954: INFO: Deleting pod "simpletest.rc-6fkw7" in namespace "gc-1490"
May  9 13:23:19.967: INFO: Deleting pod "simpletest.rc-6ptd9" in namespace "gc-1490"
May  9 13:23:19.978: INFO: Deleting pod "simpletest.rc-7hrbh" in namespace "gc-1490"
May  9 13:23:19.991: INFO: Deleting pod "simpletest.rc-7z56n" in namespace "gc-1490"
May  9 13:23:20.001: INFO: Deleting pod "simpletest.rc-82brb" in namespace "gc-1490"
May  9 13:23:20.011: INFO: Deleting pod "simpletest.rc-8bzv7" in namespace "gc-1490"
May  9 13:23:20.022: INFO: Deleting pod "simpletest.rc-8r5vr" in namespace "gc-1490"
May  9 13:23:20.034: INFO: Deleting pod "simpletest.rc-9kdnd" in namespace "gc-1490"
May  9 13:23:20.042: INFO: Deleting pod "simpletest.rc-9nhb6" in namespace "gc-1490"
May  9 13:23:20.056: INFO: Deleting pod "simpletest.rc-9rq2j" in namespace "gc-1490"
May  9 13:23:20.065: INFO: Deleting pod "simpletest.rc-bfpt5" in namespace "gc-1490"
May  9 13:23:20.075: INFO: Deleting pod "simpletest.rc-bghwh" in namespace "gc-1490"
May  9 13:23:20.085: INFO: Deleting pod "simpletest.rc-bhsmj" in namespace "gc-1490"
May  9 13:23:20.094: INFO: Deleting pod "simpletest.rc-bnlgh" in namespace "gc-1490"
May  9 13:23:20.104: INFO: Deleting pod "simpletest.rc-c5x7f" in namespace "gc-1490"
May  9 13:23:20.113: INFO: Deleting pod "simpletest.rc-cjhr7" in namespace "gc-1490"
May  9 13:23:20.123: INFO: Deleting pod "simpletest.rc-d5hnv" in namespace "gc-1490"
May  9 13:23:20.133: INFO: Deleting pod "simpletest.rc-df489" in namespace "gc-1490"
May  9 13:23:20.143: INFO: Deleting pod "simpletest.rc-dfz56" in namespace "gc-1490"
May  9 13:23:20.153: INFO: Deleting pod "simpletest.rc-dgcvx" in namespace "gc-1490"
May  9 13:23:20.168: INFO: Deleting pod "simpletest.rc-dsgt7" in namespace "gc-1490"
May  9 13:23:20.181: INFO: Deleting pod "simpletest.rc-dwp7h" in namespace "gc-1490"
May  9 13:23:20.189: INFO: Deleting pod "simpletest.rc-f2kxf" in namespace "gc-1490"
May  9 13:23:20.198: INFO: Deleting pod "simpletest.rc-ft2sh" in namespace "gc-1490"
May  9 13:23:20.212: INFO: Deleting pod "simpletest.rc-grc9s" in namespace "gc-1490"
May  9 13:23:20.226: INFO: Deleting pod "simpletest.rc-grhpl" in namespace "gc-1490"
May  9 13:23:20.240: INFO: Deleting pod "simpletest.rc-gsk9n" in namespace "gc-1490"
May  9 13:23:20.249: INFO: Deleting pod "simpletest.rc-gv6fw" in namespace "gc-1490"
May  9 13:23:20.259: INFO: Deleting pod "simpletest.rc-gwqcp" in namespace "gc-1490"
May  9 13:23:20.267: INFO: Deleting pod "simpletest.rc-h2v2n" in namespace "gc-1490"
May  9 13:23:20.278: INFO: Deleting pod "simpletest.rc-h65wt" in namespace "gc-1490"
May  9 13:23:20.288: INFO: Deleting pod "simpletest.rc-hmbj7" in namespace "gc-1490"
May  9 13:23:20.305: INFO: Deleting pod "simpletest.rc-hwsln" in namespace "gc-1490"
May  9 13:23:20.317: INFO: Deleting pod "simpletest.rc-hzc4l" in namespace "gc-1490"
May  9 13:23:20.329: INFO: Deleting pod "simpletest.rc-j286b" in namespace "gc-1490"
May  9 13:23:20.339: INFO: Deleting pod "simpletest.rc-j2pr5" in namespace "gc-1490"
May  9 13:23:20.347: INFO: Deleting pod "simpletest.rc-jcjc4" in namespace "gc-1490"
May  9 13:23:20.356: INFO: Deleting pod "simpletest.rc-jmmgc" in namespace "gc-1490"
May  9 13:23:20.366: INFO: Deleting pod "simpletest.rc-jrjxq" in namespace "gc-1490"
May  9 13:23:20.377: INFO: Deleting pod "simpletest.rc-k89xc" in namespace "gc-1490"
May  9 13:23:20.385: INFO: Deleting pod "simpletest.rc-klwqs" in namespace "gc-1490"
May  9 13:23:20.397: INFO: Deleting pod "simpletest.rc-kxrhb" in namespace "gc-1490"
May  9 13:23:20.407: INFO: Deleting pod "simpletest.rc-l5c2r" in namespace "gc-1490"
May  9 13:23:20.418: INFO: Deleting pod "simpletest.rc-l5jv7" in namespace "gc-1490"
May  9 13:23:20.435: INFO: Deleting pod "simpletest.rc-lgnqg" in namespace "gc-1490"
May  9 13:23:20.445: INFO: Deleting pod "simpletest.rc-m8jgw" in namespace "gc-1490"
May  9 13:23:20.456: INFO: Deleting pod "simpletest.rc-mdp4c" in namespace "gc-1490"
May  9 13:23:20.490: INFO: Deleting pod "simpletest.rc-mmhwg" in namespace "gc-1490"
May  9 13:23:20.542: INFO: Deleting pod "simpletest.rc-mr4fn" in namespace "gc-1490"
May  9 13:23:20.597: INFO: Deleting pod "simpletest.rc-n9vjb" in namespace "gc-1490"
May  9 13:23:20.646: INFO: Deleting pod "simpletest.rc-nflzg" in namespace "gc-1490"
May  9 13:23:20.692: INFO: Deleting pod "simpletest.rc-nkzxn" in namespace "gc-1490"
May  9 13:23:20.745: INFO: Deleting pod "simpletest.rc-nl6cm" in namespace "gc-1490"
May  9 13:23:20.795: INFO: Deleting pod "simpletest.rc-nsx86" in namespace "gc-1490"
May  9 13:23:20.845: INFO: Deleting pod "simpletest.rc-pnhmv" in namespace "gc-1490"
May  9 13:23:20.891: INFO: Deleting pod "simpletest.rc-qfgw2" in namespace "gc-1490"
May  9 13:23:20.949: INFO: Deleting pod "simpletest.rc-qnj24" in namespace "gc-1490"
May  9 13:23:20.995: INFO: Deleting pod "simpletest.rc-r4skg" in namespace "gc-1490"
May  9 13:23:21.043: INFO: Deleting pod "simpletest.rc-rs8tw" in namespace "gc-1490"
May  9 13:23:21.095: INFO: Deleting pod "simpletest.rc-rxf2f" in namespace "gc-1490"
May  9 13:23:21.143: INFO: Deleting pod "simpletest.rc-s6f26" in namespace "gc-1490"
May  9 13:23:21.195: INFO: Deleting pod "simpletest.rc-s6nqm" in namespace "gc-1490"
May  9 13:23:21.251: INFO: Deleting pod "simpletest.rc-s8p5k" in namespace "gc-1490"
May  9 13:23:21.296: INFO: Deleting pod "simpletest.rc-sb68f" in namespace "gc-1490"
May  9 13:23:21.342: INFO: Deleting pod "simpletest.rc-scrgh" in namespace "gc-1490"
May  9 13:23:21.401: INFO: Deleting pod "simpletest.rc-sf4x2" in namespace "gc-1490"
May  9 13:23:21.444: INFO: Deleting pod "simpletest.rc-sffgf" in namespace "gc-1490"
May  9 13:23:21.491: INFO: Deleting pod "simpletest.rc-sqxw2" in namespace "gc-1490"
May  9 13:23:21.541: INFO: Deleting pod "simpletest.rc-stqdk" in namespace "gc-1490"
May  9 13:23:21.597: INFO: Deleting pod "simpletest.rc-v556j" in namespace "gc-1490"
May  9 13:23:21.646: INFO: Deleting pod "simpletest.rc-v6zlj" in namespace "gc-1490"
May  9 13:23:21.693: INFO: Deleting pod "simpletest.rc-v7xqj" in namespace "gc-1490"
May  9 13:23:21.746: INFO: Deleting pod "simpletest.rc-v7zts" in namespace "gc-1490"
May  9 13:23:21.797: INFO: Deleting pod "simpletest.rc-vng2w" in namespace "gc-1490"
May  9 13:23:21.844: INFO: Deleting pod "simpletest.rc-vvjjx" in namespace "gc-1490"
May  9 13:23:21.895: INFO: Deleting pod "simpletest.rc-vwrkc" in namespace "gc-1490"
May  9 13:23:21.943: INFO: Deleting pod "simpletest.rc-w4cbh" in namespace "gc-1490"
May  9 13:23:21.994: INFO: Deleting pod "simpletest.rc-wh2ql" in namespace "gc-1490"
May  9 13:23:22.044: INFO: Deleting pod "simpletest.rc-wjx2v" in namespace "gc-1490"
May  9 13:23:22.096: INFO: Deleting pod "simpletest.rc-xpqqc" in namespace "gc-1490"
May  9 13:23:22.143: INFO: Deleting pod "simpletest.rc-xpzz7" in namespace "gc-1490"
May  9 13:23:22.198: INFO: Deleting pod "simpletest.rc-xtcs8" in namespace "gc-1490"
May  9 13:23:22.243: INFO: Deleting pod "simpletest.rc-z74sc" in namespace "gc-1490"
May  9 13:23:22.293: INFO: Deleting pod "simpletest.rc-z87cq" in namespace "gc-1490"
May  9 13:23:22.345: INFO: Deleting pod "simpletest.rc-zq57b" in namespace "gc-1490"
May  9 13:23:22.394: INFO: Deleting pod "simpletest.rc-zvbkf" in namespace "gc-1490"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  9 13:23:22.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1490" for this suite. 05/09/23 13:23:22.486
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":210,"skipped":4287,"failed":0}
------------------------------
• [SLOW TEST] [42.840 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:22:39.698
    May  9 13:22:39.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename gc 05/09/23 13:22:39.698
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:22:39.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:22:39.711
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 05/09/23 13:22:39.716
    STEP: delete the rc 05/09/23 13:22:44.724
    STEP: wait for the rc to be deleted 05/09/23 13:22:44.729
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/09/23 13:22:49.731
    STEP: Gathering metrics 05/09/23 13:23:19.741
    May  9 13:23:19.754: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" in namespace "kube-system" to be "running and ready"
    May  9 13:23:19.755: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv": Phase="Running", Reason="", readiness=true. Elapsed: 1.50194ms
    May  9 13:23:19.755: INFO: The phase of Pod kube-controller-manager-cl-gks-cncf-control-plane-gv9sv is Running (Ready = true)
    May  9 13:23:19.755: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" satisfied condition "running and ready"
    May  9 13:23:19.787: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May  9 13:23:19.787: INFO: Deleting pod "simpletest.rc-22dzs" in namespace "gc-1490"
    May  9 13:23:19.801: INFO: Deleting pod "simpletest.rc-24vjv" in namespace "gc-1490"
    May  9 13:23:19.811: INFO: Deleting pod "simpletest.rc-27cfj" in namespace "gc-1490"
    May  9 13:23:19.824: INFO: Deleting pod "simpletest.rc-2qhw2" in namespace "gc-1490"
    May  9 13:23:19.832: INFO: Deleting pod "simpletest.rc-2rfxz" in namespace "gc-1490"
    May  9 13:23:19.845: INFO: Deleting pod "simpletest.rc-2scvx" in namespace "gc-1490"
    May  9 13:23:19.857: INFO: Deleting pod "simpletest.rc-2sth5" in namespace "gc-1490"
    May  9 13:23:19.872: INFO: Deleting pod "simpletest.rc-2vdw8" in namespace "gc-1490"
    May  9 13:23:19.879: INFO: Deleting pod "simpletest.rc-464hk" in namespace "gc-1490"
    May  9 13:23:19.893: INFO: Deleting pod "simpletest.rc-4g4h6" in namespace "gc-1490"
    May  9 13:23:19.902: INFO: Deleting pod "simpletest.rc-4l56d" in namespace "gc-1490"
    May  9 13:23:19.919: INFO: Deleting pod "simpletest.rc-4qz6c" in namespace "gc-1490"
    May  9 13:23:19.930: INFO: Deleting pod "simpletest.rc-5lqmt" in namespace "gc-1490"
    May  9 13:23:19.945: INFO: Deleting pod "simpletest.rc-6bgkp" in namespace "gc-1490"
    May  9 13:23:19.954: INFO: Deleting pod "simpletest.rc-6fkw7" in namespace "gc-1490"
    May  9 13:23:19.967: INFO: Deleting pod "simpletest.rc-6ptd9" in namespace "gc-1490"
    May  9 13:23:19.978: INFO: Deleting pod "simpletest.rc-7hrbh" in namespace "gc-1490"
    May  9 13:23:19.991: INFO: Deleting pod "simpletest.rc-7z56n" in namespace "gc-1490"
    May  9 13:23:20.001: INFO: Deleting pod "simpletest.rc-82brb" in namespace "gc-1490"
    May  9 13:23:20.011: INFO: Deleting pod "simpletest.rc-8bzv7" in namespace "gc-1490"
    May  9 13:23:20.022: INFO: Deleting pod "simpletest.rc-8r5vr" in namespace "gc-1490"
    May  9 13:23:20.034: INFO: Deleting pod "simpletest.rc-9kdnd" in namespace "gc-1490"
    May  9 13:23:20.042: INFO: Deleting pod "simpletest.rc-9nhb6" in namespace "gc-1490"
    May  9 13:23:20.056: INFO: Deleting pod "simpletest.rc-9rq2j" in namespace "gc-1490"
    May  9 13:23:20.065: INFO: Deleting pod "simpletest.rc-bfpt5" in namespace "gc-1490"
    May  9 13:23:20.075: INFO: Deleting pod "simpletest.rc-bghwh" in namespace "gc-1490"
    May  9 13:23:20.085: INFO: Deleting pod "simpletest.rc-bhsmj" in namespace "gc-1490"
    May  9 13:23:20.094: INFO: Deleting pod "simpletest.rc-bnlgh" in namespace "gc-1490"
    May  9 13:23:20.104: INFO: Deleting pod "simpletest.rc-c5x7f" in namespace "gc-1490"
    May  9 13:23:20.113: INFO: Deleting pod "simpletest.rc-cjhr7" in namespace "gc-1490"
    May  9 13:23:20.123: INFO: Deleting pod "simpletest.rc-d5hnv" in namespace "gc-1490"
    May  9 13:23:20.133: INFO: Deleting pod "simpletest.rc-df489" in namespace "gc-1490"
    May  9 13:23:20.143: INFO: Deleting pod "simpletest.rc-dfz56" in namespace "gc-1490"
    May  9 13:23:20.153: INFO: Deleting pod "simpletest.rc-dgcvx" in namespace "gc-1490"
    May  9 13:23:20.168: INFO: Deleting pod "simpletest.rc-dsgt7" in namespace "gc-1490"
    May  9 13:23:20.181: INFO: Deleting pod "simpletest.rc-dwp7h" in namespace "gc-1490"
    May  9 13:23:20.189: INFO: Deleting pod "simpletest.rc-f2kxf" in namespace "gc-1490"
    May  9 13:23:20.198: INFO: Deleting pod "simpletest.rc-ft2sh" in namespace "gc-1490"
    May  9 13:23:20.212: INFO: Deleting pod "simpletest.rc-grc9s" in namespace "gc-1490"
    May  9 13:23:20.226: INFO: Deleting pod "simpletest.rc-grhpl" in namespace "gc-1490"
    May  9 13:23:20.240: INFO: Deleting pod "simpletest.rc-gsk9n" in namespace "gc-1490"
    May  9 13:23:20.249: INFO: Deleting pod "simpletest.rc-gv6fw" in namespace "gc-1490"
    May  9 13:23:20.259: INFO: Deleting pod "simpletest.rc-gwqcp" in namespace "gc-1490"
    May  9 13:23:20.267: INFO: Deleting pod "simpletest.rc-h2v2n" in namespace "gc-1490"
    May  9 13:23:20.278: INFO: Deleting pod "simpletest.rc-h65wt" in namespace "gc-1490"
    May  9 13:23:20.288: INFO: Deleting pod "simpletest.rc-hmbj7" in namespace "gc-1490"
    May  9 13:23:20.305: INFO: Deleting pod "simpletest.rc-hwsln" in namespace "gc-1490"
    May  9 13:23:20.317: INFO: Deleting pod "simpletest.rc-hzc4l" in namespace "gc-1490"
    May  9 13:23:20.329: INFO: Deleting pod "simpletest.rc-j286b" in namespace "gc-1490"
    May  9 13:23:20.339: INFO: Deleting pod "simpletest.rc-j2pr5" in namespace "gc-1490"
    May  9 13:23:20.347: INFO: Deleting pod "simpletest.rc-jcjc4" in namespace "gc-1490"
    May  9 13:23:20.356: INFO: Deleting pod "simpletest.rc-jmmgc" in namespace "gc-1490"
    May  9 13:23:20.366: INFO: Deleting pod "simpletest.rc-jrjxq" in namespace "gc-1490"
    May  9 13:23:20.377: INFO: Deleting pod "simpletest.rc-k89xc" in namespace "gc-1490"
    May  9 13:23:20.385: INFO: Deleting pod "simpletest.rc-klwqs" in namespace "gc-1490"
    May  9 13:23:20.397: INFO: Deleting pod "simpletest.rc-kxrhb" in namespace "gc-1490"
    May  9 13:23:20.407: INFO: Deleting pod "simpletest.rc-l5c2r" in namespace "gc-1490"
    May  9 13:23:20.418: INFO: Deleting pod "simpletest.rc-l5jv7" in namespace "gc-1490"
    May  9 13:23:20.435: INFO: Deleting pod "simpletest.rc-lgnqg" in namespace "gc-1490"
    May  9 13:23:20.445: INFO: Deleting pod "simpletest.rc-m8jgw" in namespace "gc-1490"
    May  9 13:23:20.456: INFO: Deleting pod "simpletest.rc-mdp4c" in namespace "gc-1490"
    May  9 13:23:20.490: INFO: Deleting pod "simpletest.rc-mmhwg" in namespace "gc-1490"
    May  9 13:23:20.542: INFO: Deleting pod "simpletest.rc-mr4fn" in namespace "gc-1490"
    May  9 13:23:20.597: INFO: Deleting pod "simpletest.rc-n9vjb" in namespace "gc-1490"
    May  9 13:23:20.646: INFO: Deleting pod "simpletest.rc-nflzg" in namespace "gc-1490"
    May  9 13:23:20.692: INFO: Deleting pod "simpletest.rc-nkzxn" in namespace "gc-1490"
    May  9 13:23:20.745: INFO: Deleting pod "simpletest.rc-nl6cm" in namespace "gc-1490"
    May  9 13:23:20.795: INFO: Deleting pod "simpletest.rc-nsx86" in namespace "gc-1490"
    May  9 13:23:20.845: INFO: Deleting pod "simpletest.rc-pnhmv" in namespace "gc-1490"
    May  9 13:23:20.891: INFO: Deleting pod "simpletest.rc-qfgw2" in namespace "gc-1490"
    May  9 13:23:20.949: INFO: Deleting pod "simpletest.rc-qnj24" in namespace "gc-1490"
    May  9 13:23:20.995: INFO: Deleting pod "simpletest.rc-r4skg" in namespace "gc-1490"
    May  9 13:23:21.043: INFO: Deleting pod "simpletest.rc-rs8tw" in namespace "gc-1490"
    May  9 13:23:21.095: INFO: Deleting pod "simpletest.rc-rxf2f" in namespace "gc-1490"
    May  9 13:23:21.143: INFO: Deleting pod "simpletest.rc-s6f26" in namespace "gc-1490"
    May  9 13:23:21.195: INFO: Deleting pod "simpletest.rc-s6nqm" in namespace "gc-1490"
    May  9 13:23:21.251: INFO: Deleting pod "simpletest.rc-s8p5k" in namespace "gc-1490"
    May  9 13:23:21.296: INFO: Deleting pod "simpletest.rc-sb68f" in namespace "gc-1490"
    May  9 13:23:21.342: INFO: Deleting pod "simpletest.rc-scrgh" in namespace "gc-1490"
    May  9 13:23:21.401: INFO: Deleting pod "simpletest.rc-sf4x2" in namespace "gc-1490"
    May  9 13:23:21.444: INFO: Deleting pod "simpletest.rc-sffgf" in namespace "gc-1490"
    May  9 13:23:21.491: INFO: Deleting pod "simpletest.rc-sqxw2" in namespace "gc-1490"
    May  9 13:23:21.541: INFO: Deleting pod "simpletest.rc-stqdk" in namespace "gc-1490"
    May  9 13:23:21.597: INFO: Deleting pod "simpletest.rc-v556j" in namespace "gc-1490"
    May  9 13:23:21.646: INFO: Deleting pod "simpletest.rc-v6zlj" in namespace "gc-1490"
    May  9 13:23:21.693: INFO: Deleting pod "simpletest.rc-v7xqj" in namespace "gc-1490"
    May  9 13:23:21.746: INFO: Deleting pod "simpletest.rc-v7zts" in namespace "gc-1490"
    May  9 13:23:21.797: INFO: Deleting pod "simpletest.rc-vng2w" in namespace "gc-1490"
    May  9 13:23:21.844: INFO: Deleting pod "simpletest.rc-vvjjx" in namespace "gc-1490"
    May  9 13:23:21.895: INFO: Deleting pod "simpletest.rc-vwrkc" in namespace "gc-1490"
    May  9 13:23:21.943: INFO: Deleting pod "simpletest.rc-w4cbh" in namespace "gc-1490"
    May  9 13:23:21.994: INFO: Deleting pod "simpletest.rc-wh2ql" in namespace "gc-1490"
    May  9 13:23:22.044: INFO: Deleting pod "simpletest.rc-wjx2v" in namespace "gc-1490"
    May  9 13:23:22.096: INFO: Deleting pod "simpletest.rc-xpqqc" in namespace "gc-1490"
    May  9 13:23:22.143: INFO: Deleting pod "simpletest.rc-xpzz7" in namespace "gc-1490"
    May  9 13:23:22.198: INFO: Deleting pod "simpletest.rc-xtcs8" in namespace "gc-1490"
    May  9 13:23:22.243: INFO: Deleting pod "simpletest.rc-z74sc" in namespace "gc-1490"
    May  9 13:23:22.293: INFO: Deleting pod "simpletest.rc-z87cq" in namespace "gc-1490"
    May  9 13:23:22.345: INFO: Deleting pod "simpletest.rc-zq57b" in namespace "gc-1490"
    May  9 13:23:22.394: INFO: Deleting pod "simpletest.rc-zvbkf" in namespace "gc-1490"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  9 13:23:22.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1490" for this suite. 05/09/23 13:23:22.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:23:22.539
May  9 13:23:22.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename namespaces 05/09/23 13:23:22.54
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:23:22.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:23:22.555
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 05/09/23 13:23:22.557
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:23:22.567
STEP: Creating a service in the namespace 05/09/23 13:23:22.568
STEP: Deleting the namespace 05/09/23 13:23:22.581
STEP: Waiting for the namespace to be removed. 05/09/23 13:23:22.587
STEP: Recreating the namespace 05/09/23 13:23:28.59
STEP: Verifying there is no service in the namespace 05/09/23 13:23:28.601
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May  9 13:23:28.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4336" for this suite. 05/09/23 13:23:28.606
STEP: Destroying namespace "nsdeletetest-4638" for this suite. 05/09/23 13:23:28.612
May  9 13:23:28.616: INFO: Namespace nsdeletetest-4638 was already deleted
STEP: Destroying namespace "nsdeletetest-7836" for this suite. 05/09/23 13:23:28.616
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":211,"skipped":4306,"failed":0}
------------------------------
• [SLOW TEST] [6.085 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:23:22.539
    May  9 13:23:22.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename namespaces 05/09/23 13:23:22.54
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:23:22.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:23:22.555
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 05/09/23 13:23:22.557
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:23:22.567
    STEP: Creating a service in the namespace 05/09/23 13:23:22.568
    STEP: Deleting the namespace 05/09/23 13:23:22.581
    STEP: Waiting for the namespace to be removed. 05/09/23 13:23:22.587
    STEP: Recreating the namespace 05/09/23 13:23:28.59
    STEP: Verifying there is no service in the namespace 05/09/23 13:23:28.601
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:23:28.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4336" for this suite. 05/09/23 13:23:28.606
    STEP: Destroying namespace "nsdeletetest-4638" for this suite. 05/09/23 13:23:28.612
    May  9 13:23:28.616: INFO: Namespace nsdeletetest-4638 was already deleted
    STEP: Destroying namespace "nsdeletetest-7836" for this suite. 05/09/23 13:23:28.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:23:28.625
May  9 13:23:28.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename runtimeclass 05/09/23 13:23:28.626
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:23:28.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:23:28.638
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  9 13:23:28.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1159" for this suite. 05/09/23 13:23:28.65
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":212,"skipped":4314,"failed":0}
------------------------------
• [0.033 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:23:28.625
    May  9 13:23:28.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename runtimeclass 05/09/23 13:23:28.626
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:23:28.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:23:28.638
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  9 13:23:28.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1159" for this suite. 05/09/23 13:23:28.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:23:28.659
May  9 13:23:28.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename var-expansion 05/09/23 13:23:28.66
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:23:28.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:23:28.674
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 05/09/23 13:23:28.676
May  9 13:23:28.686: INFO: Waiting up to 5m0s for pod "var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275" in namespace "var-expansion-6426" to be "Succeeded or Failed"
May  9 13:23:28.688: INFO: Pod "var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275": Phase="Pending", Reason="", readiness=false. Elapsed: 1.751925ms
May  9 13:23:30.691: INFO: Pod "var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004871699s
May  9 13:23:32.691: INFO: Pod "var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004844173s
May  9 13:23:34.692: INFO: Pod "var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006384273s
STEP: Saw pod success 05/09/23 13:23:34.692
May  9 13:23:34.692: INFO: Pod "var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275" satisfied condition "Succeeded or Failed"
May  9 13:23:34.694: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275 container dapi-container: <nil>
STEP: delete the pod 05/09/23 13:23:34.704
May  9 13:23:34.715: INFO: Waiting for pod var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275 to disappear
May  9 13:23:34.716: INFO: Pod var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  9 13:23:34.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6426" for this suite. 05/09/23 13:23:34.719
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":213,"skipped":4348,"failed":0}
------------------------------
• [SLOW TEST] [6.065 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:23:28.659
    May  9 13:23:28.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename var-expansion 05/09/23 13:23:28.66
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:23:28.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:23:28.674
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 05/09/23 13:23:28.676
    May  9 13:23:28.686: INFO: Waiting up to 5m0s for pod "var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275" in namespace "var-expansion-6426" to be "Succeeded or Failed"
    May  9 13:23:28.688: INFO: Pod "var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275": Phase="Pending", Reason="", readiness=false. Elapsed: 1.751925ms
    May  9 13:23:30.691: INFO: Pod "var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004871699s
    May  9 13:23:32.691: INFO: Pod "var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004844173s
    May  9 13:23:34.692: INFO: Pod "var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006384273s
    STEP: Saw pod success 05/09/23 13:23:34.692
    May  9 13:23:34.692: INFO: Pod "var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275" satisfied condition "Succeeded or Failed"
    May  9 13:23:34.694: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275 container dapi-container: <nil>
    STEP: delete the pod 05/09/23 13:23:34.704
    May  9 13:23:34.715: INFO: Waiting for pod var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275 to disappear
    May  9 13:23:34.716: INFO: Pod var-expansion-98be7fc9-34e5-4afb-9de9-f7ed1e611275 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  9 13:23:34.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6426" for this suite. 05/09/23 13:23:34.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:23:34.725
May  9 13:23:34.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename init-container 05/09/23 13:23:34.725
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:23:34.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:23:34.736
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 05/09/23 13:23:34.738
May  9 13:23:34.738: INFO: PodSpec: initContainers in spec.initContainers
May  9 13:24:19.752: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3a40a40c-65a4-4667-91ae-98bc40c5eaac", GenerateName:"", Namespace:"init-container-8211", SelfLink:"", UID:"f15b8060-4df3-4933-90c6-cafc95698a39", ResourceVersion:"35364", Generation:0, CreationTimestamp:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"738428210"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"b17179c368b1dcdde9c22920cc0cbc3ec822ba16b6078f4a10737dc3a1a94e1b", "cni.projectcalico.org/podIP":"172.25.124.231/32", "cni.projectcalico.org/podIPs":"172.25.124.231/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0004edd88), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 9, 13, 23, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0004eddb8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 9, 13, 24, 19, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0004ede00), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-5vt24", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003ee59a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5vt24", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5vt24", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5vt24", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0036d2e30), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cl-gks-cncf-ix1-md-0-48ljh", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000c090a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0036d2eb0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0036d2ed0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0036d2ed8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0036d2edc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0012b3960), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.64", PodIP:"172.25.124.231", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.124.231"}}, StartTime:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c09180)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c091f0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://a2f2755d0a0d844472cfc6e04c0ec604858c7c1188f06ae52bb3de6a3658b856", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003ee5ac0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003ee5a60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0036d2f5f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May  9 13:24:19.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8211" for this suite. 05/09/23 13:24:19.755
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":214,"skipped":4353,"failed":0}
------------------------------
• [SLOW TEST] [45.035 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:23:34.725
    May  9 13:23:34.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename init-container 05/09/23 13:23:34.725
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:23:34.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:23:34.736
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 05/09/23 13:23:34.738
    May  9 13:23:34.738: INFO: PodSpec: initContainers in spec.initContainers
    May  9 13:24:19.752: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3a40a40c-65a4-4667-91ae-98bc40c5eaac", GenerateName:"", Namespace:"init-container-8211", SelfLink:"", UID:"f15b8060-4df3-4933-90c6-cafc95698a39", ResourceVersion:"35364", Generation:0, CreationTimestamp:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"738428210"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"b17179c368b1dcdde9c22920cc0cbc3ec822ba16b6078f4a10737dc3a1a94e1b", "cni.projectcalico.org/podIP":"172.25.124.231/32", "cni.projectcalico.org/podIPs":"172.25.124.231/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0004edd88), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 9, 13, 23, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0004eddb8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 9, 13, 24, 19, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0004ede00), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-5vt24", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003ee59a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5vt24", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5vt24", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5vt24", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0036d2e30), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cl-gks-cncf-ix1-md-0-48ljh", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000c090a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0036d2eb0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0036d2ed0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0036d2ed8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0036d2edc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0012b3960), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.64", PodIP:"172.25.124.231", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.124.231"}}, StartTime:time.Date(2023, time.May, 9, 13, 23, 34, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c09180)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c091f0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://a2f2755d0a0d844472cfc6e04c0ec604858c7c1188f06ae52bb3de6a3658b856", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003ee5ac0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003ee5a60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0036d2f5f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May  9 13:24:19.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-8211" for this suite. 05/09/23 13:24:19.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:24:19.76
May  9 13:24:19.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:24:19.761
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:24:19.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:24:19.773
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:24:19.775
May  9 13:24:19.780: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d" in namespace "projected-2951" to be "Succeeded or Failed"
May  9 13:24:19.781: INFO: Pod "downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.688605ms
May  9 13:24:21.785: INFO: Pod "downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005109928s
May  9 13:24:23.785: INFO: Pod "downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005735458s
STEP: Saw pod success 05/09/23 13:24:23.785
May  9 13:24:23.785: INFO: Pod "downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d" satisfied condition "Succeeded or Failed"
May  9 13:24:23.787: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d container client-container: <nil>
STEP: delete the pod 05/09/23 13:24:23.798
May  9 13:24:23.807: INFO: Waiting for pod downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d to disappear
May  9 13:24:23.809: INFO: Pod downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  9 13:24:23.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2951" for this suite. 05/09/23 13:24:23.812
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":215,"skipped":4368,"failed":0}
------------------------------
• [4.055 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:24:19.76
    May  9 13:24:19.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:24:19.761
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:24:19.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:24:19.773
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:24:19.775
    May  9 13:24:19.780: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d" in namespace "projected-2951" to be "Succeeded or Failed"
    May  9 13:24:19.781: INFO: Pod "downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.688605ms
    May  9 13:24:21.785: INFO: Pod "downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005109928s
    May  9 13:24:23.785: INFO: Pod "downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005735458s
    STEP: Saw pod success 05/09/23 13:24:23.785
    May  9 13:24:23.785: INFO: Pod "downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d" satisfied condition "Succeeded or Failed"
    May  9 13:24:23.787: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d container client-container: <nil>
    STEP: delete the pod 05/09/23 13:24:23.798
    May  9 13:24:23.807: INFO: Waiting for pod downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d to disappear
    May  9 13:24:23.809: INFO: Pod downwardapi-volume-53cbf404-4370-4a00-96cb-42a4b46b966d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  9 13:24:23.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2951" for this suite. 05/09/23 13:24:23.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:24:23.816
May  9 13:24:23.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:24:23.817
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:24:23.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:24:23.828
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:24:23.839
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:24:24.561
STEP: Deploying the webhook pod 05/09/23 13:24:24.566
STEP: Wait for the deployment to be ready 05/09/23 13:24:24.575
May  9 13:24:24.581: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/09/23 13:24:26.586
STEP: Verifying the service has paired with the endpoint 05/09/23 13:24:26.597
May  9 13:24:27.597: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/09/23 13:24:27.6
STEP: create a configmap that should be updated by the webhook 05/09/23 13:24:27.612
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:24:27.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4006" for this suite. 05/09/23 13:24:27.628
STEP: Destroying namespace "webhook-4006-markers" for this suite. 05/09/23 13:24:27.633
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":216,"skipped":4381,"failed":0}
------------------------------
• [3.855 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:24:23.816
    May  9 13:24:23.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:24:23.817
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:24:23.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:24:23.828
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:24:23.839
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:24:24.561
    STEP: Deploying the webhook pod 05/09/23 13:24:24.566
    STEP: Wait for the deployment to be ready 05/09/23 13:24:24.575
    May  9 13:24:24.581: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/09/23 13:24:26.586
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:24:26.597
    May  9 13:24:27.597: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/09/23 13:24:27.6
    STEP: create a configmap that should be updated by the webhook 05/09/23 13:24:27.612
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:24:27.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4006" for this suite. 05/09/23 13:24:27.628
    STEP: Destroying namespace "webhook-4006-markers" for this suite. 05/09/23 13:24:27.633
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:24:27.672
May  9 13:24:27.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:24:27.673
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:24:27.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:24:27.693
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 05/09/23 13:24:27.695
May  9 13:24:27.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: rename a version 05/09/23 13:24:35.191
STEP: check the new version name is served 05/09/23 13:24:35.204
STEP: check the old version name is removed 05/09/23 13:24:38.138
STEP: check the other version is not changed 05/09/23 13:24:39.452
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:24:44.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-52" for this suite. 05/09/23 13:24:44.497
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":217,"skipped":4393,"failed":0}
------------------------------
• [SLOW TEST] [16.830 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:24:27.672
    May  9 13:24:27.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:24:27.673
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:24:27.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:24:27.693
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 05/09/23 13:24:27.695
    May  9 13:24:27.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: rename a version 05/09/23 13:24:35.191
    STEP: check the new version name is served 05/09/23 13:24:35.204
    STEP: check the old version name is removed 05/09/23 13:24:38.138
    STEP: check the other version is not changed 05/09/23 13:24:39.452
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:24:44.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-52" for this suite. 05/09/23 13:24:44.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:24:44.502
May  9 13:24:44.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename events 05/09/23 13:24:44.503
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:24:44.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:24:44.514
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 05/09/23 13:24:44.516
STEP: listing events in all namespaces 05/09/23 13:24:44.52
STEP: listing events in test namespace 05/09/23 13:24:44.523
STEP: listing events with field selection filtering on source 05/09/23 13:24:44.524
STEP: listing events with field selection filtering on reportingController 05/09/23 13:24:44.526
STEP: getting the test event 05/09/23 13:24:44.527
STEP: patching the test event 05/09/23 13:24:44.529
STEP: getting the test event 05/09/23 13:24:44.535
STEP: updating the test event 05/09/23 13:24:44.536
STEP: getting the test event 05/09/23 13:24:44.541
STEP: deleting the test event 05/09/23 13:24:44.542
STEP: listing events in all namespaces 05/09/23 13:24:44.546
STEP: listing events in test namespace 05/09/23 13:24:44.549
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
May  9 13:24:44.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4277" for this suite. 05/09/23 13:24:44.552
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":218,"skipped":4399,"failed":0}
------------------------------
• [0.056 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:24:44.502
    May  9 13:24:44.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename events 05/09/23 13:24:44.503
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:24:44.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:24:44.514
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 05/09/23 13:24:44.516
    STEP: listing events in all namespaces 05/09/23 13:24:44.52
    STEP: listing events in test namespace 05/09/23 13:24:44.523
    STEP: listing events with field selection filtering on source 05/09/23 13:24:44.524
    STEP: listing events with field selection filtering on reportingController 05/09/23 13:24:44.526
    STEP: getting the test event 05/09/23 13:24:44.527
    STEP: patching the test event 05/09/23 13:24:44.529
    STEP: getting the test event 05/09/23 13:24:44.535
    STEP: updating the test event 05/09/23 13:24:44.536
    STEP: getting the test event 05/09/23 13:24:44.541
    STEP: deleting the test event 05/09/23 13:24:44.542
    STEP: listing events in all namespaces 05/09/23 13:24:44.546
    STEP: listing events in test namespace 05/09/23 13:24:44.549
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    May  9 13:24:44.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4277" for this suite. 05/09/23 13:24:44.552
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:24:44.559
May  9 13:24:44.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename ingress 05/09/23 13:24:44.559
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:24:44.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:24:44.57
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 05/09/23 13:24:44.571
STEP: getting /apis/networking.k8s.io 05/09/23 13:24:44.573
STEP: getting /apis/networking.k8s.iov1 05/09/23 13:24:44.573
STEP: creating 05/09/23 13:24:44.574
STEP: getting 05/09/23 13:24:44.583
STEP: listing 05/09/23 13:24:44.585
STEP: watching 05/09/23 13:24:44.587
May  9 13:24:44.587: INFO: starting watch
STEP: cluster-wide listing 05/09/23 13:24:44.588
STEP: cluster-wide watching 05/09/23 13:24:44.589
May  9 13:24:44.589: INFO: starting watch
STEP: patching 05/09/23 13:24:44.59
STEP: updating 05/09/23 13:24:44.594
May  9 13:24:44.600: INFO: waiting for watch events with expected annotations
May  9 13:24:44.600: INFO: saw patched and updated annotations
STEP: patching /status 05/09/23 13:24:44.6
STEP: updating /status 05/09/23 13:24:44.603
STEP: get /status 05/09/23 13:24:44.608
STEP: deleting 05/09/23 13:24:44.612
STEP: deleting a collection 05/09/23 13:24:44.618
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
May  9 13:24:44.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-1918" for this suite. 05/09/23 13:24:44.629
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":219,"skipped":4399,"failed":0}
------------------------------
• [0.074 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:24:44.559
    May  9 13:24:44.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename ingress 05/09/23 13:24:44.559
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:24:44.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:24:44.57
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 05/09/23 13:24:44.571
    STEP: getting /apis/networking.k8s.io 05/09/23 13:24:44.573
    STEP: getting /apis/networking.k8s.iov1 05/09/23 13:24:44.573
    STEP: creating 05/09/23 13:24:44.574
    STEP: getting 05/09/23 13:24:44.583
    STEP: listing 05/09/23 13:24:44.585
    STEP: watching 05/09/23 13:24:44.587
    May  9 13:24:44.587: INFO: starting watch
    STEP: cluster-wide listing 05/09/23 13:24:44.588
    STEP: cluster-wide watching 05/09/23 13:24:44.589
    May  9 13:24:44.589: INFO: starting watch
    STEP: patching 05/09/23 13:24:44.59
    STEP: updating 05/09/23 13:24:44.594
    May  9 13:24:44.600: INFO: waiting for watch events with expected annotations
    May  9 13:24:44.600: INFO: saw patched and updated annotations
    STEP: patching /status 05/09/23 13:24:44.6
    STEP: updating /status 05/09/23 13:24:44.603
    STEP: get /status 05/09/23 13:24:44.608
    STEP: deleting 05/09/23 13:24:44.612
    STEP: deleting a collection 05/09/23 13:24:44.618
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    May  9 13:24:44.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-1918" for this suite. 05/09/23 13:24:44.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:24:44.635
May  9 13:24:44.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename statefulset 05/09/23 13:24:44.636
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:24:44.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:24:44.646
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9574 05/09/23 13:24:44.647
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 05/09/23 13:24:44.651
May  9 13:24:44.662: INFO: Found 0 stateful pods, waiting for 3
May  9 13:24:54.665: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  9 13:24:54.665: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  9 13:24:54.665: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/09/23 13:24:54.671
May  9 13:24:54.687: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/09/23 13:24:54.687
STEP: Not applying an update when the partition is greater than the number of replicas 05/09/23 13:25:04.699
STEP: Performing a canary update 05/09/23 13:25:04.699
May  9 13:25:04.715: INFO: Updating stateful set ss2
May  9 13:25:04.722: INFO: Waiting for Pod statefulset-9574/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 05/09/23 13:25:14.726
May  9 13:25:14.766: INFO: Found 2 stateful pods, waiting for 3
May  9 13:25:24.773: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  9 13:25:24.773: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  9 13:25:24.773: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 05/09/23 13:25:24.776
May  9 13:25:24.793: INFO: Updating stateful set ss2
May  9 13:25:24.799: INFO: Waiting for Pod statefulset-9574/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
May  9 13:25:34.821: INFO: Updating stateful set ss2
May  9 13:25:34.828: INFO: Waiting for StatefulSet statefulset-9574/ss2 to complete update
May  9 13:25:34.828: INFO: Waiting for Pod statefulset-9574/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  9 13:25:44.835: INFO: Deleting all statefulset in ns statefulset-9574
May  9 13:25:44.836: INFO: Scaling statefulset ss2 to 0
May  9 13:25:54.848: INFO: Waiting for statefulset status.replicas updated to 0
May  9 13:25:54.850: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  9 13:25:54.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9574" for this suite. 05/09/23 13:25:54.865
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":220,"skipped":4480,"failed":0}
------------------------------
• [SLOW TEST] [70.235 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:24:44.635
    May  9 13:24:44.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename statefulset 05/09/23 13:24:44.636
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:24:44.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:24:44.646
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9574 05/09/23 13:24:44.647
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 05/09/23 13:24:44.651
    May  9 13:24:44.662: INFO: Found 0 stateful pods, waiting for 3
    May  9 13:24:54.665: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  9 13:24:54.665: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  9 13:24:54.665: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/09/23 13:24:54.671
    May  9 13:24:54.687: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/09/23 13:24:54.687
    STEP: Not applying an update when the partition is greater than the number of replicas 05/09/23 13:25:04.699
    STEP: Performing a canary update 05/09/23 13:25:04.699
    May  9 13:25:04.715: INFO: Updating stateful set ss2
    May  9 13:25:04.722: INFO: Waiting for Pod statefulset-9574/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 05/09/23 13:25:14.726
    May  9 13:25:14.766: INFO: Found 2 stateful pods, waiting for 3
    May  9 13:25:24.773: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  9 13:25:24.773: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  9 13:25:24.773: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 05/09/23 13:25:24.776
    May  9 13:25:24.793: INFO: Updating stateful set ss2
    May  9 13:25:24.799: INFO: Waiting for Pod statefulset-9574/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    May  9 13:25:34.821: INFO: Updating stateful set ss2
    May  9 13:25:34.828: INFO: Waiting for StatefulSet statefulset-9574/ss2 to complete update
    May  9 13:25:34.828: INFO: Waiting for Pod statefulset-9574/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  9 13:25:44.835: INFO: Deleting all statefulset in ns statefulset-9574
    May  9 13:25:44.836: INFO: Scaling statefulset ss2 to 0
    May  9 13:25:54.848: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 13:25:54.850: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  9 13:25:54.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9574" for this suite. 05/09/23 13:25:54.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:25:54.871
May  9 13:25:54.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename deployment 05/09/23 13:25:54.871
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:25:54.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:25:54.883
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
May  9 13:25:54.884: INFO: Creating deployment "webserver-deployment"
May  9 13:25:54.887: INFO: Waiting for observed generation 1
May  9 13:25:56.891: INFO: Waiting for all required pods to come up
May  9 13:25:56.894: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 05/09/23 13:25:56.894
May  9 13:25:56.894: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-bmfjp" in namespace "deployment-9339" to be "running"
May  9 13:25:56.896: INFO: Pod "webserver-deployment-845c8977d9-bmfjp": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574207ms
May  9 13:25:58.899: INFO: Pod "webserver-deployment-845c8977d9-bmfjp": Phase="Running", Reason="", readiness=true. Elapsed: 2.004978057s
May  9 13:25:58.899: INFO: Pod "webserver-deployment-845c8977d9-bmfjp" satisfied condition "running"
May  9 13:25:58.899: INFO: Waiting for deployment "webserver-deployment" to complete
May  9 13:25:58.903: INFO: Updating deployment "webserver-deployment" with a non-existent image
May  9 13:25:58.909: INFO: Updating deployment webserver-deployment
May  9 13:25:58.909: INFO: Waiting for observed generation 2
May  9 13:26:00.916: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May  9 13:26:00.917: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May  9 13:26:00.919: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  9 13:26:00.927: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May  9 13:26:00.927: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May  9 13:26:00.928: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  9 13:26:00.932: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May  9 13:26:00.932: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May  9 13:26:00.941: INFO: Updating deployment webserver-deployment
May  9 13:26:00.941: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May  9 13:26:00.945: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May  9 13:26:00.950: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 13:26:00.958: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9339  e789e915-7f36-4e79-9fdd-9fc1450c4762 36487 3 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045d2af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 13:25:56 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-05-09 13:25:58 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May  9 13:26:00.967: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-9339  b77af3b2-ffeb-4a92-8187-85c150e839f3 36491 3 2023-05-09 13:25:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e789e915-7f36-4e79-9fdd-9fc1450c4762 0xc004999757 0xc004999758}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e789e915-7f36-4e79-9fdd-9fc1450c4762\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049997f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 13:26:00.967: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May  9 13:26:00.967: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-9339  69e7475b-f9d3-40d4-8e52-06ddc82da21e 36488 3 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e789e915-7f36-4e79-9fdd-9fc1450c4762 0xc004999857 0xc004999858}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e789e915-7f36-4e79-9fdd-9fc1450c4762\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049998e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May  9 13:26:00.978: INFO: Pod "webserver-deployment-69b7448995-5bjjr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5bjjr webserver-deployment-69b7448995- deployment-9339  7d460c3b-3244-4ee5-bc59-7d1dbfd7a973 36388 0 2023-05-09 13:25:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d47aa65ea1ac1129b6752ff78383a38f9a74eb82e2536919442be085a539c871 cni.projectcalico.org/podIP:172.25.72.224/32 cni.projectcalico.org/podIPs:172.25.72.224/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5d1b7 0xc003d5d1b8}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 13:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jbb9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jbb9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-skqqr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.89,PodIP:,StartTime:2023-05-09 13:25:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.978: INFO: Pod "webserver-deployment-69b7448995-5jqrz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5jqrz webserver-deployment-69b7448995- deployment-9339  9396a6a7-873e-4047-ae2c-b916db7cbfe5 36507 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5d3c7 0xc003d5d3c8}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zjlbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zjlbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.979: INFO: Pod "webserver-deployment-69b7448995-6mv57" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6mv57 webserver-deployment-69b7448995- deployment-9339  fc8186ff-910b-4a90-b8bc-488486049815 36387 0 2023-05-09 13:25:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:65473169306c245d507d7292383d58201ba0605d83796d6ebda655f79acd4cbb cni.projectcalico.org/podIP:172.25.53.146/32 cni.projectcalico.org/podIPs:172.25.53.146/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5d537 0xc003d5d538}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 13:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dgv2r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgv2r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:,StartTime:2023-05-09 13:25:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.979: INFO: Pod "webserver-deployment-69b7448995-79644" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-79644 webserver-deployment-69b7448995- deployment-9339  8693e546-5fde-47c0-956b-9432c43b4e4b 36485 0 2023-05-09 13:25:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6e1b04399c3ffb3ab9335e28ea0b61455928d05289682048d07f409858fd05e5 cni.projectcalico.org/podIP:172.25.124.241/32 cni.projectcalico.org/podIPs:172.25.124.241/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5d737 0xc003d5d738}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wfvv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wfvv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.241,StartTime:2023-05-09 13:25:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.241,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.979: INFO: Pod "webserver-deployment-69b7448995-j6ccm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-j6ccm webserver-deployment-69b7448995- deployment-9339  cfcb3e78-b0c7-43ff-9b99-98c5ca6dfce5 36502 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5d967 0xc003d5d968}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9hdpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9hdpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-skqqr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:26:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.979: INFO: Pod "webserver-deployment-69b7448995-knjlj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-knjlj webserver-deployment-69b7448995- deployment-9339  009fae3c-e4ca-4e36-ba23-fe6e4fa04f11 36408 0 2023-05-09 13:25:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:9ed10c6db4364267e7f37408b3513d09da5cd0efbcc6bbc5f5c8c56a0fc9a574 cni.projectcalico.org/podIP:172.25.124.238/32 cni.projectcalico.org/podIPs:172.25.124.238/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5db90 0xc003d5db91}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 13:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2l7b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2l7b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:,StartTime:2023-05-09 13:25:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.979: INFO: Pod "webserver-deployment-69b7448995-qsb4b" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qsb4b webserver-deployment-69b7448995- deployment-9339  2c190628-1818-4f5a-ad2d-90c12d93aa02 36509 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5dfb7 0xc003d5dfb8}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-px69t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-px69t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.979: INFO: Pod "webserver-deployment-69b7448995-r8sh2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-r8sh2 webserver-deployment-69b7448995- deployment-9339  50129e2e-dddf-4cef-bab5-5e60bc534512 36392 0 2023-05-09 13:25:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:0bfb3a7d2de06e55dffda14b444ca97b45d4ccd3e9008e639c4c9ccbb9a15a3e cni.projectcalico.org/podIP:172.25.53.142/32 cni.projectcalico.org/podIPs:172.25.53.142/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc00410a127 0xc00410a128}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 13:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sm87k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sm87k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:,StartTime:2023-05-09 13:25:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.979: INFO: Pod "webserver-deployment-845c8977d9-29d76" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-29d76 webserver-deployment-845c8977d9- deployment-9339  cdc3287a-1d70-46c5-90d1-43049c1dfda8 36494 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410a327 0xc00410a328}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jlx9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jlx9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:26:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.979: INFO: Pod "webserver-deployment-845c8977d9-2rdgr" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-2rdgr webserver-deployment-845c8977d9- deployment-9339  3b7f512b-c53a-45d3-91ca-9fc80cbb8e9f 36308 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1903903c5116e7dac46e87a7d60b0320c86cd71dbef0ca42da90c918ec84ed5e cni.projectcalico.org/podIP:172.25.72.247/32 cni.projectcalico.org/podIPs:172.25.72.247/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410a4a0 0xc00410a4a1}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.72.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wx7tf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wx7tf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-skqqr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.89,PodIP:172.25.72.247,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3adc134c58a179702f5fef7461b1b063cc0c6c51484a4f516b2c091f9abea6b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.72.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.979: INFO: Pod "webserver-deployment-845c8977d9-2z64h" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-2z64h webserver-deployment-845c8977d9- deployment-9339  70e3c36c-965b-47b1-9c8c-d944320ad46f 36302 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c90b25647283a2d467ed505ab84c35f372f2cf96847397a6684abea30e779926 cni.projectcalico.org/podIP:172.25.124.235/32 cni.projectcalico.org/podIPs:172.25.124.235/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410a6a7 0xc00410a6a8}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qkz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qkz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.235,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9f89cdfcbc618944640e3f95e5ef53aaa8a95bbfaf8d726cbec85b6dcc16b8b0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.979: INFO: Pod "webserver-deployment-845c8977d9-79grd" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-79grd webserver-deployment-845c8977d9- deployment-9339  eb6b538f-ff40-4783-a672-2c6cb54dc641 36505 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410a8a7 0xc00410a8a8}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ndzm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ndzm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.979: INFO: Pod "webserver-deployment-845c8977d9-7cwrt" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7cwrt webserver-deployment-845c8977d9- deployment-9339  ffc59ac7-6e10-408b-8eee-4a8f5a6a3d63 36295 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4d54dbd20c0c13aad847560da772060ccef48a39c8a93eb2c26c936ae84dec0a cni.projectcalico.org/podIP:172.25.53.147/32 cni.projectcalico.org/podIPs:172.25.53.147/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410aa27 0xc00410aa28}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5zx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5zx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.147,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d1fa7f9b526995b280c91e99793fa783e48ba2c6f46a80af4ec205773ac6e62f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.979: INFO: Pod "webserver-deployment-845c8977d9-8swz5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8swz5 webserver-deployment-845c8977d9- deployment-9339  1ea8c5dc-233d-456f-9d75-47210b2830cd 36503 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410ac37 0xc00410ac38}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7rx8s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7rx8s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-bmfjp" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bmfjp webserver-deployment-845c8977d9- deployment-9339  62018cae-c309-489e-8d30-03eb1d7581ca 36321 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:36f4c05a7cbc279bff2758832d1c73a241629b162da2527574536a1b029c80c9 cni.projectcalico.org/podIP:172.25.124.239/32 cni.projectcalico.org/podIPs:172.25.124.239/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410ad87 0xc00410ad88}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxn6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxn6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.239,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://29748df26395a8bdd7a313e3c6c6111e475690b9b3543ed5724a4041ad00c026,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-c9w7k" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-c9w7k webserver-deployment-845c8977d9- deployment-9339  e193b267-cabc-4d29-b1bd-11c1119358c8 36305 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f7e0eefc79a38f1ee204a3bfc77f23beaaf1314e7124a684a35befca803a1a32 cni.projectcalico.org/podIP:172.25.72.221/32 cni.projectcalico.org/podIPs:172.25.72.221/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410afe7 0xc00410afe8}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.72.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gdkns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gdkns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-skqqr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.89,PodIP:172.25.72.221,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://099b21846fa6f8b7a2cfae09bbad2dab9a91b2fba0dbb5ceec1e5d2977eec9b3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.72.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-g88kg" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-g88kg webserver-deployment-845c8977d9- deployment-9339  9e9e095a-fb8b-48cc-969d-1144bcb782c6 36292 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0d02712ca3cd8a0dccd38fd65516745496a30b0ab4f594660c47577fa6b459a2 cni.projectcalico.org/podIP:172.25.53.135/32 cni.projectcalico.org/podIPs:172.25.53.135/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410b207 0xc00410b208}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wv7vh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wv7vh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.135,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://db719cf64ea1bdf38a4bf370d948a6fdb4697bc2069234b6f091fa7cca2a36f3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-jsg7c" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jsg7c webserver-deployment-845c8977d9- deployment-9339  3142f3ce-1877-4697-ac77-bca55615d5bc 36506 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410b417 0xc00410b418}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q7n6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q7n6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-k676j" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-k676j webserver-deployment-845c8977d9- deployment-9339  045e9887-f4ac-4bf5-b139-7637f7de6346 36312 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:257dece66f72233e0000acc162066ec110d24eae18e40646421a41fd15e5bbdd cni.projectcalico.org/podIP:172.25.72.232/32 cni.projectcalico.org/podIPs:172.25.72.232/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410b587 0xc00410b588}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.72.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87mzj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87mzj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-skqqr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.89,PodIP:172.25.72.232,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9cbe291111e94e6121bb9621203089f4ad65b459487f0c171c0027a3d9c4af45,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.72.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-m25x9" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-m25x9 webserver-deployment-845c8977d9- deployment-9339  39afbd16-2e60-40c8-8ed6-20a73e793476 36501 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410b7c7 0xc00410b7c8}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vr27s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vr27s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:26:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-t986v" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-t986v webserver-deployment-845c8977d9- deployment-9339  feee0424-f1b9-4905-9f19-bf018f86901d 36499 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410b930 0xc00410b931}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfwkk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfwkk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:26:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-w696b" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-w696b webserver-deployment-845c8977d9- deployment-9339  e162983a-e749-4198-9f0f-a67a9d9033e7 36297 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8e28f62157ba11f7d84fa175e1baa5926053eeb936a375beeaaea6e8a3ec3988 cni.projectcalico.org/podIP:172.25.53.134/32 cni.projectcalico.org/podIPs:172.25.53.134/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410bac0 0xc00410bac1}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c9jkn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c9jkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.134,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9bb4f67ccb4764c41a852b542e6190b78eb849db0f2c17ec0a402a419c338949,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-xgk8l" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xgk8l webserver-deployment-845c8977d9- deployment-9339  2ed871e3-4d8d-439e-b146-34eb323f7979 36508 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410bcb7 0xc00410bcb8}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbkjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbkjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  9 13:26:00.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9339" for this suite. 05/09/23 13:26:00.988
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":221,"skipped":4497,"failed":0}
------------------------------
• [SLOW TEST] [6.134 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:25:54.871
    May  9 13:25:54.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename deployment 05/09/23 13:25:54.871
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:25:54.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:25:54.883
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    May  9 13:25:54.884: INFO: Creating deployment "webserver-deployment"
    May  9 13:25:54.887: INFO: Waiting for observed generation 1
    May  9 13:25:56.891: INFO: Waiting for all required pods to come up
    May  9 13:25:56.894: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 05/09/23 13:25:56.894
    May  9 13:25:56.894: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-bmfjp" in namespace "deployment-9339" to be "running"
    May  9 13:25:56.896: INFO: Pod "webserver-deployment-845c8977d9-bmfjp": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574207ms
    May  9 13:25:58.899: INFO: Pod "webserver-deployment-845c8977d9-bmfjp": Phase="Running", Reason="", readiness=true. Elapsed: 2.004978057s
    May  9 13:25:58.899: INFO: Pod "webserver-deployment-845c8977d9-bmfjp" satisfied condition "running"
    May  9 13:25:58.899: INFO: Waiting for deployment "webserver-deployment" to complete
    May  9 13:25:58.903: INFO: Updating deployment "webserver-deployment" with a non-existent image
    May  9 13:25:58.909: INFO: Updating deployment webserver-deployment
    May  9 13:25:58.909: INFO: Waiting for observed generation 2
    May  9 13:26:00.916: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    May  9 13:26:00.917: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    May  9 13:26:00.919: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May  9 13:26:00.927: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    May  9 13:26:00.927: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    May  9 13:26:00.928: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May  9 13:26:00.932: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    May  9 13:26:00.932: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    May  9 13:26:00.941: INFO: Updating deployment webserver-deployment
    May  9 13:26:00.941: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    May  9 13:26:00.945: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    May  9 13:26:00.950: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 13:26:00.958: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-9339  e789e915-7f36-4e79-9fdd-9fc1450c4762 36487 3 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045d2af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 13:25:56 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-05-09 13:25:58 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    May  9 13:26:00.967: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-9339  b77af3b2-ffeb-4a92-8187-85c150e839f3 36491 3 2023-05-09 13:25:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e789e915-7f36-4e79-9fdd-9fc1450c4762 0xc004999757 0xc004999758}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e789e915-7f36-4e79-9fdd-9fc1450c4762\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049997f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 13:26:00.967: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    May  9 13:26:00.967: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-9339  69e7475b-f9d3-40d4-8e52-06ddc82da21e 36488 3 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e789e915-7f36-4e79-9fdd-9fc1450c4762 0xc004999857 0xc004999858}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e789e915-7f36-4e79-9fdd-9fc1450c4762\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049998e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    May  9 13:26:00.978: INFO: Pod "webserver-deployment-69b7448995-5bjjr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5bjjr webserver-deployment-69b7448995- deployment-9339  7d460c3b-3244-4ee5-bc59-7d1dbfd7a973 36388 0 2023-05-09 13:25:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d47aa65ea1ac1129b6752ff78383a38f9a74eb82e2536919442be085a539c871 cni.projectcalico.org/podIP:172.25.72.224/32 cni.projectcalico.org/podIPs:172.25.72.224/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5d1b7 0xc003d5d1b8}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 13:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jbb9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jbb9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-skqqr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.89,PodIP:,StartTime:2023-05-09 13:25:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.978: INFO: Pod "webserver-deployment-69b7448995-5jqrz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5jqrz webserver-deployment-69b7448995- deployment-9339  9396a6a7-873e-4047-ae2c-b916db7cbfe5 36507 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5d3c7 0xc003d5d3c8}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zjlbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zjlbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.979: INFO: Pod "webserver-deployment-69b7448995-6mv57" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6mv57 webserver-deployment-69b7448995- deployment-9339  fc8186ff-910b-4a90-b8bc-488486049815 36387 0 2023-05-09 13:25:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:65473169306c245d507d7292383d58201ba0605d83796d6ebda655f79acd4cbb cni.projectcalico.org/podIP:172.25.53.146/32 cni.projectcalico.org/podIPs:172.25.53.146/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5d537 0xc003d5d538}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 13:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dgv2r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgv2r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:,StartTime:2023-05-09 13:25:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.979: INFO: Pod "webserver-deployment-69b7448995-79644" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-79644 webserver-deployment-69b7448995- deployment-9339  8693e546-5fde-47c0-956b-9432c43b4e4b 36485 0 2023-05-09 13:25:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6e1b04399c3ffb3ab9335e28ea0b61455928d05289682048d07f409858fd05e5 cni.projectcalico.org/podIP:172.25.124.241/32 cni.projectcalico.org/podIPs:172.25.124.241/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5d737 0xc003d5d738}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wfvv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wfvv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.241,StartTime:2023-05-09 13:25:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.241,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.979: INFO: Pod "webserver-deployment-69b7448995-j6ccm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-j6ccm webserver-deployment-69b7448995- deployment-9339  cfcb3e78-b0c7-43ff-9b99-98c5ca6dfce5 36502 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5d967 0xc003d5d968}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9hdpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9hdpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-skqqr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:26:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.979: INFO: Pod "webserver-deployment-69b7448995-knjlj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-knjlj webserver-deployment-69b7448995- deployment-9339  009fae3c-e4ca-4e36-ba23-fe6e4fa04f11 36408 0 2023-05-09 13:25:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:9ed10c6db4364267e7f37408b3513d09da5cd0efbcc6bbc5f5c8c56a0fc9a574 cni.projectcalico.org/podIP:172.25.124.238/32 cni.projectcalico.org/podIPs:172.25.124.238/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5db90 0xc003d5db91}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 13:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2l7b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2l7b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:,StartTime:2023-05-09 13:25:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.979: INFO: Pod "webserver-deployment-69b7448995-qsb4b" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qsb4b webserver-deployment-69b7448995- deployment-9339  2c190628-1818-4f5a-ad2d-90c12d93aa02 36509 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc003d5dfb7 0xc003d5dfb8}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-px69t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-px69t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.979: INFO: Pod "webserver-deployment-69b7448995-r8sh2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-r8sh2 webserver-deployment-69b7448995- deployment-9339  50129e2e-dddf-4cef-bab5-5e60bc534512 36392 0 2023-05-09 13:25:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:0bfb3a7d2de06e55dffda14b444ca97b45d4ccd3e9008e639c4c9ccbb9a15a3e cni.projectcalico.org/podIP:172.25.53.142/32 cni.projectcalico.org/podIPs:172.25.53.142/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 b77af3b2-ffeb-4a92-8187-85c150e839f3 0xc00410a127 0xc00410a128}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77af3b2-ffeb-4a92-8187-85c150e839f3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 13:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sm87k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sm87k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:,StartTime:2023-05-09 13:25:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.979: INFO: Pod "webserver-deployment-845c8977d9-29d76" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-29d76 webserver-deployment-845c8977d9- deployment-9339  cdc3287a-1d70-46c5-90d1-43049c1dfda8 36494 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410a327 0xc00410a328}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jlx9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jlx9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:26:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.979: INFO: Pod "webserver-deployment-845c8977d9-2rdgr" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-2rdgr webserver-deployment-845c8977d9- deployment-9339  3b7f512b-c53a-45d3-91ca-9fc80cbb8e9f 36308 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1903903c5116e7dac46e87a7d60b0320c86cd71dbef0ca42da90c918ec84ed5e cni.projectcalico.org/podIP:172.25.72.247/32 cni.projectcalico.org/podIPs:172.25.72.247/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410a4a0 0xc00410a4a1}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.72.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wx7tf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wx7tf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-skqqr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.89,PodIP:172.25.72.247,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3adc134c58a179702f5fef7461b1b063cc0c6c51484a4f516b2c091f9abea6b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.72.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.979: INFO: Pod "webserver-deployment-845c8977d9-2z64h" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-2z64h webserver-deployment-845c8977d9- deployment-9339  70e3c36c-965b-47b1-9c8c-d944320ad46f 36302 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c90b25647283a2d467ed505ab84c35f372f2cf96847397a6684abea30e779926 cni.projectcalico.org/podIP:172.25.124.235/32 cni.projectcalico.org/podIPs:172.25.124.235/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410a6a7 0xc00410a6a8}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qkz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qkz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.235,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9f89cdfcbc618944640e3f95e5ef53aaa8a95bbfaf8d726cbec85b6dcc16b8b0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.979: INFO: Pod "webserver-deployment-845c8977d9-79grd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-79grd webserver-deployment-845c8977d9- deployment-9339  eb6b538f-ff40-4783-a672-2c6cb54dc641 36505 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410a8a7 0xc00410a8a8}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ndzm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ndzm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.979: INFO: Pod "webserver-deployment-845c8977d9-7cwrt" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7cwrt webserver-deployment-845c8977d9- deployment-9339  ffc59ac7-6e10-408b-8eee-4a8f5a6a3d63 36295 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4d54dbd20c0c13aad847560da772060ccef48a39c8a93eb2c26c936ae84dec0a cni.projectcalico.org/podIP:172.25.53.147/32 cni.projectcalico.org/podIPs:172.25.53.147/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410aa27 0xc00410aa28}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5zx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5zx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.147,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d1fa7f9b526995b280c91e99793fa783e48ba2c6f46a80af4ec205773ac6e62f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.979: INFO: Pod "webserver-deployment-845c8977d9-8swz5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8swz5 webserver-deployment-845c8977d9- deployment-9339  1ea8c5dc-233d-456f-9d75-47210b2830cd 36503 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410ac37 0xc00410ac38}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7rx8s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7rx8s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-bmfjp" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bmfjp webserver-deployment-845c8977d9- deployment-9339  62018cae-c309-489e-8d30-03eb1d7581ca 36321 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:36f4c05a7cbc279bff2758832d1c73a241629b162da2527574536a1b029c80c9 cni.projectcalico.org/podIP:172.25.124.239/32 cni.projectcalico.org/podIPs:172.25.124.239/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410ad87 0xc00410ad88}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxn6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxn6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.239,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://29748df26395a8bdd7a313e3c6c6111e475690b9b3543ed5724a4041ad00c026,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-c9w7k" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-c9w7k webserver-deployment-845c8977d9- deployment-9339  e193b267-cabc-4d29-b1bd-11c1119358c8 36305 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f7e0eefc79a38f1ee204a3bfc77f23beaaf1314e7124a684a35befca803a1a32 cni.projectcalico.org/podIP:172.25.72.221/32 cni.projectcalico.org/podIPs:172.25.72.221/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410afe7 0xc00410afe8}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.72.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gdkns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gdkns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-skqqr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.89,PodIP:172.25.72.221,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://099b21846fa6f8b7a2cfae09bbad2dab9a91b2fba0dbb5ceec1e5d2977eec9b3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.72.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-g88kg" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-g88kg webserver-deployment-845c8977d9- deployment-9339  9e9e095a-fb8b-48cc-969d-1144bcb782c6 36292 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0d02712ca3cd8a0dccd38fd65516745496a30b0ab4f594660c47577fa6b459a2 cni.projectcalico.org/podIP:172.25.53.135/32 cni.projectcalico.org/podIPs:172.25.53.135/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410b207 0xc00410b208}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wv7vh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wv7vh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.135,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://db719cf64ea1bdf38a4bf370d948a6fdb4697bc2069234b6f091fa7cca2a36f3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-jsg7c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jsg7c webserver-deployment-845c8977d9- deployment-9339  3142f3ce-1877-4697-ac77-bca55615d5bc 36506 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410b417 0xc00410b418}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q7n6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q7n6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-k676j" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-k676j webserver-deployment-845c8977d9- deployment-9339  045e9887-f4ac-4bf5-b139-7637f7de6346 36312 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:257dece66f72233e0000acc162066ec110d24eae18e40646421a41fd15e5bbdd cni.projectcalico.org/podIP:172.25.72.232/32 cni.projectcalico.org/podIPs:172.25.72.232/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410b587 0xc00410b588}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.72.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87mzj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87mzj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-skqqr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.89,PodIP:172.25.72.232,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9cbe291111e94e6121bb9621203089f4ad65b459487f0c171c0027a3d9c4af45,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.72.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-m25x9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-m25x9 webserver-deployment-845c8977d9- deployment-9339  39afbd16-2e60-40c8-8ed6-20a73e793476 36501 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410b7c7 0xc00410b7c8}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vr27s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vr27s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:26:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-t986v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-t986v webserver-deployment-845c8977d9- deployment-9339  feee0424-f1b9-4905-9f19-bf018f86901d 36499 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410b930 0xc00410b931}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfwkk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfwkk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:26:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-w696b" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-w696b webserver-deployment-845c8977d9- deployment-9339  e162983a-e749-4198-9f0f-a67a9d9033e7 36297 0 2023-05-09 13:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8e28f62157ba11f7d84fa175e1baa5926053eeb936a375beeaaea6e8a3ec3988 cni.projectcalico.org/podIP:172.25.53.134/32 cni.projectcalico.org/podIPs:172.25.53.134/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410bac0 0xc00410bac1}] [] [{kube-controller-manager Update v1 2023-05-09 13:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c9jkn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c9jkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.134,StartTime:2023-05-09 13:25:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:25:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9bb4f67ccb4764c41a852b542e6190b78eb849db0f2c17ec0a402a419c338949,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 13:26:00.980: INFO: Pod "webserver-deployment-845c8977d9-xgk8l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xgk8l webserver-deployment-845c8977d9- deployment-9339  2ed871e3-4d8d-439e-b146-34eb323f7979 36508 0 2023-05-09 13:26:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 69e7475b-f9d3-40d4-8e52-06ddc82da21e 0xc00410bcb7 0xc00410bcb8}] [] [{kube-controller-manager Update v1 2023-05-09 13:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69e7475b-f9d3-40d4-8e52-06ddc82da21e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbkjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbkjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  9 13:26:00.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9339" for this suite. 05/09/23 13:26:00.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:26:01.006
May  9 13:26:01.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 13:26:01.007
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:26:01.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:26:01.033
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-eef46cd5-d063-4d05-9639-5a7de01c7ff9 05/09/23 13:26:01.035
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May  9 13:26:01.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3310" for this suite. 05/09/23 13:26:01.044
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":222,"skipped":4507,"failed":0}
------------------------------
• [0.049 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:26:01.006
    May  9 13:26:01.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 13:26:01.007
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:26:01.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:26:01.033
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-eef46cd5-d063-4d05-9639-5a7de01c7ff9 05/09/23 13:26:01.035
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 13:26:01.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3310" for this suite. 05/09/23 13:26:01.044
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:26:01.055
May  9 13:26:01.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 13:26:01.056
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:26:01.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:26:01.072
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 05/09/23 13:26:01.074
May  9 13:26:01.078: INFO: Waiting up to 5m0s for pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d" in namespace "downward-api-5389" to be "Succeeded or Failed"
May  9 13:26:01.080: INFO: Pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.971062ms
May  9 13:26:03.084: INFO: Pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005548604s
May  9 13:26:05.083: INFO: Pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004702785s
May  9 13:26:07.083: INFO: Pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004670322s
May  9 13:26:09.083: INFO: Pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.004773538s
STEP: Saw pod success 05/09/23 13:26:09.083
May  9 13:26:09.083: INFO: Pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d" satisfied condition "Succeeded or Failed"
May  9 13:26:09.085: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d container dapi-container: <nil>
STEP: delete the pod 05/09/23 13:26:09.332
May  9 13:26:09.341: INFO: Waiting for pod downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d to disappear
May  9 13:26:09.345: INFO: Pod downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  9 13:26:09.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5389" for this suite. 05/09/23 13:26:09.348
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":223,"skipped":4510,"failed":0}
------------------------------
• [SLOW TEST] [8.297 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:26:01.055
    May  9 13:26:01.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 13:26:01.056
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:26:01.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:26:01.072
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 05/09/23 13:26:01.074
    May  9 13:26:01.078: INFO: Waiting up to 5m0s for pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d" in namespace "downward-api-5389" to be "Succeeded or Failed"
    May  9 13:26:01.080: INFO: Pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.971062ms
    May  9 13:26:03.084: INFO: Pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005548604s
    May  9 13:26:05.083: INFO: Pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004702785s
    May  9 13:26:07.083: INFO: Pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004670322s
    May  9 13:26:09.083: INFO: Pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.004773538s
    STEP: Saw pod success 05/09/23 13:26:09.083
    May  9 13:26:09.083: INFO: Pod "downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d" satisfied condition "Succeeded or Failed"
    May  9 13:26:09.085: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d container dapi-container: <nil>
    STEP: delete the pod 05/09/23 13:26:09.332
    May  9 13:26:09.341: INFO: Waiting for pod downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d to disappear
    May  9 13:26:09.345: INFO: Pod downward-api-94775e7b-0953-4cde-b9f7-53d770b17d8d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  9 13:26:09.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5389" for this suite. 05/09/23 13:26:09.348
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:26:09.353
May  9 13:26:09.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename dns 05/09/23 13:26:09.353
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:26:09.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:26:09.366
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 05/09/23 13:26:09.367
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6964 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6964;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6964 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6964;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6964.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6964.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6964.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6964.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6964.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6964.svc;check="$$(dig +notcp +noall +answer +search 117.113.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.113.117_udp@PTR;check="$$(dig +tcp +noall +answer +search 117.113.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.113.117_tcp@PTR;sleep 1; done
 05/09/23 13:26:09.384
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6964 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6964;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6964 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6964;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6964.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6964.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6964.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6964.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6964.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6964.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6964.svc;check="$$(dig +notcp +noall +answer +search 117.113.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.113.117_udp@PTR;check="$$(dig +tcp +noall +answer +search 117.113.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.113.117_tcp@PTR;sleep 1; done
 05/09/23 13:26:09.384
STEP: creating a pod to probe DNS 05/09/23 13:26:09.384
STEP: submitting the pod to kubernetes 05/09/23 13:26:09.384
May  9 13:26:09.395: INFO: Waiting up to 15m0s for pod "dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2" in namespace "dns-6964" to be "running"
May  9 13:26:09.397: INFO: Pod "dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.617725ms
May  9 13:26:11.402: INFO: Pod "dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006879564s
May  9 13:26:11.402: INFO: Pod "dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2" satisfied condition "running"
STEP: retrieving the pod 05/09/23 13:26:11.402
STEP: looking for the results for each expected name from probers 05/09/23 13:26:11.404
May  9 13:26:11.407: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.409: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.411: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.413: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.415: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.417: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.419: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.421: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.430: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.432: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.434: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.436: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.438: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.439: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.441: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.443: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:11.450: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

May  9 13:26:16.458: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.460: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.462: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.464: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.465: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.467: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.469: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.470: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.479: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.480: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.482: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.483: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.485: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.487: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.489: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.491: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:16.498: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

May  9 13:26:21.454: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.456: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.457: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.459: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.461: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.462: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.464: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.466: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.474: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.476: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.478: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.479: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.481: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.482: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.484: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.486: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:21.492: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

May  9 13:26:26.457: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.460: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.461: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.463: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.465: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.466: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.468: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.470: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.478: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.480: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.482: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.484: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.485: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.487: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.488: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.490: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:26.499: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

May  9 13:26:31.456: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.458: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.460: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.462: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.464: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.466: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.468: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.469: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.478: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.479: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.481: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.482: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.484: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.486: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.488: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.489: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:31.496: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

May  9 13:26:36.455: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.458: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.460: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.465: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.473: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.475: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.477: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.485: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.486: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.488: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.490: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.491: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.493: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.495: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.496: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:36.503: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

May  9 13:26:41.456: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.458: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.460: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.462: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.464: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.466: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.468: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.470: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.478: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.480: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.482: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.483: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.485: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.487: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.488: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.490: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
May  9 13:26:41.496: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

May  9 13:26:46.497: INFO: DNS probes using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 succeeded

STEP: deleting the pod 05/09/23 13:26:46.497
STEP: deleting the test service 05/09/23 13:26:46.507
STEP: deleting the test headless service 05/09/23 13:26:46.533
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  9 13:26:46.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6964" for this suite. 05/09/23 13:26:46.546
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":224,"skipped":4513,"failed":0}
------------------------------
• [SLOW TEST] [37.198 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:26:09.353
    May  9 13:26:09.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename dns 05/09/23 13:26:09.353
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:26:09.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:26:09.366
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 05/09/23 13:26:09.367
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6964 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6964;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6964 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6964;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6964.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6964.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6964.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6964.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6964.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6964.svc;check="$$(dig +notcp +noall +answer +search 117.113.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.113.117_udp@PTR;check="$$(dig +tcp +noall +answer +search 117.113.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.113.117_tcp@PTR;sleep 1; done
     05/09/23 13:26:09.384
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6964 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6964;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6964 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6964;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6964.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6964.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6964.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6964.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6964.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6964.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6964.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6964.svc;check="$$(dig +notcp +noall +answer +search 117.113.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.113.117_udp@PTR;check="$$(dig +tcp +noall +answer +search 117.113.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.113.117_tcp@PTR;sleep 1; done
     05/09/23 13:26:09.384
    STEP: creating a pod to probe DNS 05/09/23 13:26:09.384
    STEP: submitting the pod to kubernetes 05/09/23 13:26:09.384
    May  9 13:26:09.395: INFO: Waiting up to 15m0s for pod "dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2" in namespace "dns-6964" to be "running"
    May  9 13:26:09.397: INFO: Pod "dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.617725ms
    May  9 13:26:11.402: INFO: Pod "dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006879564s
    May  9 13:26:11.402: INFO: Pod "dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 13:26:11.402
    STEP: looking for the results for each expected name from probers 05/09/23 13:26:11.404
    May  9 13:26:11.407: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.409: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.411: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.413: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.415: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.417: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.419: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.421: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.430: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.432: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.434: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.436: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.438: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.439: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.441: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.443: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:11.450: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

    May  9 13:26:16.458: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.460: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.462: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.464: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.465: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.467: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.469: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.470: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.479: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.480: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.482: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.483: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.485: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.487: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.489: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.491: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:16.498: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

    May  9 13:26:21.454: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.456: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.457: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.459: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.461: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.462: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.464: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.466: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.474: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.476: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.478: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.479: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.481: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.482: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.484: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.486: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:21.492: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

    May  9 13:26:26.457: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.460: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.461: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.463: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.465: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.466: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.468: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.470: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.478: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.480: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.482: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.484: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.485: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.487: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.488: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.490: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:26.499: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

    May  9 13:26:31.456: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.458: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.460: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.462: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.464: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.466: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.468: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.469: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.478: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.479: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.481: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.482: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.484: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.486: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.488: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.489: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:31.496: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

    May  9 13:26:36.455: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.458: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.460: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.465: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.473: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.475: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.477: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.485: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.486: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.488: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.490: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.491: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.493: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.495: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.496: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:36.503: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

    May  9 13:26:41.456: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.458: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.460: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.462: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.464: INFO: Unable to read wheezy_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.466: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.468: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.470: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.478: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.480: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.482: INFO: Unable to read jessie_udp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.483: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964 from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.485: INFO: Unable to read jessie_udp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.487: INFO: Unable to read jessie_tcp@dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.488: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.490: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc from pod dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2: the server could not find the requested resource (get pods dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2)
    May  9 13:26:41.496: INFO: Lookups using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6964 wheezy_tcp@dns-test-service.dns-6964 wheezy_udp@dns-test-service.dns-6964.svc wheezy_tcp@dns-test-service.dns-6964.svc wheezy_udp@_http._tcp.dns-test-service.dns-6964.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6964.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6964 jessie_tcp@dns-test-service.dns-6964 jessie_udp@dns-test-service.dns-6964.svc jessie_tcp@dns-test-service.dns-6964.svc jessie_udp@_http._tcp.dns-test-service.dns-6964.svc jessie_tcp@_http._tcp.dns-test-service.dns-6964.svc]

    May  9 13:26:46.497: INFO: DNS probes using dns-6964/dns-test-c2ac3ee3-5c9e-4fc7-a1e1-d1fec61d36a2 succeeded

    STEP: deleting the pod 05/09/23 13:26:46.497
    STEP: deleting the test service 05/09/23 13:26:46.507
    STEP: deleting the test headless service 05/09/23 13:26:46.533
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  9 13:26:46.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6964" for this suite. 05/09/23 13:26:46.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:26:46.552
May  9 13:26:46.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename statefulset 05/09/23 13:26:46.553
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:26:46.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:26:46.567
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-466 05/09/23 13:26:46.569
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 05/09/23 13:26:46.572
May  9 13:26:46.585: INFO: Found 0 stateful pods, waiting for 3
May  9 13:26:56.588: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  9 13:26:56.588: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  9 13:26:56.588: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May  9 13:26:56.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-466 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 13:26:56.695: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 13:26:56.695: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 13:26:56.695: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/09/23 13:27:06.703
May  9 13:27:06.719: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/09/23 13:27:06.72
STEP: Updating Pods in reverse ordinal order 05/09/23 13:27:16.734
May  9 13:27:16.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-466 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 13:27:16.837: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 13:27:16.837: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 13:27:16.837: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 05/09/23 13:27:26.855
May  9 13:27:26.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-466 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 13:27:26.950: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 13:27:26.950: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 13:27:26.950: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 13:27:36.979: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 05/09/23 13:27:46.989
May  9 13:27:46.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-466 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 13:27:47.087: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 13:27:47.087: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 13:27:47.087: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  9 13:27:57.100: INFO: Deleting all statefulset in ns statefulset-466
May  9 13:27:57.101: INFO: Scaling statefulset ss2 to 0
May  9 13:28:07.112: INFO: Waiting for statefulset status.replicas updated to 0
May  9 13:28:07.113: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  9 13:28:07.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-466" for this suite. 05/09/23 13:28:07.137
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":225,"skipped":4528,"failed":0}
------------------------------
• [SLOW TEST] [80.589 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:26:46.552
    May  9 13:26:46.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename statefulset 05/09/23 13:26:46.553
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:26:46.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:26:46.567
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-466 05/09/23 13:26:46.569
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 05/09/23 13:26:46.572
    May  9 13:26:46.585: INFO: Found 0 stateful pods, waiting for 3
    May  9 13:26:56.588: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  9 13:26:56.588: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  9 13:26:56.588: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    May  9 13:26:56.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-466 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 13:26:56.695: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 13:26:56.695: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 13:26:56.695: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/09/23 13:27:06.703
    May  9 13:27:06.719: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/09/23 13:27:06.72
    STEP: Updating Pods in reverse ordinal order 05/09/23 13:27:16.734
    May  9 13:27:16.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-466 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 13:27:16.837: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 13:27:16.837: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 13:27:16.837: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 05/09/23 13:27:26.855
    May  9 13:27:26.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-466 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 13:27:26.950: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 13:27:26.950: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 13:27:26.950: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 13:27:36.979: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 05/09/23 13:27:46.989
    May  9 13:27:46.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-466 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 13:27:47.087: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 13:27:47.087: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 13:27:47.087: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  9 13:27:57.100: INFO: Deleting all statefulset in ns statefulset-466
    May  9 13:27:57.101: INFO: Scaling statefulset ss2 to 0
    May  9 13:28:07.112: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 13:28:07.113: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  9 13:28:07.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-466" for this suite. 05/09/23 13:28:07.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:28:07.141
May  9 13:28:07.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename containers 05/09/23 13:28:07.142
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:07.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:07.156
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 05/09/23 13:28:07.157
May  9 13:28:07.163: INFO: Waiting up to 5m0s for pod "client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8" in namespace "containers-615" to be "Succeeded or Failed"
May  9 13:28:07.166: INFO: Pod "client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.956579ms
May  9 13:28:09.169: INFO: Pod "client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005695718s
May  9 13:28:11.170: INFO: Pod "client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006665391s
STEP: Saw pod success 05/09/23 13:28:11.17
May  9 13:28:11.170: INFO: Pod "client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8" satisfied condition "Succeeded or Failed"
May  9 13:28:11.172: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 13:28:11.18
May  9 13:28:11.187: INFO: Waiting for pod client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8 to disappear
May  9 13:28:11.189: INFO: Pod client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May  9 13:28:11.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-615" for this suite. 05/09/23 13:28:11.191
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":226,"skipped":4541,"failed":0}
------------------------------
• [4.056 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:28:07.141
    May  9 13:28:07.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename containers 05/09/23 13:28:07.142
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:07.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:07.156
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 05/09/23 13:28:07.157
    May  9 13:28:07.163: INFO: Waiting up to 5m0s for pod "client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8" in namespace "containers-615" to be "Succeeded or Failed"
    May  9 13:28:07.166: INFO: Pod "client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.956579ms
    May  9 13:28:09.169: INFO: Pod "client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005695718s
    May  9 13:28:11.170: INFO: Pod "client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006665391s
    STEP: Saw pod success 05/09/23 13:28:11.17
    May  9 13:28:11.170: INFO: Pod "client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8" satisfied condition "Succeeded or Failed"
    May  9 13:28:11.172: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 13:28:11.18
    May  9 13:28:11.187: INFO: Waiting for pod client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8 to disappear
    May  9 13:28:11.189: INFO: Pod client-containers-e3439ace-40f7-4cf8-b646-d0ff21d691f8 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May  9 13:28:11.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-615" for this suite. 05/09/23 13:28:11.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:28:11.197
May  9 13:28:11.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:28:11.198
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:11.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:11.209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:28:11.217
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:28:11.438
STEP: Deploying the webhook pod 05/09/23 13:28:11.446
STEP: Wait for the deployment to be ready 05/09/23 13:28:11.456
May  9 13:28:11.461: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 13:28:13.468
STEP: Verifying the service has paired with the endpoint 05/09/23 13:28:13.483
May  9 13:28:14.483: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/09/23 13:28:14.486
STEP: create a namespace for the webhook 05/09/23 13:28:14.497
STEP: create a configmap should be unconditionally rejected by the webhook 05/09/23 13:28:14.502
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:28:14.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2720" for this suite. 05/09/23 13:28:14.52
STEP: Destroying namespace "webhook-2720-markers" for this suite. 05/09/23 13:28:14.524
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":227,"skipped":4555,"failed":0}
------------------------------
• [3.358 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:28:11.197
    May  9 13:28:11.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:28:11.198
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:11.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:11.209
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:28:11.217
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:28:11.438
    STEP: Deploying the webhook pod 05/09/23 13:28:11.446
    STEP: Wait for the deployment to be ready 05/09/23 13:28:11.456
    May  9 13:28:11.461: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 13:28:13.468
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:28:13.483
    May  9 13:28:14.483: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/09/23 13:28:14.486
    STEP: create a namespace for the webhook 05/09/23 13:28:14.497
    STEP: create a configmap should be unconditionally rejected by the webhook 05/09/23 13:28:14.502
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:28:14.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2720" for this suite. 05/09/23 13:28:14.52
    STEP: Destroying namespace "webhook-2720-markers" for this suite. 05/09/23 13:28:14.524
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:28:14.556
May  9 13:28:14.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:28:14.556
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:14.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:14.569
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-6e4888b1-a05f-476a-8e10-033058c9c44c 05/09/23 13:28:14.571
STEP: Creating a pod to test consume secrets 05/09/23 13:28:14.576
May  9 13:28:14.584: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a" in namespace "projected-704" to be "Succeeded or Failed"
May  9 13:28:14.587: INFO: Pod "pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.481598ms
May  9 13:28:16.591: INFO: Pod "pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007064197s
May  9 13:28:18.590: INFO: Pod "pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006884907s
STEP: Saw pod success 05/09/23 13:28:18.59
May  9 13:28:18.591: INFO: Pod "pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a" satisfied condition "Succeeded or Failed"
May  9 13:28:18.593: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 13:28:18.597
May  9 13:28:18.606: INFO: Waiting for pod pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a to disappear
May  9 13:28:18.608: INFO: Pod pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  9 13:28:18.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-704" for this suite. 05/09/23 13:28:18.611
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":228,"skipped":4557,"failed":0}
------------------------------
• [4.059 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:28:14.556
    May  9 13:28:14.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:28:14.556
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:14.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:14.569
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-6e4888b1-a05f-476a-8e10-033058c9c44c 05/09/23 13:28:14.571
    STEP: Creating a pod to test consume secrets 05/09/23 13:28:14.576
    May  9 13:28:14.584: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a" in namespace "projected-704" to be "Succeeded or Failed"
    May  9 13:28:14.587: INFO: Pod "pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.481598ms
    May  9 13:28:16.591: INFO: Pod "pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007064197s
    May  9 13:28:18.590: INFO: Pod "pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006884907s
    STEP: Saw pod success 05/09/23 13:28:18.59
    May  9 13:28:18.591: INFO: Pod "pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a" satisfied condition "Succeeded or Failed"
    May  9 13:28:18.593: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:28:18.597
    May  9 13:28:18.606: INFO: Waiting for pod pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a to disappear
    May  9 13:28:18.608: INFO: Pod pod-projected-secrets-42c2d3b2-1174-42f6-81a9-5d049b6fb11a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  9 13:28:18.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-704" for this suite. 05/09/23 13:28:18.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:28:18.615
May  9 13:28:18.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pods 05/09/23 13:28:18.615
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:18.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:18.626
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
May  9 13:28:18.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: creating the pod 05/09/23 13:28:18.628
STEP: submitting the pod to kubernetes 05/09/23 13:28:18.629
May  9 13:28:18.634: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-6c4bffe2-1cb4-43b2-bc13-d4f7b3de895c" in namespace "pods-6885" to be "running and ready"
May  9 13:28:18.636: INFO: Pod "pod-logs-websocket-6c4bffe2-1cb4-43b2-bc13-d4f7b3de895c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.616787ms
May  9 13:28:18.636: INFO: The phase of Pod pod-logs-websocket-6c4bffe2-1cb4-43b2-bc13-d4f7b3de895c is Pending, waiting for it to be Running (with Ready = true)
May  9 13:28:20.640: INFO: Pod "pod-logs-websocket-6c4bffe2-1cb4-43b2-bc13-d4f7b3de895c": Phase="Running", Reason="", readiness=true. Elapsed: 2.005505062s
May  9 13:28:20.640: INFO: The phase of Pod pod-logs-websocket-6c4bffe2-1cb4-43b2-bc13-d4f7b3de895c is Running (Ready = true)
May  9 13:28:20.640: INFO: Pod "pod-logs-websocket-6c4bffe2-1cb4-43b2-bc13-d4f7b3de895c" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  9 13:28:20.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6885" for this suite. 05/09/23 13:28:20.659
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":229,"skipped":4570,"failed":0}
------------------------------
• [2.048 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:28:18.615
    May  9 13:28:18.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pods 05/09/23 13:28:18.615
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:18.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:18.626
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    May  9 13:28:18.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: creating the pod 05/09/23 13:28:18.628
    STEP: submitting the pod to kubernetes 05/09/23 13:28:18.629
    May  9 13:28:18.634: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-6c4bffe2-1cb4-43b2-bc13-d4f7b3de895c" in namespace "pods-6885" to be "running and ready"
    May  9 13:28:18.636: INFO: Pod "pod-logs-websocket-6c4bffe2-1cb4-43b2-bc13-d4f7b3de895c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.616787ms
    May  9 13:28:18.636: INFO: The phase of Pod pod-logs-websocket-6c4bffe2-1cb4-43b2-bc13-d4f7b3de895c is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:28:20.640: INFO: Pod "pod-logs-websocket-6c4bffe2-1cb4-43b2-bc13-d4f7b3de895c": Phase="Running", Reason="", readiness=true. Elapsed: 2.005505062s
    May  9 13:28:20.640: INFO: The phase of Pod pod-logs-websocket-6c4bffe2-1cb4-43b2-bc13-d4f7b3de895c is Running (Ready = true)
    May  9 13:28:20.640: INFO: Pod "pod-logs-websocket-6c4bffe2-1cb4-43b2-bc13-d4f7b3de895c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  9 13:28:20.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6885" for this suite. 05/09/23 13:28:20.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:28:20.664
May  9 13:28:20.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename dns 05/09/23 13:28:20.665
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:20.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:20.676
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 05/09/23 13:28:20.678
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local;sleep 1; done
 05/09/23 13:28:20.681
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local;sleep 1; done
 05/09/23 13:28:20.681
STEP: creating a pod to probe DNS 05/09/23 13:28:20.681
STEP: submitting the pod to kubernetes 05/09/23 13:28:20.681
May  9 13:28:20.692: INFO: Waiting up to 15m0s for pod "dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8" in namespace "dns-3203" to be "running"
May  9 13:28:20.693: INFO: Pod "dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.841866ms
May  9 13:28:22.697: INFO: Pod "dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005720035s
May  9 13:28:22.697: INFO: Pod "dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8" satisfied condition "running"
STEP: retrieving the pod 05/09/23 13:28:22.697
STEP: looking for the results for each expected name from probers 05/09/23 13:28:22.699
May  9 13:28:22.702: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:22.704: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:22.706: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:22.708: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:22.710: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:22.712: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:22.714: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:22.716: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:22.716: INFO: Lookups using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local]

May  9 13:28:27.720: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:27.722: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:27.723: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:27.725: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:27.727: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:27.728: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:27.730: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:27.732: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:27.732: INFO: Lookups using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local]

May  9 13:28:32.720: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:32.722: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:32.724: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:32.726: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:32.728: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:32.730: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:32.732: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:32.734: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:32.734: INFO: Lookups using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local]

May  9 13:28:37.720: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:37.722: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:37.724: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:37.726: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:37.727: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:37.729: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:37.731: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:37.732: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:37.732: INFO: Lookups using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local]

May  9 13:28:42.719: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:42.722: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:42.724: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:42.727: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:42.729: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:42.731: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:42.732: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:42.734: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:42.734: INFO: Lookups using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local]

May  9 13:28:47.719: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:47.721: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:47.723: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:47.725: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:47.727: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:47.729: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:47.731: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:47.733: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
May  9 13:28:47.733: INFO: Lookups using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local]

May  9 13:28:52.731: INFO: DNS probes using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 succeeded

STEP: deleting the pod 05/09/23 13:28:52.731
STEP: deleting the test headless service 05/09/23 13:28:52.744
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  9 13:28:52.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3203" for this suite. 05/09/23 13:28:52.763
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":230,"skipped":4591,"failed":0}
------------------------------
• [SLOW TEST] [32.104 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:28:20.664
    May  9 13:28:20.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename dns 05/09/23 13:28:20.665
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:20.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:20.676
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 05/09/23 13:28:20.678
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local;sleep 1; done
     05/09/23 13:28:20.681
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3203.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local;sleep 1; done
     05/09/23 13:28:20.681
    STEP: creating a pod to probe DNS 05/09/23 13:28:20.681
    STEP: submitting the pod to kubernetes 05/09/23 13:28:20.681
    May  9 13:28:20.692: INFO: Waiting up to 15m0s for pod "dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8" in namespace "dns-3203" to be "running"
    May  9 13:28:20.693: INFO: Pod "dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.841866ms
    May  9 13:28:22.697: INFO: Pod "dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005720035s
    May  9 13:28:22.697: INFO: Pod "dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 13:28:22.697
    STEP: looking for the results for each expected name from probers 05/09/23 13:28:22.699
    May  9 13:28:22.702: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:22.704: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:22.706: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:22.708: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:22.710: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:22.712: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:22.714: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:22.716: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:22.716: INFO: Lookups using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local]

    May  9 13:28:27.720: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:27.722: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:27.723: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:27.725: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:27.727: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:27.728: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:27.730: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:27.732: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:27.732: INFO: Lookups using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local]

    May  9 13:28:32.720: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:32.722: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:32.724: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:32.726: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:32.728: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:32.730: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:32.732: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:32.734: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:32.734: INFO: Lookups using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local]

    May  9 13:28:37.720: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:37.722: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:37.724: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:37.726: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:37.727: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:37.729: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:37.731: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:37.732: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:37.732: INFO: Lookups using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local]

    May  9 13:28:42.719: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:42.722: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:42.724: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:42.727: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:42.729: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:42.731: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:42.732: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:42.734: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:42.734: INFO: Lookups using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local]

    May  9 13:28:47.719: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:47.721: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:47.723: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:47.725: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:47.727: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:47.729: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:47.731: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:47.733: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local from pod dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8: the server could not find the requested resource (get pods dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8)
    May  9 13:28:47.733: INFO: Lookups using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3203.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3203.svc.cluster.local jessie_udp@dns-test-service-2.dns-3203.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3203.svc.cluster.local]

    May  9 13:28:52.731: INFO: DNS probes using dns-3203/dns-test-015a8720-ca39-445d-884c-d0f7ed3e6fe8 succeeded

    STEP: deleting the pod 05/09/23 13:28:52.731
    STEP: deleting the test headless service 05/09/23 13:28:52.744
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  9 13:28:52.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3203" for this suite. 05/09/23 13:28:52.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:28:52.769
May  9 13:28:52.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:28:52.769
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:52.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:52.789
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-5870b267-3e36-4bed-980c-cf3ad749b7e7 05/09/23 13:28:52.79
STEP: Creating a pod to test consume configMaps 05/09/23 13:28:52.793
May  9 13:28:52.799: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c" in namespace "projected-3492" to be "Succeeded or Failed"
May  9 13:28:52.801: INFO: Pod "pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.838832ms
May  9 13:28:54.804: INFO: Pod "pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005126423s
May  9 13:28:56.805: INFO: Pod "pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005521088s
STEP: Saw pod success 05/09/23 13:28:56.805
May  9 13:28:56.805: INFO: Pod "pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c" satisfied condition "Succeeded or Failed"
May  9 13:28:56.807: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c container agnhost-container: <nil>
STEP: delete the pod 05/09/23 13:28:56.81
May  9 13:28:56.821: INFO: Waiting for pod pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c to disappear
May  9 13:28:56.823: INFO: Pod pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  9 13:28:56.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3492" for this suite. 05/09/23 13:28:56.825
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":231,"skipped":4596,"failed":0}
------------------------------
• [4.060 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:28:52.769
    May  9 13:28:52.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:28:52.769
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:52.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:52.789
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-5870b267-3e36-4bed-980c-cf3ad749b7e7 05/09/23 13:28:52.79
    STEP: Creating a pod to test consume configMaps 05/09/23 13:28:52.793
    May  9 13:28:52.799: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c" in namespace "projected-3492" to be "Succeeded or Failed"
    May  9 13:28:52.801: INFO: Pod "pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.838832ms
    May  9 13:28:54.804: INFO: Pod "pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005126423s
    May  9 13:28:56.805: INFO: Pod "pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005521088s
    STEP: Saw pod success 05/09/23 13:28:56.805
    May  9 13:28:56.805: INFO: Pod "pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c" satisfied condition "Succeeded or Failed"
    May  9 13:28:56.807: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 13:28:56.81
    May  9 13:28:56.821: INFO: Waiting for pod pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c to disappear
    May  9 13:28:56.823: INFO: Pod pod-projected-configmaps-2e246676-ccf1-4f46-8a7b-d9755ac9d23c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  9 13:28:56.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3492" for this suite. 05/09/23 13:28:56.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:28:56.829
May  9 13:28:56.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename subpath 05/09/23 13:28:56.83
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:56.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:56.841
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/09/23 13:28:56.843
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-vlv5 05/09/23 13:28:56.858
STEP: Creating a pod to test atomic-volume-subpath 05/09/23 13:28:56.858
May  9 13:28:56.863: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vlv5" in namespace "subpath-7634" to be "Succeeded or Failed"
May  9 13:28:56.865: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.554119ms
May  9 13:28:58.868: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004962302s
May  9 13:29:00.868: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 4.00422789s
May  9 13:29:02.869: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 6.005395392s
May  9 13:29:04.867: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 8.004074336s
May  9 13:29:06.868: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 10.004146755s
May  9 13:29:08.868: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 12.004922592s
May  9 13:29:10.869: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 14.005300054s
May  9 13:29:12.868: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 16.004655109s
May  9 13:29:14.870: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 18.006184553s
May  9 13:29:16.869: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 20.005205243s
May  9 13:29:18.869: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=false. Elapsed: 22.005418124s
May  9 13:29:20.869: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00579583s
STEP: Saw pod success 05/09/23 13:29:20.869
May  9 13:29:20.869: INFO: Pod "pod-subpath-test-downwardapi-vlv5" satisfied condition "Succeeded or Failed"
May  9 13:29:20.871: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-subpath-test-downwardapi-vlv5 container test-container-subpath-downwardapi-vlv5: <nil>
STEP: delete the pod 05/09/23 13:29:20.876
May  9 13:29:20.885: INFO: Waiting for pod pod-subpath-test-downwardapi-vlv5 to disappear
May  9 13:29:20.887: INFO: Pod pod-subpath-test-downwardapi-vlv5 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-vlv5 05/09/23 13:29:20.887
May  9 13:29:20.887: INFO: Deleting pod "pod-subpath-test-downwardapi-vlv5" in namespace "subpath-7634"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  9 13:29:20.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7634" for this suite. 05/09/23 13:29:20.891
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":232,"skipped":4604,"failed":0}
------------------------------
• [SLOW TEST] [24.066 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:28:56.829
    May  9 13:28:56.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename subpath 05/09/23 13:28:56.83
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:28:56.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:28:56.841
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/09/23 13:28:56.843
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-vlv5 05/09/23 13:28:56.858
    STEP: Creating a pod to test atomic-volume-subpath 05/09/23 13:28:56.858
    May  9 13:28:56.863: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vlv5" in namespace "subpath-7634" to be "Succeeded or Failed"
    May  9 13:28:56.865: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.554119ms
    May  9 13:28:58.868: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004962302s
    May  9 13:29:00.868: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 4.00422789s
    May  9 13:29:02.869: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 6.005395392s
    May  9 13:29:04.867: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 8.004074336s
    May  9 13:29:06.868: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 10.004146755s
    May  9 13:29:08.868: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 12.004922592s
    May  9 13:29:10.869: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 14.005300054s
    May  9 13:29:12.868: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 16.004655109s
    May  9 13:29:14.870: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 18.006184553s
    May  9 13:29:16.869: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=true. Elapsed: 20.005205243s
    May  9 13:29:18.869: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Running", Reason="", readiness=false. Elapsed: 22.005418124s
    May  9 13:29:20.869: INFO: Pod "pod-subpath-test-downwardapi-vlv5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00579583s
    STEP: Saw pod success 05/09/23 13:29:20.869
    May  9 13:29:20.869: INFO: Pod "pod-subpath-test-downwardapi-vlv5" satisfied condition "Succeeded or Failed"
    May  9 13:29:20.871: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-subpath-test-downwardapi-vlv5 container test-container-subpath-downwardapi-vlv5: <nil>
    STEP: delete the pod 05/09/23 13:29:20.876
    May  9 13:29:20.885: INFO: Waiting for pod pod-subpath-test-downwardapi-vlv5 to disappear
    May  9 13:29:20.887: INFO: Pod pod-subpath-test-downwardapi-vlv5 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-vlv5 05/09/23 13:29:20.887
    May  9 13:29:20.887: INFO: Deleting pod "pod-subpath-test-downwardapi-vlv5" in namespace "subpath-7634"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  9 13:29:20.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7634" for this suite. 05/09/23 13:29:20.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:29:20.895
May  9 13:29:20.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename endpointslice 05/09/23 13:29:20.896
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:20.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:20.907
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 05/09/23 13:29:20.908
STEP: getting /apis/discovery.k8s.io 05/09/23 13:29:20.909
STEP: getting /apis/discovery.k8s.iov1 05/09/23 13:29:20.91
STEP: creating 05/09/23 13:29:20.91
STEP: getting 05/09/23 13:29:20.923
STEP: listing 05/09/23 13:29:20.925
STEP: watching 05/09/23 13:29:20.926
May  9 13:29:20.926: INFO: starting watch
STEP: cluster-wide listing 05/09/23 13:29:20.927
STEP: cluster-wide watching 05/09/23 13:29:20.928
May  9 13:29:20.928: INFO: starting watch
STEP: patching 05/09/23 13:29:20.929
STEP: updating 05/09/23 13:29:20.932
May  9 13:29:20.937: INFO: waiting for watch events with expected annotations
May  9 13:29:20.937: INFO: saw patched and updated annotations
STEP: deleting 05/09/23 13:29:20.937
STEP: deleting a collection 05/09/23 13:29:20.944
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May  9 13:29:20.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-93" for this suite. 05/09/23 13:29:20.956
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":233,"skipped":4610,"failed":0}
------------------------------
• [0.064 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:29:20.895
    May  9 13:29:20.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename endpointslice 05/09/23 13:29:20.896
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:20.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:20.907
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 05/09/23 13:29:20.908
    STEP: getting /apis/discovery.k8s.io 05/09/23 13:29:20.909
    STEP: getting /apis/discovery.k8s.iov1 05/09/23 13:29:20.91
    STEP: creating 05/09/23 13:29:20.91
    STEP: getting 05/09/23 13:29:20.923
    STEP: listing 05/09/23 13:29:20.925
    STEP: watching 05/09/23 13:29:20.926
    May  9 13:29:20.926: INFO: starting watch
    STEP: cluster-wide listing 05/09/23 13:29:20.927
    STEP: cluster-wide watching 05/09/23 13:29:20.928
    May  9 13:29:20.928: INFO: starting watch
    STEP: patching 05/09/23 13:29:20.929
    STEP: updating 05/09/23 13:29:20.932
    May  9 13:29:20.937: INFO: waiting for watch events with expected annotations
    May  9 13:29:20.937: INFO: saw patched and updated annotations
    STEP: deleting 05/09/23 13:29:20.937
    STEP: deleting a collection 05/09/23 13:29:20.944
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May  9 13:29:20.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-93" for this suite. 05/09/23 13:29:20.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:29:20.96
May  9 13:29:20.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 13:29:20.96
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:20.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:20.971
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-f9835672-f2bf-4c8e-8a14-66734fabb5cd 05/09/23 13:29:20.972
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May  9 13:29:20.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-16" for this suite. 05/09/23 13:29:20.976
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":234,"skipped":4617,"failed":0}
------------------------------
• [0.020 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:29:20.96
    May  9 13:29:20.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 13:29:20.96
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:20.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:20.971
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-f9835672-f2bf-4c8e-8a14-66734fabb5cd 05/09/23 13:29:20.972
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May  9 13:29:20.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-16" for this suite. 05/09/23 13:29:20.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:29:20.982
May  9 13:29:20.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:29:20.983
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:20.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:20.998
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8490 05/09/23 13:29:20.999
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/09/23 13:29:21.014
STEP: creating service externalsvc in namespace services-8490 05/09/23 13:29:21.014
STEP: creating replication controller externalsvc in namespace services-8490 05/09/23 13:29:21.032
I0509 13:29:21.042114      24 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8490, replica count: 2
I0509 13:29:24.093149      24 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 05/09/23 13:29:24.095
May  9 13:29:24.110: INFO: Creating new exec pod
May  9 13:29:24.116: INFO: Waiting up to 5m0s for pod "execpod8dnkf" in namespace "services-8490" to be "running"
May  9 13:29:24.118: INFO: Pod "execpod8dnkf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.944232ms
May  9 13:29:26.121: INFO: Pod "execpod8dnkf": Phase="Running", Reason="", readiness=true. Elapsed: 2.005112717s
May  9 13:29:26.121: INFO: Pod "execpod8dnkf" satisfied condition "running"
May  9 13:29:26.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8490 exec execpod8dnkf -- /bin/sh -x -c nslookup nodeport-service.services-8490.svc.cluster.local'
May  9 13:29:26.244: INFO: stderr: "+ nslookup nodeport-service.services-8490.svc.cluster.local\n"
May  9 13:29:26.244: INFO: stdout: "Server:\t\t169.254.53.53\nAddress:\t169.254.53.53#53\n\nnodeport-service.services-8490.svc.cluster.local\tcanonical name = externalsvc.services-8490.svc.cluster.local.\nName:\texternalsvc.services-8490.svc.cluster.local\nAddress: 10.101.67.7\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8490, will wait for the garbage collector to delete the pods 05/09/23 13:29:26.244
May  9 13:29:26.301: INFO: Deleting ReplicationController externalsvc took: 4.040833ms
May  9 13:29:26.401: INFO: Terminating ReplicationController externalsvc pods took: 100.86054ms
May  9 13:29:28.620: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:29:28.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8490" for this suite. 05/09/23 13:29:28.631
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":235,"skipped":4669,"failed":0}
------------------------------
• [SLOW TEST] [7.653 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:29:20.982
    May  9 13:29:20.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:29:20.983
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:20.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:20.998
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8490 05/09/23 13:29:20.999
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/09/23 13:29:21.014
    STEP: creating service externalsvc in namespace services-8490 05/09/23 13:29:21.014
    STEP: creating replication controller externalsvc in namespace services-8490 05/09/23 13:29:21.032
    I0509 13:29:21.042114      24 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8490, replica count: 2
    I0509 13:29:24.093149      24 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 05/09/23 13:29:24.095
    May  9 13:29:24.110: INFO: Creating new exec pod
    May  9 13:29:24.116: INFO: Waiting up to 5m0s for pod "execpod8dnkf" in namespace "services-8490" to be "running"
    May  9 13:29:24.118: INFO: Pod "execpod8dnkf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.944232ms
    May  9 13:29:26.121: INFO: Pod "execpod8dnkf": Phase="Running", Reason="", readiness=true. Elapsed: 2.005112717s
    May  9 13:29:26.121: INFO: Pod "execpod8dnkf" satisfied condition "running"
    May  9 13:29:26.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-8490 exec execpod8dnkf -- /bin/sh -x -c nslookup nodeport-service.services-8490.svc.cluster.local'
    May  9 13:29:26.244: INFO: stderr: "+ nslookup nodeport-service.services-8490.svc.cluster.local\n"
    May  9 13:29:26.244: INFO: stdout: "Server:\t\t169.254.53.53\nAddress:\t169.254.53.53#53\n\nnodeport-service.services-8490.svc.cluster.local\tcanonical name = externalsvc.services-8490.svc.cluster.local.\nName:\texternalsvc.services-8490.svc.cluster.local\nAddress: 10.101.67.7\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8490, will wait for the garbage collector to delete the pods 05/09/23 13:29:26.244
    May  9 13:29:26.301: INFO: Deleting ReplicationController externalsvc took: 4.040833ms
    May  9 13:29:26.401: INFO: Terminating ReplicationController externalsvc pods took: 100.86054ms
    May  9 13:29:28.620: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:29:28.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8490" for this suite. 05/09/23 13:29:28.631
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:29:28.638
May  9 13:29:28.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename security-context-test 05/09/23 13:29:28.638
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:28.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:28.65
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
May  9 13:29:28.656: INFO: Waiting up to 5m0s for pod "busybox-user-65534-057f7f1f-fb86-4346-9f50-f945b84e511f" in namespace "security-context-test-5023" to be "Succeeded or Failed"
May  9 13:29:28.659: INFO: Pod "busybox-user-65534-057f7f1f-fb86-4346-9f50-f945b84e511f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472246ms
May  9 13:29:30.663: INFO: Pod "busybox-user-65534-057f7f1f-fb86-4346-9f50-f945b84e511f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006416475s
May  9 13:29:32.661: INFO: Pod "busybox-user-65534-057f7f1f-fb86-4346-9f50-f945b84e511f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00510321s
May  9 13:29:32.661: INFO: Pod "busybox-user-65534-057f7f1f-fb86-4346-9f50-f945b84e511f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  9 13:29:32.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5023" for this suite. 05/09/23 13:29:32.665
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":236,"skipped":4713,"failed":0}
------------------------------
• [4.034 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:29:28.638
    May  9 13:29:28.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename security-context-test 05/09/23 13:29:28.638
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:28.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:28.65
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    May  9 13:29:28.656: INFO: Waiting up to 5m0s for pod "busybox-user-65534-057f7f1f-fb86-4346-9f50-f945b84e511f" in namespace "security-context-test-5023" to be "Succeeded or Failed"
    May  9 13:29:28.659: INFO: Pod "busybox-user-65534-057f7f1f-fb86-4346-9f50-f945b84e511f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472246ms
    May  9 13:29:30.663: INFO: Pod "busybox-user-65534-057f7f1f-fb86-4346-9f50-f945b84e511f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006416475s
    May  9 13:29:32.661: INFO: Pod "busybox-user-65534-057f7f1f-fb86-4346-9f50-f945b84e511f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00510321s
    May  9 13:29:32.661: INFO: Pod "busybox-user-65534-057f7f1f-fb86-4346-9f50-f945b84e511f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  9 13:29:32.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5023" for this suite. 05/09/23 13:29:32.665
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:29:32.671
May  9 13:29:32.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:29:32.672
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:32.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:32.684
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 05/09/23 13:29:32.685
May  9 13:29:32.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: mark a version not serverd 05/09/23 13:29:39.202
STEP: check the unserved version gets removed 05/09/23 13:29:39.217
STEP: check the other version is not changed 05/09/23 13:29:41.169
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:29:47.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4082" for this suite. 05/09/23 13:29:47.261
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":237,"skipped":4714,"failed":0}
------------------------------
• [SLOW TEST] [14.594 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:29:32.671
    May  9 13:29:32.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:29:32.672
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:32.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:32.684
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 05/09/23 13:29:32.685
    May  9 13:29:32.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: mark a version not serverd 05/09/23 13:29:39.202
    STEP: check the unserved version gets removed 05/09/23 13:29:39.217
    STEP: check the other version is not changed 05/09/23 13:29:41.169
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:29:47.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4082" for this suite. 05/09/23 13:29:47.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:29:47.266
May  9 13:29:47.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:29:47.267
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:47.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:47.283
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:29:47.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9602" for this suite. 05/09/23 13:29:47.289
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":238,"skipped":4740,"failed":0}
------------------------------
• [0.027 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:29:47.266
    May  9 13:29:47.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:29:47.267
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:47.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:47.283
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:29:47.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9602" for this suite. 05/09/23 13:29:47.289
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:29:47.294
May  9 13:29:47.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 13:29:47.294
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:47.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:47.308
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/09/23 13:29:47.309
May  9 13:29:47.317: INFO: Waiting up to 5m0s for pod "pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da" in namespace "emptydir-2827" to be "Succeeded or Failed"
May  9 13:29:47.319: INFO: Pod "pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.630889ms
May  9 13:29:49.323: INFO: Pod "pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005888884s
May  9 13:29:51.322: INFO: Pod "pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005403033s
STEP: Saw pod success 05/09/23 13:29:51.322
May  9 13:29:51.322: INFO: Pod "pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da" satisfied condition "Succeeded or Failed"
May  9 13:29:51.324: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da container test-container: <nil>
STEP: delete the pod 05/09/23 13:29:51.334
May  9 13:29:51.342: INFO: Waiting for pod pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da to disappear
May  9 13:29:51.344: INFO: Pod pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 13:29:51.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2827" for this suite. 05/09/23 13:29:51.347
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":239,"skipped":4743,"failed":0}
------------------------------
• [4.058 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:29:47.294
    May  9 13:29:47.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 13:29:47.294
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:47.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:47.308
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/09/23 13:29:47.309
    May  9 13:29:47.317: INFO: Waiting up to 5m0s for pod "pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da" in namespace "emptydir-2827" to be "Succeeded or Failed"
    May  9 13:29:47.319: INFO: Pod "pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.630889ms
    May  9 13:29:49.323: INFO: Pod "pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005888884s
    May  9 13:29:51.322: INFO: Pod "pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005403033s
    STEP: Saw pod success 05/09/23 13:29:51.322
    May  9 13:29:51.322: INFO: Pod "pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da" satisfied condition "Succeeded or Failed"
    May  9 13:29:51.324: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da container test-container: <nil>
    STEP: delete the pod 05/09/23 13:29:51.334
    May  9 13:29:51.342: INFO: Waiting for pod pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da to disappear
    May  9 13:29:51.344: INFO: Pod pod-feb717fa-0b22-4aa8-971e-f455c2f0f3da no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 13:29:51.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2827" for this suite. 05/09/23 13:29:51.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:29:51.352
May  9 13:29:51.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename aggregator 05/09/23 13:29:51.353
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:51.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:51.366
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
May  9 13:29:51.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 05/09/23 13:29:51.369
May  9 13:29:51.639: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May  9 13:29:53.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:29:55.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:29:57.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:29:59.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:30:01.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:30:03.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:30:05.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:30:07.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:30:09.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:30:11.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:30:13.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:30:15.797: INFO: Waited 114.713497ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 05/09/23 13:30:15.826
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/09/23 13:30:15.828
STEP: List APIServices 05/09/23 13:30:15.835
May  9 13:30:15.839: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
May  9 13:30:16.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7645" for this suite. 05/09/23 13:30:16.14
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":240,"skipped":4754,"failed":0}
------------------------------
• [SLOW TEST] [24.840 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:29:51.352
    May  9 13:29:51.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename aggregator 05/09/23 13:29:51.353
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:29:51.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:29:51.366
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    May  9 13:29:51.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 05/09/23 13:29:51.369
    May  9 13:29:51.639: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    May  9 13:29:53.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:29:55.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:29:57.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:29:59.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:30:01.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:30:03.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:30:05.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:30:07.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:30:09.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:30:11.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:30:13.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:30:15.797: INFO: Waited 114.713497ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 05/09/23 13:30:15.826
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/09/23 13:30:15.828
    STEP: List APIServices 05/09/23 13:30:15.835
    May  9 13:30:15.839: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    May  9 13:30:16.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-7645" for this suite. 05/09/23 13:30:16.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:30:16.193
May  9 13:30:16.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename runtimeclass 05/09/23 13:30:16.194
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:30:16.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:30:16.21
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
May  9 13:30:16.223: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8721 to be scheduled
May  9 13:30:16.225: INFO: 1 pods are not scheduled: [runtimeclass-8721/test-runtimeclass-runtimeclass-8721-preconfigured-handler-pd2bg(5ccdc44a-e59b-4cc5-a8be-45a90cbfefdd)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  9 13:30:18.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8721" for this suite. 05/09/23 13:30:18.239
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":241,"skipped":4780,"failed":0}
------------------------------
• [2.050 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:30:16.193
    May  9 13:30:16.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename runtimeclass 05/09/23 13:30:16.194
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:30:16.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:30:16.21
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    May  9 13:30:16.223: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8721 to be scheduled
    May  9 13:30:16.225: INFO: 1 pods are not scheduled: [runtimeclass-8721/test-runtimeclass-runtimeclass-8721-preconfigured-handler-pd2bg(5ccdc44a-e59b-4cc5-a8be-45a90cbfefdd)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  9 13:30:18.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8721" for this suite. 05/09/23 13:30:18.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:30:18.244
May  9 13:30:18.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 13:30:18.244
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:30:18.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:30:18.259
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-e62e7c63-80ee-49dd-8bf5-edb25f2b0863 05/09/23 13:30:18.26
STEP: Creating a pod to test consume secrets 05/09/23 13:30:18.264
May  9 13:30:18.272: INFO: Waiting up to 5m0s for pod "pod-secrets-82ad6646-304b-410e-8875-9810e9995711" in namespace "secrets-2157" to be "Succeeded or Failed"
May  9 13:30:18.279: INFO: Pod "pod-secrets-82ad6646-304b-410e-8875-9810e9995711": Phase="Pending", Reason="", readiness=false. Elapsed: 6.124069ms
May  9 13:30:20.282: INFO: Pod "pod-secrets-82ad6646-304b-410e-8875-9810e9995711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010020699s
May  9 13:30:22.283: INFO: Pod "pod-secrets-82ad6646-304b-410e-8875-9810e9995711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010028019s
STEP: Saw pod success 05/09/23 13:30:22.283
May  9 13:30:22.283: INFO: Pod "pod-secrets-82ad6646-304b-410e-8875-9810e9995711" satisfied condition "Succeeded or Failed"
May  9 13:30:22.285: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-secrets-82ad6646-304b-410e-8875-9810e9995711 container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 13:30:22.298
May  9 13:30:22.313: INFO: Waiting for pod pod-secrets-82ad6646-304b-410e-8875-9810e9995711 to disappear
May  9 13:30:22.315: INFO: Pod pod-secrets-82ad6646-304b-410e-8875-9810e9995711 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  9 13:30:22.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2157" for this suite. 05/09/23 13:30:22.318
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":242,"skipped":4792,"failed":0}
------------------------------
• [4.080 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:30:18.244
    May  9 13:30:18.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 13:30:18.244
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:30:18.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:30:18.259
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-e62e7c63-80ee-49dd-8bf5-edb25f2b0863 05/09/23 13:30:18.26
    STEP: Creating a pod to test consume secrets 05/09/23 13:30:18.264
    May  9 13:30:18.272: INFO: Waiting up to 5m0s for pod "pod-secrets-82ad6646-304b-410e-8875-9810e9995711" in namespace "secrets-2157" to be "Succeeded or Failed"
    May  9 13:30:18.279: INFO: Pod "pod-secrets-82ad6646-304b-410e-8875-9810e9995711": Phase="Pending", Reason="", readiness=false. Elapsed: 6.124069ms
    May  9 13:30:20.282: INFO: Pod "pod-secrets-82ad6646-304b-410e-8875-9810e9995711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010020699s
    May  9 13:30:22.283: INFO: Pod "pod-secrets-82ad6646-304b-410e-8875-9810e9995711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010028019s
    STEP: Saw pod success 05/09/23 13:30:22.283
    May  9 13:30:22.283: INFO: Pod "pod-secrets-82ad6646-304b-410e-8875-9810e9995711" satisfied condition "Succeeded or Failed"
    May  9 13:30:22.285: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-secrets-82ad6646-304b-410e-8875-9810e9995711 container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:30:22.298
    May  9 13:30:22.313: INFO: Waiting for pod pod-secrets-82ad6646-304b-410e-8875-9810e9995711 to disappear
    May  9 13:30:22.315: INFO: Pod pod-secrets-82ad6646-304b-410e-8875-9810e9995711 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  9 13:30:22.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2157" for this suite. 05/09/23 13:30:22.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:30:22.325
May  9 13:30:22.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:30:22.326
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:30:22.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:30:22.34
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 05/09/23 13:30:22.342
May  9 13:30:22.342: INFO: Creating e2e-svc-a-rvrvg
May  9 13:30:22.353: INFO: Creating e2e-svc-b-96d8v
May  9 13:30:22.368: INFO: Creating e2e-svc-c-z6dlp
STEP: deleting service collection 05/09/23 13:30:22.383
May  9 13:30:22.423: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:30:22.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5170" for this suite. 05/09/23 13:30:22.426
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":243,"skipped":4872,"failed":0}
------------------------------
• [0.105 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:30:22.325
    May  9 13:30:22.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:30:22.326
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:30:22.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:30:22.34
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 05/09/23 13:30:22.342
    May  9 13:30:22.342: INFO: Creating e2e-svc-a-rvrvg
    May  9 13:30:22.353: INFO: Creating e2e-svc-b-96d8v
    May  9 13:30:22.368: INFO: Creating e2e-svc-c-z6dlp
    STEP: deleting service collection 05/09/23 13:30:22.383
    May  9 13:30:22.423: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:30:22.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5170" for this suite. 05/09/23 13:30:22.426
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:30:22.431
May  9 13:30:22.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:30:22.432
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:30:22.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:30:22.444
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:30:22.445
May  9 13:30:22.452: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128" in namespace "projected-958" to be "Succeeded or Failed"
May  9 13:30:22.456: INFO: Pod "downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128": Phase="Pending", Reason="", readiness=false. Elapsed: 3.37724ms
May  9 13:30:24.459: INFO: Pod "downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006821547s
May  9 13:30:26.459: INFO: Pod "downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006560481s
STEP: Saw pod success 05/09/23 13:30:26.459
May  9 13:30:26.459: INFO: Pod "downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128" satisfied condition "Succeeded or Failed"
May  9 13:30:26.461: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128 container client-container: <nil>
STEP: delete the pod 05/09/23 13:30:26.465
May  9 13:30:26.477: INFO: Waiting for pod downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128 to disappear
May  9 13:30:26.479: INFO: Pod downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  9 13:30:26.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-958" for this suite. 05/09/23 13:30:26.481
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":244,"skipped":4885,"failed":0}
------------------------------
• [4.055 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:30:22.431
    May  9 13:30:22.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:30:22.432
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:30:22.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:30:22.444
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:30:22.445
    May  9 13:30:22.452: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128" in namespace "projected-958" to be "Succeeded or Failed"
    May  9 13:30:22.456: INFO: Pod "downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128": Phase="Pending", Reason="", readiness=false. Elapsed: 3.37724ms
    May  9 13:30:24.459: INFO: Pod "downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006821547s
    May  9 13:30:26.459: INFO: Pod "downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006560481s
    STEP: Saw pod success 05/09/23 13:30:26.459
    May  9 13:30:26.459: INFO: Pod "downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128" satisfied condition "Succeeded or Failed"
    May  9 13:30:26.461: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128 container client-container: <nil>
    STEP: delete the pod 05/09/23 13:30:26.465
    May  9 13:30:26.477: INFO: Waiting for pod downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128 to disappear
    May  9 13:30:26.479: INFO: Pod downwardapi-volume-c02a5844-6bcd-47a0-bf2b-b38f754ff128 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  9 13:30:26.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-958" for this suite. 05/09/23 13:30:26.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:30:26.487
May  9 13:30:26.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename statefulset 05/09/23 13:30:26.488
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:30:26.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:30:26.5
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4748 05/09/23 13:30:26.502
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-4748 05/09/23 13:30:26.506
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4748 05/09/23 13:30:26.514
May  9 13:30:26.520: INFO: Found 0 stateful pods, waiting for 1
May  9 13:30:36.524: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/09/23 13:30:36.524
May  9 13:30:36.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 13:30:36.623: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 13:30:36.624: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 13:30:36.624: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 13:30:36.626: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  9 13:30:46.633: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  9 13:30:46.633: INFO: Waiting for statefulset status.replicas updated to 0
May  9 13:30:46.643: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May  9 13:30:46.643: INFO: ss-0  cl-gks-cncf-ix1-md-0-48ljh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:26 +0000 UTC  }]
May  9 13:30:46.643: INFO: 
May  9 13:30:46.643: INFO: StatefulSet ss has not reached scale 3, at 1
May  9 13:30:47.651: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997810676s
May  9 13:30:48.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989490903s
May  9 13:30:49.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986486714s
May  9 13:30:50.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983093293s
May  9 13:30:51.664: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980189262s
May  9 13:30:52.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977346956s
May  9 13:30:53.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974414546s
May  9 13:30:54.674: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.971047474s
May  9 13:30:55.679: INFO: Verifying statefulset ss doesn't scale past 3 for another 967.757249ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4748 05/09/23 13:30:56.679
May  9 13:30:56.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 13:30:56.780: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 13:30:56.780: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 13:30:56.780: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 13:30:56.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 13:30:56.882: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  9 13:30:56.882: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 13:30:56.882: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 13:30:56.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 13:30:56.993: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  9 13:30:56.993: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 13:30:56.993: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 13:30:56.995: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  9 13:30:56.995: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  9 13:30:56.995: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 05/09/23 13:30:56.995
May  9 13:30:56.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 13:30:57.097: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 13:30:57.097: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 13:30:57.097: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 13:30:57.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 13:30:57.196: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 13:30:57.196: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 13:30:57.196: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 13:30:57.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 13:30:57.291: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 13:30:57.291: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 13:30:57.291: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 13:30:57.291: INFO: Waiting for statefulset status.replicas updated to 0
May  9 13:30:57.293: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May  9 13:31:07.300: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  9 13:31:07.300: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  9 13:31:07.300: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  9 13:31:07.310: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May  9 13:31:07.310: INFO: ss-0  cl-gks-cncf-ix1-md-0-48ljh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:26 +0000 UTC  }]
May  9 13:31:07.310: INFO: ss-1  cl-gks-cncf-ix1-md-0-879bk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:46 +0000 UTC  }]
May  9 13:31:07.310: INFO: ss-2  cl-gks-cncf-ix1-md-0-skqqr  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:46 +0000 UTC  }]
May  9 13:31:07.310: INFO: 
May  9 13:31:07.310: INFO: StatefulSet ss has not reached scale 0, at 3
May  9 13:31:08.313: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May  9 13:31:08.313: INFO: ss-0  cl-gks-cncf-ix1-md-0-48ljh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:26 +0000 UTC  }]
May  9 13:31:08.313: INFO: 
May  9 13:31:08.313: INFO: StatefulSet ss has not reached scale 0, at 1
May  9 13:31:09.316: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.994388792s
May  9 13:31:10.322: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.991339535s
May  9 13:31:11.325: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.985305974s
May  9 13:31:12.328: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.982616517s
May  9 13:31:13.333: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.979836684s
May  9 13:31:14.336: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.97459419s
May  9 13:31:15.339: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.972060571s
May  9 13:31:16.342: INFO: Verifying statefulset ss doesn't scale past 0 for another 969.200278ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4748 05/09/23 13:31:17.343
May  9 13:31:17.351: INFO: Scaling statefulset ss to 0
May  9 13:31:17.359: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  9 13:31:17.361: INFO: Deleting all statefulset in ns statefulset-4748
May  9 13:31:17.363: INFO: Scaling statefulset ss to 0
May  9 13:31:17.370: INFO: Waiting for statefulset status.replicas updated to 0
May  9 13:31:17.372: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  9 13:31:17.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4748" for this suite. 05/09/23 13:31:17.385
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":245,"skipped":4921,"failed":0}
------------------------------
• [SLOW TEST] [50.902 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:30:26.487
    May  9 13:30:26.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename statefulset 05/09/23 13:30:26.488
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:30:26.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:30:26.5
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4748 05/09/23 13:30:26.502
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-4748 05/09/23 13:30:26.506
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4748 05/09/23 13:30:26.514
    May  9 13:30:26.520: INFO: Found 0 stateful pods, waiting for 1
    May  9 13:30:36.524: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/09/23 13:30:36.524
    May  9 13:30:36.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 13:30:36.623: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 13:30:36.624: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 13:30:36.624: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 13:30:36.626: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May  9 13:30:46.633: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  9 13:30:46.633: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 13:30:46.643: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
    May  9 13:30:46.643: INFO: ss-0  cl-gks-cncf-ix1-md-0-48ljh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:26 +0000 UTC  }]
    May  9 13:30:46.643: INFO: 
    May  9 13:30:46.643: INFO: StatefulSet ss has not reached scale 3, at 1
    May  9 13:30:47.651: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997810676s
    May  9 13:30:48.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989490903s
    May  9 13:30:49.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986486714s
    May  9 13:30:50.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983093293s
    May  9 13:30:51.664: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980189262s
    May  9 13:30:52.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977346956s
    May  9 13:30:53.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974414546s
    May  9 13:30:54.674: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.971047474s
    May  9 13:30:55.679: INFO: Verifying statefulset ss doesn't scale past 3 for another 967.757249ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4748 05/09/23 13:30:56.679
    May  9 13:30:56.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 13:30:56.780: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 13:30:56.780: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 13:30:56.780: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 13:30:56.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 13:30:56.882: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May  9 13:30:56.882: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 13:30:56.882: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 13:30:56.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 13:30:56.993: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May  9 13:30:56.993: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 13:30:56.993: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 13:30:56.995: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  9 13:30:56.995: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May  9 13:30:56.995: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 05/09/23 13:30:56.995
    May  9 13:30:56.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 13:30:57.097: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 13:30:57.097: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 13:30:57.097: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 13:30:57.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 13:30:57.196: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 13:30:57.196: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 13:30:57.196: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 13:30:57.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-4748 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 13:30:57.291: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 13:30:57.291: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 13:30:57.291: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 13:30:57.291: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 13:30:57.293: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    May  9 13:31:07.300: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  9 13:31:07.300: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May  9 13:31:07.300: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May  9 13:31:07.310: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
    May  9 13:31:07.310: INFO: ss-0  cl-gks-cncf-ix1-md-0-48ljh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:26 +0000 UTC  }]
    May  9 13:31:07.310: INFO: ss-1  cl-gks-cncf-ix1-md-0-879bk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:46 +0000 UTC  }]
    May  9 13:31:07.310: INFO: ss-2  cl-gks-cncf-ix1-md-0-skqqr  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:46 +0000 UTC  }]
    May  9 13:31:07.310: INFO: 
    May  9 13:31:07.310: INFO: StatefulSet ss has not reached scale 0, at 3
    May  9 13:31:08.313: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
    May  9 13:31:08.313: INFO: ss-0  cl-gks-cncf-ix1-md-0-48ljh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 13:30:26 +0000 UTC  }]
    May  9 13:31:08.313: INFO: 
    May  9 13:31:08.313: INFO: StatefulSet ss has not reached scale 0, at 1
    May  9 13:31:09.316: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.994388792s
    May  9 13:31:10.322: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.991339535s
    May  9 13:31:11.325: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.985305974s
    May  9 13:31:12.328: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.982616517s
    May  9 13:31:13.333: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.979836684s
    May  9 13:31:14.336: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.97459419s
    May  9 13:31:15.339: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.972060571s
    May  9 13:31:16.342: INFO: Verifying statefulset ss doesn't scale past 0 for another 969.200278ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4748 05/09/23 13:31:17.343
    May  9 13:31:17.351: INFO: Scaling statefulset ss to 0
    May  9 13:31:17.359: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  9 13:31:17.361: INFO: Deleting all statefulset in ns statefulset-4748
    May  9 13:31:17.363: INFO: Scaling statefulset ss to 0
    May  9 13:31:17.370: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 13:31:17.372: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  9 13:31:17.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4748" for this suite. 05/09/23 13:31:17.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:31:17.391
May  9 13:31:17.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pods 05/09/23 13:31:17.391
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:17.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:17.406
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 05/09/23 13:31:17.407
STEP: setting up watch 05/09/23 13:31:17.407
STEP: submitting the pod to kubernetes 05/09/23 13:31:17.51
STEP: verifying the pod is in kubernetes 05/09/23 13:31:17.519
STEP: verifying pod creation was observed 05/09/23 13:31:17.524
May  9 13:31:17.524: INFO: Waiting up to 5m0s for pod "pod-submit-remove-b37a85b1-de8c-4794-b2f0-7759f65824fb" in namespace "pods-1348" to be "running"
May  9 13:31:17.528: INFO: Pod "pod-submit-remove-b37a85b1-de8c-4794-b2f0-7759f65824fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383171ms
May  9 13:31:19.532: INFO: Pod "pod-submit-remove-b37a85b1-de8c-4794-b2f0-7759f65824fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007092835s
May  9 13:31:19.532: INFO: Pod "pod-submit-remove-b37a85b1-de8c-4794-b2f0-7759f65824fb" satisfied condition "running"
STEP: deleting the pod gracefully 05/09/23 13:31:19.534
STEP: verifying pod deletion was observed 05/09/23 13:31:19.539
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  9 13:31:21.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1348" for this suite. 05/09/23 13:31:21.514
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":246,"skipped":4953,"failed":0}
------------------------------
• [4.128 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:31:17.391
    May  9 13:31:17.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pods 05/09/23 13:31:17.391
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:17.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:17.406
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 05/09/23 13:31:17.407
    STEP: setting up watch 05/09/23 13:31:17.407
    STEP: submitting the pod to kubernetes 05/09/23 13:31:17.51
    STEP: verifying the pod is in kubernetes 05/09/23 13:31:17.519
    STEP: verifying pod creation was observed 05/09/23 13:31:17.524
    May  9 13:31:17.524: INFO: Waiting up to 5m0s for pod "pod-submit-remove-b37a85b1-de8c-4794-b2f0-7759f65824fb" in namespace "pods-1348" to be "running"
    May  9 13:31:17.528: INFO: Pod "pod-submit-remove-b37a85b1-de8c-4794-b2f0-7759f65824fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383171ms
    May  9 13:31:19.532: INFO: Pod "pod-submit-remove-b37a85b1-de8c-4794-b2f0-7759f65824fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007092835s
    May  9 13:31:19.532: INFO: Pod "pod-submit-remove-b37a85b1-de8c-4794-b2f0-7759f65824fb" satisfied condition "running"
    STEP: deleting the pod gracefully 05/09/23 13:31:19.534
    STEP: verifying pod deletion was observed 05/09/23 13:31:19.539
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  9 13:31:21.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1348" for this suite. 05/09/23 13:31:21.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:31:21.519
May  9 13:31:21.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pods 05/09/23 13:31:21.52
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:21.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:21.533
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
May  9 13:31:21.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: creating the pod 05/09/23 13:31:21.535
STEP: submitting the pod to kubernetes 05/09/23 13:31:21.535
May  9 13:31:21.541: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-951d077f-414c-4b7a-b8cd-7582edadbe95" in namespace "pods-1203" to be "running and ready"
May  9 13:31:21.546: INFO: Pod "pod-exec-websocket-951d077f-414c-4b7a-b8cd-7582edadbe95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.769321ms
May  9 13:31:21.546: INFO: The phase of Pod pod-exec-websocket-951d077f-414c-4b7a-b8cd-7582edadbe95 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:31:23.549: INFO: Pod "pod-exec-websocket-951d077f-414c-4b7a-b8cd-7582edadbe95": Phase="Running", Reason="", readiness=true. Elapsed: 2.008003339s
May  9 13:31:23.549: INFO: The phase of Pod pod-exec-websocket-951d077f-414c-4b7a-b8cd-7582edadbe95 is Running (Ready = true)
May  9 13:31:23.549: INFO: Pod "pod-exec-websocket-951d077f-414c-4b7a-b8cd-7582edadbe95" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  9 13:31:23.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1203" for this suite. 05/09/23 13:31:23.614
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":247,"skipped":4992,"failed":0}
------------------------------
• [2.101 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:31:21.519
    May  9 13:31:21.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pods 05/09/23 13:31:21.52
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:21.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:21.533
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    May  9 13:31:21.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: creating the pod 05/09/23 13:31:21.535
    STEP: submitting the pod to kubernetes 05/09/23 13:31:21.535
    May  9 13:31:21.541: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-951d077f-414c-4b7a-b8cd-7582edadbe95" in namespace "pods-1203" to be "running and ready"
    May  9 13:31:21.546: INFO: Pod "pod-exec-websocket-951d077f-414c-4b7a-b8cd-7582edadbe95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.769321ms
    May  9 13:31:21.546: INFO: The phase of Pod pod-exec-websocket-951d077f-414c-4b7a-b8cd-7582edadbe95 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:31:23.549: INFO: Pod "pod-exec-websocket-951d077f-414c-4b7a-b8cd-7582edadbe95": Phase="Running", Reason="", readiness=true. Elapsed: 2.008003339s
    May  9 13:31:23.549: INFO: The phase of Pod pod-exec-websocket-951d077f-414c-4b7a-b8cd-7582edadbe95 is Running (Ready = true)
    May  9 13:31:23.549: INFO: Pod "pod-exec-websocket-951d077f-414c-4b7a-b8cd-7582edadbe95" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  9 13:31:23.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1203" for this suite. 05/09/23 13:31:23.614
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:31:23.62
May  9 13:31:23.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 13:31:23.621
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:23.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:23.635
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
May  9 13:31:23.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:31:29.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1202" for this suite. 05/09/23 13:31:29.838
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":248,"skipped":4996,"failed":0}
------------------------------
• [SLOW TEST] [6.223 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:31:23.62
    May  9 13:31:23.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 13:31:23.621
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:23.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:23.635
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    May  9 13:31:23.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:31:29.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1202" for this suite. 05/09/23 13:31:29.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:31:29.844
May  9 13:31:29.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:31:29.845
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:29.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:29.859
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:31:29.87
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:31:30.037
STEP: Deploying the webhook pod 05/09/23 13:31:30.045
STEP: Wait for the deployment to be ready 05/09/23 13:31:30.056
May  9 13:31:30.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 13:31:32.072
STEP: Verifying the service has paired with the endpoint 05/09/23 13:31:32.085
May  9 13:31:33.085: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 05/09/23 13:31:33.088
STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/09/23 13:31:33.102
STEP: Creating a configMap that should not be mutated 05/09/23 13:31:33.107
STEP: Patching a mutating webhook configuration's rules to include the create operation 05/09/23 13:31:33.116
STEP: Creating a configMap that should be mutated 05/09/23 13:31:33.122
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:31:33.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5860" for this suite. 05/09/23 13:31:33.142
STEP: Destroying namespace "webhook-5860-markers" for this suite. 05/09/23 13:31:33.146
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":249,"skipped":5036,"failed":0}
------------------------------
• [3.345 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:31:29.844
    May  9 13:31:29.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:31:29.845
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:29.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:29.859
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:31:29.87
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:31:30.037
    STEP: Deploying the webhook pod 05/09/23 13:31:30.045
    STEP: Wait for the deployment to be ready 05/09/23 13:31:30.056
    May  9 13:31:30.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 13:31:32.072
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:31:32.085
    May  9 13:31:33.085: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 05/09/23 13:31:33.088
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/09/23 13:31:33.102
    STEP: Creating a configMap that should not be mutated 05/09/23 13:31:33.107
    STEP: Patching a mutating webhook configuration's rules to include the create operation 05/09/23 13:31:33.116
    STEP: Creating a configMap that should be mutated 05/09/23 13:31:33.122
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:31:33.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5860" for this suite. 05/09/23 13:31:33.142
    STEP: Destroying namespace "webhook-5860-markers" for this suite. 05/09/23 13:31:33.146
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:31:33.19
May  9 13:31:33.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 13:31:33.19
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:33.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:33.204
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-a68ad01b-cbbe-46cd-8fc9-e353de1c5276 05/09/23 13:31:33.206
STEP: Creating a pod to test consume secrets 05/09/23 13:31:33.209
May  9 13:31:33.215: INFO: Waiting up to 5m0s for pod "pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a" in namespace "secrets-8141" to be "Succeeded or Failed"
May  9 13:31:33.219: INFO: Pod "pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.175989ms
May  9 13:31:35.222: INFO: Pod "pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007325209s
May  9 13:31:37.222: INFO: Pod "pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00732909s
STEP: Saw pod success 05/09/23 13:31:37.222
May  9 13:31:37.222: INFO: Pod "pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a" satisfied condition "Succeeded or Failed"
May  9 13:31:37.228: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 13:31:37.243
May  9 13:31:37.262: INFO: Waiting for pod pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a to disappear
May  9 13:31:37.264: INFO: Pod pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  9 13:31:37.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8141" for this suite. 05/09/23 13:31:37.267
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":250,"skipped":5046,"failed":0}
------------------------------
• [4.082 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:31:33.19
    May  9 13:31:33.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 13:31:33.19
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:33.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:33.204
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-a68ad01b-cbbe-46cd-8fc9-e353de1c5276 05/09/23 13:31:33.206
    STEP: Creating a pod to test consume secrets 05/09/23 13:31:33.209
    May  9 13:31:33.215: INFO: Waiting up to 5m0s for pod "pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a" in namespace "secrets-8141" to be "Succeeded or Failed"
    May  9 13:31:33.219: INFO: Pod "pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.175989ms
    May  9 13:31:35.222: INFO: Pod "pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007325209s
    May  9 13:31:37.222: INFO: Pod "pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00732909s
    STEP: Saw pod success 05/09/23 13:31:37.222
    May  9 13:31:37.222: INFO: Pod "pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a" satisfied condition "Succeeded or Failed"
    May  9 13:31:37.228: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:31:37.243
    May  9 13:31:37.262: INFO: Waiting for pod pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a to disappear
    May  9 13:31:37.264: INFO: Pod pod-secrets-3a3aa6c1-3fa0-4d01-89cd-84a667bb313a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  9 13:31:37.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8141" for this suite. 05/09/23 13:31:37.267
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:31:37.272
May  9 13:31:37.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pod-network-test 05/09/23 13:31:37.273
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:37.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:37.287
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-7046 05/09/23 13:31:37.289
STEP: creating a selector 05/09/23 13:31:37.289
STEP: Creating the service pods in kubernetes 05/09/23 13:31:37.289
May  9 13:31:37.289: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  9 13:31:37.318: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7046" to be "running and ready"
May  9 13:31:37.326: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.537214ms
May  9 13:31:37.326: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:31:39.330: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011942029s
May  9 13:31:39.330: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:31:41.329: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011434977s
May  9 13:31:41.329: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:31:43.329: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011532693s
May  9 13:31:43.329: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:31:45.330: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012000194s
May  9 13:31:45.330: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:31:47.330: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012732089s
May  9 13:31:47.330: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:31:49.329: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011841717s
May  9 13:31:49.330: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  9 13:31:49.330: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  9 13:31:49.332: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7046" to be "running and ready"
May  9 13:31:49.334: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.726217ms
May  9 13:31:49.334: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  9 13:31:49.334: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  9 13:31:49.336: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7046" to be "running and ready"
May  9 13:31:49.338: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.791622ms
May  9 13:31:49.338: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  9 13:31:49.338: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/09/23 13:31:49.339
May  9 13:31:49.349: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7046" to be "running"
May  9 13:31:49.353: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.55456ms
May  9 13:31:51.356: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007730784s
May  9 13:31:51.356: INFO: Pod "test-container-pod" satisfied condition "running"
May  9 13:31:51.358: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7046" to be "running"
May  9 13:31:51.360: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.92815ms
May  9 13:31:51.360: INFO: Pod "host-test-container-pod" satisfied condition "running"
May  9 13:31:51.362: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  9 13:31:51.362: INFO: Going to poll 172.25.124.252 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May  9 13:31:51.364: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.124.252 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7046 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:31:51.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:31:51.365: INFO: ExecWithOptions: Clientset creation
May  9 13:31:51.365: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7046/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.124.252+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 13:31:52.415: INFO: Found all 1 expected endpoints: [netserver-0]
May  9 13:31:52.415: INFO: Going to poll 172.25.53.158 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May  9 13:31:52.417: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.53.158 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7046 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:31:52.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:31:52.418: INFO: ExecWithOptions: Clientset creation
May  9 13:31:52.418: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7046/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.53.158+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 13:31:53.471: INFO: Found all 1 expected endpoints: [netserver-1]
May  9 13:31:53.471: INFO: Going to poll 172.25.72.246 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May  9 13:31:53.473: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.72.246 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7046 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:31:53.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:31:53.473: INFO: ExecWithOptions: Clientset creation
May  9 13:31:53.473: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7046/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.72.246+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 13:31:54.522: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May  9 13:31:54.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7046" for this suite. 05/09/23 13:31:54.526
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":251,"skipped":5046,"failed":0}
------------------------------
• [SLOW TEST] [17.260 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:31:37.272
    May  9 13:31:37.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pod-network-test 05/09/23 13:31:37.273
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:37.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:37.287
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-7046 05/09/23 13:31:37.289
    STEP: creating a selector 05/09/23 13:31:37.289
    STEP: Creating the service pods in kubernetes 05/09/23 13:31:37.289
    May  9 13:31:37.289: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  9 13:31:37.318: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7046" to be "running and ready"
    May  9 13:31:37.326: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.537214ms
    May  9 13:31:37.326: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:31:39.330: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011942029s
    May  9 13:31:39.330: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:31:41.329: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011434977s
    May  9 13:31:41.329: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:31:43.329: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011532693s
    May  9 13:31:43.329: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:31:45.330: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012000194s
    May  9 13:31:45.330: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:31:47.330: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012732089s
    May  9 13:31:47.330: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:31:49.329: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011841717s
    May  9 13:31:49.330: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  9 13:31:49.330: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  9 13:31:49.332: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7046" to be "running and ready"
    May  9 13:31:49.334: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.726217ms
    May  9 13:31:49.334: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  9 13:31:49.334: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  9 13:31:49.336: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7046" to be "running and ready"
    May  9 13:31:49.338: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.791622ms
    May  9 13:31:49.338: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  9 13:31:49.338: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/09/23 13:31:49.339
    May  9 13:31:49.349: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7046" to be "running"
    May  9 13:31:49.353: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.55456ms
    May  9 13:31:51.356: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007730784s
    May  9 13:31:51.356: INFO: Pod "test-container-pod" satisfied condition "running"
    May  9 13:31:51.358: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7046" to be "running"
    May  9 13:31:51.360: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.92815ms
    May  9 13:31:51.360: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May  9 13:31:51.362: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May  9 13:31:51.362: INFO: Going to poll 172.25.124.252 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May  9 13:31:51.364: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.124.252 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7046 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:31:51.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:31:51.365: INFO: ExecWithOptions: Clientset creation
    May  9 13:31:51.365: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7046/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.124.252+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 13:31:52.415: INFO: Found all 1 expected endpoints: [netserver-0]
    May  9 13:31:52.415: INFO: Going to poll 172.25.53.158 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May  9 13:31:52.417: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.53.158 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7046 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:31:52.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:31:52.418: INFO: ExecWithOptions: Clientset creation
    May  9 13:31:52.418: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7046/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.53.158+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 13:31:53.471: INFO: Found all 1 expected endpoints: [netserver-1]
    May  9 13:31:53.471: INFO: Going to poll 172.25.72.246 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May  9 13:31:53.473: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.72.246 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7046 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:31:53.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:31:53.473: INFO: ExecWithOptions: Clientset creation
    May  9 13:31:53.473: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7046/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.72.246+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 13:31:54.522: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May  9 13:31:54.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-7046" for this suite. 05/09/23 13:31:54.526
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:31:54.532
May  9 13:31:54.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename lease-test 05/09/23 13:31:54.532
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:54.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:54.55
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
May  9 13:31:54.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-489" for this suite. 05/09/23 13:31:54.592
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":252,"skipped":5048,"failed":0}
------------------------------
• [0.065 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:31:54.532
    May  9 13:31:54.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename lease-test 05/09/23 13:31:54.532
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:54.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:54.55
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    May  9 13:31:54.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-489" for this suite. 05/09/23 13:31:54.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:31:54.598
May  9 13:31:54.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 13:31:54.599
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:54.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:54.613
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 05/09/23 13:31:54.615
May  9 13:31:54.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 create -f -'
May  9 13:31:55.295: INFO: stderr: ""
May  9 13:31:55.295: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 13:31:55.295
May  9 13:31:55.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 13:31:55.350: INFO: stderr: ""
May  9 13:31:55.350: INFO: stdout: "update-demo-nautilus-2pqgh update-demo-nautilus-xcc2h "
May  9 13:31:55.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-2pqgh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 13:31:55.404: INFO: stderr: ""
May  9 13:31:55.404: INFO: stdout: ""
May  9 13:31:55.404: INFO: update-demo-nautilus-2pqgh is created but not running
May  9 13:32:00.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 13:32:00.462: INFO: stderr: ""
May  9 13:32:00.463: INFO: stdout: "update-demo-nautilus-2pqgh update-demo-nautilus-xcc2h "
May  9 13:32:00.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-2pqgh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 13:32:00.515: INFO: stderr: ""
May  9 13:32:00.515: INFO: stdout: "true"
May  9 13:32:00.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-2pqgh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 13:32:00.566: INFO: stderr: ""
May  9 13:32:00.566: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  9 13:32:00.566: INFO: validating pod update-demo-nautilus-2pqgh
May  9 13:32:00.569: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 13:32:00.569: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 13:32:00.569: INFO: update-demo-nautilus-2pqgh is verified up and running
May  9 13:32:00.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-xcc2h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 13:32:00.618: INFO: stderr: ""
May  9 13:32:00.618: INFO: stdout: "true"
May  9 13:32:00.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-xcc2h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 13:32:00.667: INFO: stderr: ""
May  9 13:32:00.667: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  9 13:32:00.667: INFO: validating pod update-demo-nautilus-xcc2h
May  9 13:32:00.670: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 13:32:00.671: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 13:32:00.671: INFO: update-demo-nautilus-xcc2h is verified up and running
STEP: scaling down the replication controller 05/09/23 13:32:00.671
May  9 13:32:00.671: INFO: scanned /root for discovery docs: <nil>
May  9 13:32:00.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May  9 13:32:01.735: INFO: stderr: ""
May  9 13:32:01.735: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 13:32:01.735
May  9 13:32:01.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 13:32:01.788: INFO: stderr: ""
May  9 13:32:01.788: INFO: stdout: "update-demo-nautilus-xcc2h "
May  9 13:32:01.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-xcc2h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 13:32:01.839: INFO: stderr: ""
May  9 13:32:01.839: INFO: stdout: "true"
May  9 13:32:01.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-xcc2h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 13:32:01.889: INFO: stderr: ""
May  9 13:32:01.889: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  9 13:32:01.889: INFO: validating pod update-demo-nautilus-xcc2h
May  9 13:32:01.892: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 13:32:01.892: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 13:32:01.892: INFO: update-demo-nautilus-xcc2h is verified up and running
STEP: scaling up the replication controller 05/09/23 13:32:01.892
May  9 13:32:01.893: INFO: scanned /root for discovery docs: <nil>
May  9 13:32:01.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May  9 13:32:02.958: INFO: stderr: ""
May  9 13:32:02.958: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 13:32:02.958
May  9 13:32:02.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 13:32:03.008: INFO: stderr: ""
May  9 13:32:03.008: INFO: stdout: "update-demo-nautilus-gp5gm update-demo-nautilus-xcc2h "
May  9 13:32:03.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-gp5gm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 13:32:03.057: INFO: stderr: ""
May  9 13:32:03.057: INFO: stdout: "true"
May  9 13:32:03.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-gp5gm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 13:32:03.108: INFO: stderr: ""
May  9 13:32:03.108: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  9 13:32:03.108: INFO: validating pod update-demo-nautilus-gp5gm
May  9 13:32:03.111: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 13:32:03.111: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 13:32:03.111: INFO: update-demo-nautilus-gp5gm is verified up and running
May  9 13:32:03.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-xcc2h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 13:32:03.161: INFO: stderr: ""
May  9 13:32:03.161: INFO: stdout: "true"
May  9 13:32:03.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-xcc2h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 13:32:03.210: INFO: stderr: ""
May  9 13:32:03.210: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  9 13:32:03.210: INFO: validating pod update-demo-nautilus-xcc2h
May  9 13:32:03.213: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 13:32:03.214: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 13:32:03.214: INFO: update-demo-nautilus-xcc2h is verified up and running
STEP: using delete to clean up resources 05/09/23 13:32:03.214
May  9 13:32:03.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 delete --grace-period=0 --force -f -'
May  9 13:32:03.269: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 13:32:03.269: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  9 13:32:03.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get rc,svc -l name=update-demo --no-headers'
May  9 13:32:03.325: INFO: stderr: "No resources found in kubectl-6955 namespace.\n"
May  9 13:32:03.325: INFO: stdout: ""
May  9 13:32:03.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  9 13:32:03.377: INFO: stderr: ""
May  9 13:32:03.377: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 13:32:03.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6955" for this suite. 05/09/23 13:32:03.38
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":253,"skipped":5085,"failed":0}
------------------------------
• [SLOW TEST] [8.786 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:31:54.598
    May  9 13:31:54.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 13:31:54.599
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:31:54.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:31:54.613
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 05/09/23 13:31:54.615
    May  9 13:31:54.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 create -f -'
    May  9 13:31:55.295: INFO: stderr: ""
    May  9 13:31:55.295: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 13:31:55.295
    May  9 13:31:55.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 13:31:55.350: INFO: stderr: ""
    May  9 13:31:55.350: INFO: stdout: "update-demo-nautilus-2pqgh update-demo-nautilus-xcc2h "
    May  9 13:31:55.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-2pqgh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 13:31:55.404: INFO: stderr: ""
    May  9 13:31:55.404: INFO: stdout: ""
    May  9 13:31:55.404: INFO: update-demo-nautilus-2pqgh is created but not running
    May  9 13:32:00.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 13:32:00.462: INFO: stderr: ""
    May  9 13:32:00.463: INFO: stdout: "update-demo-nautilus-2pqgh update-demo-nautilus-xcc2h "
    May  9 13:32:00.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-2pqgh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 13:32:00.515: INFO: stderr: ""
    May  9 13:32:00.515: INFO: stdout: "true"
    May  9 13:32:00.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-2pqgh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 13:32:00.566: INFO: stderr: ""
    May  9 13:32:00.566: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  9 13:32:00.566: INFO: validating pod update-demo-nautilus-2pqgh
    May  9 13:32:00.569: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 13:32:00.569: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 13:32:00.569: INFO: update-demo-nautilus-2pqgh is verified up and running
    May  9 13:32:00.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-xcc2h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 13:32:00.618: INFO: stderr: ""
    May  9 13:32:00.618: INFO: stdout: "true"
    May  9 13:32:00.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-xcc2h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 13:32:00.667: INFO: stderr: ""
    May  9 13:32:00.667: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  9 13:32:00.667: INFO: validating pod update-demo-nautilus-xcc2h
    May  9 13:32:00.670: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 13:32:00.671: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 13:32:00.671: INFO: update-demo-nautilus-xcc2h is verified up and running
    STEP: scaling down the replication controller 05/09/23 13:32:00.671
    May  9 13:32:00.671: INFO: scanned /root for discovery docs: <nil>
    May  9 13:32:00.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    May  9 13:32:01.735: INFO: stderr: ""
    May  9 13:32:01.735: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 13:32:01.735
    May  9 13:32:01.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 13:32:01.788: INFO: stderr: ""
    May  9 13:32:01.788: INFO: stdout: "update-demo-nautilus-xcc2h "
    May  9 13:32:01.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-xcc2h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 13:32:01.839: INFO: stderr: ""
    May  9 13:32:01.839: INFO: stdout: "true"
    May  9 13:32:01.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-xcc2h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 13:32:01.889: INFO: stderr: ""
    May  9 13:32:01.889: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  9 13:32:01.889: INFO: validating pod update-demo-nautilus-xcc2h
    May  9 13:32:01.892: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 13:32:01.892: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 13:32:01.892: INFO: update-demo-nautilus-xcc2h is verified up and running
    STEP: scaling up the replication controller 05/09/23 13:32:01.892
    May  9 13:32:01.893: INFO: scanned /root for discovery docs: <nil>
    May  9 13:32:01.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    May  9 13:32:02.958: INFO: stderr: ""
    May  9 13:32:02.958: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 13:32:02.958
    May  9 13:32:02.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 13:32:03.008: INFO: stderr: ""
    May  9 13:32:03.008: INFO: stdout: "update-demo-nautilus-gp5gm update-demo-nautilus-xcc2h "
    May  9 13:32:03.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-gp5gm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 13:32:03.057: INFO: stderr: ""
    May  9 13:32:03.057: INFO: stdout: "true"
    May  9 13:32:03.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-gp5gm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 13:32:03.108: INFO: stderr: ""
    May  9 13:32:03.108: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  9 13:32:03.108: INFO: validating pod update-demo-nautilus-gp5gm
    May  9 13:32:03.111: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 13:32:03.111: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 13:32:03.111: INFO: update-demo-nautilus-gp5gm is verified up and running
    May  9 13:32:03.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-xcc2h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 13:32:03.161: INFO: stderr: ""
    May  9 13:32:03.161: INFO: stdout: "true"
    May  9 13:32:03.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods update-demo-nautilus-xcc2h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 13:32:03.210: INFO: stderr: ""
    May  9 13:32:03.210: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  9 13:32:03.210: INFO: validating pod update-demo-nautilus-xcc2h
    May  9 13:32:03.213: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 13:32:03.214: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 13:32:03.214: INFO: update-demo-nautilus-xcc2h is verified up and running
    STEP: using delete to clean up resources 05/09/23 13:32:03.214
    May  9 13:32:03.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 delete --grace-period=0 --force -f -'
    May  9 13:32:03.269: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 13:32:03.269: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May  9 13:32:03.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get rc,svc -l name=update-demo --no-headers'
    May  9 13:32:03.325: INFO: stderr: "No resources found in kubectl-6955 namespace.\n"
    May  9 13:32:03.325: INFO: stdout: ""
    May  9 13:32:03.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6955 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  9 13:32:03.377: INFO: stderr: ""
    May  9 13:32:03.377: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 13:32:03.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6955" for this suite. 05/09/23 13:32:03.38
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:32:03.385
May  9 13:32:03.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:32:03.386
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:03.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:03.401
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9685 05/09/23 13:32:03.402
STEP: changing the ExternalName service to type=NodePort 05/09/23 13:32:03.407
STEP: creating replication controller externalname-service in namespace services-9685 05/09/23 13:32:03.429
I0509 13:32:03.435259      24 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9685, replica count: 2
I0509 13:32:06.487282      24 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 13:32:06.487: INFO: Creating new exec pod
May  9 13:32:06.495: INFO: Waiting up to 5m0s for pod "execpodqrrc9" in namespace "services-9685" to be "running"
May  9 13:32:06.498: INFO: Pod "execpodqrrc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.23582ms
May  9 13:32:08.503: INFO: Pod "execpodqrrc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008313349s
May  9 13:32:08.503: INFO: Pod "execpodqrrc9" satisfied condition "running"
May  9 13:32:09.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9685 exec execpodqrrc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May  9 13:32:09.606: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  9 13:32:09.606: INFO: stdout: "externalname-service-cj72t"
May  9 13:32:09.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9685 exec execpodqrrc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.132.14 80'
May  9 13:32:09.700: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.132.14 80\nConnection to 10.109.132.14 80 port [tcp/http] succeeded!\n"
May  9 13:32:09.700: INFO: stdout: "externalname-service-cj72t"
May  9 13:32:09.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9685 exec execpodqrrc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.73 30085'
May  9 13:32:09.808: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.73 30085\nConnection to 192.168.1.73 30085 port [tcp/*] succeeded!\n"
May  9 13:32:09.808: INFO: stdout: "externalname-service-cj72t"
May  9 13:32:09.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9685 exec execpodqrrc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.64 30085'
May  9 13:32:09.906: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.64 30085\nConnection to 192.168.1.64 30085 port [tcp/*] succeeded!\n"
May  9 13:32:09.906: INFO: stdout: ""
May  9 13:32:10.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9685 exec execpodqrrc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.64 30085'
May  9 13:32:11.011: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.64 30085\nConnection to 192.168.1.64 30085 port [tcp/*] succeeded!\n"
May  9 13:32:11.011: INFO: stdout: "externalname-service-hlnf5"
May  9 13:32:11.011: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:32:11.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9685" for this suite. 05/09/23 13:32:11.037
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":254,"skipped":5087,"failed":0}
------------------------------
• [SLOW TEST] [7.657 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:32:03.385
    May  9 13:32:03.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:32:03.386
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:03.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:03.401
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9685 05/09/23 13:32:03.402
    STEP: changing the ExternalName service to type=NodePort 05/09/23 13:32:03.407
    STEP: creating replication controller externalname-service in namespace services-9685 05/09/23 13:32:03.429
    I0509 13:32:03.435259      24 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9685, replica count: 2
    I0509 13:32:06.487282      24 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 13:32:06.487: INFO: Creating new exec pod
    May  9 13:32:06.495: INFO: Waiting up to 5m0s for pod "execpodqrrc9" in namespace "services-9685" to be "running"
    May  9 13:32:06.498: INFO: Pod "execpodqrrc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.23582ms
    May  9 13:32:08.503: INFO: Pod "execpodqrrc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008313349s
    May  9 13:32:08.503: INFO: Pod "execpodqrrc9" satisfied condition "running"
    May  9 13:32:09.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9685 exec execpodqrrc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    May  9 13:32:09.606: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May  9 13:32:09.606: INFO: stdout: "externalname-service-cj72t"
    May  9 13:32:09.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9685 exec execpodqrrc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.132.14 80'
    May  9 13:32:09.700: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.132.14 80\nConnection to 10.109.132.14 80 port [tcp/http] succeeded!\n"
    May  9 13:32:09.700: INFO: stdout: "externalname-service-cj72t"
    May  9 13:32:09.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9685 exec execpodqrrc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.73 30085'
    May  9 13:32:09.808: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.73 30085\nConnection to 192.168.1.73 30085 port [tcp/*] succeeded!\n"
    May  9 13:32:09.808: INFO: stdout: "externalname-service-cj72t"
    May  9 13:32:09.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9685 exec execpodqrrc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.64 30085'
    May  9 13:32:09.906: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.64 30085\nConnection to 192.168.1.64 30085 port [tcp/*] succeeded!\n"
    May  9 13:32:09.906: INFO: stdout: ""
    May  9 13:32:10.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-9685 exec execpodqrrc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.64 30085'
    May  9 13:32:11.011: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.64 30085\nConnection to 192.168.1.64 30085 port [tcp/*] succeeded!\n"
    May  9 13:32:11.011: INFO: stdout: "externalname-service-hlnf5"
    May  9 13:32:11.011: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:32:11.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9685" for this suite. 05/09/23 13:32:11.037
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:32:11.043
May  9 13:32:11.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename statefulset 05/09/23 13:32:11.043
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:11.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:11.06
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6792 05/09/23 13:32:11.062
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
May  9 13:32:11.078: INFO: Found 0 stateful pods, waiting for 1
May  9 13:32:21.082: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 05/09/23 13:32:21.087
W0509 13:32:21.096907      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  9 13:32:21.102: INFO: Found 1 stateful pods, waiting for 2
May  9 13:32:31.109: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  9 13:32:31.109: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 05/09/23 13:32:31.113
STEP: Delete all of the StatefulSets 05/09/23 13:32:31.115
STEP: Verify that StatefulSets have been deleted 05/09/23 13:32:31.12
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  9 13:32:31.123: INFO: Deleting all statefulset in ns statefulset-6792
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  9 13:32:31.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6792" for this suite. 05/09/23 13:32:31.133
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":255,"skipped":5093,"failed":0}
------------------------------
• [SLOW TEST] [20.099 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:32:11.043
    May  9 13:32:11.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename statefulset 05/09/23 13:32:11.043
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:11.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:11.06
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6792 05/09/23 13:32:11.062
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    May  9 13:32:11.078: INFO: Found 0 stateful pods, waiting for 1
    May  9 13:32:21.082: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 05/09/23 13:32:21.087
    W0509 13:32:21.096907      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  9 13:32:21.102: INFO: Found 1 stateful pods, waiting for 2
    May  9 13:32:31.109: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  9 13:32:31.109: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 05/09/23 13:32:31.113
    STEP: Delete all of the StatefulSets 05/09/23 13:32:31.115
    STEP: Verify that StatefulSets have been deleted 05/09/23 13:32:31.12
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  9 13:32:31.123: INFO: Deleting all statefulset in ns statefulset-6792
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  9 13:32:31.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6792" for this suite. 05/09/23 13:32:31.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:32:31.142
May  9 13:32:31.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename svcaccounts 05/09/23 13:32:31.143
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:31.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:31.158
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 05/09/23 13:32:31.16
STEP: watching for the ServiceAccount to be added 05/09/23 13:32:31.165
STEP: patching the ServiceAccount 05/09/23 13:32:31.165
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/09/23 13:32:31.17
STEP: deleting the ServiceAccount 05/09/23 13:32:31.172
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  9 13:32:31.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7508" for this suite. 05/09/23 13:32:31.185
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":256,"skipped":5102,"failed":0}
------------------------------
• [0.048 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:32:31.142
    May  9 13:32:31.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 13:32:31.143
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:31.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:31.158
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 05/09/23 13:32:31.16
    STEP: watching for the ServiceAccount to be added 05/09/23 13:32:31.165
    STEP: patching the ServiceAccount 05/09/23 13:32:31.165
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/09/23 13:32:31.17
    STEP: deleting the ServiceAccount 05/09/23 13:32:31.172
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  9 13:32:31.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7508" for this suite. 05/09/23 13:32:31.185
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:32:31.191
May  9 13:32:31.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename daemonsets 05/09/23 13:32:31.192
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:31.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:31.203
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
May  9 13:32:31.217: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 05/09/23 13:32:31.221
May  9 13:32:31.224: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:32:31.224: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 05/09/23 13:32:31.224
May  9 13:32:31.246: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:32:31.246: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 13:32:32.250: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  9 13:32:32.250: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 05/09/23 13:32:32.252
May  9 13:32:32.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:32:32.268: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/09/23 13:32:32.268
May  9 13:32:32.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:32:32.284: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 13:32:33.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:32:33.287: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 13:32:34.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:32:34.290: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 13:32:35.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:32:35.287: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 13:32:36.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  9 13:32:36.289: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/09/23 13:32:36.293
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5825, will wait for the garbage collector to delete the pods 05/09/23 13:32:36.293
May  9 13:32:36.350: INFO: Deleting DaemonSet.extensions daemon-set took: 4.490109ms
May  9 13:32:36.451: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.368196ms
May  9 13:32:39.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:32:39.053: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  9 13:32:39.055: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40407"},"items":null}

May  9 13:32:39.057: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40407"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  9 13:32:39.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5825" for this suite. 05/09/23 13:32:39.077
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":257,"skipped":5104,"failed":0}
------------------------------
• [SLOW TEST] [7.891 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:32:31.191
    May  9 13:32:31.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename daemonsets 05/09/23 13:32:31.192
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:31.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:31.203
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    May  9 13:32:31.217: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 05/09/23 13:32:31.221
    May  9 13:32:31.224: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:32:31.224: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 05/09/23 13:32:31.224
    May  9 13:32:31.246: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:32:31.246: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 13:32:32.250: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  9 13:32:32.250: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 05/09/23 13:32:32.252
    May  9 13:32:32.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:32:32.268: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/09/23 13:32:32.268
    May  9 13:32:32.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:32:32.284: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 13:32:33.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:32:33.287: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 13:32:34.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:32:34.290: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 13:32:35.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:32:35.287: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 13:32:36.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  9 13:32:36.289: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/09/23 13:32:36.293
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5825, will wait for the garbage collector to delete the pods 05/09/23 13:32:36.293
    May  9 13:32:36.350: INFO: Deleting DaemonSet.extensions daemon-set took: 4.490109ms
    May  9 13:32:36.451: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.368196ms
    May  9 13:32:39.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:32:39.053: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  9 13:32:39.055: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40407"},"items":null}

    May  9 13:32:39.057: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40407"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:32:39.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5825" for this suite. 05/09/23 13:32:39.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:32:39.082
May  9 13:32:39.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename endpointslice 05/09/23 13:32:39.083
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:39.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:39.096
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
May  9 13:32:39.103: INFO: Endpoints addresses: [192.168.1.33 192.168.1.34 192.168.1.96] , ports: [6443]
May  9 13:32:39.103: INFO: EndpointSlices addresses: [192.168.1.33 192.168.1.34 192.168.1.96] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May  9 13:32:39.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7909" for this suite. 05/09/23 13:32:39.106
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":258,"skipped":5116,"failed":0}
------------------------------
• [0.029 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:32:39.082
    May  9 13:32:39.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename endpointslice 05/09/23 13:32:39.083
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:39.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:39.096
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    May  9 13:32:39.103: INFO: Endpoints addresses: [192.168.1.33 192.168.1.34 192.168.1.96] , ports: [6443]
    May  9 13:32:39.103: INFO: EndpointSlices addresses: [192.168.1.33 192.168.1.34 192.168.1.96] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May  9 13:32:39.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7909" for this suite. 05/09/23 13:32:39.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:32:39.111
May  9 13:32:39.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename resourcequota 05/09/23 13:32:39.112
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:39.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:39.124
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 05/09/23 13:32:39.125
STEP: Getting a ResourceQuota 05/09/23 13:32:39.129
STEP: Listing all ResourceQuotas with LabelSelector 05/09/23 13:32:39.132
STEP: Patching the ResourceQuota 05/09/23 13:32:39.134
STEP: Deleting a Collection of ResourceQuotas 05/09/23 13:32:39.14
STEP: Verifying the deleted ResourceQuota 05/09/23 13:32:39.147
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  9 13:32:39.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2617" for this suite. 05/09/23 13:32:39.152
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":259,"skipped":5123,"failed":0}
------------------------------
• [0.045 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:32:39.111
    May  9 13:32:39.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename resourcequota 05/09/23 13:32:39.112
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:39.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:39.124
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 05/09/23 13:32:39.125
    STEP: Getting a ResourceQuota 05/09/23 13:32:39.129
    STEP: Listing all ResourceQuotas with LabelSelector 05/09/23 13:32:39.132
    STEP: Patching the ResourceQuota 05/09/23 13:32:39.134
    STEP: Deleting a Collection of ResourceQuotas 05/09/23 13:32:39.14
    STEP: Verifying the deleted ResourceQuota 05/09/23 13:32:39.147
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  9 13:32:39.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2617" for this suite. 05/09/23 13:32:39.152
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:32:39.157
May  9 13:32:39.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 13:32:39.157
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:39.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:39.169
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
May  9 13:32:39.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6162 version'
May  9 13:32:39.216: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
May  9 13:32:39.216: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.8\", GitCommit:\"0ce7342c984110dfc93657d64df5dc3b2c0d1fe9\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:39:54Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.8\", GitCommit:\"0ce7342c984110dfc93657d64df5dc3b2c0d1fe9\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:33:02Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 13:32:39.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6162" for this suite. 05/09/23 13:32:39.219
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":260,"skipped":5123,"failed":0}
------------------------------
• [0.067 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:32:39.157
    May  9 13:32:39.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 13:32:39.157
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:39.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:39.169
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    May  9 13:32:39.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-6162 version'
    May  9 13:32:39.216: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    May  9 13:32:39.216: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.8\", GitCommit:\"0ce7342c984110dfc93657d64df5dc3b2c0d1fe9\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:39:54Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.8\", GitCommit:\"0ce7342c984110dfc93657d64df5dc3b2c0d1fe9\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:33:02Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 13:32:39.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6162" for this suite. 05/09/23 13:32:39.219
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:32:39.224
May  9 13:32:39.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:32:39.225
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:39.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:39.237
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-7b45ba12-d866-49a3-9435-2fa01060666f 05/09/23 13:32:39.239
STEP: Creating a pod to test consume secrets 05/09/23 13:32:39.242
May  9 13:32:39.249: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c" in namespace "projected-272" to be "Succeeded or Failed"
May  9 13:32:39.252: INFO: Pod "pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.511314ms
May  9 13:32:41.256: INFO: Pod "pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006683357s
May  9 13:32:43.257: INFO: Pod "pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007709336s
STEP: Saw pod success 05/09/23 13:32:43.257
May  9 13:32:43.257: INFO: Pod "pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c" satisfied condition "Succeeded or Failed"
May  9 13:32:43.259: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c container projected-secret-volume-test: <nil>
STEP: delete the pod 05/09/23 13:32:43.262
May  9 13:32:43.272: INFO: Waiting for pod pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c to disappear
May  9 13:32:43.274: INFO: Pod pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  9 13:32:43.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-272" for this suite. 05/09/23 13:32:43.277
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":261,"skipped":5126,"failed":0}
------------------------------
• [4.060 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:32:39.224
    May  9 13:32:39.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:32:39.225
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:39.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:39.237
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-7b45ba12-d866-49a3-9435-2fa01060666f 05/09/23 13:32:39.239
    STEP: Creating a pod to test consume secrets 05/09/23 13:32:39.242
    May  9 13:32:39.249: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c" in namespace "projected-272" to be "Succeeded or Failed"
    May  9 13:32:39.252: INFO: Pod "pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.511314ms
    May  9 13:32:41.256: INFO: Pod "pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006683357s
    May  9 13:32:43.257: INFO: Pod "pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007709336s
    STEP: Saw pod success 05/09/23 13:32:43.257
    May  9 13:32:43.257: INFO: Pod "pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c" satisfied condition "Succeeded or Failed"
    May  9 13:32:43.259: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:32:43.262
    May  9 13:32:43.272: INFO: Waiting for pod pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c to disappear
    May  9 13:32:43.274: INFO: Pod pod-projected-secrets-3371b0c7-6ad4-4f9b-a687-f6973148128c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  9 13:32:43.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-272" for this suite. 05/09/23 13:32:43.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:32:43.286
May  9 13:32:43.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename runtimeclass 05/09/23 13:32:43.286
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:43.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:43.299
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
May  9 13:32:43.310: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8943 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  9 13:32:43.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8943" for this suite. 05/09/23 13:32:43.323
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":262,"skipped":5189,"failed":0}
------------------------------
• [0.043 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:32:43.286
    May  9 13:32:43.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename runtimeclass 05/09/23 13:32:43.286
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:43.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:43.299
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    May  9 13:32:43.310: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8943 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  9 13:32:43.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8943" for this suite. 05/09/23 13:32:43.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:32:43.329
May  9 13:32:43.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:32:43.33
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:43.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:43.342
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 05/09/23 13:32:43.345
STEP: watching for the Service to be added 05/09/23 13:32:43.356
May  9 13:32:43.357: INFO: Found Service test-service-wgwpr in namespace services-8296 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May  9 13:32:43.357: INFO: Service test-service-wgwpr created
STEP: Getting /status 05/09/23 13:32:43.357
May  9 13:32:43.359: INFO: Service test-service-wgwpr has LoadBalancer: {[]}
STEP: patching the ServiceStatus 05/09/23 13:32:43.359
STEP: watching for the Service to be patched 05/09/23 13:32:43.364
May  9 13:32:43.365: INFO: observed Service test-service-wgwpr in namespace services-8296 with annotations: map[] & LoadBalancer: {[]}
May  9 13:32:43.365: INFO: Found Service test-service-wgwpr in namespace services-8296 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May  9 13:32:43.365: INFO: Service test-service-wgwpr has service status patched
STEP: updating the ServiceStatus 05/09/23 13:32:43.365
May  9 13:32:43.371: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 05/09/23 13:32:43.371
May  9 13:32:43.372: INFO: Observed Service test-service-wgwpr in namespace services-8296 with annotations: map[] & Conditions: {[]}
May  9 13:32:43.372: INFO: Observed event: &Service{ObjectMeta:{test-service-wgwpr  services-8296  4f87e9aa-757f-4a12-85d2-22ac58a088bd 40482 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.110.142.189,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.110.142.189],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May  9 13:32:43.372: INFO: Found Service test-service-wgwpr in namespace services-8296 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  9 13:32:43.372: INFO: Service test-service-wgwpr has service status updated
STEP: patching the service 05/09/23 13:32:43.372
STEP: watching for the Service to be patched 05/09/23 13:32:43.382
May  9 13:32:43.384: INFO: observed Service test-service-wgwpr in namespace services-8296 with labels: map[test-service-static:true]
May  9 13:32:43.384: INFO: observed Service test-service-wgwpr in namespace services-8296 with labels: map[test-service-static:true]
May  9 13:32:43.384: INFO: observed Service test-service-wgwpr in namespace services-8296 with labels: map[test-service-static:true]
May  9 13:32:43.384: INFO: Found Service test-service-wgwpr in namespace services-8296 with labels: map[test-service:patched test-service-static:true]
May  9 13:32:43.384: INFO: Service test-service-wgwpr patched
STEP: deleting the service 05/09/23 13:32:43.384
STEP: watching for the Service to be deleted 05/09/23 13:32:43.406
May  9 13:32:43.406: INFO: Observed event: ADDED
May  9 13:32:43.407: INFO: Observed event: MODIFIED
May  9 13:32:43.407: INFO: Observed event: MODIFIED
May  9 13:32:43.407: INFO: Observed event: MODIFIED
May  9 13:32:43.407: INFO: Found Service test-service-wgwpr in namespace services-8296 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May  9 13:32:43.407: INFO: Service test-service-wgwpr deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:32:43.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8296" for this suite. 05/09/23 13:32:43.41
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":263,"skipped":5194,"failed":0}
------------------------------
• [0.085 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:32:43.329
    May  9 13:32:43.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:32:43.33
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:43.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:43.342
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 05/09/23 13:32:43.345
    STEP: watching for the Service to be added 05/09/23 13:32:43.356
    May  9 13:32:43.357: INFO: Found Service test-service-wgwpr in namespace services-8296 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    May  9 13:32:43.357: INFO: Service test-service-wgwpr created
    STEP: Getting /status 05/09/23 13:32:43.357
    May  9 13:32:43.359: INFO: Service test-service-wgwpr has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 05/09/23 13:32:43.359
    STEP: watching for the Service to be patched 05/09/23 13:32:43.364
    May  9 13:32:43.365: INFO: observed Service test-service-wgwpr in namespace services-8296 with annotations: map[] & LoadBalancer: {[]}
    May  9 13:32:43.365: INFO: Found Service test-service-wgwpr in namespace services-8296 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    May  9 13:32:43.365: INFO: Service test-service-wgwpr has service status patched
    STEP: updating the ServiceStatus 05/09/23 13:32:43.365
    May  9 13:32:43.371: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 05/09/23 13:32:43.371
    May  9 13:32:43.372: INFO: Observed Service test-service-wgwpr in namespace services-8296 with annotations: map[] & Conditions: {[]}
    May  9 13:32:43.372: INFO: Observed event: &Service{ObjectMeta:{test-service-wgwpr  services-8296  4f87e9aa-757f-4a12-85d2-22ac58a088bd 40482 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.110.142.189,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.110.142.189],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    May  9 13:32:43.372: INFO: Found Service test-service-wgwpr in namespace services-8296 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  9 13:32:43.372: INFO: Service test-service-wgwpr has service status updated
    STEP: patching the service 05/09/23 13:32:43.372
    STEP: watching for the Service to be patched 05/09/23 13:32:43.382
    May  9 13:32:43.384: INFO: observed Service test-service-wgwpr in namespace services-8296 with labels: map[test-service-static:true]
    May  9 13:32:43.384: INFO: observed Service test-service-wgwpr in namespace services-8296 with labels: map[test-service-static:true]
    May  9 13:32:43.384: INFO: observed Service test-service-wgwpr in namespace services-8296 with labels: map[test-service-static:true]
    May  9 13:32:43.384: INFO: Found Service test-service-wgwpr in namespace services-8296 with labels: map[test-service:patched test-service-static:true]
    May  9 13:32:43.384: INFO: Service test-service-wgwpr patched
    STEP: deleting the service 05/09/23 13:32:43.384
    STEP: watching for the Service to be deleted 05/09/23 13:32:43.406
    May  9 13:32:43.406: INFO: Observed event: ADDED
    May  9 13:32:43.407: INFO: Observed event: MODIFIED
    May  9 13:32:43.407: INFO: Observed event: MODIFIED
    May  9 13:32:43.407: INFO: Observed event: MODIFIED
    May  9 13:32:43.407: INFO: Found Service test-service-wgwpr in namespace services-8296 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    May  9 13:32:43.407: INFO: Service test-service-wgwpr deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:32:43.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8296" for this suite. 05/09/23 13:32:43.41
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:32:43.415
May  9 13:32:43.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename watch 05/09/23 13:32:43.416
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:43.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:43.43
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 05/09/23 13:32:43.432
STEP: creating a watch on configmaps with label B 05/09/23 13:32:43.432
STEP: creating a watch on configmaps with label A or B 05/09/23 13:32:43.433
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/09/23 13:32:43.434
May  9 13:32:43.438: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40491 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 13:32:43.438: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40491 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/09/23 13:32:43.438
May  9 13:32:43.443: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40492 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 13:32:43.443: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40492 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/09/23 13:32:43.443
May  9 13:32:43.448: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40493 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 13:32:43.448: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40493 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/09/23 13:32:43.448
May  9 13:32:43.452: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40494 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 13:32:43.452: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40494 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/09/23 13:32:43.452
May  9 13:32:43.455: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7094  056df9d3-78d6-47f4-af16-32bd8eaa2651 40495 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 13:32:43.455: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7094  056df9d3-78d6-47f4-af16-32bd8eaa2651 40495 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/09/23 13:32:53.455
May  9 13:32:53.461: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7094  056df9d3-78d6-47f4-af16-32bd8eaa2651 40580 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 13:32:53.461: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7094  056df9d3-78d6-47f4-af16-32bd8eaa2651 40580 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  9 13:33:03.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7094" for this suite. 05/09/23 13:33:03.465
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":264,"skipped":5209,"failed":0}
------------------------------
• [SLOW TEST] [20.054 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:32:43.415
    May  9 13:32:43.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename watch 05/09/23 13:32:43.416
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:32:43.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:32:43.43
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 05/09/23 13:32:43.432
    STEP: creating a watch on configmaps with label B 05/09/23 13:32:43.432
    STEP: creating a watch on configmaps with label A or B 05/09/23 13:32:43.433
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/09/23 13:32:43.434
    May  9 13:32:43.438: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40491 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 13:32:43.438: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40491 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/09/23 13:32:43.438
    May  9 13:32:43.443: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40492 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 13:32:43.443: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40492 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/09/23 13:32:43.443
    May  9 13:32:43.448: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40493 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 13:32:43.448: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40493 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/09/23 13:32:43.448
    May  9 13:32:43.452: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40494 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 13:32:43.452: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7094  bb1af27a-f76b-4d15-a47c-f9dd693325cd 40494 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/09/23 13:32:43.452
    May  9 13:32:43.455: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7094  056df9d3-78d6-47f4-af16-32bd8eaa2651 40495 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 13:32:43.455: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7094  056df9d3-78d6-47f4-af16-32bd8eaa2651 40495 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/09/23 13:32:53.455
    May  9 13:32:53.461: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7094  056df9d3-78d6-47f4-af16-32bd8eaa2651 40580 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 13:32:53.461: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7094  056df9d3-78d6-47f4-af16-32bd8eaa2651 40580 0 2023-05-09 13:32:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 13:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  9 13:33:03.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7094" for this suite. 05/09/23 13:33:03.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:33:03.472
May  9 13:33:03.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:33:03.473
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:03.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:03.486
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:33:03.487
May  9 13:33:03.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058" in namespace "projected-7189" to be "Succeeded or Failed"
May  9 13:33:03.499: INFO: Pod "downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058": Phase="Pending", Reason="", readiness=false. Elapsed: 4.170199ms
May  9 13:33:05.502: INFO: Pod "downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007240777s
May  9 13:33:07.502: INFO: Pod "downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007116674s
STEP: Saw pod success 05/09/23 13:33:07.502
May  9 13:33:07.502: INFO: Pod "downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058" satisfied condition "Succeeded or Failed"
May  9 13:33:07.504: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058 container client-container: <nil>
STEP: delete the pod 05/09/23 13:33:07.508
May  9 13:33:07.519: INFO: Waiting for pod downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058 to disappear
May  9 13:33:07.521: INFO: Pod downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  9 13:33:07.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7189" for this suite. 05/09/23 13:33:07.525
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":265,"skipped":5249,"failed":0}
------------------------------
• [4.057 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:33:03.472
    May  9 13:33:03.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:33:03.473
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:03.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:03.486
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:33:03.487
    May  9 13:33:03.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058" in namespace "projected-7189" to be "Succeeded or Failed"
    May  9 13:33:03.499: INFO: Pod "downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058": Phase="Pending", Reason="", readiness=false. Elapsed: 4.170199ms
    May  9 13:33:05.502: INFO: Pod "downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007240777s
    May  9 13:33:07.502: INFO: Pod "downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007116674s
    STEP: Saw pod success 05/09/23 13:33:07.502
    May  9 13:33:07.502: INFO: Pod "downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058" satisfied condition "Succeeded or Failed"
    May  9 13:33:07.504: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058 container client-container: <nil>
    STEP: delete the pod 05/09/23 13:33:07.508
    May  9 13:33:07.519: INFO: Waiting for pod downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058 to disappear
    May  9 13:33:07.521: INFO: Pod downwardapi-volume-1125b520-00ab-4031-9257-e11c4e476058 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  9 13:33:07.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7189" for this suite. 05/09/23 13:33:07.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:33:07.53
May  9 13:33:07.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename init-container 05/09/23 13:33:07.53
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:07.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:07.544
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 05/09/23 13:33:07.546
May  9 13:33:07.546: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May  9 13:33:12.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9057" for this suite. 05/09/23 13:33:12.726
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":266,"skipped":5268,"failed":0}
------------------------------
• [SLOW TEST] [5.201 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:33:07.53
    May  9 13:33:07.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename init-container 05/09/23 13:33:07.53
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:07.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:07.544
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 05/09/23 13:33:07.546
    May  9 13:33:07.546: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May  9 13:33:12.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9057" for this suite. 05/09/23 13:33:12.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:33:12.731
May  9 13:33:12.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 13:33:12.732
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:12.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:12.746
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-0716c7fe-e14e-40ac-a7c7-ef476ec40fbd 05/09/23 13:33:12.747
STEP: Creating a pod to test consume configMaps 05/09/23 13:33:12.752
May  9 13:33:12.757: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206" in namespace "configmap-5485" to be "Succeeded or Failed"
May  9 13:33:12.759: INFO: Pod "pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206": Phase="Pending", Reason="", readiness=false. Elapsed: 1.819553ms
May  9 13:33:14.765: INFO: Pod "pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007777247s
May  9 13:33:16.764: INFO: Pod "pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006545764s
STEP: Saw pod success 05/09/23 13:33:16.764
May  9 13:33:16.764: INFO: Pod "pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206" satisfied condition "Succeeded or Failed"
May  9 13:33:16.766: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 13:33:16.77
May  9 13:33:16.779: INFO: Waiting for pod pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206 to disappear
May  9 13:33:16.781: INFO: Pod pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  9 13:33:16.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5485" for this suite. 05/09/23 13:33:16.784
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":267,"skipped":5294,"failed":0}
------------------------------
• [4.057 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:33:12.731
    May  9 13:33:12.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 13:33:12.732
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:12.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:12.746
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-0716c7fe-e14e-40ac-a7c7-ef476ec40fbd 05/09/23 13:33:12.747
    STEP: Creating a pod to test consume configMaps 05/09/23 13:33:12.752
    May  9 13:33:12.757: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206" in namespace "configmap-5485" to be "Succeeded or Failed"
    May  9 13:33:12.759: INFO: Pod "pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206": Phase="Pending", Reason="", readiness=false. Elapsed: 1.819553ms
    May  9 13:33:14.765: INFO: Pod "pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007777247s
    May  9 13:33:16.764: INFO: Pod "pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006545764s
    STEP: Saw pod success 05/09/23 13:33:16.764
    May  9 13:33:16.764: INFO: Pod "pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206" satisfied condition "Succeeded or Failed"
    May  9 13:33:16.766: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 13:33:16.77
    May  9 13:33:16.779: INFO: Waiting for pod pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206 to disappear
    May  9 13:33:16.781: INFO: Pod pod-configmaps-fa0502af-f7df-499d-8913-d97b8b31d206 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 13:33:16.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5485" for this suite. 05/09/23 13:33:16.784
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:33:16.788
May  9 13:33:16.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename statefulset 05/09/23 13:33:16.789
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:16.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:16.801
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3424 05/09/23 13:33:16.802
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-3424 05/09/23 13:33:16.81
May  9 13:33:16.819: INFO: Found 0 stateful pods, waiting for 1
May  9 13:33:26.822: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 05/09/23 13:33:26.827
STEP: Getting /status 05/09/23 13:33:26.832
May  9 13:33:26.835: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 05/09/23 13:33:26.835
May  9 13:33:26.852: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 05/09/23 13:33:26.852
May  9 13:33:26.853: INFO: Observed &StatefulSet event: ADDED
May  9 13:33:26.853: INFO: Found Statefulset ss in namespace statefulset-3424 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  9 13:33:26.853: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 05/09/23 13:33:26.853
May  9 13:33:26.853: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  9 13:33:26.858: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 05/09/23 13:33:26.858
May  9 13:33:26.859: INFO: Observed &StatefulSet event: ADDED
May  9 13:33:26.859: INFO: Observed Statefulset ss in namespace statefulset-3424 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  9 13:33:26.859: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  9 13:33:26.859: INFO: Deleting all statefulset in ns statefulset-3424
May  9 13:33:26.861: INFO: Scaling statefulset ss to 0
May  9 13:33:36.877: INFO: Waiting for statefulset status.replicas updated to 0
May  9 13:33:36.879: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  9 13:33:36.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3424" for this suite. 05/09/23 13:33:36.894
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":268,"skipped":5294,"failed":0}
------------------------------
• [SLOW TEST] [20.112 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:33:16.788
    May  9 13:33:16.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename statefulset 05/09/23 13:33:16.789
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:16.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:16.801
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3424 05/09/23 13:33:16.802
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-3424 05/09/23 13:33:16.81
    May  9 13:33:16.819: INFO: Found 0 stateful pods, waiting for 1
    May  9 13:33:26.822: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 05/09/23 13:33:26.827
    STEP: Getting /status 05/09/23 13:33:26.832
    May  9 13:33:26.835: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 05/09/23 13:33:26.835
    May  9 13:33:26.852: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 05/09/23 13:33:26.852
    May  9 13:33:26.853: INFO: Observed &StatefulSet event: ADDED
    May  9 13:33:26.853: INFO: Found Statefulset ss in namespace statefulset-3424 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  9 13:33:26.853: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 05/09/23 13:33:26.853
    May  9 13:33:26.853: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  9 13:33:26.858: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 05/09/23 13:33:26.858
    May  9 13:33:26.859: INFO: Observed &StatefulSet event: ADDED
    May  9 13:33:26.859: INFO: Observed Statefulset ss in namespace statefulset-3424 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  9 13:33:26.859: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  9 13:33:26.859: INFO: Deleting all statefulset in ns statefulset-3424
    May  9 13:33:26.861: INFO: Scaling statefulset ss to 0
    May  9 13:33:36.877: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 13:33:36.879: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  9 13:33:36.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3424" for this suite. 05/09/23 13:33:36.894
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:33:36.901
May  9 13:33:36.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:33:36.901
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:36.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:36.915
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:33:36.927
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:33:37.203
STEP: Deploying the webhook pod 05/09/23 13:33:37.21
STEP: Wait for the deployment to be ready 05/09/23 13:33:37.22
May  9 13:33:37.229: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 13:33:39.236
STEP: Verifying the service has paired with the endpoint 05/09/23 13:33:39.247
May  9 13:33:40.247: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 05/09/23 13:33:40.25
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/09/23 13:33:40.251
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/09/23 13:33:40.251
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/09/23 13:33:40.251
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/09/23 13:33:40.251
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/09/23 13:33:40.252
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/09/23 13:33:40.252
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:33:40.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7309" for this suite. 05/09/23 13:33:40.255
STEP: Destroying namespace "webhook-7309-markers" for this suite. 05/09/23 13:33:40.261
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":269,"skipped":5313,"failed":0}
------------------------------
• [3.401 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:33:36.901
    May  9 13:33:36.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:33:36.901
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:36.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:36.915
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:33:36.927
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:33:37.203
    STEP: Deploying the webhook pod 05/09/23 13:33:37.21
    STEP: Wait for the deployment to be ready 05/09/23 13:33:37.22
    May  9 13:33:37.229: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 13:33:39.236
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:33:39.247
    May  9 13:33:40.247: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 05/09/23 13:33:40.25
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/09/23 13:33:40.251
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/09/23 13:33:40.251
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/09/23 13:33:40.251
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/09/23 13:33:40.251
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/09/23 13:33:40.252
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/09/23 13:33:40.252
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:33:40.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7309" for this suite. 05/09/23 13:33:40.255
    STEP: Destroying namespace "webhook-7309-markers" for this suite. 05/09/23 13:33:40.261
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:33:40.304
May  9 13:33:40.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename hostport 05/09/23 13:33:40.305
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:40.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:40.321
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/09/23 13:33:40.325
May  9 13:33:40.331: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4031" to be "running and ready"
May  9 13:33:40.333: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.788916ms
May  9 13:33:40.333: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:33:42.336: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005039364s
May  9 13:33:42.336: INFO: The phase of Pod pod1 is Running (Ready = true)
May  9 13:33:42.336: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.1.73 on the node which pod1 resides and expect scheduled 05/09/23 13:33:42.336
May  9 13:33:42.341: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4031" to be "running and ready"
May  9 13:33:42.343: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.966082ms
May  9 13:33:42.343: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:33:44.347: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.006132749s
May  9 13:33:44.347: INFO: The phase of Pod pod2 is Running (Ready = false)
May  9 13:33:46.348: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.006368327s
May  9 13:33:46.348: INFO: The phase of Pod pod2 is Running (Ready = true)
May  9 13:33:46.348: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.1.73 but use UDP protocol on the node which pod2 resides 05/09/23 13:33:46.348
May  9 13:33:46.354: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4031" to be "running and ready"
May  9 13:33:46.356: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26369ms
May  9 13:33:46.356: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:33:48.359: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005742326s
May  9 13:33:48.359: INFO: The phase of Pod pod3 is Running (Ready = true)
May  9 13:33:48.359: INFO: Pod "pod3" satisfied condition "running and ready"
May  9 13:33:48.363: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4031" to be "running and ready"
May  9 13:33:48.365: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.807531ms
May  9 13:33:48.365: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May  9 13:33:50.368: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.00490313s
May  9 13:33:50.368: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
May  9 13:33:50.368: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/09/23 13:33:50.37
May  9 13:33:50.370: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.1.73 http://127.0.0.1:54323/hostname] Namespace:hostport-4031 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:33:50.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:33:50.371: INFO: ExecWithOptions: Clientset creation
May  9 13:33:50.371: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4031/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.1.73+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.1.73, port: 54323 05/09/23 13:33:50.425
May  9 13:33:50.425: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.1.73:54323/hostname] Namespace:hostport-4031 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:33:50.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:33:50.425: INFO: ExecWithOptions: Clientset creation
May  9 13:33:50.425: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4031/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.1.73%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.1.73, port: 54323 UDP 05/09/23 13:33:50.472
May  9 13:33:50.472: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.1.73 54323] Namespace:hostport-4031 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:33:50.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:33:50.473: INFO: ExecWithOptions: Clientset creation
May  9 13:33:50.473: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4031/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.1.73+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
May  9 13:33:55.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-4031" for this suite. 05/09/23 13:33:55.526
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":270,"skipped":5324,"failed":0}
------------------------------
• [SLOW TEST] [15.227 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:33:40.304
    May  9 13:33:40.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename hostport 05/09/23 13:33:40.305
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:40.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:40.321
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/09/23 13:33:40.325
    May  9 13:33:40.331: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4031" to be "running and ready"
    May  9 13:33:40.333: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.788916ms
    May  9 13:33:40.333: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:33:42.336: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005039364s
    May  9 13:33:42.336: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  9 13:33:42.336: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.1.73 on the node which pod1 resides and expect scheduled 05/09/23 13:33:42.336
    May  9 13:33:42.341: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4031" to be "running and ready"
    May  9 13:33:42.343: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.966082ms
    May  9 13:33:42.343: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:33:44.347: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.006132749s
    May  9 13:33:44.347: INFO: The phase of Pod pod2 is Running (Ready = false)
    May  9 13:33:46.348: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.006368327s
    May  9 13:33:46.348: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  9 13:33:46.348: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.1.73 but use UDP protocol on the node which pod2 resides 05/09/23 13:33:46.348
    May  9 13:33:46.354: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4031" to be "running and ready"
    May  9 13:33:46.356: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26369ms
    May  9 13:33:46.356: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:33:48.359: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005742326s
    May  9 13:33:48.359: INFO: The phase of Pod pod3 is Running (Ready = true)
    May  9 13:33:48.359: INFO: Pod "pod3" satisfied condition "running and ready"
    May  9 13:33:48.363: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4031" to be "running and ready"
    May  9 13:33:48.365: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.807531ms
    May  9 13:33:48.365: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:33:50.368: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.00490313s
    May  9 13:33:50.368: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    May  9 13:33:50.368: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/09/23 13:33:50.37
    May  9 13:33:50.370: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.1.73 http://127.0.0.1:54323/hostname] Namespace:hostport-4031 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:33:50.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:33:50.371: INFO: ExecWithOptions: Clientset creation
    May  9 13:33:50.371: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4031/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.1.73+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.1.73, port: 54323 05/09/23 13:33:50.425
    May  9 13:33:50.425: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.1.73:54323/hostname] Namespace:hostport-4031 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:33:50.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:33:50.425: INFO: ExecWithOptions: Clientset creation
    May  9 13:33:50.425: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4031/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.1.73%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.1.73, port: 54323 UDP 05/09/23 13:33:50.472
    May  9 13:33:50.472: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.1.73 54323] Namespace:hostport-4031 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:33:50.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:33:50.473: INFO: ExecWithOptions: Clientset creation
    May  9 13:33:50.473: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4031/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.1.73+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    May  9 13:33:55.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-4031" for this suite. 05/09/23 13:33:55.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:33:55.532
May  9 13:33:55.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir-wrapper 05/09/23 13:33:55.532
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:55.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:55.545
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 05/09/23 13:33:55.546
STEP: Creating RC which spawns configmap-volume pods 05/09/23 13:33:55.786
May  9 13:33:55.894: INFO: Pod name wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/09/23 13:33:55.894
May  9 13:33:55.894: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:33:55.935: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 41.69032ms
May  9 13:33:57.940: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046304548s
May  9 13:33:59.940: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046518698s
May  9 13:34:01.940: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046403703s
May  9 13:34:03.939: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.045128318s
May  9 13:34:05.940: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.046097798s
May  9 13:34:07.939: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.045288581s
May  9 13:34:09.940: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Running", Reason="", readiness=true. Elapsed: 14.04596383s
May  9 13:34:09.940: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd" satisfied condition "running"
May  9 13:34:09.940: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-g8lbx" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:09.942: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-g8lbx": Phase="Running", Reason="", readiness=true. Elapsed: 2.396302ms
May  9 13:34:09.942: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-g8lbx" satisfied condition "running"
May  9 13:34:09.942: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-xb4hn" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:09.945: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-xb4hn": Phase="Running", Reason="", readiness=true. Elapsed: 2.313735ms
May  9 13:34:09.945: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-xb4hn" satisfied condition "running"
May  9 13:34:09.945: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-zdmgn" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:09.947: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-zdmgn": Phase="Running", Reason="", readiness=true. Elapsed: 2.142298ms
May  9 13:34:09.947: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-zdmgn" satisfied condition "running"
May  9 13:34:09.947: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-zzjl8" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:09.949: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-zzjl8": Phase="Running", Reason="", readiness=true. Elapsed: 2.064419ms
May  9 13:34:09.949: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-zzjl8" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb in namespace emptydir-wrapper-591, will wait for the garbage collector to delete the pods 05/09/23 13:34:09.949
May  9 13:34:10.007: INFO: Deleting ReplicationController wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb took: 5.111681ms
May  9 13:34:10.107: INFO: Terminating ReplicationController wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb pods took: 100.429983ms
STEP: Creating RC which spawns configmap-volume pods 05/09/23 13:34:13.912
May  9 13:34:13.922: INFO: Pod name wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29: Found 0 pods out of 5
May  9 13:34:18.927: INFO: Pod name wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/09/23 13:34:18.927
May  9 13:34:18.927: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:18.929: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Pending", Reason="", readiness=false. Elapsed: 1.906029ms
May  9 13:34:20.933: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005721573s
May  9 13:34:22.933: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005779083s
May  9 13:34:24.934: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007046475s
May  9 13:34:26.933: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006142874s
May  9 13:34:28.934: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00693349s
May  9 13:34:30.933: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Running", Reason="", readiness=true. Elapsed: 12.006050228s
May  9 13:34:30.933: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77" satisfied condition "running"
May  9 13:34:30.933: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-59clf" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:30.936: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-59clf": Phase="Running", Reason="", readiness=true. Elapsed: 2.35341ms
May  9 13:34:30.936: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-59clf" satisfied condition "running"
May  9 13:34:30.936: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-hcgcw" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:30.938: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-hcgcw": Phase="Running", Reason="", readiness=true. Elapsed: 1.954491ms
May  9 13:34:30.938: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-hcgcw" satisfied condition "running"
May  9 13:34:30.938: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-nwpxm" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:30.939: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-nwpxm": Phase="Running", Reason="", readiness=true. Elapsed: 1.834031ms
May  9 13:34:30.939: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-nwpxm" satisfied condition "running"
May  9 13:34:30.939: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-t2t2f" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:30.941: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-t2t2f": Phase="Running", Reason="", readiness=true. Elapsed: 1.803194ms
May  9 13:34:30.941: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-t2t2f" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29 in namespace emptydir-wrapper-591, will wait for the garbage collector to delete the pods 05/09/23 13:34:30.941
May  9 13:34:30.999: INFO: Deleting ReplicationController wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29 took: 5.555027ms
May  9 13:34:31.100: INFO: Terminating ReplicationController wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29 pods took: 101.01503ms
STEP: Creating RC which spawns configmap-volume pods 05/09/23 13:34:33.904
May  9 13:34:33.915: INFO: Pod name wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5: Found 0 pods out of 5
May  9 13:34:38.920: INFO: Pod name wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/09/23 13:34:38.92
May  9 13:34:38.920: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:38.922: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.250324ms
May  9 13:34:40.926: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006543807s
May  9 13:34:42.926: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006052569s
May  9 13:34:44.927: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006991966s
May  9 13:34:46.926: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006153476s
May  9 13:34:48.925: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7": Phase="Running", Reason="", readiness=true. Elapsed: 10.00545665s
May  9 13:34:48.925: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7" satisfied condition "running"
May  9 13:34:48.925: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-6wbj6" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:48.928: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-6wbj6": Phase="Running", Reason="", readiness=true. Elapsed: 2.801173ms
May  9 13:34:48.928: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-6wbj6" satisfied condition "running"
May  9 13:34:48.928: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-bm5ks" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:48.932: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-bm5ks": Phase="Running", Reason="", readiness=true. Elapsed: 3.623219ms
May  9 13:34:48.932: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-bm5ks" satisfied condition "running"
May  9 13:34:48.932: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-mht8w" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:48.934: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-mht8w": Phase="Running", Reason="", readiness=true. Elapsed: 2.010969ms
May  9 13:34:48.934: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-mht8w" satisfied condition "running"
May  9 13:34:48.934: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-rzfzs" in namespace "emptydir-wrapper-591" to be "running"
May  9 13:34:48.936: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-rzfzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.008343ms
May  9 13:34:48.936: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-rzfzs" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5 in namespace emptydir-wrapper-591, will wait for the garbage collector to delete the pods 05/09/23 13:34:48.936
May  9 13:34:48.996: INFO: Deleting ReplicationController wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5 took: 7.292033ms
May  9 13:34:49.096: INFO: Terminating ReplicationController wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5 pods took: 100.302765ms
STEP: Cleaning up the configMaps 05/09/23 13:34:52.097
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
May  9 13:34:52.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-591" for this suite. 05/09/23 13:34:52.321
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":271,"skipped":5332,"failed":0}
------------------------------
• [SLOW TEST] [56.794 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:33:55.532
    May  9 13:33:55.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir-wrapper 05/09/23 13:33:55.532
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:33:55.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:33:55.545
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 05/09/23 13:33:55.546
    STEP: Creating RC which spawns configmap-volume pods 05/09/23 13:33:55.786
    May  9 13:33:55.894: INFO: Pod name wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/09/23 13:33:55.894
    May  9 13:33:55.894: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:33:55.935: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 41.69032ms
    May  9 13:33:57.940: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046304548s
    May  9 13:33:59.940: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046518698s
    May  9 13:34:01.940: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046403703s
    May  9 13:34:03.939: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.045128318s
    May  9 13:34:05.940: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.046097798s
    May  9 13:34:07.939: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.045288581s
    May  9 13:34:09.940: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd": Phase="Running", Reason="", readiness=true. Elapsed: 14.04596383s
    May  9 13:34:09.940: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-7jbwd" satisfied condition "running"
    May  9 13:34:09.940: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-g8lbx" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:09.942: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-g8lbx": Phase="Running", Reason="", readiness=true. Elapsed: 2.396302ms
    May  9 13:34:09.942: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-g8lbx" satisfied condition "running"
    May  9 13:34:09.942: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-xb4hn" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:09.945: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-xb4hn": Phase="Running", Reason="", readiness=true. Elapsed: 2.313735ms
    May  9 13:34:09.945: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-xb4hn" satisfied condition "running"
    May  9 13:34:09.945: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-zdmgn" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:09.947: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-zdmgn": Phase="Running", Reason="", readiness=true. Elapsed: 2.142298ms
    May  9 13:34:09.947: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-zdmgn" satisfied condition "running"
    May  9 13:34:09.947: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-zzjl8" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:09.949: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-zzjl8": Phase="Running", Reason="", readiness=true. Elapsed: 2.064419ms
    May  9 13:34:09.949: INFO: Pod "wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb-zzjl8" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb in namespace emptydir-wrapper-591, will wait for the garbage collector to delete the pods 05/09/23 13:34:09.949
    May  9 13:34:10.007: INFO: Deleting ReplicationController wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb took: 5.111681ms
    May  9 13:34:10.107: INFO: Terminating ReplicationController wrapped-volume-race-12fc5249-985e-4e31-b2e4-73a2d11946bb pods took: 100.429983ms
    STEP: Creating RC which spawns configmap-volume pods 05/09/23 13:34:13.912
    May  9 13:34:13.922: INFO: Pod name wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29: Found 0 pods out of 5
    May  9 13:34:18.927: INFO: Pod name wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/09/23 13:34:18.927
    May  9 13:34:18.927: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:18.929: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Pending", Reason="", readiness=false. Elapsed: 1.906029ms
    May  9 13:34:20.933: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005721573s
    May  9 13:34:22.933: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005779083s
    May  9 13:34:24.934: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007046475s
    May  9 13:34:26.933: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006142874s
    May  9 13:34:28.934: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00693349s
    May  9 13:34:30.933: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77": Phase="Running", Reason="", readiness=true. Elapsed: 12.006050228s
    May  9 13:34:30.933: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-56n77" satisfied condition "running"
    May  9 13:34:30.933: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-59clf" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:30.936: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-59clf": Phase="Running", Reason="", readiness=true. Elapsed: 2.35341ms
    May  9 13:34:30.936: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-59clf" satisfied condition "running"
    May  9 13:34:30.936: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-hcgcw" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:30.938: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-hcgcw": Phase="Running", Reason="", readiness=true. Elapsed: 1.954491ms
    May  9 13:34:30.938: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-hcgcw" satisfied condition "running"
    May  9 13:34:30.938: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-nwpxm" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:30.939: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-nwpxm": Phase="Running", Reason="", readiness=true. Elapsed: 1.834031ms
    May  9 13:34:30.939: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-nwpxm" satisfied condition "running"
    May  9 13:34:30.939: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-t2t2f" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:30.941: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-t2t2f": Phase="Running", Reason="", readiness=true. Elapsed: 1.803194ms
    May  9 13:34:30.941: INFO: Pod "wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29-t2t2f" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29 in namespace emptydir-wrapper-591, will wait for the garbage collector to delete the pods 05/09/23 13:34:30.941
    May  9 13:34:30.999: INFO: Deleting ReplicationController wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29 took: 5.555027ms
    May  9 13:34:31.100: INFO: Terminating ReplicationController wrapped-volume-race-1b35484e-2907-44df-abfb-8e685a633e29 pods took: 101.01503ms
    STEP: Creating RC which spawns configmap-volume pods 05/09/23 13:34:33.904
    May  9 13:34:33.915: INFO: Pod name wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5: Found 0 pods out of 5
    May  9 13:34:38.920: INFO: Pod name wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/09/23 13:34:38.92
    May  9 13:34:38.920: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:38.922: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.250324ms
    May  9 13:34:40.926: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006543807s
    May  9 13:34:42.926: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006052569s
    May  9 13:34:44.927: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006991966s
    May  9 13:34:46.926: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006153476s
    May  9 13:34:48.925: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7": Phase="Running", Reason="", readiness=true. Elapsed: 10.00545665s
    May  9 13:34:48.925: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-2qms7" satisfied condition "running"
    May  9 13:34:48.925: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-6wbj6" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:48.928: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-6wbj6": Phase="Running", Reason="", readiness=true. Elapsed: 2.801173ms
    May  9 13:34:48.928: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-6wbj6" satisfied condition "running"
    May  9 13:34:48.928: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-bm5ks" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:48.932: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-bm5ks": Phase="Running", Reason="", readiness=true. Elapsed: 3.623219ms
    May  9 13:34:48.932: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-bm5ks" satisfied condition "running"
    May  9 13:34:48.932: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-mht8w" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:48.934: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-mht8w": Phase="Running", Reason="", readiness=true. Elapsed: 2.010969ms
    May  9 13:34:48.934: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-mht8w" satisfied condition "running"
    May  9 13:34:48.934: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-rzfzs" in namespace "emptydir-wrapper-591" to be "running"
    May  9 13:34:48.936: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-rzfzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.008343ms
    May  9 13:34:48.936: INFO: Pod "wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5-rzfzs" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5 in namespace emptydir-wrapper-591, will wait for the garbage collector to delete the pods 05/09/23 13:34:48.936
    May  9 13:34:48.996: INFO: Deleting ReplicationController wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5 took: 7.292033ms
    May  9 13:34:49.096: INFO: Terminating ReplicationController wrapped-volume-race-efe4333f-5c5a-4738-877d-2b3047570ff5 pods took: 100.302765ms
    STEP: Cleaning up the configMaps 05/09/23 13:34:52.097
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    May  9 13:34:52.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-591" for this suite. 05/09/23 13:34:52.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:34:52.327
May  9 13:34:52.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:34:52.328
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:34:52.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:34:52.344
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-20b09119-31bf-4fb2-90b7-53630637395e 05/09/23 13:34:52.346
STEP: Creating a pod to test consume configMaps 05/09/23 13:34:52.35
May  9 13:34:52.358: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698" in namespace "projected-4768" to be "Succeeded or Failed"
May  9 13:34:52.363: INFO: Pod "pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698": Phase="Pending", Reason="", readiness=false. Elapsed: 5.306975ms
May  9 13:34:54.367: INFO: Pod "pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009096236s
May  9 13:34:56.368: INFO: Pod "pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009943294s
STEP: Saw pod success 05/09/23 13:34:56.368
May  9 13:34:56.368: INFO: Pod "pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698" satisfied condition "Succeeded or Failed"
May  9 13:34:56.370: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 13:34:56.381
May  9 13:34:56.390: INFO: Waiting for pod pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698 to disappear
May  9 13:34:56.392: INFO: Pod pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  9 13:34:56.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4768" for this suite. 05/09/23 13:34:56.395
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":272,"skipped":5365,"failed":0}
------------------------------
• [4.072 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:34:52.327
    May  9 13:34:52.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:34:52.328
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:34:52.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:34:52.344
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-20b09119-31bf-4fb2-90b7-53630637395e 05/09/23 13:34:52.346
    STEP: Creating a pod to test consume configMaps 05/09/23 13:34:52.35
    May  9 13:34:52.358: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698" in namespace "projected-4768" to be "Succeeded or Failed"
    May  9 13:34:52.363: INFO: Pod "pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698": Phase="Pending", Reason="", readiness=false. Elapsed: 5.306975ms
    May  9 13:34:54.367: INFO: Pod "pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009096236s
    May  9 13:34:56.368: INFO: Pod "pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009943294s
    STEP: Saw pod success 05/09/23 13:34:56.368
    May  9 13:34:56.368: INFO: Pod "pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698" satisfied condition "Succeeded or Failed"
    May  9 13:34:56.370: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 13:34:56.381
    May  9 13:34:56.390: INFO: Waiting for pod pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698 to disappear
    May  9 13:34:56.392: INFO: Pod pod-projected-configmaps-efd329ee-b89c-44a1-b038-dfbb18a20698 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  9 13:34:56.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4768" for this suite. 05/09/23 13:34:56.395
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:34:56.4
May  9 13:34:56.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubelet-test 05/09/23 13:34:56.4
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:34:56.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:34:56.413
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 05/09/23 13:34:56.419
May  9 13:34:56.419: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases85941520-cee4-4b85-a433-020e2a5f69d7" in namespace "kubelet-test-7319" to be "completed"
May  9 13:34:56.423: INFO: Pod "agnhost-host-aliases85941520-cee4-4b85-a433-020e2a5f69d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.994545ms
May  9 13:34:58.426: INFO: Pod "agnhost-host-aliases85941520-cee4-4b85-a433-020e2a5f69d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006900103s
May  9 13:35:00.427: INFO: Pod "agnhost-host-aliases85941520-cee4-4b85-a433-020e2a5f69d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007627735s
May  9 13:35:00.427: INFO: Pod "agnhost-host-aliases85941520-cee4-4b85-a433-020e2a5f69d7" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  9 13:35:00.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7319" for this suite. 05/09/23 13:35:00.436
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":273,"skipped":5367,"failed":0}
------------------------------
• [4.042 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:34:56.4
    May  9 13:34:56.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubelet-test 05/09/23 13:34:56.4
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:34:56.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:34:56.413
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 05/09/23 13:34:56.419
    May  9 13:34:56.419: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases85941520-cee4-4b85-a433-020e2a5f69d7" in namespace "kubelet-test-7319" to be "completed"
    May  9 13:34:56.423: INFO: Pod "agnhost-host-aliases85941520-cee4-4b85-a433-020e2a5f69d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.994545ms
    May  9 13:34:58.426: INFO: Pod "agnhost-host-aliases85941520-cee4-4b85-a433-020e2a5f69d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006900103s
    May  9 13:35:00.427: INFO: Pod "agnhost-host-aliases85941520-cee4-4b85-a433-020e2a5f69d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007627735s
    May  9 13:35:00.427: INFO: Pod "agnhost-host-aliases85941520-cee4-4b85-a433-020e2a5f69d7" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  9 13:35:00.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7319" for this suite. 05/09/23 13:35:00.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:35:00.442
May  9 13:35:00.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 13:35:00.443
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:35:00.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:35:00.457
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 05/09/23 13:35:00.459
May  9 13:35:00.465: INFO: Waiting up to 5m0s for pod "pod-70745c86-b70e-402d-bfe7-c921af2c4161" in namespace "emptydir-7680" to be "Succeeded or Failed"
May  9 13:35:00.469: INFO: Pod "pod-70745c86-b70e-402d-bfe7-c921af2c4161": Phase="Pending", Reason="", readiness=false. Elapsed: 3.934221ms
May  9 13:35:02.472: INFO: Pod "pod-70745c86-b70e-402d-bfe7-c921af2c4161": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007049618s
May  9 13:35:04.472: INFO: Pod "pod-70745c86-b70e-402d-bfe7-c921af2c4161": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006854853s
STEP: Saw pod success 05/09/23 13:35:04.472
May  9 13:35:04.472: INFO: Pod "pod-70745c86-b70e-402d-bfe7-c921af2c4161" satisfied condition "Succeeded or Failed"
May  9 13:35:04.474: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-70745c86-b70e-402d-bfe7-c921af2c4161 container test-container: <nil>
STEP: delete the pod 05/09/23 13:35:04.482
May  9 13:35:04.497: INFO: Waiting for pod pod-70745c86-b70e-402d-bfe7-c921af2c4161 to disappear
May  9 13:35:04.499: INFO: Pod pod-70745c86-b70e-402d-bfe7-c921af2c4161 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 13:35:04.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7680" for this suite. 05/09/23 13:35:04.502
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":274,"skipped":5379,"failed":0}
------------------------------
• [4.064 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:35:00.442
    May  9 13:35:00.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 13:35:00.443
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:35:00.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:35:00.457
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/09/23 13:35:00.459
    May  9 13:35:00.465: INFO: Waiting up to 5m0s for pod "pod-70745c86-b70e-402d-bfe7-c921af2c4161" in namespace "emptydir-7680" to be "Succeeded or Failed"
    May  9 13:35:00.469: INFO: Pod "pod-70745c86-b70e-402d-bfe7-c921af2c4161": Phase="Pending", Reason="", readiness=false. Elapsed: 3.934221ms
    May  9 13:35:02.472: INFO: Pod "pod-70745c86-b70e-402d-bfe7-c921af2c4161": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007049618s
    May  9 13:35:04.472: INFO: Pod "pod-70745c86-b70e-402d-bfe7-c921af2c4161": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006854853s
    STEP: Saw pod success 05/09/23 13:35:04.472
    May  9 13:35:04.472: INFO: Pod "pod-70745c86-b70e-402d-bfe7-c921af2c4161" satisfied condition "Succeeded or Failed"
    May  9 13:35:04.474: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-70745c86-b70e-402d-bfe7-c921af2c4161 container test-container: <nil>
    STEP: delete the pod 05/09/23 13:35:04.482
    May  9 13:35:04.497: INFO: Waiting for pod pod-70745c86-b70e-402d-bfe7-c921af2c4161 to disappear
    May  9 13:35:04.499: INFO: Pod pod-70745c86-b70e-402d-bfe7-c921af2c4161 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 13:35:04.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7680" for this suite. 05/09/23 13:35:04.502
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:35:04.507
May  9 13:35:04.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename job 05/09/23 13:35:04.507
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:35:04.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:35:04.522
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 05/09/23 13:35:04.524
STEP: Ensuring job reaches completions 05/09/23 13:35:04.531
STEP: Ensuring pods with index for job exist 05/09/23 13:35:12.535
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  9 13:35:12.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6690" for this suite. 05/09/23 13:35:12.541
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":275,"skipped":5383,"failed":0}
------------------------------
• [SLOW TEST] [8.041 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:35:04.507
    May  9 13:35:04.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename job 05/09/23 13:35:04.507
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:35:04.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:35:04.522
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 05/09/23 13:35:04.524
    STEP: Ensuring job reaches completions 05/09/23 13:35:04.531
    STEP: Ensuring pods with index for job exist 05/09/23 13:35:12.535
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  9 13:35:12.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6690" for this suite. 05/09/23 13:35:12.541
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:35:12.548
May  9 13:35:12.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 13:35:12.549
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:35:12.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:35:12.565
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 05/09/23 13:35:12.566
May  9 13:35:12.574: INFO: Waiting up to 5m0s for pod "downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867" in namespace "downward-api-9004" to be "Succeeded or Failed"
May  9 13:35:12.580: INFO: Pod "downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044278ms
May  9 13:35:14.583: INFO: Pod "downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009108474s
May  9 13:35:16.583: INFO: Pod "downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00910466s
STEP: Saw pod success 05/09/23 13:35:16.583
May  9 13:35:16.583: INFO: Pod "downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867" satisfied condition "Succeeded or Failed"
May  9 13:35:16.585: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867 container dapi-container: <nil>
STEP: delete the pod 05/09/23 13:35:16.589
May  9 13:35:16.601: INFO: Waiting for pod downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867 to disappear
May  9 13:35:16.603: INFO: Pod downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  9 13:35:16.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9004" for this suite. 05/09/23 13:35:16.606
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":276,"skipped":5386,"failed":0}
------------------------------
• [4.062 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:35:12.548
    May  9 13:35:12.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 13:35:12.549
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:35:12.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:35:12.565
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 05/09/23 13:35:12.566
    May  9 13:35:12.574: INFO: Waiting up to 5m0s for pod "downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867" in namespace "downward-api-9004" to be "Succeeded or Failed"
    May  9 13:35:12.580: INFO: Pod "downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044278ms
    May  9 13:35:14.583: INFO: Pod "downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009108474s
    May  9 13:35:16.583: INFO: Pod "downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00910466s
    STEP: Saw pod success 05/09/23 13:35:16.583
    May  9 13:35:16.583: INFO: Pod "downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867" satisfied condition "Succeeded or Failed"
    May  9 13:35:16.585: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867 container dapi-container: <nil>
    STEP: delete the pod 05/09/23 13:35:16.589
    May  9 13:35:16.601: INFO: Waiting for pod downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867 to disappear
    May  9 13:35:16.603: INFO: Pod downward-api-fb382e6c-10b0-4437-8a10-d1768bc51867 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  9 13:35:16.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9004" for this suite. 05/09/23 13:35:16.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:35:16.612
May  9 13:35:16.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename deployment 05/09/23 13:35:16.612
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:35:16.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:35:16.626
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
May  9 13:35:16.628: INFO: Creating deployment "test-recreate-deployment"
May  9 13:35:16.632: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May  9 13:35:16.647: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May  9 13:35:18.653: INFO: Waiting deployment "test-recreate-deployment" to complete
May  9 13:35:18.655: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May  9 13:35:18.661: INFO: Updating deployment test-recreate-deployment
May  9 13:35:18.661: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 13:35:18.765: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7955  f7ab34fa-67f3-4813-9c0d-2f38c6a80bc0 42634 2 2023-05-09 13:35:16 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00330ed58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-09 13:35:18 +0000 UTC,LastTransitionTime:2023-05-09 13:35:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-05-09 13:35:18 +0000 UTC,LastTransitionTime:2023-05-09 13:35:16 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May  9 13:35:18.767: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-7955  7d13af28-4708-49d7-95cb-42bfa0eebaa9 42632 1 2023-05-09 13:35:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f7ab34fa-67f3-4813-9c0d-2f38c6a80bc0 0xc00330f220 0xc00330f221}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7ab34fa-67f3-4813-9c0d-2f38c6a80bc0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00330f2b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 13:35:18.767: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May  9 13:35:18.767: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-7955  07e3ddc5-d3f9-4e85-8800-b4a5e3f0eeca 42621 2 2023-05-09 13:35:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f7ab34fa-67f3-4813-9c0d-2f38c6a80bc0 0xc00330f107 0xc00330f108}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7ab34fa-67f3-4813-9c0d-2f38c6a80bc0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00330f1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 13:35:18.769: INFO: Pod "test-recreate-deployment-9d58999df-l582f" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-l582f test-recreate-deployment-9d58999df- deployment-7955  06d803e2-0770-4fc2-9804-83b44b9443ee 42633 0 2023-05-09 13:35:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 7d13af28-4708-49d7-95cb-42bfa0eebaa9 0xc00330f710 0xc00330f711}] [] [{kube-controller-manager Update v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d13af28-4708-49d7-95cb-42bfa0eebaa9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fxhbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fxhbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:35:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:35:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:35:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:35:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:,StartTime:2023-05-09 13:35:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  9 13:35:18.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7955" for this suite. 05/09/23 13:35:18.772
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":277,"skipped":5402,"failed":0}
------------------------------
• [2.165 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:35:16.612
    May  9 13:35:16.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename deployment 05/09/23 13:35:16.612
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:35:16.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:35:16.626
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    May  9 13:35:16.628: INFO: Creating deployment "test-recreate-deployment"
    May  9 13:35:16.632: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    May  9 13:35:16.647: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    May  9 13:35:18.653: INFO: Waiting deployment "test-recreate-deployment" to complete
    May  9 13:35:18.655: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    May  9 13:35:18.661: INFO: Updating deployment test-recreate-deployment
    May  9 13:35:18.661: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 13:35:18.765: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-7955  f7ab34fa-67f3-4813-9c0d-2f38c6a80bc0 42634 2 2023-05-09 13:35:16 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00330ed58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-09 13:35:18 +0000 UTC,LastTransitionTime:2023-05-09 13:35:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-05-09 13:35:18 +0000 UTC,LastTransitionTime:2023-05-09 13:35:16 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    May  9 13:35:18.767: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-7955  7d13af28-4708-49d7-95cb-42bfa0eebaa9 42632 1 2023-05-09 13:35:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f7ab34fa-67f3-4813-9c0d-2f38c6a80bc0 0xc00330f220 0xc00330f221}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7ab34fa-67f3-4813-9c0d-2f38c6a80bc0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00330f2b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 13:35:18.767: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    May  9 13:35:18.767: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-7955  07e3ddc5-d3f9-4e85-8800-b4a5e3f0eeca 42621 2 2023-05-09 13:35:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f7ab34fa-67f3-4813-9c0d-2f38c6a80bc0 0xc00330f107 0xc00330f108}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7ab34fa-67f3-4813-9c0d-2f38c6a80bc0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00330f1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 13:35:18.769: INFO: Pod "test-recreate-deployment-9d58999df-l582f" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-l582f test-recreate-deployment-9d58999df- deployment-7955  06d803e2-0770-4fc2-9804-83b44b9443ee 42633 0 2023-05-09 13:35:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 7d13af28-4708-49d7-95cb-42bfa0eebaa9 0xc00330f710 0xc00330f711}] [] [{kube-controller-manager Update v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d13af28-4708-49d7-95cb-42bfa0eebaa9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:35:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fxhbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fxhbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:35:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:35:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:35:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:35:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:,StartTime:2023-05-09 13:35:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  9 13:35:18.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7955" for this suite. 05/09/23 13:35:18.772
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:35:18.777
May  9 13:35:18.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename taint-multiple-pods 05/09/23 13:35:18.777
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:35:18.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:35:18.791
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
May  9 13:35:18.792: INFO: Waiting up to 1m0s for all nodes to be ready
May  9 13:36:18.820: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
May  9 13:36:18.823: INFO: Starting informer...
STEP: Starting pods... 05/09/23 13:36:18.823
May  9 13:36:19.036: INFO: Pod1 is running on cl-gks-cncf-ix1-md-0-48ljh. Tainting Node
May  9 13:36:19.242: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-4047" to be "running"
May  9 13:36:19.245: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.162408ms
May  9 13:36:21.247: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004750253s
May  9 13:36:21.247: INFO: Pod "taint-eviction-b1" satisfied condition "running"
May  9 13:36:21.247: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-4047" to be "running"
May  9 13:36:21.249: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.100309ms
May  9 13:36:21.249: INFO: Pod "taint-eviction-b2" satisfied condition "running"
May  9 13:36:21.249: INFO: Pod2 is running on cl-gks-cncf-ix1-md-0-48ljh. Tainting Node
STEP: Trying to apply a taint on the Node 05/09/23 13:36:21.249
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 13:36:21.26
STEP: Waiting for Pod1 and Pod2 to be deleted 05/09/23 13:36:21.264
May  9 13:36:27.105: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May  9 13:36:47.137: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 13:36:47.146
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
May  9 13:36:47.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4047" for this suite. 05/09/23 13:36:47.152
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":278,"skipped":5402,"failed":0}
------------------------------
• [SLOW TEST] [88.382 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:35:18.777
    May  9 13:35:18.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename taint-multiple-pods 05/09/23 13:35:18.777
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:35:18.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:35:18.791
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    May  9 13:35:18.792: INFO: Waiting up to 1m0s for all nodes to be ready
    May  9 13:36:18.820: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    May  9 13:36:18.823: INFO: Starting informer...
    STEP: Starting pods... 05/09/23 13:36:18.823
    May  9 13:36:19.036: INFO: Pod1 is running on cl-gks-cncf-ix1-md-0-48ljh. Tainting Node
    May  9 13:36:19.242: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-4047" to be "running"
    May  9 13:36:19.245: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.162408ms
    May  9 13:36:21.247: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004750253s
    May  9 13:36:21.247: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    May  9 13:36:21.247: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-4047" to be "running"
    May  9 13:36:21.249: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.100309ms
    May  9 13:36:21.249: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    May  9 13:36:21.249: INFO: Pod2 is running on cl-gks-cncf-ix1-md-0-48ljh. Tainting Node
    STEP: Trying to apply a taint on the Node 05/09/23 13:36:21.249
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 13:36:21.26
    STEP: Waiting for Pod1 and Pod2 to be deleted 05/09/23 13:36:21.264
    May  9 13:36:27.105: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    May  9 13:36:47.137: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 13:36:47.146
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:36:47.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-4047" for this suite. 05/09/23 13:36:47.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:36:47.16
May  9 13:36:47.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename var-expansion 05/09/23 13:36:47.161
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:36:47.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:36:47.18
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 05/09/23 13:36:47.181
May  9 13:36:47.187: INFO: Waiting up to 5m0s for pod "var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0" in namespace "var-expansion-6290" to be "Succeeded or Failed"
May  9 13:36:47.197: INFO: Pod "var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.366194ms
May  9 13:36:49.201: INFO: Pod "var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01316954s
May  9 13:36:51.201: INFO: Pod "var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013937785s
STEP: Saw pod success 05/09/23 13:36:51.201
May  9 13:36:51.202: INFO: Pod "var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0" satisfied condition "Succeeded or Failed"
May  9 13:36:51.204: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0 container dapi-container: <nil>
STEP: delete the pod 05/09/23 13:36:51.216
May  9 13:36:51.225: INFO: Waiting for pod var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0 to disappear
May  9 13:36:51.227: INFO: Pod var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  9 13:36:51.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6290" for this suite. 05/09/23 13:36:51.23
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":279,"skipped":5427,"failed":0}
------------------------------
• [4.074 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:36:47.16
    May  9 13:36:47.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename var-expansion 05/09/23 13:36:47.161
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:36:47.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:36:47.18
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 05/09/23 13:36:47.181
    May  9 13:36:47.187: INFO: Waiting up to 5m0s for pod "var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0" in namespace "var-expansion-6290" to be "Succeeded or Failed"
    May  9 13:36:47.197: INFO: Pod "var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.366194ms
    May  9 13:36:49.201: INFO: Pod "var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01316954s
    May  9 13:36:51.201: INFO: Pod "var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013937785s
    STEP: Saw pod success 05/09/23 13:36:51.201
    May  9 13:36:51.202: INFO: Pod "var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0" satisfied condition "Succeeded or Failed"
    May  9 13:36:51.204: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0 container dapi-container: <nil>
    STEP: delete the pod 05/09/23 13:36:51.216
    May  9 13:36:51.225: INFO: Waiting for pod var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0 to disappear
    May  9 13:36:51.227: INFO: Pod var-expansion-98d1a275-4293-4151-b129-5d6b8bd566e0 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  9 13:36:51.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6290" for this suite. 05/09/23 13:36:51.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:36:51.235
May  9 13:36:51.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename daemonsets 05/09/23 13:36:51.236
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:36:51.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:36:51.248
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
May  9 13:36:51.263: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 13:36:51.27
May  9 13:36:51.275: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:51.275: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:51.275: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:51.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:36:51.279: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
May  9 13:36:52.285: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:52.285: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:52.285: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:52.288: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 13:36:52.288: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 13:36:53.283: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:53.284: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:53.284: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:53.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 13:36:53.287: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 05/09/23 13:36:53.294
STEP: Check that daemon pods images are updated. 05/09/23 13:36:53.302
May  9 13:36:53.305: INFO: Wrong image for pod: daemon-set-7ppjz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  9 13:36:53.305: INFO: Wrong image for pod: daemon-set-h4t2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  9 13:36:53.305: INFO: Wrong image for pod: daemon-set-vsdp5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  9 13:36:53.309: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:53.309: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:53.309: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:54.313: INFO: Wrong image for pod: daemon-set-h4t2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  9 13:36:54.313: INFO: Wrong image for pod: daemon-set-vsdp5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  9 13:36:54.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:54.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:54.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:55.313: INFO: Wrong image for pod: daemon-set-h4t2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  9 13:36:55.313: INFO: Pod daemon-set-p4447 is not available
May  9 13:36:55.313: INFO: Wrong image for pod: daemon-set-vsdp5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  9 13:36:55.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:55.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:55.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:56.313: INFO: Wrong image for pod: daemon-set-h4t2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  9 13:36:56.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:56.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:56.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:57.313: INFO: Wrong image for pod: daemon-set-h4t2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  9 13:36:57.313: INFO: Pod daemon-set-nvq9p is not available
May  9 13:36:57.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:57.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:57.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:58.319: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:58.319: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:58.319: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:59.317: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:59.317: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:36:59.317: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:37:00.313: INFO: Pod daemon-set-sf27v is not available
May  9 13:37:00.317: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:37:00.317: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:37:00.317: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 05/09/23 13:37:00.317
May  9 13:37:00.321: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:37:00.321: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:37:00.321: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:37:00.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 13:37:00.323: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
May  9 13:37:01.328: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:37:01.328: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:37:01.328: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:37:01.330: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 13:37:01.330: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/09/23 13:37:01.34
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4777, will wait for the garbage collector to delete the pods 05/09/23 13:37:01.34
May  9 13:37:01.398: INFO: Deleting DaemonSet.extensions daemon-set took: 4.847038ms
May  9 13:37:01.498: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.569078ms
May  9 13:37:03.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:37:03.702: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  9 13:37:03.704: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43283"},"items":null}

May  9 13:37:03.706: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43283"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  9 13:37:03.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4777" for this suite. 05/09/23 13:37:03.717
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":280,"skipped":5445,"failed":0}
------------------------------
• [SLOW TEST] [12.487 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:36:51.235
    May  9 13:36:51.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename daemonsets 05/09/23 13:36:51.236
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:36:51.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:36:51.248
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    May  9 13:36:51.263: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 13:36:51.27
    May  9 13:36:51.275: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:51.275: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:51.275: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:51.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:36:51.279: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
    May  9 13:36:52.285: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:52.285: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:52.285: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:52.288: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 13:36:52.288: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 13:36:53.283: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:53.284: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:53.284: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:53.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 13:36:53.287: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 05/09/23 13:36:53.294
    STEP: Check that daemon pods images are updated. 05/09/23 13:36:53.302
    May  9 13:36:53.305: INFO: Wrong image for pod: daemon-set-7ppjz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  9 13:36:53.305: INFO: Wrong image for pod: daemon-set-h4t2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  9 13:36:53.305: INFO: Wrong image for pod: daemon-set-vsdp5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  9 13:36:53.309: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:53.309: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:53.309: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:54.313: INFO: Wrong image for pod: daemon-set-h4t2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  9 13:36:54.313: INFO: Wrong image for pod: daemon-set-vsdp5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  9 13:36:54.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:54.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:54.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:55.313: INFO: Wrong image for pod: daemon-set-h4t2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  9 13:36:55.313: INFO: Pod daemon-set-p4447 is not available
    May  9 13:36:55.313: INFO: Wrong image for pod: daemon-set-vsdp5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  9 13:36:55.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:55.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:55.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:56.313: INFO: Wrong image for pod: daemon-set-h4t2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  9 13:36:56.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:56.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:56.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:57.313: INFO: Wrong image for pod: daemon-set-h4t2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  9 13:36:57.313: INFO: Pod daemon-set-nvq9p is not available
    May  9 13:36:57.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:57.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:57.316: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:58.319: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:58.319: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:58.319: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:59.317: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:59.317: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:36:59.317: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:37:00.313: INFO: Pod daemon-set-sf27v is not available
    May  9 13:37:00.317: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:37:00.317: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:37:00.317: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 05/09/23 13:37:00.317
    May  9 13:37:00.321: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:37:00.321: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:37:00.321: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:37:00.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 13:37:00.323: INFO: Node cl-gks-cncf-ix1-md-0-skqqr is running 0 daemon pod, expected 1
    May  9 13:37:01.328: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:37:01.328: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:37:01.328: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:37:01.330: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 13:37:01.330: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/09/23 13:37:01.34
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4777, will wait for the garbage collector to delete the pods 05/09/23 13:37:01.34
    May  9 13:37:01.398: INFO: Deleting DaemonSet.extensions daemon-set took: 4.847038ms
    May  9 13:37:01.498: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.569078ms
    May  9 13:37:03.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:37:03.702: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  9 13:37:03.704: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43283"},"items":null}

    May  9 13:37:03.706: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43283"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:37:03.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4777" for this suite. 05/09/23 13:37:03.717
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:37:03.723
May  9 13:37:03.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 13:37:03.723
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:03.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:03.743
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-6485/configmap-test-42d51058-5790-4296-9b54-5768ea5daccb 05/09/23 13:37:03.744
STEP: Creating a pod to test consume configMaps 05/09/23 13:37:03.748
May  9 13:37:03.755: INFO: Waiting up to 5m0s for pod "pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f" in namespace "configmap-6485" to be "Succeeded or Failed"
May  9 13:37:03.759: INFO: Pod "pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.751683ms
May  9 13:37:05.762: INFO: Pod "pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007078963s
May  9 13:37:07.762: INFO: Pod "pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006931466s
STEP: Saw pod success 05/09/23 13:37:07.762
May  9 13:37:07.762: INFO: Pod "pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f" satisfied condition "Succeeded or Failed"
May  9 13:37:07.764: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f container env-test: <nil>
STEP: delete the pod 05/09/23 13:37:07.769
May  9 13:37:07.780: INFO: Waiting for pod pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f to disappear
May  9 13:37:07.782: INFO: Pod pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May  9 13:37:07.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6485" for this suite. 05/09/23 13:37:07.784
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":281,"skipped":5449,"failed":0}
------------------------------
• [4.066 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:37:03.723
    May  9 13:37:03.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 13:37:03.723
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:03.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:03.743
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-6485/configmap-test-42d51058-5790-4296-9b54-5768ea5daccb 05/09/23 13:37:03.744
    STEP: Creating a pod to test consume configMaps 05/09/23 13:37:03.748
    May  9 13:37:03.755: INFO: Waiting up to 5m0s for pod "pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f" in namespace "configmap-6485" to be "Succeeded or Failed"
    May  9 13:37:03.759: INFO: Pod "pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.751683ms
    May  9 13:37:05.762: INFO: Pod "pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007078963s
    May  9 13:37:07.762: INFO: Pod "pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006931466s
    STEP: Saw pod success 05/09/23 13:37:07.762
    May  9 13:37:07.762: INFO: Pod "pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f" satisfied condition "Succeeded or Failed"
    May  9 13:37:07.764: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f container env-test: <nil>
    STEP: delete the pod 05/09/23 13:37:07.769
    May  9 13:37:07.780: INFO: Waiting for pod pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f to disappear
    May  9 13:37:07.782: INFO: Pod pod-configmaps-72f39c80-14a7-4aa7-a628-38c6b955003f no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 13:37:07.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6485" for this suite. 05/09/23 13:37:07.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:37:07.789
May  9 13:37:07.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 13:37:07.79
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:07.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:07.805
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-50f6278e-9a7a-4085-bcd7-b731f2dca751 05/09/23 13:37:07.806
STEP: Creating a pod to test consume configMaps 05/09/23 13:37:07.81
May  9 13:37:07.816: INFO: Waiting up to 5m0s for pod "pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f" in namespace "configmap-6977" to be "Succeeded or Failed"
May  9 13:37:07.818: INFO: Pod "pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.998253ms
May  9 13:37:09.822: INFO: Pod "pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006341447s
May  9 13:37:11.821: INFO: Pod "pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005664363s
STEP: Saw pod success 05/09/23 13:37:11.821
May  9 13:37:11.821: INFO: Pod "pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f" satisfied condition "Succeeded or Failed"
May  9 13:37:11.823: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f container agnhost-container: <nil>
STEP: delete the pod 05/09/23 13:37:11.827
May  9 13:37:11.836: INFO: Waiting for pod pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f to disappear
May  9 13:37:11.838: INFO: Pod pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  9 13:37:11.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6977" for this suite. 05/09/23 13:37:11.841
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":282,"skipped":5471,"failed":0}
------------------------------
• [4.056 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:37:07.789
    May  9 13:37:07.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 13:37:07.79
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:07.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:07.805
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-50f6278e-9a7a-4085-bcd7-b731f2dca751 05/09/23 13:37:07.806
    STEP: Creating a pod to test consume configMaps 05/09/23 13:37:07.81
    May  9 13:37:07.816: INFO: Waiting up to 5m0s for pod "pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f" in namespace "configmap-6977" to be "Succeeded or Failed"
    May  9 13:37:07.818: INFO: Pod "pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.998253ms
    May  9 13:37:09.822: INFO: Pod "pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006341447s
    May  9 13:37:11.821: INFO: Pod "pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005664363s
    STEP: Saw pod success 05/09/23 13:37:11.821
    May  9 13:37:11.821: INFO: Pod "pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f" satisfied condition "Succeeded or Failed"
    May  9 13:37:11.823: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 13:37:11.827
    May  9 13:37:11.836: INFO: Waiting for pod pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f to disappear
    May  9 13:37:11.838: INFO: Pod pod-configmaps-270f3355-4f10-4efd-8740-f1fa2616c10f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 13:37:11.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6977" for this suite. 05/09/23 13:37:11.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:37:11.848
May  9 13:37:11.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:37:11.848
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:11.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:11.861
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:37:11.872
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:37:12.298
STEP: Deploying the webhook pod 05/09/23 13:37:12.305
STEP: Wait for the deployment to be ready 05/09/23 13:37:12.315
May  9 13:37:12.320: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 13:37:14.328
STEP: Verifying the service has paired with the endpoint 05/09/23 13:37:14.341
May  9 13:37:15.341: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 05/09/23 13:37:15.391
STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 13:37:15.415
STEP: Deleting the collection of validation webhooks 05/09/23 13:37:15.432
STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 13:37:15.471
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:37:15.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6824" for this suite. 05/09/23 13:37:15.484
STEP: Destroying namespace "webhook-6824-markers" for this suite. 05/09/23 13:37:15.488
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":283,"skipped":5508,"failed":0}
------------------------------
• [3.679 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:37:11.848
    May  9 13:37:11.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:37:11.848
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:11.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:11.861
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:37:11.872
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:37:12.298
    STEP: Deploying the webhook pod 05/09/23 13:37:12.305
    STEP: Wait for the deployment to be ready 05/09/23 13:37:12.315
    May  9 13:37:12.320: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 13:37:14.328
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:37:14.341
    May  9 13:37:15.341: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 05/09/23 13:37:15.391
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 13:37:15.415
    STEP: Deleting the collection of validation webhooks 05/09/23 13:37:15.432
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 13:37:15.471
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:37:15.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6824" for this suite. 05/09/23 13:37:15.484
    STEP: Destroying namespace "webhook-6824-markers" for this suite. 05/09/23 13:37:15.488
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:37:15.528
May  9 13:37:15.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename svcaccounts 05/09/23 13:37:15.528
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:15.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:15.545
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
May  9 13:37:15.565: INFO: created pod pod-service-account-defaultsa
May  9 13:37:15.565: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May  9 13:37:15.570: INFO: created pod pod-service-account-mountsa
May  9 13:37:15.570: INFO: pod pod-service-account-mountsa service account token volume mount: true
May  9 13:37:15.577: INFO: created pod pod-service-account-nomountsa
May  9 13:37:15.577: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May  9 13:37:15.583: INFO: created pod pod-service-account-defaultsa-mountspec
May  9 13:37:15.583: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May  9 13:37:15.592: INFO: created pod pod-service-account-mountsa-mountspec
May  9 13:37:15.592: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May  9 13:37:15.596: INFO: created pod pod-service-account-nomountsa-mountspec
May  9 13:37:15.596: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May  9 13:37:15.602: INFO: created pod pod-service-account-defaultsa-nomountspec
May  9 13:37:15.602: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May  9 13:37:15.609: INFO: created pod pod-service-account-mountsa-nomountspec
May  9 13:37:15.609: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May  9 13:37:15.616: INFO: created pod pod-service-account-nomountsa-nomountspec
May  9 13:37:15.616: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  9 13:37:15.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6935" for this suite. 05/09/23 13:37:15.626
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":284,"skipped":5526,"failed":0}
------------------------------
• [0.104 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:37:15.528
    May  9 13:37:15.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 13:37:15.528
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:15.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:15.545
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    May  9 13:37:15.565: INFO: created pod pod-service-account-defaultsa
    May  9 13:37:15.565: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    May  9 13:37:15.570: INFO: created pod pod-service-account-mountsa
    May  9 13:37:15.570: INFO: pod pod-service-account-mountsa service account token volume mount: true
    May  9 13:37:15.577: INFO: created pod pod-service-account-nomountsa
    May  9 13:37:15.577: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    May  9 13:37:15.583: INFO: created pod pod-service-account-defaultsa-mountspec
    May  9 13:37:15.583: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    May  9 13:37:15.592: INFO: created pod pod-service-account-mountsa-mountspec
    May  9 13:37:15.592: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    May  9 13:37:15.596: INFO: created pod pod-service-account-nomountsa-mountspec
    May  9 13:37:15.596: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    May  9 13:37:15.602: INFO: created pod pod-service-account-defaultsa-nomountspec
    May  9 13:37:15.602: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    May  9 13:37:15.609: INFO: created pod pod-service-account-mountsa-nomountspec
    May  9 13:37:15.609: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    May  9 13:37:15.616: INFO: created pod pod-service-account-nomountsa-nomountspec
    May  9 13:37:15.616: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  9 13:37:15.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6935" for this suite. 05/09/23 13:37:15.626
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:37:15.632
May  9 13:37:15.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename crd-webhook 05/09/23 13:37:15.633
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:15.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:15.645
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/09/23 13:37:15.647
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/09/23 13:37:16.239
STEP: Deploying the custom resource conversion webhook pod 05/09/23 13:37:16.245
STEP: Wait for the deployment to be ready 05/09/23 13:37:16.256
May  9 13:37:16.266: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May  9 13:37:18.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 37, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 37, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 37, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 37, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/09/23 13:37:20.278
STEP: Verifying the service has paired with the endpoint 05/09/23 13:37:20.292
May  9 13:37:21.292: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
May  9 13:37:21.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Creating a v1 custom resource 05/09/23 13:37:23.859
STEP: Create a v2 custom resource 05/09/23 13:37:23.871
STEP: List CRs in v1 05/09/23 13:37:23.908
STEP: List CRs in v2 05/09/23 13:37:23.911
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:37:24.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6957" for this suite. 05/09/23 13:37:24.426
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":285,"skipped":5526,"failed":0}
------------------------------
• [SLOW TEST] [8.842 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:37:15.632
    May  9 13:37:15.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename crd-webhook 05/09/23 13:37:15.633
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:15.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:15.645
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/09/23 13:37:15.647
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/09/23 13:37:16.239
    STEP: Deploying the custom resource conversion webhook pod 05/09/23 13:37:16.245
    STEP: Wait for the deployment to be ready 05/09/23 13:37:16.256
    May  9 13:37:16.266: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    May  9 13:37:18.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 13, 37, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 37, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 37, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 37, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/09/23 13:37:20.278
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:37:20.292
    May  9 13:37:21.292: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    May  9 13:37:21.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Creating a v1 custom resource 05/09/23 13:37:23.859
    STEP: Create a v2 custom resource 05/09/23 13:37:23.871
    STEP: List CRs in v1 05/09/23 13:37:23.908
    STEP: List CRs in v2 05/09/23 13:37:23.911
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:37:24.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-6957" for this suite. 05/09/23 13:37:24.426
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:37:24.474
May  9 13:37:24.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename cronjob 05/09/23 13:37:24.475
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:24.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:24.489
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 05/09/23 13:37:24.491
STEP: creating 05/09/23 13:37:24.491
STEP: getting 05/09/23 13:37:24.496
STEP: listing 05/09/23 13:37:24.497
STEP: watching 05/09/23 13:37:24.501
May  9 13:37:24.501: INFO: starting watch
STEP: cluster-wide listing 05/09/23 13:37:24.502
STEP: cluster-wide watching 05/09/23 13:37:24.504
May  9 13:37:24.504: INFO: starting watch
STEP: patching 05/09/23 13:37:24.504
STEP: updating 05/09/23 13:37:24.509
May  9 13:37:24.516: INFO: waiting for watch events with expected annotations
May  9 13:37:24.516: INFO: saw patched and updated annotations
STEP: patching /status 05/09/23 13:37:24.516
STEP: updating /status 05/09/23 13:37:24.527
STEP: get /status 05/09/23 13:37:24.536
STEP: deleting 05/09/23 13:37:24.538
STEP: deleting a collection 05/09/23 13:37:24.549
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  9 13:37:24.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9505" for this suite. 05/09/23 13:37:24.562
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":286,"skipped":5533,"failed":0}
------------------------------
• [0.095 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:37:24.474
    May  9 13:37:24.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename cronjob 05/09/23 13:37:24.475
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:24.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:24.489
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 05/09/23 13:37:24.491
    STEP: creating 05/09/23 13:37:24.491
    STEP: getting 05/09/23 13:37:24.496
    STEP: listing 05/09/23 13:37:24.497
    STEP: watching 05/09/23 13:37:24.501
    May  9 13:37:24.501: INFO: starting watch
    STEP: cluster-wide listing 05/09/23 13:37:24.502
    STEP: cluster-wide watching 05/09/23 13:37:24.504
    May  9 13:37:24.504: INFO: starting watch
    STEP: patching 05/09/23 13:37:24.504
    STEP: updating 05/09/23 13:37:24.509
    May  9 13:37:24.516: INFO: waiting for watch events with expected annotations
    May  9 13:37:24.516: INFO: saw patched and updated annotations
    STEP: patching /status 05/09/23 13:37:24.516
    STEP: updating /status 05/09/23 13:37:24.527
    STEP: get /status 05/09/23 13:37:24.536
    STEP: deleting 05/09/23 13:37:24.538
    STEP: deleting a collection 05/09/23 13:37:24.549
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  9 13:37:24.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9505" for this suite. 05/09/23 13:37:24.562
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:37:24.57
May  9 13:37:24.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 13:37:24.571
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:24.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:24.587
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 05/09/23 13:37:24.588
May  9 13:37:24.589: INFO: namespace kubectl-2921
May  9 13:37:24.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2921 create -f -'
May  9 13:37:25.310: INFO: stderr: ""
May  9 13:37:25.310: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/09/23 13:37:25.31
May  9 13:37:26.314: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 13:37:26.314: INFO: Found 1 / 1
May  9 13:37:26.314: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  9 13:37:26.316: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 13:37:26.316: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  9 13:37:26.316: INFO: wait on agnhost-primary startup in kubectl-2921 
May  9 13:37:26.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2921 logs agnhost-primary-6mz8d agnhost-primary'
May  9 13:37:26.377: INFO: stderr: ""
May  9 13:37:26.377: INFO: stdout: "Paused\n"
STEP: exposing RC 05/09/23 13:37:26.377
May  9 13:37:26.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2921 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May  9 13:37:26.440: INFO: stderr: ""
May  9 13:37:26.440: INFO: stdout: "service/rm2 exposed\n"
May  9 13:37:26.444: INFO: Service rm2 in namespace kubectl-2921 found.
STEP: exposing service 05/09/23 13:37:28.45
May  9 13:37:28.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2921 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May  9 13:37:28.522: INFO: stderr: ""
May  9 13:37:28.522: INFO: stdout: "service/rm3 exposed\n"
May  9 13:37:28.525: INFO: Service rm3 in namespace kubectl-2921 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 13:37:30.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2921" for this suite. 05/09/23 13:37:30.534
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":287,"skipped":5535,"failed":0}
------------------------------
• [SLOW TEST] [5.968 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:37:24.57
    May  9 13:37:24.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 13:37:24.571
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:24.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:24.587
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 05/09/23 13:37:24.588
    May  9 13:37:24.589: INFO: namespace kubectl-2921
    May  9 13:37:24.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2921 create -f -'
    May  9 13:37:25.310: INFO: stderr: ""
    May  9 13:37:25.310: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/09/23 13:37:25.31
    May  9 13:37:26.314: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 13:37:26.314: INFO: Found 1 / 1
    May  9 13:37:26.314: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May  9 13:37:26.316: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 13:37:26.316: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  9 13:37:26.316: INFO: wait on agnhost-primary startup in kubectl-2921 
    May  9 13:37:26.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2921 logs agnhost-primary-6mz8d agnhost-primary'
    May  9 13:37:26.377: INFO: stderr: ""
    May  9 13:37:26.377: INFO: stdout: "Paused\n"
    STEP: exposing RC 05/09/23 13:37:26.377
    May  9 13:37:26.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2921 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    May  9 13:37:26.440: INFO: stderr: ""
    May  9 13:37:26.440: INFO: stdout: "service/rm2 exposed\n"
    May  9 13:37:26.444: INFO: Service rm2 in namespace kubectl-2921 found.
    STEP: exposing service 05/09/23 13:37:28.45
    May  9 13:37:28.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-2921 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    May  9 13:37:28.522: INFO: stderr: ""
    May  9 13:37:28.522: INFO: stdout: "service/rm3 exposed\n"
    May  9 13:37:28.525: INFO: Service rm3 in namespace kubectl-2921 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 13:37:30.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2921" for this suite. 05/09/23 13:37:30.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:37:30.539
May  9 13:37:30.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 13:37:30.539
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:30.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:30.554
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:37:30.555
May  9 13:37:30.563: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46" in namespace "downward-api-8448" to be "Succeeded or Failed"
May  9 13:37:30.565: INFO: Pod "downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12231ms
May  9 13:37:32.568: INFO: Pod "downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46": Phase="Running", Reason="", readiness=false. Elapsed: 2.00463754s
May  9 13:37:34.571: INFO: Pod "downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007787945s
STEP: Saw pod success 05/09/23 13:37:34.571
May  9 13:37:34.571: INFO: Pod "downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46" satisfied condition "Succeeded or Failed"
May  9 13:37:34.573: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46 container client-container: <nil>
STEP: delete the pod 05/09/23 13:37:34.584
May  9 13:37:34.602: INFO: Waiting for pod downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46 to disappear
May  9 13:37:34.604: INFO: Pod downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  9 13:37:34.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8448" for this suite. 05/09/23 13:37:34.607
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":288,"skipped":5542,"failed":0}
------------------------------
• [4.072 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:37:30.539
    May  9 13:37:30.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 13:37:30.539
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:30.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:30.554
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:37:30.555
    May  9 13:37:30.563: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46" in namespace "downward-api-8448" to be "Succeeded or Failed"
    May  9 13:37:30.565: INFO: Pod "downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12231ms
    May  9 13:37:32.568: INFO: Pod "downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46": Phase="Running", Reason="", readiness=false. Elapsed: 2.00463754s
    May  9 13:37:34.571: INFO: Pod "downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007787945s
    STEP: Saw pod success 05/09/23 13:37:34.571
    May  9 13:37:34.571: INFO: Pod "downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46" satisfied condition "Succeeded or Failed"
    May  9 13:37:34.573: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46 container client-container: <nil>
    STEP: delete the pod 05/09/23 13:37:34.584
    May  9 13:37:34.602: INFO: Waiting for pod downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46 to disappear
    May  9 13:37:34.604: INFO: Pod downwardapi-volume-53a39dd2-fc70-4490-b6d3-96f4dbae8f46 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  9 13:37:34.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8448" for this suite. 05/09/23 13:37:34.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:37:34.612
May  9 13:37:34.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 13:37:34.612
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:34.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:34.625
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-df2ecb05-879a-4b9f-835f-20c6a54106fe 05/09/23 13:37:34.627
STEP: Creating a pod to test consume configMaps 05/09/23 13:37:34.645
May  9 13:37:34.651: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5" in namespace "configmap-8083" to be "Succeeded or Failed"
May  9 13:37:34.656: INFO: Pod "pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.541526ms
May  9 13:37:36.660: INFO: Pod "pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5": Phase="Running", Reason="", readiness=false. Elapsed: 2.008380006s
May  9 13:37:38.659: INFO: Pod "pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008149779s
STEP: Saw pod success 05/09/23 13:37:38.659
May  9 13:37:38.660: INFO: Pod "pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5" satisfied condition "Succeeded or Failed"
May  9 13:37:38.666: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5 container configmap-volume-test: <nil>
STEP: delete the pod 05/09/23 13:37:38.67
May  9 13:37:38.680: INFO: Waiting for pod pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5 to disappear
May  9 13:37:38.682: INFO: Pod pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  9 13:37:38.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8083" for this suite. 05/09/23 13:37:38.685
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":289,"skipped":5557,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:37:34.612
    May  9 13:37:34.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 13:37:34.612
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:34.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:34.625
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-df2ecb05-879a-4b9f-835f-20c6a54106fe 05/09/23 13:37:34.627
    STEP: Creating a pod to test consume configMaps 05/09/23 13:37:34.645
    May  9 13:37:34.651: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5" in namespace "configmap-8083" to be "Succeeded or Failed"
    May  9 13:37:34.656: INFO: Pod "pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.541526ms
    May  9 13:37:36.660: INFO: Pod "pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5": Phase="Running", Reason="", readiness=false. Elapsed: 2.008380006s
    May  9 13:37:38.659: INFO: Pod "pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008149779s
    STEP: Saw pod success 05/09/23 13:37:38.659
    May  9 13:37:38.660: INFO: Pod "pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5" satisfied condition "Succeeded or Failed"
    May  9 13:37:38.666: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5 container configmap-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:37:38.67
    May  9 13:37:38.680: INFO: Waiting for pod pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5 to disappear
    May  9 13:37:38.682: INFO: Pod pod-configmaps-b7e9495d-03ab-4756-bbb8-25cfa5c032b5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 13:37:38.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8083" for this suite. 05/09/23 13:37:38.685
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:37:38.69
May  9 13:37:38.690: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubectl 05/09/23 13:37:38.69
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:38.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:38.704
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/09/23 13:37:38.705
May  9 13:37:38.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-5726 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May  9 13:37:38.770: INFO: stderr: ""
May  9 13:37:38.770: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 05/09/23 13:37:38.77
STEP: verifying the pod e2e-test-httpd-pod was created 05/09/23 13:37:43.823
May  9 13:37:43.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-5726 get pod e2e-test-httpd-pod -o json'
May  9 13:37:43.873: INFO: stderr: ""
May  9 13:37:43.873: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"7541bd55ad0905c7a94399f5126ba7c53c7534def1fdd7f20f7759f86b8a855c\",\n            \"cni.projectcalico.org/podIP\": \"172.25.124.242/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.25.124.242/32\"\n        },\n        \"creationTimestamp\": \"2023-05-09T13:37:38Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5726\",\n        \"resourceVersion\": \"43928\",\n        \"uid\": \"ef9e24de-c72c-48f5-9c95-b0bb3db89978\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-75xp2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cl-gks-cncf-ix1-md-0-48ljh\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-75xp2\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T13:37:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T13:37:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T13:37:40Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T13:37:38Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://a796988096e618c7a5f962a8332a2e560d800bc48acb0fd29f51088c5a13ded6\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-09T13:37:39Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.64\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.124.242\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.124.242\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-09T13:37:38Z\"\n    }\n}\n"
STEP: replace the image in the pod 05/09/23 13:37:43.874
May  9 13:37:43.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-5726 replace -f -'
May  9 13:37:44.557: INFO: stderr: ""
May  9 13:37:44.557: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 05/09/23 13:37:44.557
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
May  9 13:37:44.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-5726 delete pods e2e-test-httpd-pod'
May  9 13:37:46.265: INFO: stderr: ""
May  9 13:37:46.265: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  9 13:37:46.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5726" for this suite. 05/09/23 13:37:46.269
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":290,"skipped":5559,"failed":0}
------------------------------
• [SLOW TEST] [7.587 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:37:38.69
    May  9 13:37:38.690: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubectl 05/09/23 13:37:38.69
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:38.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:38.704
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/09/23 13:37:38.705
    May  9 13:37:38.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-5726 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May  9 13:37:38.770: INFO: stderr: ""
    May  9 13:37:38.770: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 05/09/23 13:37:38.77
    STEP: verifying the pod e2e-test-httpd-pod was created 05/09/23 13:37:43.823
    May  9 13:37:43.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-5726 get pod e2e-test-httpd-pod -o json'
    May  9 13:37:43.873: INFO: stderr: ""
    May  9 13:37:43.873: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"7541bd55ad0905c7a94399f5126ba7c53c7534def1fdd7f20f7759f86b8a855c\",\n            \"cni.projectcalico.org/podIP\": \"172.25.124.242/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.25.124.242/32\"\n        },\n        \"creationTimestamp\": \"2023-05-09T13:37:38Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5726\",\n        \"resourceVersion\": \"43928\",\n        \"uid\": \"ef9e24de-c72c-48f5-9c95-b0bb3db89978\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-75xp2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cl-gks-cncf-ix1-md-0-48ljh\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-75xp2\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T13:37:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T13:37:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T13:37:40Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T13:37:38Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://a796988096e618c7a5f962a8332a2e560d800bc48acb0fd29f51088c5a13ded6\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-09T13:37:39Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.64\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.124.242\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.124.242\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-09T13:37:38Z\"\n    }\n}\n"
    STEP: replace the image in the pod 05/09/23 13:37:43.874
    May  9 13:37:43.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-5726 replace -f -'
    May  9 13:37:44.557: INFO: stderr: ""
    May  9 13:37:44.557: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 05/09/23 13:37:44.557
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    May  9 13:37:44.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=kubectl-5726 delete pods e2e-test-httpd-pod'
    May  9 13:37:46.265: INFO: stderr: ""
    May  9 13:37:46.265: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  9 13:37:46.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5726" for this suite. 05/09/23 13:37:46.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:37:46.279
May  9 13:37:46.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:37:46.28
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:46.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:46.296
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-7297 05/09/23 13:37:46.297
May  9 13:37:46.306: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-7297" to be "running and ready"
May  9 13:37:46.308: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.402945ms
May  9 13:37:46.308: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May  9 13:37:48.311: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.005641882s
May  9 13:37:48.311: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
May  9 13:37:48.312: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
May  9 13:37:48.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7297 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May  9 13:37:48.413: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May  9 13:37:48.413: INFO: stdout: "iptables"
May  9 13:37:48.413: INFO: proxyMode: iptables
May  9 13:37:48.426: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May  9 13:37:48.428: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-7297 05/09/23 13:37:48.428
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7297 05/09/23 13:37:48.44
I0509 13:37:48.446124      24 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7297, replica count: 3
I0509 13:37:51.497678      24 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 13:37:51.502: INFO: Creating new exec pod
May  9 13:37:51.506: INFO: Waiting up to 5m0s for pod "execpod-affinity2vc7j" in namespace "services-7297" to be "running"
May  9 13:37:51.508: INFO: Pod "execpod-affinity2vc7j": Phase="Pending", Reason="", readiness=false. Elapsed: 1.809465ms
May  9 13:37:53.512: INFO: Pod "execpod-affinity2vc7j": Phase="Running", Reason="", readiness=true. Elapsed: 2.006180585s
May  9 13:37:53.512: INFO: Pod "execpod-affinity2vc7j" satisfied condition "running"
May  9 13:37:54.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7297 exec execpod-affinity2vc7j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
May  9 13:37:55.621: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May  9 13:37:55.621: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:37:55.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7297 exec execpod-affinity2vc7j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.102.53 80'
May  9 13:37:55.719: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.102.53 80\nConnection to 10.110.102.53 80 port [tcp/http] succeeded!\n"
May  9 13:37:55.719: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:37:55.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7297 exec execpod-affinity2vc7j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.102.53:80/ ; done'
May  9 13:37:55.858: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n"
May  9 13:37:55.858: INFO: stdout: "\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4"
May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
May  9 13:37:55.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7297 exec execpod-affinity2vc7j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.110.102.53:80/'
May  9 13:37:55.963: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n"
May  9 13:37:55.963: INFO: stdout: "affinity-clusterip-timeout-smdv4"
May  9 13:38:15.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7297 exec execpod-affinity2vc7j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.110.102.53:80/'
May  9 13:38:16.073: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n"
May  9 13:38:16.073: INFO: stdout: "affinity-clusterip-timeout-wvp4w"
May  9 13:38:16.073: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7297, will wait for the garbage collector to delete the pods 05/09/23 13:38:16.085
May  9 13:38:16.145: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.65772ms
May  9 13:38:16.246: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.952837ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:38:18.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7297" for this suite. 05/09/23 13:38:18.475
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":291,"skipped":5628,"failed":0}
------------------------------
• [SLOW TEST] [32.203 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:37:46.279
    May  9 13:37:46.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:37:46.28
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:37:46.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:37:46.296
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-7297 05/09/23 13:37:46.297
    May  9 13:37:46.306: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-7297" to be "running and ready"
    May  9 13:37:46.308: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.402945ms
    May  9 13:37:46.308: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:37:48.311: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.005641882s
    May  9 13:37:48.311: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    May  9 13:37:48.312: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    May  9 13:37:48.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7297 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    May  9 13:37:48.413: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    May  9 13:37:48.413: INFO: stdout: "iptables"
    May  9 13:37:48.413: INFO: proxyMode: iptables
    May  9 13:37:48.426: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    May  9 13:37:48.428: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-7297 05/09/23 13:37:48.428
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-7297 05/09/23 13:37:48.44
    I0509 13:37:48.446124      24 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7297, replica count: 3
    I0509 13:37:51.497678      24 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 13:37:51.502: INFO: Creating new exec pod
    May  9 13:37:51.506: INFO: Waiting up to 5m0s for pod "execpod-affinity2vc7j" in namespace "services-7297" to be "running"
    May  9 13:37:51.508: INFO: Pod "execpod-affinity2vc7j": Phase="Pending", Reason="", readiness=false. Elapsed: 1.809465ms
    May  9 13:37:53.512: INFO: Pod "execpod-affinity2vc7j": Phase="Running", Reason="", readiness=true. Elapsed: 2.006180585s
    May  9 13:37:53.512: INFO: Pod "execpod-affinity2vc7j" satisfied condition "running"
    May  9 13:37:54.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7297 exec execpod-affinity2vc7j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    May  9 13:37:55.621: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    May  9 13:37:55.621: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:37:55.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7297 exec execpod-affinity2vc7j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.102.53 80'
    May  9 13:37:55.719: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.102.53 80\nConnection to 10.110.102.53 80 port [tcp/http] succeeded!\n"
    May  9 13:37:55.719: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:37:55.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7297 exec execpod-affinity2vc7j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.102.53:80/ ; done'
    May  9 13:37:55.858: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n"
    May  9 13:37:55.858: INFO: stdout: "\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4\naffinity-clusterip-timeout-smdv4"
    May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.858: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.859: INFO: Received response from host: affinity-clusterip-timeout-smdv4
    May  9 13:37:55.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7297 exec execpod-affinity2vc7j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.110.102.53:80/'
    May  9 13:37:55.963: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n"
    May  9 13:37:55.963: INFO: stdout: "affinity-clusterip-timeout-smdv4"
    May  9 13:38:15.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-7297 exec execpod-affinity2vc7j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.110.102.53:80/'
    May  9 13:38:16.073: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.110.102.53:80/\n"
    May  9 13:38:16.073: INFO: stdout: "affinity-clusterip-timeout-wvp4w"
    May  9 13:38:16.073: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7297, will wait for the garbage collector to delete the pods 05/09/23 13:38:16.085
    May  9 13:38:16.145: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.65772ms
    May  9 13:38:16.246: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.952837ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:38:18.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7297" for this suite. 05/09/23 13:38:18.475
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:38:18.482
May  9 13:38:18.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename security-context 05/09/23 13:38:18.483
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:18.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:18.5
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/09/23 13:38:18.501
May  9 13:38:18.510: INFO: Waiting up to 5m0s for pod "security-context-6959d0da-6d99-4e83-9374-af0549bea5fb" in namespace "security-context-1722" to be "Succeeded or Failed"
May  9 13:38:18.512: INFO: Pod "security-context-6959d0da-6d99-4e83-9374-af0549bea5fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.537611ms
May  9 13:38:20.519: INFO: Pod "security-context-6959d0da-6d99-4e83-9374-af0549bea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 2.00912409s
May  9 13:38:22.516: INFO: Pod "security-context-6959d0da-6d99-4e83-9374-af0549bea5fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006012045s
STEP: Saw pod success 05/09/23 13:38:22.516
May  9 13:38:22.516: INFO: Pod "security-context-6959d0da-6d99-4e83-9374-af0549bea5fb" satisfied condition "Succeeded or Failed"
May  9 13:38:22.518: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod security-context-6959d0da-6d99-4e83-9374-af0549bea5fb container test-container: <nil>
STEP: delete the pod 05/09/23 13:38:22.522
May  9 13:38:22.535: INFO: Waiting for pod security-context-6959d0da-6d99-4e83-9374-af0549bea5fb to disappear
May  9 13:38:22.538: INFO: Pod security-context-6959d0da-6d99-4e83-9374-af0549bea5fb no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  9 13:38:22.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1722" for this suite. 05/09/23 13:38:22.541
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":292,"skipped":5631,"failed":0}
------------------------------
• [4.065 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:38:18.482
    May  9 13:38:18.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename security-context 05/09/23 13:38:18.483
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:18.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:18.5
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/09/23 13:38:18.501
    May  9 13:38:18.510: INFO: Waiting up to 5m0s for pod "security-context-6959d0da-6d99-4e83-9374-af0549bea5fb" in namespace "security-context-1722" to be "Succeeded or Failed"
    May  9 13:38:18.512: INFO: Pod "security-context-6959d0da-6d99-4e83-9374-af0549bea5fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.537611ms
    May  9 13:38:20.519: INFO: Pod "security-context-6959d0da-6d99-4e83-9374-af0549bea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 2.00912409s
    May  9 13:38:22.516: INFO: Pod "security-context-6959d0da-6d99-4e83-9374-af0549bea5fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006012045s
    STEP: Saw pod success 05/09/23 13:38:22.516
    May  9 13:38:22.516: INFO: Pod "security-context-6959d0da-6d99-4e83-9374-af0549bea5fb" satisfied condition "Succeeded or Failed"
    May  9 13:38:22.518: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod security-context-6959d0da-6d99-4e83-9374-af0549bea5fb container test-container: <nil>
    STEP: delete the pod 05/09/23 13:38:22.522
    May  9 13:38:22.535: INFO: Waiting for pod security-context-6959d0da-6d99-4e83-9374-af0549bea5fb to disappear
    May  9 13:38:22.538: INFO: Pod security-context-6959d0da-6d99-4e83-9374-af0549bea5fb no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  9 13:38:22.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-1722" for this suite. 05/09/23 13:38:22.541
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:38:22.547
May  9 13:38:22.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 13:38:22.548
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:22.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:22.569
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-d8e2b262-5ec6-4108-85ca-10f6a4848e7d 05/09/23 13:38:22.571
STEP: Creating a pod to test consume secrets 05/09/23 13:38:22.575
May  9 13:38:22.600: INFO: Waiting up to 5m0s for pod "pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6" in namespace "secrets-8141" to be "Succeeded or Failed"
May  9 13:38:22.614: INFO: Pod "pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.46247ms
May  9 13:38:24.618: INFO: Pod "pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6": Phase="Running", Reason="", readiness=false. Elapsed: 2.017653127s
May  9 13:38:26.618: INFO: Pod "pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017676335s
STEP: Saw pod success 05/09/23 13:38:26.618
May  9 13:38:26.618: INFO: Pod "pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6" satisfied condition "Succeeded or Failed"
May  9 13:38:26.621: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6 container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 13:38:26.625
May  9 13:38:26.636: INFO: Waiting for pod pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6 to disappear
May  9 13:38:26.638: INFO: Pod pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  9 13:38:26.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8141" for this suite. 05/09/23 13:38:26.642
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":293,"skipped":5632,"failed":0}
------------------------------
• [4.102 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:38:22.547
    May  9 13:38:22.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 13:38:22.548
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:22.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:22.569
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-d8e2b262-5ec6-4108-85ca-10f6a4848e7d 05/09/23 13:38:22.571
    STEP: Creating a pod to test consume secrets 05/09/23 13:38:22.575
    May  9 13:38:22.600: INFO: Waiting up to 5m0s for pod "pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6" in namespace "secrets-8141" to be "Succeeded or Failed"
    May  9 13:38:22.614: INFO: Pod "pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.46247ms
    May  9 13:38:24.618: INFO: Pod "pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6": Phase="Running", Reason="", readiness=false. Elapsed: 2.017653127s
    May  9 13:38:26.618: INFO: Pod "pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017676335s
    STEP: Saw pod success 05/09/23 13:38:26.618
    May  9 13:38:26.618: INFO: Pod "pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6" satisfied condition "Succeeded or Failed"
    May  9 13:38:26.621: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6 container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:38:26.625
    May  9 13:38:26.636: INFO: Waiting for pod pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6 to disappear
    May  9 13:38:26.638: INFO: Pod pod-secrets-39f02df2-0b39-4b73-9b80-f9c9c45f27b6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  9 13:38:26.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8141" for this suite. 05/09/23 13:38:26.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:38:26.651
May  9 13:38:26.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename var-expansion 05/09/23 13:38:26.651
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:26.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:26.666
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
May  9 13:38:26.679: INFO: Waiting up to 2m0s for pod "var-expansion-86808400-c019-4d95-8fee-b945d95dd2f1" in namespace "var-expansion-8036" to be "container 0 failed with reason CreateContainerConfigError"
May  9 13:38:26.687: INFO: Pod "var-expansion-86808400-c019-4d95-8fee-b945d95dd2f1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.850857ms
May  9 13:38:28.690: INFO: Pod "var-expansion-86808400-c019-4d95-8fee-b945d95dd2f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010879484s
May  9 13:38:28.690: INFO: Pod "var-expansion-86808400-c019-4d95-8fee-b945d95dd2f1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May  9 13:38:28.690: INFO: Deleting pod "var-expansion-86808400-c019-4d95-8fee-b945d95dd2f1" in namespace "var-expansion-8036"
May  9 13:38:28.697: INFO: Wait up to 5m0s for pod "var-expansion-86808400-c019-4d95-8fee-b945d95dd2f1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  9 13:38:30.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8036" for this suite. 05/09/23 13:38:30.704
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":294,"skipped":5655,"failed":0}
------------------------------
• [4.058 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:38:26.651
    May  9 13:38:26.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename var-expansion 05/09/23 13:38:26.651
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:26.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:26.666
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    May  9 13:38:26.679: INFO: Waiting up to 2m0s for pod "var-expansion-86808400-c019-4d95-8fee-b945d95dd2f1" in namespace "var-expansion-8036" to be "container 0 failed with reason CreateContainerConfigError"
    May  9 13:38:26.687: INFO: Pod "var-expansion-86808400-c019-4d95-8fee-b945d95dd2f1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.850857ms
    May  9 13:38:28.690: INFO: Pod "var-expansion-86808400-c019-4d95-8fee-b945d95dd2f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010879484s
    May  9 13:38:28.690: INFO: Pod "var-expansion-86808400-c019-4d95-8fee-b945d95dd2f1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May  9 13:38:28.690: INFO: Deleting pod "var-expansion-86808400-c019-4d95-8fee-b945d95dd2f1" in namespace "var-expansion-8036"
    May  9 13:38:28.697: INFO: Wait up to 5m0s for pod "var-expansion-86808400-c019-4d95-8fee-b945d95dd2f1" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  9 13:38:30.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8036" for this suite. 05/09/23 13:38:30.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:38:30.709
May  9 13:38:30.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename daemonsets 05/09/23 13:38:30.709
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:30.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:30.722
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 05/09/23 13:38:30.734
STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 13:38:30.739
May  9 13:38:30.741: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:30.741: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:30.741: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:30.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:38:30.743: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
May  9 13:38:31.747: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:31.747: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:31.747: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:31.750: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 13:38:31.750: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
May  9 13:38:32.747: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:32.747: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:32.747: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:32.749: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 13:38:32.749: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/09/23 13:38:32.751
May  9 13:38:32.768: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:32.768: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:32.768: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:32.770: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 13:38:32.770: INFO: Node cl-gks-cncf-ix1-md-0-879bk is running 0 daemon pod, expected 1
May  9 13:38:33.775: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:33.775: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:33.775: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:38:33.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 13:38:33.777: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 05/09/23 13:38:33.777
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/09/23 13:38:33.781
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4379, will wait for the garbage collector to delete the pods 05/09/23 13:38:33.781
May  9 13:38:33.839: INFO: Deleting DaemonSet.extensions daemon-set took: 5.542822ms
May  9 13:38:33.940: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.830885ms
May  9 13:38:36.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:38:36.543: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  9 13:38:36.545: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"44519"},"items":null}

May  9 13:38:36.546: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"44519"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  9 13:38:36.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4379" for this suite. 05/09/23 13:38:36.557
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":295,"skipped":5661,"failed":0}
------------------------------
• [SLOW TEST] [5.856 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:38:30.709
    May  9 13:38:30.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename daemonsets 05/09/23 13:38:30.709
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:30.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:30.722
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 05/09/23 13:38:30.734
    STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 13:38:30.739
    May  9 13:38:30.741: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:30.741: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:30.741: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:30.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:38:30.743: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
    May  9 13:38:31.747: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:31.747: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:31.747: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:31.750: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 13:38:31.750: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
    May  9 13:38:32.747: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:32.747: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:32.747: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:32.749: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 13:38:32.749: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/09/23 13:38:32.751
    May  9 13:38:32.768: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:32.768: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:32.768: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:32.770: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 13:38:32.770: INFO: Node cl-gks-cncf-ix1-md-0-879bk is running 0 daemon pod, expected 1
    May  9 13:38:33.775: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:33.775: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:33.775: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:38:33.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 13:38:33.777: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 05/09/23 13:38:33.777
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/09/23 13:38:33.781
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4379, will wait for the garbage collector to delete the pods 05/09/23 13:38:33.781
    May  9 13:38:33.839: INFO: Deleting DaemonSet.extensions daemon-set took: 5.542822ms
    May  9 13:38:33.940: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.830885ms
    May  9 13:38:36.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:38:36.543: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  9 13:38:36.545: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"44519"},"items":null}

    May  9 13:38:36.546: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"44519"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:38:36.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4379" for this suite. 05/09/23 13:38:36.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:38:36.566
May  9 13:38:36.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 13:38:36.566
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:36.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:36.579
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-c8e2813c-80e9-488a-bbf6-b1dcccf459ff 05/09/23 13:38:36.602
STEP: Creating a pod to test consume secrets 05/09/23 13:38:36.607
May  9 13:38:36.616: INFO: Waiting up to 5m0s for pod "pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba" in namespace "secrets-877" to be "Succeeded or Failed"
May  9 13:38:36.619: INFO: Pod "pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.146871ms
May  9 13:38:38.622: INFO: Pod "pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba": Phase="Running", Reason="", readiness=false. Elapsed: 2.006146441s
May  9 13:38:40.623: INFO: Pod "pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007373252s
STEP: Saw pod success 05/09/23 13:38:40.623
May  9 13:38:40.623: INFO: Pod "pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba" satisfied condition "Succeeded or Failed"
May  9 13:38:40.625: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 13:38:40.63
May  9 13:38:40.640: INFO: Waiting for pod pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba to disappear
May  9 13:38:40.642: INFO: Pod pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  9 13:38:40.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-877" for this suite. 05/09/23 13:38:40.645
STEP: Destroying namespace "secret-namespace-3399" for this suite. 05/09/23 13:38:40.649
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":296,"skipped":5684,"failed":0}
------------------------------
• [4.088 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:38:36.566
    May  9 13:38:36.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 13:38:36.566
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:36.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:36.579
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-c8e2813c-80e9-488a-bbf6-b1dcccf459ff 05/09/23 13:38:36.602
    STEP: Creating a pod to test consume secrets 05/09/23 13:38:36.607
    May  9 13:38:36.616: INFO: Waiting up to 5m0s for pod "pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba" in namespace "secrets-877" to be "Succeeded or Failed"
    May  9 13:38:36.619: INFO: Pod "pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.146871ms
    May  9 13:38:38.622: INFO: Pod "pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba": Phase="Running", Reason="", readiness=false. Elapsed: 2.006146441s
    May  9 13:38:40.623: INFO: Pod "pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007373252s
    STEP: Saw pod success 05/09/23 13:38:40.623
    May  9 13:38:40.623: INFO: Pod "pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba" satisfied condition "Succeeded or Failed"
    May  9 13:38:40.625: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:38:40.63
    May  9 13:38:40.640: INFO: Waiting for pod pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba to disappear
    May  9 13:38:40.642: INFO: Pod pod-secrets-f8e202a3-23d7-46bb-97d6-5637f329daba no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  9 13:38:40.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-877" for this suite. 05/09/23 13:38:40.645
    STEP: Destroying namespace "secret-namespace-3399" for this suite. 05/09/23 13:38:40.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:38:40.654
May  9 13:38:40.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename resourcequota 05/09/23 13:38:40.655
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:40.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:40.669
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 05/09/23 13:38:40.671
STEP: Getting a ResourceQuota 05/09/23 13:38:40.675
STEP: Updating a ResourceQuota 05/09/23 13:38:40.678
STEP: Verifying a ResourceQuota was modified 05/09/23 13:38:40.682
STEP: Deleting a ResourceQuota 05/09/23 13:38:40.686
STEP: Verifying the deleted ResourceQuota 05/09/23 13:38:40.69
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  9 13:38:40.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9209" for this suite. 05/09/23 13:38:40.695
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":297,"skipped":5697,"failed":0}
------------------------------
• [0.046 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:38:40.654
    May  9 13:38:40.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename resourcequota 05/09/23 13:38:40.655
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:40.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:40.669
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 05/09/23 13:38:40.671
    STEP: Getting a ResourceQuota 05/09/23 13:38:40.675
    STEP: Updating a ResourceQuota 05/09/23 13:38:40.678
    STEP: Verifying a ResourceQuota was modified 05/09/23 13:38:40.682
    STEP: Deleting a ResourceQuota 05/09/23 13:38:40.686
    STEP: Verifying the deleted ResourceQuota 05/09/23 13:38:40.69
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  9 13:38:40.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9209" for this suite. 05/09/23 13:38:40.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:38:40.701
May  9 13:38:40.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 13:38:40.701
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:40.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:40.718
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/09/23 13:38:40.719
May  9 13:38:40.728: INFO: Waiting up to 5m0s for pod "pod-85863aa1-fb84-4454-864b-148439456456" in namespace "emptydir-2459" to be "Succeeded or Failed"
May  9 13:38:40.730: INFO: Pod "pod-85863aa1-fb84-4454-864b-148439456456": Phase="Pending", Reason="", readiness=false. Elapsed: 1.924044ms
May  9 13:38:42.733: INFO: Pod "pod-85863aa1-fb84-4454-864b-148439456456": Phase="Running", Reason="", readiness=false. Elapsed: 2.004831494s
May  9 13:38:44.740: INFO: Pod "pod-85863aa1-fb84-4454-864b-148439456456": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011557104s
STEP: Saw pod success 05/09/23 13:38:44.74
May  9 13:38:44.740: INFO: Pod "pod-85863aa1-fb84-4454-864b-148439456456" satisfied condition "Succeeded or Failed"
May  9 13:38:44.744: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-85863aa1-fb84-4454-864b-148439456456 container test-container: <nil>
STEP: delete the pod 05/09/23 13:38:44.75
May  9 13:38:44.759: INFO: Waiting for pod pod-85863aa1-fb84-4454-864b-148439456456 to disappear
May  9 13:38:44.760: INFO: Pod pod-85863aa1-fb84-4454-864b-148439456456 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 13:38:44.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2459" for this suite. 05/09/23 13:38:44.763
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":298,"skipped":5711,"failed":0}
------------------------------
• [4.067 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:38:40.701
    May  9 13:38:40.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 13:38:40.701
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:40.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:40.718
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/09/23 13:38:40.719
    May  9 13:38:40.728: INFO: Waiting up to 5m0s for pod "pod-85863aa1-fb84-4454-864b-148439456456" in namespace "emptydir-2459" to be "Succeeded or Failed"
    May  9 13:38:40.730: INFO: Pod "pod-85863aa1-fb84-4454-864b-148439456456": Phase="Pending", Reason="", readiness=false. Elapsed: 1.924044ms
    May  9 13:38:42.733: INFO: Pod "pod-85863aa1-fb84-4454-864b-148439456456": Phase="Running", Reason="", readiness=false. Elapsed: 2.004831494s
    May  9 13:38:44.740: INFO: Pod "pod-85863aa1-fb84-4454-864b-148439456456": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011557104s
    STEP: Saw pod success 05/09/23 13:38:44.74
    May  9 13:38:44.740: INFO: Pod "pod-85863aa1-fb84-4454-864b-148439456456" satisfied condition "Succeeded or Failed"
    May  9 13:38:44.744: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-85863aa1-fb84-4454-864b-148439456456 container test-container: <nil>
    STEP: delete the pod 05/09/23 13:38:44.75
    May  9 13:38:44.759: INFO: Waiting for pod pod-85863aa1-fb84-4454-864b-148439456456 to disappear
    May  9 13:38:44.760: INFO: Pod pod-85863aa1-fb84-4454-864b-148439456456 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 13:38:44.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2459" for this suite. 05/09/23 13:38:44.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:38:44.769
May  9 13:38:44.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename downward-api 05/09/23 13:38:44.77
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:44.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:44.783
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 05/09/23 13:38:44.785
May  9 13:38:44.790: INFO: Waiting up to 5m0s for pod "downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e" in namespace "downward-api-6974" to be "Succeeded or Failed"
May  9 13:38:44.794: INFO: Pod "downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.738027ms
May  9 13:38:46.798: INFO: Pod "downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007633429s
May  9 13:38:48.798: INFO: Pod "downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008223076s
STEP: Saw pod success 05/09/23 13:38:48.798
May  9 13:38:48.799: INFO: Pod "downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e" satisfied condition "Succeeded or Failed"
May  9 13:38:48.801: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e container dapi-container: <nil>
STEP: delete the pod 05/09/23 13:38:48.806
May  9 13:38:48.817: INFO: Waiting for pod downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e to disappear
May  9 13:38:48.818: INFO: Pod downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  9 13:38:48.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6974" for this suite. 05/09/23 13:38:48.821
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":299,"skipped":5762,"failed":0}
------------------------------
• [4.057 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:38:44.769
    May  9 13:38:44.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename downward-api 05/09/23 13:38:44.77
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:44.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:44.783
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 05/09/23 13:38:44.785
    May  9 13:38:44.790: INFO: Waiting up to 5m0s for pod "downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e" in namespace "downward-api-6974" to be "Succeeded or Failed"
    May  9 13:38:44.794: INFO: Pod "downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.738027ms
    May  9 13:38:46.798: INFO: Pod "downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007633429s
    May  9 13:38:48.798: INFO: Pod "downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008223076s
    STEP: Saw pod success 05/09/23 13:38:48.798
    May  9 13:38:48.799: INFO: Pod "downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e" satisfied condition "Succeeded or Failed"
    May  9 13:38:48.801: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e container dapi-container: <nil>
    STEP: delete the pod 05/09/23 13:38:48.806
    May  9 13:38:48.817: INFO: Waiting for pod downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e to disappear
    May  9 13:38:48.818: INFO: Pod downward-api-320cdda4-04fc-4002-bf74-5723ed7d056e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  9 13:38:48.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6974" for this suite. 05/09/23 13:38:48.821
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:38:48.827
May  9 13:38:48.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename namespaces 05/09/23 13:38:48.827
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:48.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:48.839
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 05/09/23 13:38:48.84
STEP: patching the Namespace 05/09/23 13:38:48.851
STEP: get the Namespace and ensuring it has the label 05/09/23 13:38:48.855
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May  9 13:38:48.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6019" for this suite. 05/09/23 13:38:48.859
STEP: Destroying namespace "nspatchtest-6b986d78-7028-4978-a11e-0bb1dc885600-1199" for this suite. 05/09/23 13:38:48.864
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":300,"skipped":5765,"failed":0}
------------------------------
• [0.045 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:38:48.827
    May  9 13:38:48.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename namespaces 05/09/23 13:38:48.827
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:48.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:48.839
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 05/09/23 13:38:48.84
    STEP: patching the Namespace 05/09/23 13:38:48.851
    STEP: get the Namespace and ensuring it has the label 05/09/23 13:38:48.855
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:38:48.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6019" for this suite. 05/09/23 13:38:48.859
    STEP: Destroying namespace "nspatchtest-6b986d78-7028-4978-a11e-0bb1dc885600-1199" for this suite. 05/09/23 13:38:48.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:38:48.873
May  9 13:38:48.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename deployment 05/09/23 13:38:48.873
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:48.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:48.885
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 05/09/23 13:38:48.889
May  9 13:38:48.889: INFO: Creating simple deployment test-deployment-cvcs6
May  9 13:38:48.899: INFO: deployment "test-deployment-cvcs6" doesn't have the required revision set
STEP: Getting /status 05/09/23 13:38:50.908
May  9 13:38:50.911: INFO: Deployment test-deployment-cvcs6 has Conditions: [{Available True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvcs6-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 05/09/23 13:38:50.911
May  9 13:38:50.918: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 38, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 38, 48, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-cvcs6-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 05/09/23 13:38:50.918
May  9 13:38:50.920: INFO: Observed &Deployment event: ADDED
May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvcs6-777898ffcc"}
May  9 13:38:50.920: INFO: Observed &Deployment event: MODIFIED
May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvcs6-777898ffcc"}
May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  9 13:38:50.920: INFO: Observed &Deployment event: MODIFIED
May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cvcs6-777898ffcc" is progressing.}
May  9 13:38:50.920: INFO: Observed &Deployment event: MODIFIED
May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvcs6-777898ffcc" has successfully progressed.}
May  9 13:38:50.920: INFO: Observed &Deployment event: MODIFIED
May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvcs6-777898ffcc" has successfully progressed.}
May  9 13:38:50.920: INFO: Found Deployment test-deployment-cvcs6 in namespace deployment-9898 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  9 13:38:50.920: INFO: Deployment test-deployment-cvcs6 has an updated status
STEP: patching the Statefulset Status 05/09/23 13:38:50.92
May  9 13:38:50.920: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  9 13:38:50.927: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 05/09/23 13:38:50.927
May  9 13:38:50.928: INFO: Observed &Deployment event: ADDED
May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvcs6-777898ffcc"}
May  9 13:38:50.928: INFO: Observed &Deployment event: MODIFIED
May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvcs6-777898ffcc"}
May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  9 13:38:50.928: INFO: Observed &Deployment event: MODIFIED
May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cvcs6-777898ffcc" is progressing.}
May  9 13:38:50.928: INFO: Observed &Deployment event: MODIFIED
May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvcs6-777898ffcc" has successfully progressed.}
May  9 13:38:50.928: INFO: Observed &Deployment event: MODIFIED
May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvcs6-777898ffcc" has successfully progressed.}
May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  9 13:38:50.928: INFO: Observed &Deployment event: MODIFIED
May  9 13:38:50.928: INFO: Found deployment test-deployment-cvcs6 in namespace deployment-9898 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
May  9 13:38:50.928: INFO: Deployment test-deployment-cvcs6 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 13:38:50.931: INFO: Deployment "test-deployment-cvcs6":
&Deployment{ObjectMeta:{test-deployment-cvcs6  deployment-9898  388e12bf-3027-4f70-b779-fe5c2b159eef 44745 1 2023-05-09 13:38:48 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-09 13:38:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-09 13:38:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-09 13:38:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034ab848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-cvcs6-777898ffcc",LastUpdateTime:2023-05-09 13:38:50 +0000 UTC,LastTransitionTime:2023-05-09 13:38:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  9 13:38:50.933: INFO: New ReplicaSet "test-deployment-cvcs6-777898ffcc" of Deployment "test-deployment-cvcs6":
&ReplicaSet{ObjectMeta:{test-deployment-cvcs6-777898ffcc  deployment-9898  2fa17362-5622-4fef-af8e-3704d20b8b0c 44738 1 2023-05-09 13:38:48 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-cvcs6 388e12bf-3027-4f70-b779-fe5c2b159eef 0xc0034abf00 0xc0034abf01}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:38:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"388e12bf-3027-4f70-b779-fe5c2b159eef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:38:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034abfa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  9 13:38:50.938: INFO: Pod "test-deployment-cvcs6-777898ffcc-7j9wc" is available:
&Pod{ObjectMeta:{test-deployment-cvcs6-777898ffcc-7j9wc test-deployment-cvcs6-777898ffcc- deployment-9898  4c26e564-4a5c-4c84-a096-e9ceabeefbfb 44737 0 2023-05-09 13:38:48 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:ff7e54b0eb4e709b603aa2aeba93a225d9fe726e952883296aa109ddb45dac14 cni.projectcalico.org/podIP:172.25.53.139/32 cni.projectcalico.org/podIPs:172.25.53.139/32] [{apps/v1 ReplicaSet test-deployment-cvcs6-777898ffcc 2fa17362-5622-4fef-af8e-3704d20b8b0c 0xc0034c23b0 0xc0034c23b1}] [] [{kube-controller-manager Update v1 2023-05-09 13:38:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fa17362-5622-4fef-af8e-3704d20b8b0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:38:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:38:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4wgrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4wgrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:38:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:38:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.139,StartTime:2023-05-09 13:38:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:38:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b4263403a15d26e914cc88e9ae43744c085dffe8ba37717f36cfce6cdc3f9da6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  9 13:38:50.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9898" for this suite. 05/09/23 13:38:50.941
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":301,"skipped":5772,"failed":0}
------------------------------
• [2.072 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:38:48.873
    May  9 13:38:48.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename deployment 05/09/23 13:38:48.873
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:48.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:48.885
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 05/09/23 13:38:48.889
    May  9 13:38:48.889: INFO: Creating simple deployment test-deployment-cvcs6
    May  9 13:38:48.899: INFO: deployment "test-deployment-cvcs6" doesn't have the required revision set
    STEP: Getting /status 05/09/23 13:38:50.908
    May  9 13:38:50.911: INFO: Deployment test-deployment-cvcs6 has Conditions: [{Available True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvcs6-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 05/09/23 13:38:50.911
    May  9 13:38:50.918: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 38, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 38, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 38, 48, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-cvcs6-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 05/09/23 13:38:50.918
    May  9 13:38:50.920: INFO: Observed &Deployment event: ADDED
    May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvcs6-777898ffcc"}
    May  9 13:38:50.920: INFO: Observed &Deployment event: MODIFIED
    May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvcs6-777898ffcc"}
    May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  9 13:38:50.920: INFO: Observed &Deployment event: MODIFIED
    May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cvcs6-777898ffcc" is progressing.}
    May  9 13:38:50.920: INFO: Observed &Deployment event: MODIFIED
    May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvcs6-777898ffcc" has successfully progressed.}
    May  9 13:38:50.920: INFO: Observed &Deployment event: MODIFIED
    May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  9 13:38:50.920: INFO: Observed Deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvcs6-777898ffcc" has successfully progressed.}
    May  9 13:38:50.920: INFO: Found Deployment test-deployment-cvcs6 in namespace deployment-9898 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  9 13:38:50.920: INFO: Deployment test-deployment-cvcs6 has an updated status
    STEP: patching the Statefulset Status 05/09/23 13:38:50.92
    May  9 13:38:50.920: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  9 13:38:50.927: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 05/09/23 13:38:50.927
    May  9 13:38:50.928: INFO: Observed &Deployment event: ADDED
    May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvcs6-777898ffcc"}
    May  9 13:38:50.928: INFO: Observed &Deployment event: MODIFIED
    May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cvcs6-777898ffcc"}
    May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  9 13:38:50.928: INFO: Observed &Deployment event: MODIFIED
    May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:48 +0000 UTC 2023-05-09 13:38:48 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cvcs6-777898ffcc" is progressing.}
    May  9 13:38:50.928: INFO: Observed &Deployment event: MODIFIED
    May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvcs6-777898ffcc" has successfully progressed.}
    May  9 13:38:50.928: INFO: Observed &Deployment event: MODIFIED
    May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 13:38:50 +0000 UTC 2023-05-09 13:38:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cvcs6-777898ffcc" has successfully progressed.}
    May  9 13:38:50.928: INFO: Observed deployment test-deployment-cvcs6 in namespace deployment-9898 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  9 13:38:50.928: INFO: Observed &Deployment event: MODIFIED
    May  9 13:38:50.928: INFO: Found deployment test-deployment-cvcs6 in namespace deployment-9898 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    May  9 13:38:50.928: INFO: Deployment test-deployment-cvcs6 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 13:38:50.931: INFO: Deployment "test-deployment-cvcs6":
    &Deployment{ObjectMeta:{test-deployment-cvcs6  deployment-9898  388e12bf-3027-4f70-b779-fe5c2b159eef 44745 1 2023-05-09 13:38:48 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-09 13:38:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-09 13:38:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-09 13:38:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034ab848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-cvcs6-777898ffcc",LastUpdateTime:2023-05-09 13:38:50 +0000 UTC,LastTransitionTime:2023-05-09 13:38:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  9 13:38:50.933: INFO: New ReplicaSet "test-deployment-cvcs6-777898ffcc" of Deployment "test-deployment-cvcs6":
    &ReplicaSet{ObjectMeta:{test-deployment-cvcs6-777898ffcc  deployment-9898  2fa17362-5622-4fef-af8e-3704d20b8b0c 44738 1 2023-05-09 13:38:48 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-cvcs6 388e12bf-3027-4f70-b779-fe5c2b159eef 0xc0034abf00 0xc0034abf01}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:38:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"388e12bf-3027-4f70-b779-fe5c2b159eef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:38:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034abfa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  9 13:38:50.938: INFO: Pod "test-deployment-cvcs6-777898ffcc-7j9wc" is available:
    &Pod{ObjectMeta:{test-deployment-cvcs6-777898ffcc-7j9wc test-deployment-cvcs6-777898ffcc- deployment-9898  4c26e564-4a5c-4c84-a096-e9ceabeefbfb 44737 0 2023-05-09 13:38:48 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:ff7e54b0eb4e709b603aa2aeba93a225d9fe726e952883296aa109ddb45dac14 cni.projectcalico.org/podIP:172.25.53.139/32 cni.projectcalico.org/podIPs:172.25.53.139/32] [{apps/v1 ReplicaSet test-deployment-cvcs6-777898ffcc 2fa17362-5622-4fef-af8e-3704d20b8b0c 0xc0034c23b0 0xc0034c23b1}] [] [{kube-controller-manager Update v1 2023-05-09 13:38:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fa17362-5622-4fef-af8e-3704d20b8b0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 13:38:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 13:38:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.53.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4wgrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4wgrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-879bk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:38:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:38:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:38:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.73,PodIP:172.25.53.139,StartTime:2023-05-09 13:38:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:38:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b4263403a15d26e914cc88e9ae43744c085dffe8ba37717f36cfce6cdc3f9da6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.53.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  9 13:38:50.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9898" for this suite. 05/09/23 13:38:50.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:38:50.945
May  9 13:38:50.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename tables 05/09/23 13:38:50.946
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:50.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:50.962
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
May  9 13:38:50.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7545" for this suite. 05/09/23 13:38:50.968
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":302,"skipped":5786,"failed":0}
------------------------------
• [0.027 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:38:50.945
    May  9 13:38:50.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename tables 05/09/23 13:38:50.946
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:50.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:50.962
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    May  9 13:38:50.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-7545" for this suite. 05/09/23 13:38:50.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:38:50.973
May  9 13:38:50.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename job 05/09/23 13:38:50.973
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:50.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:50.989
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 05/09/23 13:38:50.99
STEP: Ensuring job reaches completions 05/09/23 13:38:50.996
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  9 13:39:00.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6617" for this suite. 05/09/23 13:39:01.002
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":303,"skipped":5795,"failed":0}
------------------------------
• [SLOW TEST] [10.038 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:38:50.973
    May  9 13:38:50.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename job 05/09/23 13:38:50.973
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:38:50.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:38:50.989
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 05/09/23 13:38:50.99
    STEP: Ensuring job reaches completions 05/09/23 13:38:50.996
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  9 13:39:00.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6617" for this suite. 05/09/23 13:39:01.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:39:01.011
May  9 13:39:01.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 13:39:01.012
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:01.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:01.026
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
May  9 13:39:01.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:39:02.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1949" for this suite. 05/09/23 13:39:02.073
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":304,"skipped":5808,"failed":0}
------------------------------
• [1.067 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:39:01.011
    May  9 13:39:01.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 13:39:01.012
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:01.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:01.026
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    May  9 13:39:01.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:39:02.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1949" for this suite. 05/09/23 13:39:02.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:39:02.078
May  9 13:39:02.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename resourcequota 05/09/23 13:39:02.079
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:02.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:02.093
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 05/09/23 13:39:02.095
STEP: Counting existing ResourceQuota 05/09/23 13:39:07.098
STEP: Creating a ResourceQuota 05/09/23 13:39:12.101
STEP: Ensuring resource quota status is calculated 05/09/23 13:39:12.105
STEP: Creating a Secret 05/09/23 13:39:14.108
STEP: Ensuring resource quota status captures secret creation 05/09/23 13:39:14.118
STEP: Deleting a secret 05/09/23 13:39:16.122
STEP: Ensuring resource quota status released usage 05/09/23 13:39:16.126
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  9 13:39:18.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4670" for this suite. 05/09/23 13:39:18.134
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":305,"skipped":5816,"failed":0}
------------------------------
• [SLOW TEST] [16.061 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:39:02.078
    May  9 13:39:02.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename resourcequota 05/09/23 13:39:02.079
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:02.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:02.093
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 05/09/23 13:39:02.095
    STEP: Counting existing ResourceQuota 05/09/23 13:39:07.098
    STEP: Creating a ResourceQuota 05/09/23 13:39:12.101
    STEP: Ensuring resource quota status is calculated 05/09/23 13:39:12.105
    STEP: Creating a Secret 05/09/23 13:39:14.108
    STEP: Ensuring resource quota status captures secret creation 05/09/23 13:39:14.118
    STEP: Deleting a secret 05/09/23 13:39:16.122
    STEP: Ensuring resource quota status released usage 05/09/23 13:39:16.126
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  9 13:39:18.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4670" for this suite. 05/09/23 13:39:18.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:39:18.14
May  9 13:39:18.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename ingressclass 05/09/23 13:39:18.141
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:18.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:18.155
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 05/09/23 13:39:18.156
STEP: getting /apis/networking.k8s.io 05/09/23 13:39:18.157
STEP: getting /apis/networking.k8s.iov1 05/09/23 13:39:18.157
STEP: creating 05/09/23 13:39:18.158
STEP: getting 05/09/23 13:39:18.171
STEP: listing 05/09/23 13:39:18.172
STEP: watching 05/09/23 13:39:18.174
May  9 13:39:18.174: INFO: starting watch
STEP: patching 05/09/23 13:39:18.174
STEP: updating 05/09/23 13:39:18.178
May  9 13:39:18.181: INFO: waiting for watch events with expected annotations
May  9 13:39:18.181: INFO: saw patched and updated annotations
STEP: deleting 05/09/23 13:39:18.181
STEP: deleting a collection 05/09/23 13:39:18.188
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
May  9 13:39:18.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1562" for this suite. 05/09/23 13:39:18.201
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":306,"skipped":5824,"failed":0}
------------------------------
• [0.066 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:39:18.14
    May  9 13:39:18.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename ingressclass 05/09/23 13:39:18.141
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:18.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:18.155
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 05/09/23 13:39:18.156
    STEP: getting /apis/networking.k8s.io 05/09/23 13:39:18.157
    STEP: getting /apis/networking.k8s.iov1 05/09/23 13:39:18.157
    STEP: creating 05/09/23 13:39:18.158
    STEP: getting 05/09/23 13:39:18.171
    STEP: listing 05/09/23 13:39:18.172
    STEP: watching 05/09/23 13:39:18.174
    May  9 13:39:18.174: INFO: starting watch
    STEP: patching 05/09/23 13:39:18.174
    STEP: updating 05/09/23 13:39:18.178
    May  9 13:39:18.181: INFO: waiting for watch events with expected annotations
    May  9 13:39:18.181: INFO: saw patched and updated annotations
    STEP: deleting 05/09/23 13:39:18.181
    STEP: deleting a collection 05/09/23 13:39:18.188
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    May  9 13:39:18.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-1562" for this suite. 05/09/23 13:39:18.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:39:18.207
May  9 13:39:18.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:39:18.207
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:18.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:18.219
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:39:18.22
May  9 13:39:18.227: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe" in namespace "projected-3743" to be "Succeeded or Failed"
May  9 13:39:18.232: INFO: Pod "downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.100772ms
May  9 13:39:20.236: INFO: Pod "downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008543284s
May  9 13:39:22.236: INFO: Pod "downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008727041s
STEP: Saw pod success 05/09/23 13:39:22.236
May  9 13:39:22.236: INFO: Pod "downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe" satisfied condition "Succeeded or Failed"
May  9 13:39:22.238: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe container client-container: <nil>
STEP: delete the pod 05/09/23 13:39:22.243
May  9 13:39:22.254: INFO: Waiting for pod downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe to disappear
May  9 13:39:22.256: INFO: Pod downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  9 13:39:22.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3743" for this suite. 05/09/23 13:39:22.259
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":307,"skipped":5863,"failed":0}
------------------------------
• [4.056 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:39:18.207
    May  9 13:39:18.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:39:18.207
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:18.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:18.219
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:39:18.22
    May  9 13:39:18.227: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe" in namespace "projected-3743" to be "Succeeded or Failed"
    May  9 13:39:18.232: INFO: Pod "downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.100772ms
    May  9 13:39:20.236: INFO: Pod "downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008543284s
    May  9 13:39:22.236: INFO: Pod "downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008727041s
    STEP: Saw pod success 05/09/23 13:39:22.236
    May  9 13:39:22.236: INFO: Pod "downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe" satisfied condition "Succeeded or Failed"
    May  9 13:39:22.238: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe container client-container: <nil>
    STEP: delete the pod 05/09/23 13:39:22.243
    May  9 13:39:22.254: INFO: Waiting for pod downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe to disappear
    May  9 13:39:22.256: INFO: Pod downwardapi-volume-fac8667b-93ca-42c3-b753-9d3e12bb1cbe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  9 13:39:22.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3743" for this suite. 05/09/23 13:39:22.259
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:39:22.263
May  9 13:39:22.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 13:39:22.264
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:22.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:22.281
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-2e4c05f2-6bb8-4c83-bbab-de41b9a18d71 05/09/23 13:39:22.286
STEP: Creating secret with name s-test-opt-upd-d25a43be-1f26-4a1a-968a-013430ccc613 05/09/23 13:39:22.289
STEP: Creating the pod 05/09/23 13:39:22.292
May  9 13:39:22.299: INFO: Waiting up to 5m0s for pod "pod-secrets-61ac7a5c-6d4b-4ce4-bf79-e1abf8e4e88a" in namespace "secrets-8747" to be "running and ready"
May  9 13:39:22.305: INFO: Pod "pod-secrets-61ac7a5c-6d4b-4ce4-bf79-e1abf8e4e88a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.634728ms
May  9 13:39:22.305: INFO: The phase of Pod pod-secrets-61ac7a5c-6d4b-4ce4-bf79-e1abf8e4e88a is Pending, waiting for it to be Running (with Ready = true)
May  9 13:39:24.308: INFO: Pod "pod-secrets-61ac7a5c-6d4b-4ce4-bf79-e1abf8e4e88a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00950239s
May  9 13:39:24.309: INFO: The phase of Pod pod-secrets-61ac7a5c-6d4b-4ce4-bf79-e1abf8e4e88a is Running (Ready = true)
May  9 13:39:24.309: INFO: Pod "pod-secrets-61ac7a5c-6d4b-4ce4-bf79-e1abf8e4e88a" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-2e4c05f2-6bb8-4c83-bbab-de41b9a18d71 05/09/23 13:39:24.325
STEP: Updating secret s-test-opt-upd-d25a43be-1f26-4a1a-968a-013430ccc613 05/09/23 13:39:24.33
STEP: Creating secret with name s-test-opt-create-6f3d07bf-8e4e-4b75-a729-3d39f408b5ce 05/09/23 13:39:24.333
STEP: waiting to observe update in volume 05/09/23 13:39:24.337
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  9 13:39:26.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8747" for this suite. 05/09/23 13:39:26.357
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":308,"skipped":5863,"failed":0}
------------------------------
• [4.099 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:39:22.263
    May  9 13:39:22.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 13:39:22.264
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:22.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:22.281
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-2e4c05f2-6bb8-4c83-bbab-de41b9a18d71 05/09/23 13:39:22.286
    STEP: Creating secret with name s-test-opt-upd-d25a43be-1f26-4a1a-968a-013430ccc613 05/09/23 13:39:22.289
    STEP: Creating the pod 05/09/23 13:39:22.292
    May  9 13:39:22.299: INFO: Waiting up to 5m0s for pod "pod-secrets-61ac7a5c-6d4b-4ce4-bf79-e1abf8e4e88a" in namespace "secrets-8747" to be "running and ready"
    May  9 13:39:22.305: INFO: Pod "pod-secrets-61ac7a5c-6d4b-4ce4-bf79-e1abf8e4e88a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.634728ms
    May  9 13:39:22.305: INFO: The phase of Pod pod-secrets-61ac7a5c-6d4b-4ce4-bf79-e1abf8e4e88a is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:39:24.308: INFO: Pod "pod-secrets-61ac7a5c-6d4b-4ce4-bf79-e1abf8e4e88a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00950239s
    May  9 13:39:24.309: INFO: The phase of Pod pod-secrets-61ac7a5c-6d4b-4ce4-bf79-e1abf8e4e88a is Running (Ready = true)
    May  9 13:39:24.309: INFO: Pod "pod-secrets-61ac7a5c-6d4b-4ce4-bf79-e1abf8e4e88a" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-2e4c05f2-6bb8-4c83-bbab-de41b9a18d71 05/09/23 13:39:24.325
    STEP: Updating secret s-test-opt-upd-d25a43be-1f26-4a1a-968a-013430ccc613 05/09/23 13:39:24.33
    STEP: Creating secret with name s-test-opt-create-6f3d07bf-8e4e-4b75-a729-3d39f408b5ce 05/09/23 13:39:24.333
    STEP: waiting to observe update in volume 05/09/23 13:39:24.337
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  9 13:39:26.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8747" for this suite. 05/09/23 13:39:26.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:39:26.365
May  9 13:39:26.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename job 05/09/23 13:39:26.366
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:26.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:26.38
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 05/09/23 13:39:26.381
STEP: Ensuring active pods == parallelism 05/09/23 13:39:26.394
STEP: Orphaning one of the Job's Pods 05/09/23 13:39:28.397
May  9 13:39:28.909: INFO: Successfully updated pod "adopt-release-5j54r"
STEP: Checking that the Job readopts the Pod 05/09/23 13:39:28.909
May  9 13:39:28.909: INFO: Waiting up to 15m0s for pod "adopt-release-5j54r" in namespace "job-2124" to be "adopted"
May  9 13:39:28.912: INFO: Pod "adopt-release-5j54r": Phase="Running", Reason="", readiness=true. Elapsed: 2.410921ms
May  9 13:39:30.916: INFO: Pod "adopt-release-5j54r": Phase="Running", Reason="", readiness=true. Elapsed: 2.006787115s
May  9 13:39:30.916: INFO: Pod "adopt-release-5j54r" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 05/09/23 13:39:30.916
May  9 13:39:31.429: INFO: Successfully updated pod "adopt-release-5j54r"
STEP: Checking that the Job releases the Pod 05/09/23 13:39:31.429
May  9 13:39:31.429: INFO: Waiting up to 15m0s for pod "adopt-release-5j54r" in namespace "job-2124" to be "released"
May  9 13:39:31.431: INFO: Pod "adopt-release-5j54r": Phase="Running", Reason="", readiness=true. Elapsed: 2.036387ms
May  9 13:39:33.434: INFO: Pod "adopt-release-5j54r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005105276s
May  9 13:39:33.434: INFO: Pod "adopt-release-5j54r" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  9 13:39:33.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2124" for this suite. 05/09/23 13:39:33.437
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":309,"skipped":5910,"failed":0}
------------------------------
• [SLOW TEST] [7.077 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:39:26.365
    May  9 13:39:26.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename job 05/09/23 13:39:26.366
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:26.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:26.38
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 05/09/23 13:39:26.381
    STEP: Ensuring active pods == parallelism 05/09/23 13:39:26.394
    STEP: Orphaning one of the Job's Pods 05/09/23 13:39:28.397
    May  9 13:39:28.909: INFO: Successfully updated pod "adopt-release-5j54r"
    STEP: Checking that the Job readopts the Pod 05/09/23 13:39:28.909
    May  9 13:39:28.909: INFO: Waiting up to 15m0s for pod "adopt-release-5j54r" in namespace "job-2124" to be "adopted"
    May  9 13:39:28.912: INFO: Pod "adopt-release-5j54r": Phase="Running", Reason="", readiness=true. Elapsed: 2.410921ms
    May  9 13:39:30.916: INFO: Pod "adopt-release-5j54r": Phase="Running", Reason="", readiness=true. Elapsed: 2.006787115s
    May  9 13:39:30.916: INFO: Pod "adopt-release-5j54r" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 05/09/23 13:39:30.916
    May  9 13:39:31.429: INFO: Successfully updated pod "adopt-release-5j54r"
    STEP: Checking that the Job releases the Pod 05/09/23 13:39:31.429
    May  9 13:39:31.429: INFO: Waiting up to 15m0s for pod "adopt-release-5j54r" in namespace "job-2124" to be "released"
    May  9 13:39:31.431: INFO: Pod "adopt-release-5j54r": Phase="Running", Reason="", readiness=true. Elapsed: 2.036387ms
    May  9 13:39:33.434: INFO: Pod "adopt-release-5j54r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005105276s
    May  9 13:39:33.434: INFO: Pod "adopt-release-5j54r" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  9 13:39:33.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2124" for this suite. 05/09/23 13:39:33.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:39:33.443
May  9 13:39:33.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 13:39:33.444
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:33.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:33.458
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-a7dde15b-915a-4864-83d3-9bb9e8cf927d 05/09/23 13:39:33.462
STEP: Creating the pod 05/09/23 13:39:33.467
May  9 13:39:33.473: INFO: Waiting up to 5m0s for pod "pod-configmaps-66c5b1bd-e88c-401a-9ae7-923e262f6dd9" in namespace "configmap-468" to be "running and ready"
May  9 13:39:33.479: INFO: Pod "pod-configmaps-66c5b1bd-e88c-401a-9ae7-923e262f6dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.870025ms
May  9 13:39:33.479: INFO: The phase of Pod pod-configmaps-66c5b1bd-e88c-401a-9ae7-923e262f6dd9 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:39:35.482: INFO: Pod "pod-configmaps-66c5b1bd-e88c-401a-9ae7-923e262f6dd9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009181456s
May  9 13:39:35.482: INFO: The phase of Pod pod-configmaps-66c5b1bd-e88c-401a-9ae7-923e262f6dd9 is Running (Ready = true)
May  9 13:39:35.482: INFO: Pod "pod-configmaps-66c5b1bd-e88c-401a-9ae7-923e262f6dd9" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-a7dde15b-915a-4864-83d3-9bb9e8cf927d 05/09/23 13:39:35.488
STEP: waiting to observe update in volume 05/09/23 13:39:35.493
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  9 13:39:37.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-468" for this suite. 05/09/23 13:39:37.506
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":310,"skipped":5922,"failed":0}
------------------------------
• [4.069 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:39:33.443
    May  9 13:39:33.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 13:39:33.444
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:33.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:33.458
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-a7dde15b-915a-4864-83d3-9bb9e8cf927d 05/09/23 13:39:33.462
    STEP: Creating the pod 05/09/23 13:39:33.467
    May  9 13:39:33.473: INFO: Waiting up to 5m0s for pod "pod-configmaps-66c5b1bd-e88c-401a-9ae7-923e262f6dd9" in namespace "configmap-468" to be "running and ready"
    May  9 13:39:33.479: INFO: Pod "pod-configmaps-66c5b1bd-e88c-401a-9ae7-923e262f6dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.870025ms
    May  9 13:39:33.479: INFO: The phase of Pod pod-configmaps-66c5b1bd-e88c-401a-9ae7-923e262f6dd9 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:39:35.482: INFO: Pod "pod-configmaps-66c5b1bd-e88c-401a-9ae7-923e262f6dd9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009181456s
    May  9 13:39:35.482: INFO: The phase of Pod pod-configmaps-66c5b1bd-e88c-401a-9ae7-923e262f6dd9 is Running (Ready = true)
    May  9 13:39:35.482: INFO: Pod "pod-configmaps-66c5b1bd-e88c-401a-9ae7-923e262f6dd9" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-a7dde15b-915a-4864-83d3-9bb9e8cf927d 05/09/23 13:39:35.488
    STEP: waiting to observe update in volume 05/09/23 13:39:35.493
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 13:39:37.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-468" for this suite. 05/09/23 13:39:37.506
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:39:37.512
May  9 13:39:37.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename init-container 05/09/23 13:39:37.513
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:37.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:37.528
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 05/09/23 13:39:37.529
May  9 13:39:37.529: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May  9 13:39:42.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5614" for this suite. 05/09/23 13:39:42.481
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":311,"skipped":5924,"failed":0}
------------------------------
• [4.976 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:39:37.512
    May  9 13:39:37.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename init-container 05/09/23 13:39:37.513
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:37.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:37.528
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 05/09/23 13:39:37.529
    May  9 13:39:37.529: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May  9 13:39:42.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5614" for this suite. 05/09/23 13:39:42.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:39:42.489
May  9 13:39:42.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename secrets 05/09/23 13:39:42.49
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:42.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:42.504
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-19e2d770-da1e-41f9-8a69-31b3833adb8b 05/09/23 13:39:42.506
STEP: Creating a pod to test consume secrets 05/09/23 13:39:42.513
May  9 13:39:42.520: INFO: Waiting up to 5m0s for pod "pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4" in namespace "secrets-9816" to be "Succeeded or Failed"
May  9 13:39:42.525: INFO: Pod "pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036916ms
May  9 13:39:44.527: INFO: Pod "pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4": Phase="Running", Reason="", readiness=false. Elapsed: 2.006987445s
May  9 13:39:46.529: INFO: Pod "pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008674217s
STEP: Saw pod success 05/09/23 13:39:46.529
May  9 13:39:46.529: INFO: Pod "pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4" satisfied condition "Succeeded or Failed"
May  9 13:39:46.531: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4 container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 13:39:46.535
May  9 13:39:46.546: INFO: Waiting for pod pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4 to disappear
May  9 13:39:46.548: INFO: Pod pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  9 13:39:46.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9816" for this suite. 05/09/23 13:39:46.551
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":312,"skipped":5937,"failed":0}
------------------------------
• [4.066 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:39:42.489
    May  9 13:39:42.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename secrets 05/09/23 13:39:42.49
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:42.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:42.504
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-19e2d770-da1e-41f9-8a69-31b3833adb8b 05/09/23 13:39:42.506
    STEP: Creating a pod to test consume secrets 05/09/23 13:39:42.513
    May  9 13:39:42.520: INFO: Waiting up to 5m0s for pod "pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4" in namespace "secrets-9816" to be "Succeeded or Failed"
    May  9 13:39:42.525: INFO: Pod "pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036916ms
    May  9 13:39:44.527: INFO: Pod "pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4": Phase="Running", Reason="", readiness=false. Elapsed: 2.006987445s
    May  9 13:39:46.529: INFO: Pod "pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008674217s
    STEP: Saw pod success 05/09/23 13:39:46.529
    May  9 13:39:46.529: INFO: Pod "pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4" satisfied condition "Succeeded or Failed"
    May  9 13:39:46.531: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4 container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:39:46.535
    May  9 13:39:46.546: INFO: Waiting for pod pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4 to disappear
    May  9 13:39:46.548: INFO: Pod pod-secrets-3ddbddb2-946e-46da-8ac6-454e575042b4 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  9 13:39:46.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9816" for this suite. 05/09/23 13:39:46.551
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:39:46.556
May  9 13:39:46.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename sched-preemption 05/09/23 13:39:46.556
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:46.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:46.57
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May  9 13:39:46.584: INFO: Waiting up to 1m0s for all nodes to be ready
May  9 13:40:46.621: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 05/09/23 13:40:46.623
May  9 13:40:46.640: INFO: Created pod: pod0-0-sched-preemption-low-priority
May  9 13:40:46.651: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May  9 13:40:46.680: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May  9 13:40:46.691: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May  9 13:40:46.704: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May  9 13:40:46.711: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/09/23 13:40:46.711
May  9 13:40:46.711: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4107" to be "running"
May  9 13:40:46.714: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.193971ms
May  9 13:40:48.717: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006154305s
May  9 13:40:50.719: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008135539s
May  9 13:40:52.717: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006373285s
May  9 13:40:54.717: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.00634311s
May  9 13:40:54.717: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May  9 13:40:54.717: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4107" to be "running"
May  9 13:40:54.720: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.431569ms
May  9 13:40:54.720: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May  9 13:40:54.720: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4107" to be "running"
May  9 13:40:54.722: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.876282ms
May  9 13:40:56.724: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.00440386s
May  9 13:40:56.724: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May  9 13:40:56.724: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4107" to be "running"
May  9 13:40:56.726: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.730966ms
May  9 13:40:56.726: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May  9 13:40:56.726: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4107" to be "running"
May  9 13:40:56.728: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.628761ms
May  9 13:40:56.728: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May  9 13:40:56.728: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4107" to be "running"
May  9 13:40:56.730: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.779918ms
May  9 13:40:56.730: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 05/09/23 13:40:56.73
May  9 13:40:56.737: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
May  9 13:40:56.739: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.849731ms
May  9 13:40:58.743: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005772051s
May  9 13:41:00.743: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005301157s
May  9 13:41:00.743: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May  9 13:41:00.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4107" for this suite. 05/09/23 13:41:00.77
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":313,"skipped":5938,"failed":0}
------------------------------
• [SLOW TEST] [74.249 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:39:46.556
    May  9 13:39:46.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename sched-preemption 05/09/23 13:39:46.556
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:39:46.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:39:46.57
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May  9 13:39:46.584: INFO: Waiting up to 1m0s for all nodes to be ready
    May  9 13:40:46.621: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 05/09/23 13:40:46.623
    May  9 13:40:46.640: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May  9 13:40:46.651: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May  9 13:40:46.680: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May  9 13:40:46.691: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May  9 13:40:46.704: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May  9 13:40:46.711: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/09/23 13:40:46.711
    May  9 13:40:46.711: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4107" to be "running"
    May  9 13:40:46.714: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.193971ms
    May  9 13:40:48.717: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006154305s
    May  9 13:40:50.719: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008135539s
    May  9 13:40:52.717: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006373285s
    May  9 13:40:54.717: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.00634311s
    May  9 13:40:54.717: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May  9 13:40:54.717: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4107" to be "running"
    May  9 13:40:54.720: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.431569ms
    May  9 13:40:54.720: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May  9 13:40:54.720: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4107" to be "running"
    May  9 13:40:54.722: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.876282ms
    May  9 13:40:56.724: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.00440386s
    May  9 13:40:56.724: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May  9 13:40:56.724: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4107" to be "running"
    May  9 13:40:56.726: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.730966ms
    May  9 13:40:56.726: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May  9 13:40:56.726: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4107" to be "running"
    May  9 13:40:56.728: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.628761ms
    May  9 13:40:56.728: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May  9 13:40:56.728: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4107" to be "running"
    May  9 13:40:56.730: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.779918ms
    May  9 13:40:56.730: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 05/09/23 13:40:56.73
    May  9 13:40:56.737: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    May  9 13:40:56.739: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.849731ms
    May  9 13:40:58.743: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005772051s
    May  9 13:41:00.743: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005301157s
    May  9 13:41:00.743: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:41:00.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4107" for this suite. 05/09/23 13:41:00.77
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:41:00.805
May  9 13:41:00.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pods 05/09/23 13:41:00.805
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:41:00.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:41:00.82
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 05/09/23 13:41:00.821
May  9 13:41:00.827: INFO: Waiting up to 5m0s for pod "pod-n2gvc" in namespace "pods-8774" to be "running"
May  9 13:41:00.831: INFO: Pod "pod-n2gvc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568504ms
May  9 13:41:02.835: INFO: Pod "pod-n2gvc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008062683s
May  9 13:41:02.835: INFO: Pod "pod-n2gvc" satisfied condition "running"
STEP: patching /status 05/09/23 13:41:02.835
May  9 13:41:02.842: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  9 13:41:02.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8774" for this suite. 05/09/23 13:41:02.845
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":314,"skipped":5942,"failed":0}
------------------------------
• [2.045 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:41:00.805
    May  9 13:41:00.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pods 05/09/23 13:41:00.805
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:41:00.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:41:00.82
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 05/09/23 13:41:00.821
    May  9 13:41:00.827: INFO: Waiting up to 5m0s for pod "pod-n2gvc" in namespace "pods-8774" to be "running"
    May  9 13:41:00.831: INFO: Pod "pod-n2gvc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568504ms
    May  9 13:41:02.835: INFO: Pod "pod-n2gvc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008062683s
    May  9 13:41:02.835: INFO: Pod "pod-n2gvc" satisfied condition "running"
    STEP: patching /status 05/09/23 13:41:02.835
    May  9 13:41:02.842: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  9 13:41:02.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8774" for this suite. 05/09/23 13:41:02.845
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:41:02.85
May  9 13:41:02.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename resourcequota 05/09/23 13:41:02.85
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:41:02.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:41:02.865
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 05/09/23 13:41:02.866
STEP: Ensuring ResourceQuota status is calculated 05/09/23 13:41:02.87
STEP: Creating a ResourceQuota with not terminating scope 05/09/23 13:41:04.873
STEP: Ensuring ResourceQuota status is calculated 05/09/23 13:41:04.878
STEP: Creating a long running pod 05/09/23 13:41:06.884
STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/09/23 13:41:06.912
STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/09/23 13:41:08.915
STEP: Deleting the pod 05/09/23 13:41:10.921
STEP: Ensuring resource quota status released the pod usage 05/09/23 13:41:10.932
STEP: Creating a terminating pod 05/09/23 13:41:12.936
STEP: Ensuring resource quota with terminating scope captures the pod usage 05/09/23 13:41:12.948
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/09/23 13:41:14.951
STEP: Deleting the pod 05/09/23 13:41:16.955
STEP: Ensuring resource quota status released the pod usage 05/09/23 13:41:16.964
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  9 13:41:18.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-868" for this suite. 05/09/23 13:41:18.97
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":315,"skipped":5942,"failed":0}
------------------------------
• [SLOW TEST] [16.126 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:41:02.85
    May  9 13:41:02.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename resourcequota 05/09/23 13:41:02.85
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:41:02.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:41:02.865
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 05/09/23 13:41:02.866
    STEP: Ensuring ResourceQuota status is calculated 05/09/23 13:41:02.87
    STEP: Creating a ResourceQuota with not terminating scope 05/09/23 13:41:04.873
    STEP: Ensuring ResourceQuota status is calculated 05/09/23 13:41:04.878
    STEP: Creating a long running pod 05/09/23 13:41:06.884
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/09/23 13:41:06.912
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/09/23 13:41:08.915
    STEP: Deleting the pod 05/09/23 13:41:10.921
    STEP: Ensuring resource quota status released the pod usage 05/09/23 13:41:10.932
    STEP: Creating a terminating pod 05/09/23 13:41:12.936
    STEP: Ensuring resource quota with terminating scope captures the pod usage 05/09/23 13:41:12.948
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/09/23 13:41:14.951
    STEP: Deleting the pod 05/09/23 13:41:16.955
    STEP: Ensuring resource quota status released the pod usage 05/09/23 13:41:16.964
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  9 13:41:18.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-868" for this suite. 05/09/23 13:41:18.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:41:18.977
May  9 13:41:18.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename sched-pred 05/09/23 13:41:18.977
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:41:18.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:41:18.992
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May  9 13:41:18.993: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  9 13:41:18.998: INFO: Waiting for terminating namespaces to be deleted...
May  9 13:41:19.000: INFO: 
Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-48ljh before test
May  9 13:41:19.005: INFO: calico-node-fdkdq from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.005: INFO: 	Container calico-node ready: true, restart count 0
May  9 13:41:19.005: INFO: csi-cinder-nodeplugin-f688q from kube-system started at 2023-05-09 11:59:12 +0000 UTC (3 container statuses recorded)
May  9 13:41:19.005: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May  9 13:41:19.005: INFO: 	Container liveness-probe ready: true, restart count 0
May  9 13:41:19.005: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  9 13:41:19.005: INFO: kube-proxy-lbgl4 from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.005: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 13:41:19.005: INFO: node-local-dns-vg5zf from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.005: INFO: 	Container node-cache ready: true, restart count 0
May  9 13:41:19.005: INFO: sonobuoy from sonobuoy started at 2023-05-09 12:27:44 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.005: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  9 13:41:19.005: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-nvk78 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:41:19.005: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:41:19.005: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 13:41:19.005: INFO: 
Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-879bk before test
May  9 13:41:19.010: INFO: calico-node-gdlh8 from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.010: INFO: 	Container calico-node ready: true, restart count 0
May  9 13:41:19.010: INFO: csi-cinder-nodeplugin-hzf44 from kube-system started at 2023-05-09 11:59:09 +0000 UTC (3 container statuses recorded)
May  9 13:41:19.010: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May  9 13:41:19.010: INFO: 	Container liveness-probe ready: true, restart count 0
May  9 13:41:19.010: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  9 13:41:19.010: INFO: kube-proxy-pcp2c from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.010: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 13:41:19.010: INFO: node-local-dns-hvfcl from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.010: INFO: 	Container node-cache ready: true, restart count 0
May  9 13:41:19.010: INFO: sonobuoy-e2e-job-588b06ca4a824bd1 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:41:19.010: INFO: 	Container e2e ready: true, restart count 0
May  9 13:41:19.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:41:19.010: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-6xh5z from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:41:19.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:41:19.010: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 13:41:19.010: INFO: 
Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-skqqr before test
May  9 13:41:19.015: INFO: calico-kube-controllers-578488968b-2nchl from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.015: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  9 13:41:19.015: INFO: calico-node-fm8kr from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.015: INFO: 	Container calico-node ready: true, restart count 0
May  9 13:41:19.015: INFO: coredns-677bf479b7-t4l9l from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.015: INFO: 	Container coredns ready: true, restart count 0
May  9 13:41:19.015: INFO: coredns-677bf479b7-t7hjx from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.015: INFO: 	Container coredns ready: true, restart count 0
May  9 13:41:19.015: INFO: csi-cinder-nodeplugin-f82jf from kube-system started at 2023-05-09 11:59:06 +0000 UTC (3 container statuses recorded)
May  9 13:41:19.015: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May  9 13:41:19.015: INFO: 	Container liveness-probe ready: true, restart count 0
May  9 13:41:19.015: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  9 13:41:19.015: INFO: kube-proxy-d9lqj from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.015: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 13:41:19.015: INFO: metrics-server-5974658d45-zx6c9 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.015: INFO: 	Container metrics-server ready: true, restart count 0
May  9 13:41:19.015: INFO: node-local-dns-hfgs9 from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.015: INFO: 	Container node-cache ready: true, restart count 0
May  9 13:41:19.015: INFO: snapshot-controller-677c67cc64-78dwg from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.015: INFO: 	Container snapshot-controller ready: true, restart count 0
May  9 13:41:19.015: INFO: snapshot-controller-677c67cc64-jrp67 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:41:19.015: INFO: 	Container snapshot-controller ready: true, restart count 0
May  9 13:41:19.015: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-67f2c from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:41:19.015: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:41:19.015: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/09/23 13:41:19.016
May  9 13:41:19.023: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9134" to be "running"
May  9 13:41:19.025: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.640203ms
May  9 13:41:21.029: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005659398s
May  9 13:41:21.029: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/09/23 13:41:21.03
STEP: Trying to apply a random label on the found node. 05/09/23 13:41:21.041
STEP: verifying the node has the label kubernetes.io/e2e-394463f0-69f5-46b5-81ac-abab6577df6c 95 05/09/23 13:41:21.05
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/09/23 13:41:21.052
May  9 13:41:21.057: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-9134" to be "not pending"
May  9 13:41:21.059: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.819545ms
May  9 13:41:23.063: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005800957s
May  9 13:41:23.063: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.1.64 on the node which pod4 resides and expect not scheduled 05/09/23 13:41:23.063
May  9 13:41:23.067: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-9134" to be "not pending"
May  9 13:41:23.077: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.32803ms
May  9 13:41:25.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014493486s
May  9 13:41:27.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01233188s
May  9 13:41:29.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013067551s
May  9 13:41:31.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012352078s
May  9 13:41:33.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013565756s
May  9 13:41:35.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012544499s
May  9 13:41:37.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012668699s
May  9 13:41:39.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013261562s
May  9 13:41:41.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013722994s
May  9 13:41:43.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.012993408s
May  9 13:41:45.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014009231s
May  9 13:41:47.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.013632284s
May  9 13:41:49.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012533088s
May  9 13:41:51.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013251363s
May  9 13:41:53.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.013017614s
May  9 13:41:55.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.013527019s
May  9 13:41:57.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012319582s
May  9 13:41:59.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.013976449s
May  9 13:42:01.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012862249s
May  9 13:42:03.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.012728633s
May  9 13:42:05.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.013173348s
May  9 13:42:07.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.012356549s
May  9 13:42:09.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012772455s
May  9 13:42:11.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.014688637s
May  9 13:42:13.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013050799s
May  9 13:42:15.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012133953s
May  9 13:42:17.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.012221337s
May  9 13:42:19.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012509573s
May  9 13:42:21.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012110498s
May  9 13:42:23.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012369834s
May  9 13:42:25.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.012002034s
May  9 13:42:27.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.012382419s
May  9 13:42:29.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.014034191s
May  9 13:42:31.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.012028278s
May  9 13:42:33.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011858668s
May  9 13:42:35.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012212031s
May  9 13:42:37.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011980558s
May  9 13:42:39.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.012231312s
May  9 13:42:41.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.012257968s
May  9 13:42:43.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012683502s
May  9 13:42:45.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011915702s
May  9 13:42:47.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012877643s
May  9 13:42:49.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012474783s
May  9 13:42:51.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.012301928s
May  9 13:42:53.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.013338414s
May  9 13:42:55.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.013690845s
May  9 13:42:57.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.012251362s
May  9 13:42:59.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.014175442s
May  9 13:43:01.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.012355091s
May  9 13:43:03.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013178059s
May  9 13:43:05.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.01237641s
May  9 13:43:07.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012634607s
May  9 13:43:09.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.012354826s
May  9 13:43:11.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012497739s
May  9 13:43:13.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01318709s
May  9 13:43:15.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.01368328s
May  9 13:43:17.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012564223s
May  9 13:43:19.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012232332s
May  9 13:43:21.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.014064267s
May  9 13:43:23.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013218743s
May  9 13:43:25.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.013036178s
May  9 13:43:27.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.012241825s
May  9 13:43:29.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.013707133s
May  9 13:43:31.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.012112481s
May  9 13:43:33.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.012487788s
May  9 13:43:35.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.013062811s
May  9 13:43:37.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.012622467s
May  9 13:43:39.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.012951636s
May  9 13:43:41.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.013157978s
May  9 13:43:43.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.012937154s
May  9 13:43:45.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.013554413s
May  9 13:43:47.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.012313921s
May  9 13:43:49.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.012847405s
May  9 13:43:51.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.012081012s
May  9 13:43:53.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.011814296s
May  9 13:43:55.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.012396904s
May  9 13:43:57.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.012039208s
May  9 13:43:59.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.01340468s
May  9 13:44:01.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.012167674s
May  9 13:44:03.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.012334227s
May  9 13:44:05.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.012761905s
May  9 13:44:07.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.012498152s
May  9 13:44:09.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.01291958s
May  9 13:44:11.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.01209883s
May  9 13:44:13.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.012351451s
May  9 13:44:15.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.012161998s
May  9 13:44:17.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.012619752s
May  9 13:44:19.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.012426455s
May  9 13:44:21.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.013984739s
May  9 13:44:23.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.012504334s
May  9 13:44:25.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.013367377s
May  9 13:44:27.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.011774429s
May  9 13:44:29.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.012315801s
May  9 13:44:31.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.012382727s
May  9 13:44:33.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.013418944s
May  9 13:44:35.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.012360599s
May  9 13:44:37.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.012792538s
May  9 13:44:39.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.012868778s
May  9 13:44:41.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.014365688s
May  9 13:44:43.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.013425635s
May  9 13:44:45.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.013177805s
May  9 13:44:47.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.012966792s
May  9 13:44:49.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.014025672s
May  9 13:44:51.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.01213906s
May  9 13:44:53.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.012971962s
May  9 13:44:55.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.0128222s
May  9 13:44:57.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.012860531s
May  9 13:44:59.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.014195088s
May  9 13:45:01.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.012574293s
May  9 13:45:03.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.012593377s
May  9 13:45:05.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.012936142s
May  9 13:45:07.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.012540559s
May  9 13:45:09.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.012456585s
May  9 13:45:11.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.012479039s
May  9 13:45:13.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.013031508s
May  9 13:45:15.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.013407677s
May  9 13:45:17.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.012452748s
May  9 13:45:19.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.014067429s
May  9 13:45:21.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.013495109s
May  9 13:45:23.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.012336809s
May  9 13:45:25.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.014186865s
May  9 13:45:27.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.012373619s
May  9 13:45:29.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.014230991s
May  9 13:45:31.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.012567181s
May  9 13:45:33.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.012229715s
May  9 13:45:35.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.013505424s
May  9 13:45:37.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.012787384s
May  9 13:45:39.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.012909268s
May  9 13:45:41.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.013480453s
May  9 13:45:43.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.012369551s
May  9 13:45:45.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.012723984s
May  9 13:45:47.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.012601816s
May  9 13:45:49.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.013497562s
May  9 13:45:51.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.011978465s
May  9 13:45:53.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.013151641s
May  9 13:45:55.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.013532162s
May  9 13:45:57.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.01197495s
May  9 13:45:59.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.014185327s
May  9 13:46:01.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.012751459s
May  9 13:46:03.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.012959764s
May  9 13:46:05.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.01299717s
May  9 13:46:07.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.012453382s
May  9 13:46:09.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.012606578s
May  9 13:46:11.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.01329075s
May  9 13:46:13.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.012181723s
May  9 13:46:15.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.012470643s
May  9 13:46:17.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.012590997s
May  9 13:46:19.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.012369885s
May  9 13:46:21.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012555214s
May  9 13:46:23.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011932177s
May  9 13:46:23.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.014035993s
STEP: removing the label kubernetes.io/e2e-394463f0-69f5-46b5-81ac-abab6577df6c off the node cl-gks-cncf-ix1-md-0-48ljh 05/09/23 13:46:23.081
STEP: verifying the node doesn't have the label kubernetes.io/e2e-394463f0-69f5-46b5-81ac-abab6577df6c 05/09/23 13:46:23.091
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May  9 13:46:23.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9134" for this suite. 05/09/23 13:46:23.096
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":316,"skipped":5952,"failed":0}
------------------------------
• [SLOW TEST] [304.124 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:41:18.977
    May  9 13:41:18.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename sched-pred 05/09/23 13:41:18.977
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:41:18.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:41:18.992
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May  9 13:41:18.993: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  9 13:41:18.998: INFO: Waiting for terminating namespaces to be deleted...
    May  9 13:41:19.000: INFO: 
    Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-48ljh before test
    May  9 13:41:19.005: INFO: calico-node-fdkdq from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.005: INFO: 	Container calico-node ready: true, restart count 0
    May  9 13:41:19.005: INFO: csi-cinder-nodeplugin-f688q from kube-system started at 2023-05-09 11:59:12 +0000 UTC (3 container statuses recorded)
    May  9 13:41:19.005: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    May  9 13:41:19.005: INFO: 	Container liveness-probe ready: true, restart count 0
    May  9 13:41:19.005: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May  9 13:41:19.005: INFO: kube-proxy-lbgl4 from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.005: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 13:41:19.005: INFO: node-local-dns-vg5zf from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.005: INFO: 	Container node-cache ready: true, restart count 0
    May  9 13:41:19.005: INFO: sonobuoy from sonobuoy started at 2023-05-09 12:27:44 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.005: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  9 13:41:19.005: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-nvk78 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:41:19.005: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:41:19.005: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 13:41:19.005: INFO: 
    Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-879bk before test
    May  9 13:41:19.010: INFO: calico-node-gdlh8 from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.010: INFO: 	Container calico-node ready: true, restart count 0
    May  9 13:41:19.010: INFO: csi-cinder-nodeplugin-hzf44 from kube-system started at 2023-05-09 11:59:09 +0000 UTC (3 container statuses recorded)
    May  9 13:41:19.010: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    May  9 13:41:19.010: INFO: 	Container liveness-probe ready: true, restart count 0
    May  9 13:41:19.010: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May  9 13:41:19.010: INFO: kube-proxy-pcp2c from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.010: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 13:41:19.010: INFO: node-local-dns-hvfcl from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.010: INFO: 	Container node-cache ready: true, restart count 0
    May  9 13:41:19.010: INFO: sonobuoy-e2e-job-588b06ca4a824bd1 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:41:19.010: INFO: 	Container e2e ready: true, restart count 0
    May  9 13:41:19.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:41:19.010: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-6xh5z from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:41:19.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:41:19.010: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 13:41:19.010: INFO: 
    Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-skqqr before test
    May  9 13:41:19.015: INFO: calico-kube-controllers-578488968b-2nchl from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.015: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  9 13:41:19.015: INFO: calico-node-fm8kr from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.015: INFO: 	Container calico-node ready: true, restart count 0
    May  9 13:41:19.015: INFO: coredns-677bf479b7-t4l9l from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.015: INFO: 	Container coredns ready: true, restart count 0
    May  9 13:41:19.015: INFO: coredns-677bf479b7-t7hjx from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.015: INFO: 	Container coredns ready: true, restart count 0
    May  9 13:41:19.015: INFO: csi-cinder-nodeplugin-f82jf from kube-system started at 2023-05-09 11:59:06 +0000 UTC (3 container statuses recorded)
    May  9 13:41:19.015: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    May  9 13:41:19.015: INFO: 	Container liveness-probe ready: true, restart count 0
    May  9 13:41:19.015: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May  9 13:41:19.015: INFO: kube-proxy-d9lqj from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.015: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 13:41:19.015: INFO: metrics-server-5974658d45-zx6c9 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.015: INFO: 	Container metrics-server ready: true, restart count 0
    May  9 13:41:19.015: INFO: node-local-dns-hfgs9 from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.015: INFO: 	Container node-cache ready: true, restart count 0
    May  9 13:41:19.015: INFO: snapshot-controller-677c67cc64-78dwg from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.015: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  9 13:41:19.015: INFO: snapshot-controller-677c67cc64-jrp67 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:41:19.015: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  9 13:41:19.015: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-67f2c from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:41:19.015: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:41:19.015: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/09/23 13:41:19.016
    May  9 13:41:19.023: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9134" to be "running"
    May  9 13:41:19.025: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.640203ms
    May  9 13:41:21.029: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005659398s
    May  9 13:41:21.029: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/09/23 13:41:21.03
    STEP: Trying to apply a random label on the found node. 05/09/23 13:41:21.041
    STEP: verifying the node has the label kubernetes.io/e2e-394463f0-69f5-46b5-81ac-abab6577df6c 95 05/09/23 13:41:21.05
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/09/23 13:41:21.052
    May  9 13:41:21.057: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-9134" to be "not pending"
    May  9 13:41:21.059: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.819545ms
    May  9 13:41:23.063: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005800957s
    May  9 13:41:23.063: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.1.64 on the node which pod4 resides and expect not scheduled 05/09/23 13:41:23.063
    May  9 13:41:23.067: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-9134" to be "not pending"
    May  9 13:41:23.077: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.32803ms
    May  9 13:41:25.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014493486s
    May  9 13:41:27.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01233188s
    May  9 13:41:29.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013067551s
    May  9 13:41:31.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012352078s
    May  9 13:41:33.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013565756s
    May  9 13:41:35.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012544499s
    May  9 13:41:37.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012668699s
    May  9 13:41:39.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013261562s
    May  9 13:41:41.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013722994s
    May  9 13:41:43.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.012993408s
    May  9 13:41:45.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014009231s
    May  9 13:41:47.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.013632284s
    May  9 13:41:49.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012533088s
    May  9 13:41:51.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013251363s
    May  9 13:41:53.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.013017614s
    May  9 13:41:55.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.013527019s
    May  9 13:41:57.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012319582s
    May  9 13:41:59.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.013976449s
    May  9 13:42:01.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012862249s
    May  9 13:42:03.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.012728633s
    May  9 13:42:05.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.013173348s
    May  9 13:42:07.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.012356549s
    May  9 13:42:09.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012772455s
    May  9 13:42:11.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.014688637s
    May  9 13:42:13.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013050799s
    May  9 13:42:15.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012133953s
    May  9 13:42:17.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.012221337s
    May  9 13:42:19.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012509573s
    May  9 13:42:21.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012110498s
    May  9 13:42:23.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012369834s
    May  9 13:42:25.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.012002034s
    May  9 13:42:27.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.012382419s
    May  9 13:42:29.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.014034191s
    May  9 13:42:31.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.012028278s
    May  9 13:42:33.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011858668s
    May  9 13:42:35.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012212031s
    May  9 13:42:37.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011980558s
    May  9 13:42:39.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.012231312s
    May  9 13:42:41.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.012257968s
    May  9 13:42:43.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012683502s
    May  9 13:42:45.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011915702s
    May  9 13:42:47.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012877643s
    May  9 13:42:49.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012474783s
    May  9 13:42:51.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.012301928s
    May  9 13:42:53.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.013338414s
    May  9 13:42:55.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.013690845s
    May  9 13:42:57.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.012251362s
    May  9 13:42:59.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.014175442s
    May  9 13:43:01.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.012355091s
    May  9 13:43:03.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013178059s
    May  9 13:43:05.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.01237641s
    May  9 13:43:07.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012634607s
    May  9 13:43:09.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.012354826s
    May  9 13:43:11.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012497739s
    May  9 13:43:13.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01318709s
    May  9 13:43:15.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.01368328s
    May  9 13:43:17.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012564223s
    May  9 13:43:19.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012232332s
    May  9 13:43:21.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.014064267s
    May  9 13:43:23.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013218743s
    May  9 13:43:25.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.013036178s
    May  9 13:43:27.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.012241825s
    May  9 13:43:29.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.013707133s
    May  9 13:43:31.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.012112481s
    May  9 13:43:33.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.012487788s
    May  9 13:43:35.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.013062811s
    May  9 13:43:37.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.012622467s
    May  9 13:43:39.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.012951636s
    May  9 13:43:41.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.013157978s
    May  9 13:43:43.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.012937154s
    May  9 13:43:45.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.013554413s
    May  9 13:43:47.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.012313921s
    May  9 13:43:49.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.012847405s
    May  9 13:43:51.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.012081012s
    May  9 13:43:53.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.011814296s
    May  9 13:43:55.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.012396904s
    May  9 13:43:57.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.012039208s
    May  9 13:43:59.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.01340468s
    May  9 13:44:01.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.012167674s
    May  9 13:44:03.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.012334227s
    May  9 13:44:05.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.012761905s
    May  9 13:44:07.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.012498152s
    May  9 13:44:09.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.01291958s
    May  9 13:44:11.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.01209883s
    May  9 13:44:13.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.012351451s
    May  9 13:44:15.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.012161998s
    May  9 13:44:17.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.012619752s
    May  9 13:44:19.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.012426455s
    May  9 13:44:21.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.013984739s
    May  9 13:44:23.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.012504334s
    May  9 13:44:25.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.013367377s
    May  9 13:44:27.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.011774429s
    May  9 13:44:29.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.012315801s
    May  9 13:44:31.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.012382727s
    May  9 13:44:33.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.013418944s
    May  9 13:44:35.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.012360599s
    May  9 13:44:37.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.012792538s
    May  9 13:44:39.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.012868778s
    May  9 13:44:41.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.014365688s
    May  9 13:44:43.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.013425635s
    May  9 13:44:45.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.013177805s
    May  9 13:44:47.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.012966792s
    May  9 13:44:49.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.014025672s
    May  9 13:44:51.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.01213906s
    May  9 13:44:53.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.012971962s
    May  9 13:44:55.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.0128222s
    May  9 13:44:57.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.012860531s
    May  9 13:44:59.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.014195088s
    May  9 13:45:01.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.012574293s
    May  9 13:45:03.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.012593377s
    May  9 13:45:05.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.012936142s
    May  9 13:45:07.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.012540559s
    May  9 13:45:09.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.012456585s
    May  9 13:45:11.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.012479039s
    May  9 13:45:13.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.013031508s
    May  9 13:45:15.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.013407677s
    May  9 13:45:17.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.012452748s
    May  9 13:45:19.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.014067429s
    May  9 13:45:21.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.013495109s
    May  9 13:45:23.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.012336809s
    May  9 13:45:25.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.014186865s
    May  9 13:45:27.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.012373619s
    May  9 13:45:29.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.014230991s
    May  9 13:45:31.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.012567181s
    May  9 13:45:33.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.012229715s
    May  9 13:45:35.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.013505424s
    May  9 13:45:37.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.012787384s
    May  9 13:45:39.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.012909268s
    May  9 13:45:41.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.013480453s
    May  9 13:45:43.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.012369551s
    May  9 13:45:45.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.012723984s
    May  9 13:45:47.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.012601816s
    May  9 13:45:49.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.013497562s
    May  9 13:45:51.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.011978465s
    May  9 13:45:53.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.013151641s
    May  9 13:45:55.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.013532162s
    May  9 13:45:57.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.01197495s
    May  9 13:45:59.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.014185327s
    May  9 13:46:01.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.012751459s
    May  9 13:46:03.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.012959764s
    May  9 13:46:05.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.01299717s
    May  9 13:46:07.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.012453382s
    May  9 13:46:09.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.012606578s
    May  9 13:46:11.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.01329075s
    May  9 13:46:13.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.012181723s
    May  9 13:46:15.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.012470643s
    May  9 13:46:17.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.012590997s
    May  9 13:46:19.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.012369885s
    May  9 13:46:21.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012555214s
    May  9 13:46:23.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011932177s
    May  9 13:46:23.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.014035993s
    STEP: removing the label kubernetes.io/e2e-394463f0-69f5-46b5-81ac-abab6577df6c off the node cl-gks-cncf-ix1-md-0-48ljh 05/09/23 13:46:23.081
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-394463f0-69f5-46b5-81ac-abab6577df6c 05/09/23 13:46:23.091
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:46:23.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9134" for this suite. 05/09/23 13:46:23.096
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:46:23.103
May  9 13:46:23.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename replicaset 05/09/23 13:46:23.104
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:23.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:23.117
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 05/09/23 13:46:23.121
STEP: Verify that the required pods have come up. 05/09/23 13:46:23.125
May  9 13:46:23.128: INFO: Pod name sample-pod: Found 0 pods out of 1
May  9 13:46:28.131: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/09/23 13:46:28.131
STEP: Getting /status 05/09/23 13:46:28.131
May  9 13:46:28.136: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 05/09/23 13:46:28.136
May  9 13:46:28.143: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 05/09/23 13:46:28.143
May  9 13:46:28.144: INFO: Observed &ReplicaSet event: ADDED
May  9 13:46:28.144: INFO: Observed &ReplicaSet event: MODIFIED
May  9 13:46:28.144: INFO: Observed &ReplicaSet event: MODIFIED
May  9 13:46:28.144: INFO: Observed &ReplicaSet event: MODIFIED
May  9 13:46:28.144: INFO: Found replicaset test-rs in namespace replicaset-722 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  9 13:46:28.144: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 05/09/23 13:46:28.144
May  9 13:46:28.145: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  9 13:46:28.150: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 05/09/23 13:46:28.15
May  9 13:46:28.151: INFO: Observed &ReplicaSet event: ADDED
May  9 13:46:28.151: INFO: Observed &ReplicaSet event: MODIFIED
May  9 13:46:28.151: INFO: Observed &ReplicaSet event: MODIFIED
May  9 13:46:28.151: INFO: Observed &ReplicaSet event: MODIFIED
May  9 13:46:28.151: INFO: Observed replicaset test-rs in namespace replicaset-722 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  9 13:46:28.151: INFO: Observed &ReplicaSet event: MODIFIED
May  9 13:46:28.151: INFO: Found replicaset test-rs in namespace replicaset-722 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
May  9 13:46:28.151: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  9 13:46:28.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-722" for this suite. 05/09/23 13:46:28.155
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":317,"skipped":5978,"failed":0}
------------------------------
• [SLOW TEST] [5.056 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:46:23.103
    May  9 13:46:23.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename replicaset 05/09/23 13:46:23.104
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:23.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:23.117
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 05/09/23 13:46:23.121
    STEP: Verify that the required pods have come up. 05/09/23 13:46:23.125
    May  9 13:46:23.128: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  9 13:46:28.131: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/09/23 13:46:28.131
    STEP: Getting /status 05/09/23 13:46:28.131
    May  9 13:46:28.136: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 05/09/23 13:46:28.136
    May  9 13:46:28.143: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 05/09/23 13:46:28.143
    May  9 13:46:28.144: INFO: Observed &ReplicaSet event: ADDED
    May  9 13:46:28.144: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 13:46:28.144: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 13:46:28.144: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 13:46:28.144: INFO: Found replicaset test-rs in namespace replicaset-722 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  9 13:46:28.144: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 05/09/23 13:46:28.144
    May  9 13:46:28.145: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  9 13:46:28.150: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 05/09/23 13:46:28.15
    May  9 13:46:28.151: INFO: Observed &ReplicaSet event: ADDED
    May  9 13:46:28.151: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 13:46:28.151: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 13:46:28.151: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 13:46:28.151: INFO: Observed replicaset test-rs in namespace replicaset-722 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  9 13:46:28.151: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 13:46:28.151: INFO: Found replicaset test-rs in namespace replicaset-722 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    May  9 13:46:28.151: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  9 13:46:28.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-722" for this suite. 05/09/23 13:46:28.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:46:28.16
May  9 13:46:28.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename gc 05/09/23 13:46:28.161
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:28.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:28.174
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 05/09/23 13:46:28.178
STEP: delete the rc 05/09/23 13:46:33.194
STEP: wait for the rc to be deleted 05/09/23 13:46:33.201
May  9 13:46:34.210: INFO: 80 pods remaining
May  9 13:46:34.211: INFO: 80 pods has nil DeletionTimestamp
May  9 13:46:34.211: INFO: 
May  9 13:46:35.216: INFO: 71 pods remaining
May  9 13:46:35.216: INFO: 71 pods has nil DeletionTimestamp
May  9 13:46:35.216: INFO: 
May  9 13:46:36.209: INFO: 60 pods remaining
May  9 13:46:36.209: INFO: 60 pods has nil DeletionTimestamp
May  9 13:46:36.209: INFO: 
May  9 13:46:37.209: INFO: 40 pods remaining
May  9 13:46:37.209: INFO: 40 pods has nil DeletionTimestamp
May  9 13:46:37.209: INFO: 
May  9 13:46:38.208: INFO: 31 pods remaining
May  9 13:46:38.208: INFO: 31 pods has nil DeletionTimestamp
May  9 13:46:38.208: INFO: 
May  9 13:46:39.212: INFO: 20 pods remaining
May  9 13:46:39.212: INFO: 20 pods has nil DeletionTimestamp
May  9 13:46:39.212: INFO: 
STEP: Gathering metrics 05/09/23 13:46:40.206
May  9 13:46:40.230: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" in namespace "kube-system" to be "running and ready"
May  9 13:46:40.232: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv": Phase="Running", Reason="", readiness=true. Elapsed: 2.170242ms
May  9 13:46:40.232: INFO: The phase of Pod kube-controller-manager-cl-gks-cncf-control-plane-gv9sv is Running (Ready = true)
May  9 13:46:40.232: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" satisfied condition "running and ready"
May  9 13:46:40.270: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  9 13:46:40.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1171" for this suite. 05/09/23 13:46:40.273
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":318,"skipped":5993,"failed":0}
------------------------------
• [SLOW TEST] [12.118 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:46:28.16
    May  9 13:46:28.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename gc 05/09/23 13:46:28.161
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:28.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:28.174
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 05/09/23 13:46:28.178
    STEP: delete the rc 05/09/23 13:46:33.194
    STEP: wait for the rc to be deleted 05/09/23 13:46:33.201
    May  9 13:46:34.210: INFO: 80 pods remaining
    May  9 13:46:34.211: INFO: 80 pods has nil DeletionTimestamp
    May  9 13:46:34.211: INFO: 
    May  9 13:46:35.216: INFO: 71 pods remaining
    May  9 13:46:35.216: INFO: 71 pods has nil DeletionTimestamp
    May  9 13:46:35.216: INFO: 
    May  9 13:46:36.209: INFO: 60 pods remaining
    May  9 13:46:36.209: INFO: 60 pods has nil DeletionTimestamp
    May  9 13:46:36.209: INFO: 
    May  9 13:46:37.209: INFO: 40 pods remaining
    May  9 13:46:37.209: INFO: 40 pods has nil DeletionTimestamp
    May  9 13:46:37.209: INFO: 
    May  9 13:46:38.208: INFO: 31 pods remaining
    May  9 13:46:38.208: INFO: 31 pods has nil DeletionTimestamp
    May  9 13:46:38.208: INFO: 
    May  9 13:46:39.212: INFO: 20 pods remaining
    May  9 13:46:39.212: INFO: 20 pods has nil DeletionTimestamp
    May  9 13:46:39.212: INFO: 
    STEP: Gathering metrics 05/09/23 13:46:40.206
    May  9 13:46:40.230: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" in namespace "kube-system" to be "running and ready"
    May  9 13:46:40.232: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv": Phase="Running", Reason="", readiness=true. Elapsed: 2.170242ms
    May  9 13:46:40.232: INFO: The phase of Pod kube-controller-manager-cl-gks-cncf-control-plane-gv9sv is Running (Ready = true)
    May  9 13:46:40.232: INFO: Pod "kube-controller-manager-cl-gks-cncf-control-plane-gv9sv" satisfied condition "running and ready"
    May  9 13:46:40.270: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  9 13:46:40.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1171" for this suite. 05/09/23 13:46:40.273
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:46:40.278
May  9 13:46:40.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename security-context-test 05/09/23 13:46:40.279
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:40.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:40.292
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
May  9 13:46:40.299: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc" in namespace "security-context-test-4298" to be "Succeeded or Failed"
May  9 13:46:40.301: INFO: Pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.758527ms
May  9 13:46:42.304: INFO: Pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005220551s
May  9 13:46:44.306: INFO: Pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006543378s
May  9 13:46:46.304: INFO: Pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004776868s
May  9 13:46:46.304: INFO: Pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc" satisfied condition "Succeeded or Failed"
May  9 13:46:46.604: INFO: Got logs for pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  9 13:46:46.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4298" for this suite. 05/09/23 13:46:46.608
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":319,"skipped":5993,"failed":0}
------------------------------
• [SLOW TEST] [6.333 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:46:40.278
    May  9 13:46:40.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename security-context-test 05/09/23 13:46:40.279
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:40.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:40.292
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    May  9 13:46:40.299: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc" in namespace "security-context-test-4298" to be "Succeeded or Failed"
    May  9 13:46:40.301: INFO: Pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.758527ms
    May  9 13:46:42.304: INFO: Pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005220551s
    May  9 13:46:44.306: INFO: Pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006543378s
    May  9 13:46:46.304: INFO: Pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004776868s
    May  9 13:46:46.304: INFO: Pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc" satisfied condition "Succeeded or Failed"
    May  9 13:46:46.604: INFO: Got logs for pod "busybox-privileged-false-5306093f-7a75-4b70-8d01-36582c7f13fc": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  9 13:46:46.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4298" for this suite. 05/09/23 13:46:46.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:46:46.612
May  9 13:46:46.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 13:46:46.613
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:46.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:46.626
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 05/09/23 13:46:46.628
May  9 13:46:46.634: INFO: Waiting up to 5m0s for pod "pod-d2be552e-92e6-47d2-8d91-8321893f0594" in namespace "emptydir-519" to be "Succeeded or Failed"
May  9 13:46:46.639: INFO: Pod "pod-d2be552e-92e6-47d2-8d91-8321893f0594": Phase="Pending", Reason="", readiness=false. Elapsed: 5.167147ms
May  9 13:46:48.643: INFO: Pod "pod-d2be552e-92e6-47d2-8d91-8321893f0594": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0089332s
May  9 13:46:50.643: INFO: Pod "pod-d2be552e-92e6-47d2-8d91-8321893f0594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008771817s
STEP: Saw pod success 05/09/23 13:46:50.643
May  9 13:46:50.643: INFO: Pod "pod-d2be552e-92e6-47d2-8d91-8321893f0594" satisfied condition "Succeeded or Failed"
May  9 13:46:50.646: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-d2be552e-92e6-47d2-8d91-8321893f0594 container test-container: <nil>
STEP: delete the pod 05/09/23 13:46:50.654
May  9 13:46:50.667: INFO: Waiting for pod pod-d2be552e-92e6-47d2-8d91-8321893f0594 to disappear
May  9 13:46:50.669: INFO: Pod pod-d2be552e-92e6-47d2-8d91-8321893f0594 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 13:46:50.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-519" for this suite. 05/09/23 13:46:50.673
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":320,"skipped":6018,"failed":0}
------------------------------
• [4.065 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:46:46.612
    May  9 13:46:46.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 13:46:46.613
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:46.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:46.626
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/09/23 13:46:46.628
    May  9 13:46:46.634: INFO: Waiting up to 5m0s for pod "pod-d2be552e-92e6-47d2-8d91-8321893f0594" in namespace "emptydir-519" to be "Succeeded or Failed"
    May  9 13:46:46.639: INFO: Pod "pod-d2be552e-92e6-47d2-8d91-8321893f0594": Phase="Pending", Reason="", readiness=false. Elapsed: 5.167147ms
    May  9 13:46:48.643: INFO: Pod "pod-d2be552e-92e6-47d2-8d91-8321893f0594": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0089332s
    May  9 13:46:50.643: INFO: Pod "pod-d2be552e-92e6-47d2-8d91-8321893f0594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008771817s
    STEP: Saw pod success 05/09/23 13:46:50.643
    May  9 13:46:50.643: INFO: Pod "pod-d2be552e-92e6-47d2-8d91-8321893f0594" satisfied condition "Succeeded or Failed"
    May  9 13:46:50.646: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-d2be552e-92e6-47d2-8d91-8321893f0594 container test-container: <nil>
    STEP: delete the pod 05/09/23 13:46:50.654
    May  9 13:46:50.667: INFO: Waiting for pod pod-d2be552e-92e6-47d2-8d91-8321893f0594 to disappear
    May  9 13:46:50.669: INFO: Pod pod-d2be552e-92e6-47d2-8d91-8321893f0594 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 13:46:50.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-519" for this suite. 05/09/23 13:46:50.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:46:50.678
May  9 13:46:50.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename daemonsets 05/09/23 13:46:50.679
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:50.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:50.693
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 05/09/23 13:46:50.709
STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 13:46:50.714
May  9 13:46:50.719: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:46:50.719: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:46:50.719: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:46:50.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:46:50.721: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
May  9 13:46:51.727: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:46:51.727: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:46:51.727: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:46:51.729: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 13:46:51.729: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
May  9 13:46:52.727: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:46:52.727: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:46:52.727: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May  9 13:46:52.729: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 13:46:52.729: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 05/09/23 13:46:52.732
May  9 13:46:52.734: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 05/09/23 13:46:52.734
May  9 13:46:52.742: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 05/09/23 13:46:52.742
May  9 13:46:52.744: INFO: Observed &DaemonSet event: ADDED
May  9 13:46:52.744: INFO: Observed &DaemonSet event: MODIFIED
May  9 13:46:52.744: INFO: Observed &DaemonSet event: MODIFIED
May  9 13:46:52.744: INFO: Observed &DaemonSet event: MODIFIED
May  9 13:46:52.744: INFO: Observed &DaemonSet event: MODIFIED
May  9 13:46:52.744: INFO: Found daemon set daemon-set in namespace daemonsets-7152 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  9 13:46:52.744: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 05/09/23 13:46:52.744
STEP: watching for the daemon set status to be patched 05/09/23 13:46:52.75
May  9 13:46:52.751: INFO: Observed &DaemonSet event: ADDED
May  9 13:46:52.751: INFO: Observed &DaemonSet event: MODIFIED
May  9 13:46:52.751: INFO: Observed &DaemonSet event: MODIFIED
May  9 13:46:52.751: INFO: Observed &DaemonSet event: MODIFIED
May  9 13:46:52.751: INFO: Observed &DaemonSet event: MODIFIED
May  9 13:46:52.751: INFO: Observed daemon set daemon-set in namespace daemonsets-7152 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  9 13:46:52.752: INFO: Observed &DaemonSet event: MODIFIED
May  9 13:46:52.752: INFO: Found daemon set daemon-set in namespace daemonsets-7152 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
May  9 13:46:52.752: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/09/23 13:46:52.755
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7152, will wait for the garbage collector to delete the pods 05/09/23 13:46:52.755
May  9 13:46:52.813: INFO: Deleting DaemonSet.extensions daemon-set took: 5.214277ms
May  9 13:46:52.913: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.580781ms
May  9 13:46:55.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 13:46:55.317: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  9 13:46:55.319: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49062"},"items":null}

May  9 13:46:55.321: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49062"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  9 13:46:55.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7152" for this suite. 05/09/23 13:46:55.333
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":321,"skipped":6047,"failed":0}
------------------------------
• [4.661 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:46:50.678
    May  9 13:46:50.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename daemonsets 05/09/23 13:46:50.679
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:50.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:50.693
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 05/09/23 13:46:50.709
    STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 13:46:50.714
    May  9 13:46:50.719: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:46:50.719: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:46:50.719: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:46:50.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:46:50.721: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
    May  9 13:46:51.727: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:46:51.727: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:46:51.727: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:46:51.729: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 13:46:51.729: INFO: Node cl-gks-cncf-ix1-md-0-48ljh is running 0 daemon pod, expected 1
    May  9 13:46:52.727: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-7p8m8 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:46:52.727: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-8hpm7 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:46:52.727: INFO: DaemonSet pods can't tolerate node cl-gks-cncf-control-plane-gv9sv with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May  9 13:46:52.729: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 13:46:52.729: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 05/09/23 13:46:52.732
    May  9 13:46:52.734: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 05/09/23 13:46:52.734
    May  9 13:46:52.742: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 05/09/23 13:46:52.742
    May  9 13:46:52.744: INFO: Observed &DaemonSet event: ADDED
    May  9 13:46:52.744: INFO: Observed &DaemonSet event: MODIFIED
    May  9 13:46:52.744: INFO: Observed &DaemonSet event: MODIFIED
    May  9 13:46:52.744: INFO: Observed &DaemonSet event: MODIFIED
    May  9 13:46:52.744: INFO: Observed &DaemonSet event: MODIFIED
    May  9 13:46:52.744: INFO: Found daemon set daemon-set in namespace daemonsets-7152 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  9 13:46:52.744: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 05/09/23 13:46:52.744
    STEP: watching for the daemon set status to be patched 05/09/23 13:46:52.75
    May  9 13:46:52.751: INFO: Observed &DaemonSet event: ADDED
    May  9 13:46:52.751: INFO: Observed &DaemonSet event: MODIFIED
    May  9 13:46:52.751: INFO: Observed &DaemonSet event: MODIFIED
    May  9 13:46:52.751: INFO: Observed &DaemonSet event: MODIFIED
    May  9 13:46:52.751: INFO: Observed &DaemonSet event: MODIFIED
    May  9 13:46:52.751: INFO: Observed daemon set daemon-set in namespace daemonsets-7152 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  9 13:46:52.752: INFO: Observed &DaemonSet event: MODIFIED
    May  9 13:46:52.752: INFO: Found daemon set daemon-set in namespace daemonsets-7152 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    May  9 13:46:52.752: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/09/23 13:46:52.755
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7152, will wait for the garbage collector to delete the pods 05/09/23 13:46:52.755
    May  9 13:46:52.813: INFO: Deleting DaemonSet.extensions daemon-set took: 5.214277ms
    May  9 13:46:52.913: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.580781ms
    May  9 13:46:55.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 13:46:55.317: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  9 13:46:55.319: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49062"},"items":null}

    May  9 13:46:55.321: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49062"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:46:55.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7152" for this suite. 05/09/23 13:46:55.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:46:55.339
May  9 13:46:55.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:46:55.34
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:55.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:55.354
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:46:55.356
May  9 13:46:55.362: INFO: Waiting up to 5m0s for pod "downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a" in namespace "projected-892" to be "Succeeded or Failed"
May  9 13:46:55.367: INFO: Pod "downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.94255ms
May  9 13:46:57.371: INFO: Pod "downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008080819s
May  9 13:46:59.371: INFO: Pod "downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008700884s
STEP: Saw pod success 05/09/23 13:46:59.371
May  9 13:46:59.371: INFO: Pod "downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a" satisfied condition "Succeeded or Failed"
May  9 13:46:59.373: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a container client-container: <nil>
STEP: delete the pod 05/09/23 13:46:59.378
May  9 13:46:59.386: INFO: Waiting for pod downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a to disappear
May  9 13:46:59.388: INFO: Pod downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  9 13:46:59.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-892" for this suite. 05/09/23 13:46:59.391
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":322,"skipped":6062,"failed":0}
------------------------------
• [4.056 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:46:55.339
    May  9 13:46:55.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:46:55.34
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:55.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:55.354
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:46:55.356
    May  9 13:46:55.362: INFO: Waiting up to 5m0s for pod "downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a" in namespace "projected-892" to be "Succeeded or Failed"
    May  9 13:46:55.367: INFO: Pod "downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.94255ms
    May  9 13:46:57.371: INFO: Pod "downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008080819s
    May  9 13:46:59.371: INFO: Pod "downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008700884s
    STEP: Saw pod success 05/09/23 13:46:59.371
    May  9 13:46:59.371: INFO: Pod "downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a" satisfied condition "Succeeded or Failed"
    May  9 13:46:59.373: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a container client-container: <nil>
    STEP: delete the pod 05/09/23 13:46:59.378
    May  9 13:46:59.386: INFO: Waiting for pod downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a to disappear
    May  9 13:46:59.388: INFO: Pod downwardapi-volume-03439b49-6d32-42fb-9941-55fa1eea1b2a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  9 13:46:59.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-892" for this suite. 05/09/23 13:46:59.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:46:59.396
May  9 13:46:59.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pods 05/09/23 13:46:59.396
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:59.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:59.412
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
May  9 13:46:59.422: INFO: Waiting up to 5m0s for pod "server-envvars-43bb9e45-7b8d-471d-91b3-6ed70ebb7c1b" in namespace "pods-4601" to be "running and ready"
May  9 13:46:59.424: INFO: Pod "server-envvars-43bb9e45-7b8d-471d-91b3-6ed70ebb7c1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.357628ms
May  9 13:46:59.424: INFO: The phase of Pod server-envvars-43bb9e45-7b8d-471d-91b3-6ed70ebb7c1b is Pending, waiting for it to be Running (with Ready = true)
May  9 13:47:01.427: INFO: Pod "server-envvars-43bb9e45-7b8d-471d-91b3-6ed70ebb7c1b": Phase="Running", Reason="", readiness=true. Elapsed: 2.005146822s
May  9 13:47:01.427: INFO: The phase of Pod server-envvars-43bb9e45-7b8d-471d-91b3-6ed70ebb7c1b is Running (Ready = true)
May  9 13:47:01.427: INFO: Pod "server-envvars-43bb9e45-7b8d-471d-91b3-6ed70ebb7c1b" satisfied condition "running and ready"
May  9 13:47:01.446: INFO: Waiting up to 5m0s for pod "client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5" in namespace "pods-4601" to be "Succeeded or Failed"
May  9 13:47:01.448: INFO: Pod "client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.876904ms
May  9 13:47:03.452: INFO: Pod "client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5": Phase="Running", Reason="", readiness=false. Elapsed: 2.005820134s
May  9 13:47:05.453: INFO: Pod "client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006739858s
STEP: Saw pod success 05/09/23 13:47:05.453
May  9 13:47:05.453: INFO: Pod "client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5" satisfied condition "Succeeded or Failed"
May  9 13:47:05.455: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5 container env3cont: <nil>
STEP: delete the pod 05/09/23 13:47:05.459
May  9 13:47:05.470: INFO: Waiting for pod client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5 to disappear
May  9 13:47:05.472: INFO: Pod client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  9 13:47:05.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4601" for this suite. 05/09/23 13:47:05.475
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":323,"skipped":6074,"failed":0}
------------------------------
• [SLOW TEST] [6.086 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:46:59.396
    May  9 13:46:59.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pods 05/09/23 13:46:59.396
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:46:59.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:46:59.412
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    May  9 13:46:59.422: INFO: Waiting up to 5m0s for pod "server-envvars-43bb9e45-7b8d-471d-91b3-6ed70ebb7c1b" in namespace "pods-4601" to be "running and ready"
    May  9 13:46:59.424: INFO: Pod "server-envvars-43bb9e45-7b8d-471d-91b3-6ed70ebb7c1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.357628ms
    May  9 13:46:59.424: INFO: The phase of Pod server-envvars-43bb9e45-7b8d-471d-91b3-6ed70ebb7c1b is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:47:01.427: INFO: Pod "server-envvars-43bb9e45-7b8d-471d-91b3-6ed70ebb7c1b": Phase="Running", Reason="", readiness=true. Elapsed: 2.005146822s
    May  9 13:47:01.427: INFO: The phase of Pod server-envvars-43bb9e45-7b8d-471d-91b3-6ed70ebb7c1b is Running (Ready = true)
    May  9 13:47:01.427: INFO: Pod "server-envvars-43bb9e45-7b8d-471d-91b3-6ed70ebb7c1b" satisfied condition "running and ready"
    May  9 13:47:01.446: INFO: Waiting up to 5m0s for pod "client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5" in namespace "pods-4601" to be "Succeeded or Failed"
    May  9 13:47:01.448: INFO: Pod "client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.876904ms
    May  9 13:47:03.452: INFO: Pod "client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5": Phase="Running", Reason="", readiness=false. Elapsed: 2.005820134s
    May  9 13:47:05.453: INFO: Pod "client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006739858s
    STEP: Saw pod success 05/09/23 13:47:05.453
    May  9 13:47:05.453: INFO: Pod "client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5" satisfied condition "Succeeded or Failed"
    May  9 13:47:05.455: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5 container env3cont: <nil>
    STEP: delete the pod 05/09/23 13:47:05.459
    May  9 13:47:05.470: INFO: Waiting for pod client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5 to disappear
    May  9 13:47:05.472: INFO: Pod client-envvars-4f704543-0c10-419e-b5ef-ba1008e3d2d5 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  9 13:47:05.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4601" for this suite. 05/09/23 13:47:05.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:47:05.483
May  9 13:47:05.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename container-probe 05/09/23 13:47:05.483
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:47:05.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:47:05.503
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  9 13:48:05.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1029" for this suite. 05/09/23 13:48:05.518
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":324,"skipped":6101,"failed":0}
------------------------------
• [SLOW TEST] [60.041 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:47:05.483
    May  9 13:47:05.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename container-probe 05/09/23 13:47:05.483
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:47:05.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:47:05.503
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  9 13:48:05.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1029" for this suite. 05/09/23 13:48:05.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:48:05.524
May  9 13:48:05.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename sched-pred 05/09/23 13:48:05.525
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:48:05.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:48:05.539
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May  9 13:48:05.541: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  9 13:48:05.547: INFO: Waiting for terminating namespaces to be deleted...
May  9 13:48:05.549: INFO: 
Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-48ljh before test
May  9 13:48:05.554: INFO: test-webserver-2e2abcbc-7094-4ef7-9c4f-eb7d0b742959 from container-probe-1029 started at 2023-05-09 13:47:05 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.554: INFO: 	Container test-webserver ready: false, restart count 0
May  9 13:48:05.554: INFO: calico-node-fdkdq from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.554: INFO: 	Container calico-node ready: true, restart count 0
May  9 13:48:05.554: INFO: csi-cinder-nodeplugin-f688q from kube-system started at 2023-05-09 11:59:12 +0000 UTC (3 container statuses recorded)
May  9 13:48:05.554: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May  9 13:48:05.554: INFO: 	Container liveness-probe ready: true, restart count 0
May  9 13:48:05.554: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  9 13:48:05.554: INFO: kube-proxy-lbgl4 from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.554: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 13:48:05.554: INFO: node-local-dns-vg5zf from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.554: INFO: 	Container node-cache ready: true, restart count 0
May  9 13:48:05.554: INFO: sonobuoy from sonobuoy started at 2023-05-09 12:27:44 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.554: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  9 13:48:05.554: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-nvk78 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:48:05.554: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:48:05.554: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 13:48:05.554: INFO: 
Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-879bk before test
May  9 13:48:05.560: INFO: calico-node-gdlh8 from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.560: INFO: 	Container calico-node ready: true, restart count 0
May  9 13:48:05.560: INFO: csi-cinder-nodeplugin-hzf44 from kube-system started at 2023-05-09 11:59:09 +0000 UTC (3 container statuses recorded)
May  9 13:48:05.560: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May  9 13:48:05.560: INFO: 	Container liveness-probe ready: true, restart count 0
May  9 13:48:05.560: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  9 13:48:05.560: INFO: kube-proxy-pcp2c from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.560: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 13:48:05.560: INFO: node-local-dns-hvfcl from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.560: INFO: 	Container node-cache ready: true, restart count 0
May  9 13:48:05.560: INFO: sonobuoy-e2e-job-588b06ca4a824bd1 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:48:05.560: INFO: 	Container e2e ready: true, restart count 0
May  9 13:48:05.560: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:48:05.560: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-6xh5z from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:48:05.560: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:48:05.560: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 13:48:05.560: INFO: 
Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-skqqr before test
May  9 13:48:05.566: INFO: calico-kube-controllers-578488968b-2nchl from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.566: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  9 13:48:05.566: INFO: calico-node-fm8kr from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.566: INFO: 	Container calico-node ready: true, restart count 0
May  9 13:48:05.566: INFO: coredns-677bf479b7-t4l9l from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.566: INFO: 	Container coredns ready: true, restart count 0
May  9 13:48:05.566: INFO: coredns-677bf479b7-t7hjx from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.566: INFO: 	Container coredns ready: true, restart count 0
May  9 13:48:05.566: INFO: csi-cinder-nodeplugin-f82jf from kube-system started at 2023-05-09 11:59:06 +0000 UTC (3 container statuses recorded)
May  9 13:48:05.566: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May  9 13:48:05.566: INFO: 	Container liveness-probe ready: true, restart count 0
May  9 13:48:05.566: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  9 13:48:05.566: INFO: kube-proxy-d9lqj from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.566: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 13:48:05.566: INFO: metrics-server-5974658d45-zx6c9 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.566: INFO: 	Container metrics-server ready: true, restart count 0
May  9 13:48:05.566: INFO: node-local-dns-hfgs9 from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.566: INFO: 	Container node-cache ready: true, restart count 0
May  9 13:48:05.566: INFO: snapshot-controller-677c67cc64-78dwg from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.566: INFO: 	Container snapshot-controller ready: true, restart count 0
May  9 13:48:05.566: INFO: snapshot-controller-677c67cc64-jrp67 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:48:05.566: INFO: 	Container snapshot-controller ready: true, restart count 0
May  9 13:48:05.566: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-67f2c from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:48:05.566: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:48:05.566: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node cl-gks-cncf-ix1-md-0-48ljh 05/09/23 13:48:05.586
STEP: verifying the node has the label node cl-gks-cncf-ix1-md-0-879bk 05/09/23 13:48:05.596
STEP: verifying the node has the label node cl-gks-cncf-ix1-md-0-skqqr 05/09/23 13:48:05.607
May  9 13:48:05.624: INFO: Pod test-webserver-2e2abcbc-7094-4ef7-9c4f-eb7d0b742959 requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-48ljh
May  9 13:48:05.624: INFO: Pod calico-kube-controllers-578488968b-2nchl requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-skqqr
May  9 13:48:05.624: INFO: Pod calico-node-fdkdq requesting resource cpu=250m on Node cl-gks-cncf-ix1-md-0-48ljh
May  9 13:48:05.624: INFO: Pod calico-node-fm8kr requesting resource cpu=250m on Node cl-gks-cncf-ix1-md-0-skqqr
May  9 13:48:05.624: INFO: Pod calico-node-gdlh8 requesting resource cpu=250m on Node cl-gks-cncf-ix1-md-0-879bk
May  9 13:48:05.624: INFO: Pod coredns-677bf479b7-t4l9l requesting resource cpu=100m on Node cl-gks-cncf-ix1-md-0-skqqr
May  9 13:48:05.624: INFO: Pod coredns-677bf479b7-t7hjx requesting resource cpu=100m on Node cl-gks-cncf-ix1-md-0-skqqr
May  9 13:48:05.624: INFO: Pod csi-cinder-nodeplugin-f688q requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-48ljh
May  9 13:48:05.624: INFO: Pod csi-cinder-nodeplugin-f82jf requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-skqqr
May  9 13:48:05.624: INFO: Pod csi-cinder-nodeplugin-hzf44 requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-879bk
May  9 13:48:05.624: INFO: Pod kube-proxy-d9lqj requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-skqqr
May  9 13:48:05.624: INFO: Pod kube-proxy-lbgl4 requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-48ljh
May  9 13:48:05.624: INFO: Pod kube-proxy-pcp2c requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-879bk
May  9 13:48:05.624: INFO: Pod metrics-server-5974658d45-zx6c9 requesting resource cpu=100m on Node cl-gks-cncf-ix1-md-0-skqqr
May  9 13:48:05.624: INFO: Pod node-local-dns-hfgs9 requesting resource cpu=25m on Node cl-gks-cncf-ix1-md-0-skqqr
May  9 13:48:05.624: INFO: Pod node-local-dns-hvfcl requesting resource cpu=25m on Node cl-gks-cncf-ix1-md-0-879bk
May  9 13:48:05.624: INFO: Pod node-local-dns-vg5zf requesting resource cpu=25m on Node cl-gks-cncf-ix1-md-0-48ljh
May  9 13:48:05.624: INFO: Pod snapshot-controller-677c67cc64-78dwg requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-skqqr
May  9 13:48:05.624: INFO: Pod snapshot-controller-677c67cc64-jrp67 requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-skqqr
May  9 13:48:05.624: INFO: Pod sonobuoy requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-48ljh
May  9 13:48:05.624: INFO: Pod sonobuoy-e2e-job-588b06ca4a824bd1 requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-879bk
May  9 13:48:05.624: INFO: Pod sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-67f2c requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-skqqr
May  9 13:48:05.624: INFO: Pod sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-6xh5z requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-879bk
May  9 13:48:05.624: INFO: Pod sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-nvk78 requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-48ljh
STEP: Starting Pods to consume most of the cluster CPU. 05/09/23 13:48:05.624
May  9 13:48:05.624: INFO: Creating a pod which consumes cpu=5407m on Node cl-gks-cncf-ix1-md-0-48ljh
May  9 13:48:05.630: INFO: Creating a pod which consumes cpu=5407m on Node cl-gks-cncf-ix1-md-0-879bk
May  9 13:48:05.636: INFO: Creating a pod which consumes cpu=5197m on Node cl-gks-cncf-ix1-md-0-skqqr
May  9 13:48:05.643: INFO: Waiting up to 5m0s for pod "filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41" in namespace "sched-pred-7801" to be "running"
May  9 13:48:05.648: INFO: Pod "filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41": Phase="Pending", Reason="", readiness=false. Elapsed: 5.126301ms
May  9 13:48:07.652: INFO: Pod "filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41": Phase="Running", Reason="", readiness=true. Elapsed: 2.008506085s
May  9 13:48:07.652: INFO: Pod "filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41" satisfied condition "running"
May  9 13:48:07.652: INFO: Waiting up to 5m0s for pod "filler-pod-391e3aae-69f7-4734-99c4-0adde4293885" in namespace "sched-pred-7801" to be "running"
May  9 13:48:07.654: INFO: Pod "filler-pod-391e3aae-69f7-4734-99c4-0adde4293885": Phase="Running", Reason="", readiness=true. Elapsed: 2.120678ms
May  9 13:48:07.654: INFO: Pod "filler-pod-391e3aae-69f7-4734-99c4-0adde4293885" satisfied condition "running"
May  9 13:48:07.654: INFO: Waiting up to 5m0s for pod "filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6" in namespace "sched-pred-7801" to be "running"
May  9 13:48:07.656: INFO: Pod "filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6": Phase="Running", Reason="", readiness=true. Elapsed: 1.987674ms
May  9 13:48:07.656: INFO: Pod "filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 05/09/23 13:48:07.656
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41.175d7dbcd19e8454], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7801/filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41 to cl-gks-cncf-ix1-md-0-48ljh] 05/09/23 13:48:07.659
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41.175d7dbcf10b8eb3], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/09/23 13:48:07.659
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41.175d7dbcf309c4df], Reason = [Created], Message = [Created container filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41] 05/09/23 13:48:07.659
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41.175d7dbcf66b2d77], Reason = [Started], Message = [Started container filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41] 05/09/23 13:48:07.659
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-391e3aae-69f7-4734-99c4-0adde4293885.175d7dbcd1ead196], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7801/filler-pod-391e3aae-69f7-4734-99c4-0adde4293885 to cl-gks-cncf-ix1-md-0-879bk] 05/09/23 13:48:07.659
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-391e3aae-69f7-4734-99c4-0adde4293885.175d7dbcf18128f8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/09/23 13:48:07.659
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-391e3aae-69f7-4734-99c4-0adde4293885.175d7dbcf3622dad], Reason = [Created], Message = [Created container filler-pod-391e3aae-69f7-4734-99c4-0adde4293885] 05/09/23 13:48:07.659
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-391e3aae-69f7-4734-99c4-0adde4293885.175d7dbcf6d22372], Reason = [Started], Message = [Started container filler-pod-391e3aae-69f7-4734-99c4-0adde4293885] 05/09/23 13:48:07.659
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6.175d7dbcd28ef758], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7801/filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6 to cl-gks-cncf-ix1-md-0-skqqr] 05/09/23 13:48:07.659
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6.175d7dbcf1cf1f89], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/09/23 13:48:07.659
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6.175d7dbcf3f971e2], Reason = [Created], Message = [Created container filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6] 05/09/23 13:48:07.659
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6.175d7dbcf7265573], Reason = [Started], Message = [Started container filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6] 05/09/23 13:48:07.659
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.175d7dbd4a8b1753], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 05/09/23 13:48:07.669
STEP: removing the label node off the node cl-gks-cncf-ix1-md-0-48ljh 05/09/23 13:48:08.668
STEP: verifying the node doesn't have the label node 05/09/23 13:48:08.678
STEP: removing the label node off the node cl-gks-cncf-ix1-md-0-879bk 05/09/23 13:48:08.681
STEP: verifying the node doesn't have the label node 05/09/23 13:48:08.692
STEP: removing the label node off the node cl-gks-cncf-ix1-md-0-skqqr 05/09/23 13:48:08.694
STEP: verifying the node doesn't have the label node 05/09/23 13:48:08.704
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May  9 13:48:08.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7801" for this suite. 05/09/23 13:48:08.709
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":325,"skipped":6114,"failed":0}
------------------------------
• [3.191 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:48:05.524
    May  9 13:48:05.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename sched-pred 05/09/23 13:48:05.525
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:48:05.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:48:05.539
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May  9 13:48:05.541: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  9 13:48:05.547: INFO: Waiting for terminating namespaces to be deleted...
    May  9 13:48:05.549: INFO: 
    Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-48ljh before test
    May  9 13:48:05.554: INFO: test-webserver-2e2abcbc-7094-4ef7-9c4f-eb7d0b742959 from container-probe-1029 started at 2023-05-09 13:47:05 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.554: INFO: 	Container test-webserver ready: false, restart count 0
    May  9 13:48:05.554: INFO: calico-node-fdkdq from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.554: INFO: 	Container calico-node ready: true, restart count 0
    May  9 13:48:05.554: INFO: csi-cinder-nodeplugin-f688q from kube-system started at 2023-05-09 11:59:12 +0000 UTC (3 container statuses recorded)
    May  9 13:48:05.554: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    May  9 13:48:05.554: INFO: 	Container liveness-probe ready: true, restart count 0
    May  9 13:48:05.554: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May  9 13:48:05.554: INFO: kube-proxy-lbgl4 from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.554: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 13:48:05.554: INFO: node-local-dns-vg5zf from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.554: INFO: 	Container node-cache ready: true, restart count 0
    May  9 13:48:05.554: INFO: sonobuoy from sonobuoy started at 2023-05-09 12:27:44 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.554: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  9 13:48:05.554: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-nvk78 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:48:05.554: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:48:05.554: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 13:48:05.554: INFO: 
    Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-879bk before test
    May  9 13:48:05.560: INFO: calico-node-gdlh8 from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.560: INFO: 	Container calico-node ready: true, restart count 0
    May  9 13:48:05.560: INFO: csi-cinder-nodeplugin-hzf44 from kube-system started at 2023-05-09 11:59:09 +0000 UTC (3 container statuses recorded)
    May  9 13:48:05.560: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    May  9 13:48:05.560: INFO: 	Container liveness-probe ready: true, restart count 0
    May  9 13:48:05.560: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May  9 13:48:05.560: INFO: kube-proxy-pcp2c from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.560: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 13:48:05.560: INFO: node-local-dns-hvfcl from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.560: INFO: 	Container node-cache ready: true, restart count 0
    May  9 13:48:05.560: INFO: sonobuoy-e2e-job-588b06ca4a824bd1 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:48:05.560: INFO: 	Container e2e ready: true, restart count 0
    May  9 13:48:05.560: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:48:05.560: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-6xh5z from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:48:05.560: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:48:05.560: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 13:48:05.560: INFO: 
    Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-skqqr before test
    May  9 13:48:05.566: INFO: calico-kube-controllers-578488968b-2nchl from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.566: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  9 13:48:05.566: INFO: calico-node-fm8kr from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.566: INFO: 	Container calico-node ready: true, restart count 0
    May  9 13:48:05.566: INFO: coredns-677bf479b7-t4l9l from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.566: INFO: 	Container coredns ready: true, restart count 0
    May  9 13:48:05.566: INFO: coredns-677bf479b7-t7hjx from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.566: INFO: 	Container coredns ready: true, restart count 0
    May  9 13:48:05.566: INFO: csi-cinder-nodeplugin-f82jf from kube-system started at 2023-05-09 11:59:06 +0000 UTC (3 container statuses recorded)
    May  9 13:48:05.566: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    May  9 13:48:05.566: INFO: 	Container liveness-probe ready: true, restart count 0
    May  9 13:48:05.566: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May  9 13:48:05.566: INFO: kube-proxy-d9lqj from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.566: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 13:48:05.566: INFO: metrics-server-5974658d45-zx6c9 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.566: INFO: 	Container metrics-server ready: true, restart count 0
    May  9 13:48:05.566: INFO: node-local-dns-hfgs9 from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.566: INFO: 	Container node-cache ready: true, restart count 0
    May  9 13:48:05.566: INFO: snapshot-controller-677c67cc64-78dwg from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.566: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  9 13:48:05.566: INFO: snapshot-controller-677c67cc64-jrp67 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:48:05.566: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  9 13:48:05.566: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-67f2c from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:48:05.566: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:48:05.566: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node cl-gks-cncf-ix1-md-0-48ljh 05/09/23 13:48:05.586
    STEP: verifying the node has the label node cl-gks-cncf-ix1-md-0-879bk 05/09/23 13:48:05.596
    STEP: verifying the node has the label node cl-gks-cncf-ix1-md-0-skqqr 05/09/23 13:48:05.607
    May  9 13:48:05.624: INFO: Pod test-webserver-2e2abcbc-7094-4ef7-9c4f-eb7d0b742959 requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-48ljh
    May  9 13:48:05.624: INFO: Pod calico-kube-controllers-578488968b-2nchl requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-skqqr
    May  9 13:48:05.624: INFO: Pod calico-node-fdkdq requesting resource cpu=250m on Node cl-gks-cncf-ix1-md-0-48ljh
    May  9 13:48:05.624: INFO: Pod calico-node-fm8kr requesting resource cpu=250m on Node cl-gks-cncf-ix1-md-0-skqqr
    May  9 13:48:05.624: INFO: Pod calico-node-gdlh8 requesting resource cpu=250m on Node cl-gks-cncf-ix1-md-0-879bk
    May  9 13:48:05.624: INFO: Pod coredns-677bf479b7-t4l9l requesting resource cpu=100m on Node cl-gks-cncf-ix1-md-0-skqqr
    May  9 13:48:05.624: INFO: Pod coredns-677bf479b7-t7hjx requesting resource cpu=100m on Node cl-gks-cncf-ix1-md-0-skqqr
    May  9 13:48:05.624: INFO: Pod csi-cinder-nodeplugin-f688q requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-48ljh
    May  9 13:48:05.624: INFO: Pod csi-cinder-nodeplugin-f82jf requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-skqqr
    May  9 13:48:05.624: INFO: Pod csi-cinder-nodeplugin-hzf44 requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-879bk
    May  9 13:48:05.624: INFO: Pod kube-proxy-d9lqj requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-skqqr
    May  9 13:48:05.624: INFO: Pod kube-proxy-lbgl4 requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-48ljh
    May  9 13:48:05.624: INFO: Pod kube-proxy-pcp2c requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-879bk
    May  9 13:48:05.624: INFO: Pod metrics-server-5974658d45-zx6c9 requesting resource cpu=100m on Node cl-gks-cncf-ix1-md-0-skqqr
    May  9 13:48:05.624: INFO: Pod node-local-dns-hfgs9 requesting resource cpu=25m on Node cl-gks-cncf-ix1-md-0-skqqr
    May  9 13:48:05.624: INFO: Pod node-local-dns-hvfcl requesting resource cpu=25m on Node cl-gks-cncf-ix1-md-0-879bk
    May  9 13:48:05.624: INFO: Pod node-local-dns-vg5zf requesting resource cpu=25m on Node cl-gks-cncf-ix1-md-0-48ljh
    May  9 13:48:05.624: INFO: Pod snapshot-controller-677c67cc64-78dwg requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-skqqr
    May  9 13:48:05.624: INFO: Pod snapshot-controller-677c67cc64-jrp67 requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-skqqr
    May  9 13:48:05.624: INFO: Pod sonobuoy requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-48ljh
    May  9 13:48:05.624: INFO: Pod sonobuoy-e2e-job-588b06ca4a824bd1 requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-879bk
    May  9 13:48:05.624: INFO: Pod sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-67f2c requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-skqqr
    May  9 13:48:05.624: INFO: Pod sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-6xh5z requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-879bk
    May  9 13:48:05.624: INFO: Pod sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-nvk78 requesting resource cpu=0m on Node cl-gks-cncf-ix1-md-0-48ljh
    STEP: Starting Pods to consume most of the cluster CPU. 05/09/23 13:48:05.624
    May  9 13:48:05.624: INFO: Creating a pod which consumes cpu=5407m on Node cl-gks-cncf-ix1-md-0-48ljh
    May  9 13:48:05.630: INFO: Creating a pod which consumes cpu=5407m on Node cl-gks-cncf-ix1-md-0-879bk
    May  9 13:48:05.636: INFO: Creating a pod which consumes cpu=5197m on Node cl-gks-cncf-ix1-md-0-skqqr
    May  9 13:48:05.643: INFO: Waiting up to 5m0s for pod "filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41" in namespace "sched-pred-7801" to be "running"
    May  9 13:48:05.648: INFO: Pod "filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41": Phase="Pending", Reason="", readiness=false. Elapsed: 5.126301ms
    May  9 13:48:07.652: INFO: Pod "filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41": Phase="Running", Reason="", readiness=true. Elapsed: 2.008506085s
    May  9 13:48:07.652: INFO: Pod "filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41" satisfied condition "running"
    May  9 13:48:07.652: INFO: Waiting up to 5m0s for pod "filler-pod-391e3aae-69f7-4734-99c4-0adde4293885" in namespace "sched-pred-7801" to be "running"
    May  9 13:48:07.654: INFO: Pod "filler-pod-391e3aae-69f7-4734-99c4-0adde4293885": Phase="Running", Reason="", readiness=true. Elapsed: 2.120678ms
    May  9 13:48:07.654: INFO: Pod "filler-pod-391e3aae-69f7-4734-99c4-0adde4293885" satisfied condition "running"
    May  9 13:48:07.654: INFO: Waiting up to 5m0s for pod "filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6" in namespace "sched-pred-7801" to be "running"
    May  9 13:48:07.656: INFO: Pod "filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6": Phase="Running", Reason="", readiness=true. Elapsed: 1.987674ms
    May  9 13:48:07.656: INFO: Pod "filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 05/09/23 13:48:07.656
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41.175d7dbcd19e8454], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7801/filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41 to cl-gks-cncf-ix1-md-0-48ljh] 05/09/23 13:48:07.659
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41.175d7dbcf10b8eb3], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/09/23 13:48:07.659
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41.175d7dbcf309c4df], Reason = [Created], Message = [Created container filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41] 05/09/23 13:48:07.659
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41.175d7dbcf66b2d77], Reason = [Started], Message = [Started container filler-pod-1bbad489-5fe4-4f9e-b1ef-dae86f527f41] 05/09/23 13:48:07.659
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-391e3aae-69f7-4734-99c4-0adde4293885.175d7dbcd1ead196], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7801/filler-pod-391e3aae-69f7-4734-99c4-0adde4293885 to cl-gks-cncf-ix1-md-0-879bk] 05/09/23 13:48:07.659
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-391e3aae-69f7-4734-99c4-0adde4293885.175d7dbcf18128f8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/09/23 13:48:07.659
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-391e3aae-69f7-4734-99c4-0adde4293885.175d7dbcf3622dad], Reason = [Created], Message = [Created container filler-pod-391e3aae-69f7-4734-99c4-0adde4293885] 05/09/23 13:48:07.659
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-391e3aae-69f7-4734-99c4-0adde4293885.175d7dbcf6d22372], Reason = [Started], Message = [Started container filler-pod-391e3aae-69f7-4734-99c4-0adde4293885] 05/09/23 13:48:07.659
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6.175d7dbcd28ef758], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7801/filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6 to cl-gks-cncf-ix1-md-0-skqqr] 05/09/23 13:48:07.659
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6.175d7dbcf1cf1f89], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/09/23 13:48:07.659
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6.175d7dbcf3f971e2], Reason = [Created], Message = [Created container filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6] 05/09/23 13:48:07.659
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6.175d7dbcf7265573], Reason = [Started], Message = [Started container filler-pod-ac52369f-c274-4ab9-ab51-3436196539a6] 05/09/23 13:48:07.659
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.175d7dbd4a8b1753], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 05/09/23 13:48:07.669
    STEP: removing the label node off the node cl-gks-cncf-ix1-md-0-48ljh 05/09/23 13:48:08.668
    STEP: verifying the node doesn't have the label node 05/09/23 13:48:08.678
    STEP: removing the label node off the node cl-gks-cncf-ix1-md-0-879bk 05/09/23 13:48:08.681
    STEP: verifying the node doesn't have the label node 05/09/23 13:48:08.692
    STEP: removing the label node off the node cl-gks-cncf-ix1-md-0-skqqr 05/09/23 13:48:08.694
    STEP: verifying the node doesn't have the label node 05/09/23 13:48:08.704
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:48:08.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7801" for this suite. 05/09/23 13:48:08.709
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:48:08.715
May  9 13:48:08.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename disruption 05/09/23 13:48:08.716
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:48:08.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:48:08.727
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:48:08.729
May  9 13:48:08.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename disruption-2 05/09/23 13:48:08.729
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:48:08.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:48:08.742
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 05/09/23 13:48:08.747
STEP: Waiting for the pdb to be processed 05/09/23 13:48:10.756
STEP: Waiting for the pdb to be processed 05/09/23 13:48:12.767
STEP: listing a collection of PDBs across all namespaces 05/09/23 13:48:12.771
STEP: listing a collection of PDBs in namespace disruption-3139 05/09/23 13:48:12.773
STEP: deleting a collection of PDBs 05/09/23 13:48:12.776
STEP: Waiting for the PDB collection to be deleted 05/09/23 13:48:12.784
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
May  9 13:48:12.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-8449" for this suite. 05/09/23 13:48:12.789
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  9 13:48:12.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3139" for this suite. 05/09/23 13:48:12.798
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":326,"skipped":6129,"failed":0}
------------------------------
• [4.087 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:48:08.715
    May  9 13:48:08.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename disruption 05/09/23 13:48:08.716
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:48:08.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:48:08.727
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:48:08.729
    May  9 13:48:08.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename disruption-2 05/09/23 13:48:08.729
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:48:08.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:48:08.742
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 05/09/23 13:48:08.747
    STEP: Waiting for the pdb to be processed 05/09/23 13:48:10.756
    STEP: Waiting for the pdb to be processed 05/09/23 13:48:12.767
    STEP: listing a collection of PDBs across all namespaces 05/09/23 13:48:12.771
    STEP: listing a collection of PDBs in namespace disruption-3139 05/09/23 13:48:12.773
    STEP: deleting a collection of PDBs 05/09/23 13:48:12.776
    STEP: Waiting for the PDB collection to be deleted 05/09/23 13:48:12.784
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    May  9 13:48:12.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-8449" for this suite. 05/09/23 13:48:12.789
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  9 13:48:12.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3139" for this suite. 05/09/23 13:48:12.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:48:12.803
May  9 13:48:12.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:48:12.804
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:48:12.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:48:12.816
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-303e5364-ebeb-4685-b636-4c54e3414bab 05/09/23 13:48:12.818
STEP: Creating a pod to test consume secrets 05/09/23 13:48:12.822
May  9 13:48:12.827: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc" in namespace "projected-169" to be "Succeeded or Failed"
May  9 13:48:12.831: INFO: Pod "pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.885628ms
May  9 13:48:14.834: INFO: Pod "pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc": Phase="Running", Reason="", readiness=false. Elapsed: 2.00664889s
May  9 13:48:16.835: INFO: Pod "pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007551176s
STEP: Saw pod success 05/09/23 13:48:16.835
May  9 13:48:16.835: INFO: Pod "pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc" satisfied condition "Succeeded or Failed"
May  9 13:48:16.838: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc container projected-secret-volume-test: <nil>
STEP: delete the pod 05/09/23 13:48:16.843
May  9 13:48:16.852: INFO: Waiting for pod pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc to disappear
May  9 13:48:16.854: INFO: Pod pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  9 13:48:16.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-169" for this suite. 05/09/23 13:48:16.857
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":327,"skipped":6159,"failed":0}
------------------------------
• [4.059 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:48:12.803
    May  9 13:48:12.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:48:12.804
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:48:12.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:48:12.816
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-303e5364-ebeb-4685-b636-4c54e3414bab 05/09/23 13:48:12.818
    STEP: Creating a pod to test consume secrets 05/09/23 13:48:12.822
    May  9 13:48:12.827: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc" in namespace "projected-169" to be "Succeeded or Failed"
    May  9 13:48:12.831: INFO: Pod "pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.885628ms
    May  9 13:48:14.834: INFO: Pod "pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc": Phase="Running", Reason="", readiness=false. Elapsed: 2.00664889s
    May  9 13:48:16.835: INFO: Pod "pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007551176s
    STEP: Saw pod success 05/09/23 13:48:16.835
    May  9 13:48:16.835: INFO: Pod "pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc" satisfied condition "Succeeded or Failed"
    May  9 13:48:16.838: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:48:16.843
    May  9 13:48:16.852: INFO: Waiting for pod pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc to disappear
    May  9 13:48:16.854: INFO: Pod pod-projected-secrets-7b635a36-aeac-4304-af06-a19cd090efdc no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  9 13:48:16.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-169" for this suite. 05/09/23 13:48:16.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:48:16.865
May  9 13:48:16.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pod-network-test 05/09/23 13:48:16.865
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:48:16.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:48:16.884
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9739 05/09/23 13:48:16.886
STEP: creating a selector 05/09/23 13:48:16.886
STEP: Creating the service pods in kubernetes 05/09/23 13:48:16.886
May  9 13:48:16.886: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  9 13:48:16.915: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9739" to be "running and ready"
May  9 13:48:16.917: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015898ms
May  9 13:48:16.917: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:48:18.921: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005314075s
May  9 13:48:18.921: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:48:20.922: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006433415s
May  9 13:48:20.922: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:48:22.921: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005457903s
May  9 13:48:22.921: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:48:24.922: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006329459s
May  9 13:48:24.922: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:48:26.921: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005433025s
May  9 13:48:26.921: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:48:28.922: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.006479359s
May  9 13:48:28.922: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  9 13:48:28.922: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  9 13:48:28.924: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9739" to be "running and ready"
May  9 13:48:28.926: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.692422ms
May  9 13:48:28.926: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  9 13:48:28.926: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  9 13:48:28.927: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9739" to be "running and ready"
May  9 13:48:28.929: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.678706ms
May  9 13:48:28.929: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  9 13:48:28.929: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/09/23 13:48:28.931
May  9 13:48:28.935: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9739" to be "running"
May  9 13:48:28.937: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.867015ms
May  9 13:48:30.941: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005364685s
May  9 13:48:30.941: INFO: Pod "test-container-pod" satisfied condition "running"
May  9 13:48:30.943: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  9 13:48:30.943: INFO: Breadth first check of 172.25.124.243 on host 192.168.1.64...
May  9 13:48:30.945: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.124.238:9080/dial?request=hostname&protocol=http&host=172.25.124.243&port=8083&tries=1'] Namespace:pod-network-test-9739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:48:30.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:48:30.945: INFO: ExecWithOptions: Clientset creation
May  9 13:48:30.945: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.124.238%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.124.243%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  9 13:48:31.005: INFO: Waiting for responses: map[]
May  9 13:48:31.005: INFO: reached 172.25.124.243 after 0/1 tries
May  9 13:48:31.005: INFO: Breadth first check of 172.25.53.160 on host 192.168.1.73...
May  9 13:48:31.008: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.124.238:9080/dial?request=hostname&protocol=http&host=172.25.53.160&port=8083&tries=1'] Namespace:pod-network-test-9739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:48:31.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:48:31.008: INFO: ExecWithOptions: Clientset creation
May  9 13:48:31.008: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.124.238%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.53.160%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  9 13:48:31.063: INFO: Waiting for responses: map[]
May  9 13:48:31.063: INFO: reached 172.25.53.160 after 0/1 tries
May  9 13:48:31.063: INFO: Breadth first check of 172.25.72.211 on host 192.168.1.89...
May  9 13:48:31.065: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.124.238:9080/dial?request=hostname&protocol=http&host=172.25.72.211&port=8083&tries=1'] Namespace:pod-network-test-9739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:48:31.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:48:31.066: INFO: ExecWithOptions: Clientset creation
May  9 13:48:31.066: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.124.238%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.72.211%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  9 13:48:31.113: INFO: Waiting for responses: map[]
May  9 13:48:31.113: INFO: reached 172.25.72.211 after 0/1 tries
May  9 13:48:31.113: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May  9 13:48:31.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9739" for this suite. 05/09/23 13:48:31.117
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":328,"skipped":6223,"failed":0}
------------------------------
• [SLOW TEST] [14.257 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:48:16.865
    May  9 13:48:16.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pod-network-test 05/09/23 13:48:16.865
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:48:16.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:48:16.884
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9739 05/09/23 13:48:16.886
    STEP: creating a selector 05/09/23 13:48:16.886
    STEP: Creating the service pods in kubernetes 05/09/23 13:48:16.886
    May  9 13:48:16.886: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  9 13:48:16.915: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9739" to be "running and ready"
    May  9 13:48:16.917: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015898ms
    May  9 13:48:16.917: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:48:18.921: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005314075s
    May  9 13:48:18.921: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:48:20.922: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006433415s
    May  9 13:48:20.922: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:48:22.921: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005457903s
    May  9 13:48:22.921: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:48:24.922: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006329459s
    May  9 13:48:24.922: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:48:26.921: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005433025s
    May  9 13:48:26.921: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:48:28.922: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.006479359s
    May  9 13:48:28.922: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  9 13:48:28.922: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  9 13:48:28.924: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9739" to be "running and ready"
    May  9 13:48:28.926: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.692422ms
    May  9 13:48:28.926: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  9 13:48:28.926: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  9 13:48:28.927: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9739" to be "running and ready"
    May  9 13:48:28.929: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.678706ms
    May  9 13:48:28.929: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  9 13:48:28.929: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/09/23 13:48:28.931
    May  9 13:48:28.935: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9739" to be "running"
    May  9 13:48:28.937: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.867015ms
    May  9 13:48:30.941: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005364685s
    May  9 13:48:30.941: INFO: Pod "test-container-pod" satisfied condition "running"
    May  9 13:48:30.943: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May  9 13:48:30.943: INFO: Breadth first check of 172.25.124.243 on host 192.168.1.64...
    May  9 13:48:30.945: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.124.238:9080/dial?request=hostname&protocol=http&host=172.25.124.243&port=8083&tries=1'] Namespace:pod-network-test-9739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:48:30.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:48:30.945: INFO: ExecWithOptions: Clientset creation
    May  9 13:48:30.945: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.124.238%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.124.243%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  9 13:48:31.005: INFO: Waiting for responses: map[]
    May  9 13:48:31.005: INFO: reached 172.25.124.243 after 0/1 tries
    May  9 13:48:31.005: INFO: Breadth first check of 172.25.53.160 on host 192.168.1.73...
    May  9 13:48:31.008: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.124.238:9080/dial?request=hostname&protocol=http&host=172.25.53.160&port=8083&tries=1'] Namespace:pod-network-test-9739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:48:31.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:48:31.008: INFO: ExecWithOptions: Clientset creation
    May  9 13:48:31.008: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.124.238%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.53.160%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  9 13:48:31.063: INFO: Waiting for responses: map[]
    May  9 13:48:31.063: INFO: reached 172.25.53.160 after 0/1 tries
    May  9 13:48:31.063: INFO: Breadth first check of 172.25.72.211 on host 192.168.1.89...
    May  9 13:48:31.065: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.124.238:9080/dial?request=hostname&protocol=http&host=172.25.72.211&port=8083&tries=1'] Namespace:pod-network-test-9739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:48:31.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:48:31.066: INFO: ExecWithOptions: Clientset creation
    May  9 13:48:31.066: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.124.238%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.72.211%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  9 13:48:31.113: INFO: Waiting for responses: map[]
    May  9 13:48:31.113: INFO: reached 172.25.72.211 after 0/1 tries
    May  9 13:48:31.113: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May  9 13:48:31.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9739" for this suite. 05/09/23 13:48:31.117
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:48:31.122
May  9 13:48:31.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename svcaccounts 05/09/23 13:48:31.122
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:48:31.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:48:31.137
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
May  9 13:48:31.148: INFO: created pod
May  9 13:48:31.148: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-92" to be "Succeeded or Failed"
May  9 13:48:31.153: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.962678ms
May  9 13:48:33.156: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00755879s
May  9 13:48:35.156: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00782309s
STEP: Saw pod success 05/09/23 13:48:35.156
May  9 13:48:35.156: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May  9 13:49:05.157: INFO: polling logs
May  9 13:49:05.162: INFO: Pod logs: 
I0509 13:48:31.761890       1 log.go:195] OK: Got token
I0509 13:48:31.761913       1 log.go:195] validating with in-cluster discovery
I0509 13:48:31.762121       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0509 13:48:31.762140       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-92:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683640711, NotBefore:1683640111, IssuedAt:1683640111, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-92", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"82ca5529-0fe2-4ca1-afe5-c6bcdd3361eb"}}}
I0509 13:48:31.771237       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0509 13:48:31.775479       1 log.go:195] OK: Validated signature on JWT
I0509 13:48:31.775542       1 log.go:195] OK: Got valid claims from token!
I0509 13:48:31.775559       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-92:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683640711, NotBefore:1683640111, IssuedAt:1683640111, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-92", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"82ca5529-0fe2-4ca1-afe5-c6bcdd3361eb"}}}

May  9 13:49:05.162: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  9 13:49:05.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-92" for this suite. 05/09/23 13:49:05.175
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":329,"skipped":6227,"failed":0}
------------------------------
• [SLOW TEST] [34.068 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:48:31.122
    May  9 13:48:31.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 13:48:31.122
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:48:31.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:48:31.137
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    May  9 13:48:31.148: INFO: created pod
    May  9 13:48:31.148: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-92" to be "Succeeded or Failed"
    May  9 13:48:31.153: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.962678ms
    May  9 13:48:33.156: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00755879s
    May  9 13:48:35.156: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00782309s
    STEP: Saw pod success 05/09/23 13:48:35.156
    May  9 13:48:35.156: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    May  9 13:49:05.157: INFO: polling logs
    May  9 13:49:05.162: INFO: Pod logs: 
    I0509 13:48:31.761890       1 log.go:195] OK: Got token
    I0509 13:48:31.761913       1 log.go:195] validating with in-cluster discovery
    I0509 13:48:31.762121       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0509 13:48:31.762140       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-92:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683640711, NotBefore:1683640111, IssuedAt:1683640111, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-92", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"82ca5529-0fe2-4ca1-afe5-c6bcdd3361eb"}}}
    I0509 13:48:31.771237       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0509 13:48:31.775479       1 log.go:195] OK: Validated signature on JWT
    I0509 13:48:31.775542       1 log.go:195] OK: Got valid claims from token!
    I0509 13:48:31.775559       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-92:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683640711, NotBefore:1683640111, IssuedAt:1683640111, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-92", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"82ca5529-0fe2-4ca1-afe5-c6bcdd3361eb"}}}

    May  9 13:49:05.162: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  9 13:49:05.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-92" for this suite. 05/09/23 13:49:05.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:49:05.19
May  9 13:49:05.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename var-expansion 05/09/23 13:49:05.191
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:49:05.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:49:05.205
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 05/09/23 13:49:05.207
May  9 13:49:05.213: INFO: Waiting up to 2m0s for pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0" in namespace "var-expansion-6052" to be "running"
May  9 13:49:05.215: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.716377ms
May  9 13:49:07.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004763144s
May  9 13:49:09.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004817402s
May  9 13:49:11.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006293275s
May  9 13:49:13.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004676289s
May  9 13:49:15.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005417181s
May  9 13:49:17.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005416631s
May  9 13:49:19.221: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007374868s
May  9 13:49:21.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005886363s
May  9 13:49:23.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005843858s
May  9 13:49:25.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004935374s
May  9 13:49:27.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005312929s
May  9 13:49:29.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005914696s
May  9 13:49:31.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005142356s
May  9 13:49:33.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005294185s
May  9 13:49:35.217: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004181593s
May  9 13:49:37.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00519617s
May  9 13:49:39.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005968635s
May  9 13:49:41.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005526606s
May  9 13:49:43.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005188773s
May  9 13:49:45.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005106545s
May  9 13:49:47.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004549272s
May  9 13:49:49.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006701774s
May  9 13:49:51.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005068188s
May  9 13:49:53.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 48.00489485s
May  9 13:49:55.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005551719s
May  9 13:49:57.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005042387s
May  9 13:49:59.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 54.00592829s
May  9 13:50:01.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005158758s
May  9 13:50:03.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004865292s
May  9 13:50:05.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004977585s
May  9 13:50:07.217: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004031933s
May  9 13:50:09.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00621212s
May  9 13:50:11.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006240928s
May  9 13:50:13.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005435605s
May  9 13:50:15.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005748009s
May  9 13:50:17.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005489493s
May  9 13:50:19.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.00660947s
May  9 13:50:21.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.006591969s
May  9 13:50:23.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005134253s
May  9 13:50:25.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006175592s
May  9 13:50:27.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00455373s
May  9 13:50:29.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006134151s
May  9 13:50:31.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005251351s
May  9 13:50:33.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005627264s
May  9 13:50:35.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.005308982s
May  9 13:50:37.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005138258s
May  9 13:50:39.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005472293s
May  9 13:50:41.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006625616s
May  9 13:50:43.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005888358s
May  9 13:50:45.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005159392s
May  9 13:50:47.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00471802s
May  9 13:50:49.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004582549s
May  9 13:50:51.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005644154s
May  9 13:50:53.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005820502s
May  9 13:50:55.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007040412s
May  9 13:50:57.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004752361s
May  9 13:50:59.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006627625s
May  9 13:51:01.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006907678s
May  9 13:51:03.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005681391s
May  9 13:51:05.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005370573s
May  9 13:51:05.221: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007591461s
STEP: updating the pod 05/09/23 13:51:05.221
May  9 13:51:05.733: INFO: Successfully updated pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0"
STEP: waiting for pod running 05/09/23 13:51:05.733
May  9 13:51:05.734: INFO: Waiting up to 2m0s for pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0" in namespace "var-expansion-6052" to be "running"
May  9 13:51:05.736: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.161847ms
May  9 13:51:07.739: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.005403073s
May  9 13:51:07.739: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0" satisfied condition "running"
STEP: deleting the pod gracefully 05/09/23 13:51:07.739
May  9 13:51:07.739: INFO: Deleting pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0" in namespace "var-expansion-6052"
May  9 13:51:07.746: INFO: Wait up to 5m0s for pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  9 13:51:39.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6052" for this suite. 05/09/23 13:51:39.755
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":330,"skipped":6233,"failed":0}
------------------------------
• [SLOW TEST] [154.570 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:49:05.19
    May  9 13:49:05.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename var-expansion 05/09/23 13:49:05.191
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:49:05.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:49:05.205
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 05/09/23 13:49:05.207
    May  9 13:49:05.213: INFO: Waiting up to 2m0s for pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0" in namespace "var-expansion-6052" to be "running"
    May  9 13:49:05.215: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.716377ms
    May  9 13:49:07.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004763144s
    May  9 13:49:09.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004817402s
    May  9 13:49:11.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006293275s
    May  9 13:49:13.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004676289s
    May  9 13:49:15.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005417181s
    May  9 13:49:17.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005416631s
    May  9 13:49:19.221: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007374868s
    May  9 13:49:21.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005886363s
    May  9 13:49:23.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005843858s
    May  9 13:49:25.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004935374s
    May  9 13:49:27.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005312929s
    May  9 13:49:29.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005914696s
    May  9 13:49:31.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005142356s
    May  9 13:49:33.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005294185s
    May  9 13:49:35.217: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004181593s
    May  9 13:49:37.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00519617s
    May  9 13:49:39.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005968635s
    May  9 13:49:41.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005526606s
    May  9 13:49:43.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005188773s
    May  9 13:49:45.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005106545s
    May  9 13:49:47.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004549272s
    May  9 13:49:49.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006701774s
    May  9 13:49:51.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005068188s
    May  9 13:49:53.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 48.00489485s
    May  9 13:49:55.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005551719s
    May  9 13:49:57.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005042387s
    May  9 13:49:59.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 54.00592829s
    May  9 13:50:01.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005158758s
    May  9 13:50:03.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004865292s
    May  9 13:50:05.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004977585s
    May  9 13:50:07.217: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004031933s
    May  9 13:50:09.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00621212s
    May  9 13:50:11.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006240928s
    May  9 13:50:13.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005435605s
    May  9 13:50:15.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005748009s
    May  9 13:50:17.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005489493s
    May  9 13:50:19.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.00660947s
    May  9 13:50:21.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.006591969s
    May  9 13:50:23.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005134253s
    May  9 13:50:25.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006175592s
    May  9 13:50:27.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00455373s
    May  9 13:50:29.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006134151s
    May  9 13:50:31.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005251351s
    May  9 13:50:33.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005627264s
    May  9 13:50:35.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.005308982s
    May  9 13:50:37.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005138258s
    May  9 13:50:39.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005472293s
    May  9 13:50:41.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006625616s
    May  9 13:50:43.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005888358s
    May  9 13:50:45.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005159392s
    May  9 13:50:47.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00471802s
    May  9 13:50:49.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004582549s
    May  9 13:50:51.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005644154s
    May  9 13:50:53.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005820502s
    May  9 13:50:55.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007040412s
    May  9 13:50:57.218: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004752361s
    May  9 13:50:59.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006627625s
    May  9 13:51:01.220: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006907678s
    May  9 13:51:03.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005681391s
    May  9 13:51:05.219: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005370573s
    May  9 13:51:05.221: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007591461s
    STEP: updating the pod 05/09/23 13:51:05.221
    May  9 13:51:05.733: INFO: Successfully updated pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0"
    STEP: waiting for pod running 05/09/23 13:51:05.733
    May  9 13:51:05.734: INFO: Waiting up to 2m0s for pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0" in namespace "var-expansion-6052" to be "running"
    May  9 13:51:05.736: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.161847ms
    May  9 13:51:07.739: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.005403073s
    May  9 13:51:07.739: INFO: Pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0" satisfied condition "running"
    STEP: deleting the pod gracefully 05/09/23 13:51:07.739
    May  9 13:51:07.739: INFO: Deleting pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0" in namespace "var-expansion-6052"
    May  9 13:51:07.746: INFO: Wait up to 5m0s for pod "var-expansion-3adee81a-9aa7-4618-be42-30985e251bc0" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  9 13:51:39.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6052" for this suite. 05/09/23 13:51:39.755
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:51:39.76
May  9 13:51:39.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:51:39.76
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:51:39.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:51:39.774
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-9d8dc2d8-1bd9-4d47-aa51-03b75211df29 05/09/23 13:51:39.776
STEP: Creating a pod to test consume configMaps 05/09/23 13:51:39.779
May  9 13:51:39.785: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6" in namespace "projected-1007" to be "Succeeded or Failed"
May  9 13:51:39.789: INFO: Pod "pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.505049ms
May  9 13:51:41.793: INFO: Pod "pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008262782s
May  9 13:51:43.795: INFO: Pod "pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009779544s
STEP: Saw pod success 05/09/23 13:51:43.795
May  9 13:51:43.795: INFO: Pod "pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6" satisfied condition "Succeeded or Failed"
May  9 13:51:43.797: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6 container projected-configmap-volume-test: <nil>
STEP: delete the pod 05/09/23 13:51:43.807
May  9 13:51:43.818: INFO: Waiting for pod pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6 to disappear
May  9 13:51:43.821: INFO: Pod pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  9 13:51:43.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1007" for this suite. 05/09/23 13:51:43.824
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":331,"skipped":6233,"failed":0}
------------------------------
• [4.069 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:51:39.76
    May  9 13:51:39.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:51:39.76
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:51:39.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:51:39.774
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-9d8dc2d8-1bd9-4d47-aa51-03b75211df29 05/09/23 13:51:39.776
    STEP: Creating a pod to test consume configMaps 05/09/23 13:51:39.779
    May  9 13:51:39.785: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6" in namespace "projected-1007" to be "Succeeded or Failed"
    May  9 13:51:39.789: INFO: Pod "pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.505049ms
    May  9 13:51:41.793: INFO: Pod "pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008262782s
    May  9 13:51:43.795: INFO: Pod "pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009779544s
    STEP: Saw pod success 05/09/23 13:51:43.795
    May  9 13:51:43.795: INFO: Pod "pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6" satisfied condition "Succeeded or Failed"
    May  9 13:51:43.797: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:51:43.807
    May  9 13:51:43.818: INFO: Waiting for pod pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6 to disappear
    May  9 13:51:43.821: INFO: Pod pod-projected-configmaps-f1b0a3e7-560e-4b64-a7c2-193bdb51e0d6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  9 13:51:43.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1007" for this suite. 05/09/23 13:51:43.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:51:43.829
May  9 13:51:43.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename certificates 05/09/23 13:51:43.83
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:51:43.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:51:43.844
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 05/09/23 13:51:44.477
STEP: getting /apis/certificates.k8s.io 05/09/23 13:51:44.479
STEP: getting /apis/certificates.k8s.io/v1 05/09/23 13:51:44.48
STEP: creating 05/09/23 13:51:44.481
STEP: getting 05/09/23 13:51:44.494
STEP: listing 05/09/23 13:51:44.496
STEP: watching 05/09/23 13:51:44.498
May  9 13:51:44.498: INFO: starting watch
STEP: patching 05/09/23 13:51:44.499
STEP: updating 05/09/23 13:51:44.503
May  9 13:51:44.508: INFO: waiting for watch events with expected annotations
May  9 13:51:44.508: INFO: saw patched and updated annotations
STEP: getting /approval 05/09/23 13:51:44.508
STEP: patching /approval 05/09/23 13:51:44.51
STEP: updating /approval 05/09/23 13:51:44.515
STEP: getting /status 05/09/23 13:51:44.52
STEP: patching /status 05/09/23 13:51:44.522
STEP: updating /status 05/09/23 13:51:44.527
STEP: deleting 05/09/23 13:51:44.532
STEP: deleting a collection 05/09/23 13:51:44.54
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:51:44.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5230" for this suite. 05/09/23 13:51:44.555
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":332,"skipped":6254,"failed":0}
------------------------------
• [0.730 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:51:43.829
    May  9 13:51:43.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename certificates 05/09/23 13:51:43.83
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:51:43.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:51:43.844
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 05/09/23 13:51:44.477
    STEP: getting /apis/certificates.k8s.io 05/09/23 13:51:44.479
    STEP: getting /apis/certificates.k8s.io/v1 05/09/23 13:51:44.48
    STEP: creating 05/09/23 13:51:44.481
    STEP: getting 05/09/23 13:51:44.494
    STEP: listing 05/09/23 13:51:44.496
    STEP: watching 05/09/23 13:51:44.498
    May  9 13:51:44.498: INFO: starting watch
    STEP: patching 05/09/23 13:51:44.499
    STEP: updating 05/09/23 13:51:44.503
    May  9 13:51:44.508: INFO: waiting for watch events with expected annotations
    May  9 13:51:44.508: INFO: saw patched and updated annotations
    STEP: getting /approval 05/09/23 13:51:44.508
    STEP: patching /approval 05/09/23 13:51:44.51
    STEP: updating /approval 05/09/23 13:51:44.515
    STEP: getting /status 05/09/23 13:51:44.52
    STEP: patching /status 05/09/23 13:51:44.522
    STEP: updating /status 05/09/23 13:51:44.527
    STEP: deleting 05/09/23 13:51:44.532
    STEP: deleting a collection 05/09/23 13:51:44.54
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:51:44.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-5230" for this suite. 05/09/23 13:51:44.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:51:44.56
May  9 13:51:44.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 13:51:44.561
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:51:44.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:51:44.577
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-636/configmap-test-5ce72ffe-6630-416c-aa22-2b2a0a770893 05/09/23 13:51:44.578
STEP: Creating a pod to test consume configMaps 05/09/23 13:51:44.582
May  9 13:51:44.591: INFO: Waiting up to 5m0s for pod "pod-configmaps-002031ea-e487-4120-b53a-18973e031e36" in namespace "configmap-636" to be "Succeeded or Failed"
May  9 13:51:44.602: INFO: Pod "pod-configmaps-002031ea-e487-4120-b53a-18973e031e36": Phase="Pending", Reason="", readiness=false. Elapsed: 10.446681ms
May  9 13:51:46.606: INFO: Pod "pod-configmaps-002031ea-e487-4120-b53a-18973e031e36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015091038s
May  9 13:51:48.606: INFO: Pod "pod-configmaps-002031ea-e487-4120-b53a-18973e031e36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014404221s
STEP: Saw pod success 05/09/23 13:51:48.606
May  9 13:51:48.606: INFO: Pod "pod-configmaps-002031ea-e487-4120-b53a-18973e031e36" satisfied condition "Succeeded or Failed"
May  9 13:51:48.607: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-002031ea-e487-4120-b53a-18973e031e36 container env-test: <nil>
STEP: delete the pod 05/09/23 13:51:48.615
May  9 13:51:48.623: INFO: Waiting for pod pod-configmaps-002031ea-e487-4120-b53a-18973e031e36 to disappear
May  9 13:51:48.625: INFO: Pod pod-configmaps-002031ea-e487-4120-b53a-18973e031e36 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May  9 13:51:48.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-636" for this suite. 05/09/23 13:51:48.627
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":333,"skipped":6279,"failed":0}
------------------------------
• [4.072 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:51:44.56
    May  9 13:51:44.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 13:51:44.561
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:51:44.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:51:44.577
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-636/configmap-test-5ce72ffe-6630-416c-aa22-2b2a0a770893 05/09/23 13:51:44.578
    STEP: Creating a pod to test consume configMaps 05/09/23 13:51:44.582
    May  9 13:51:44.591: INFO: Waiting up to 5m0s for pod "pod-configmaps-002031ea-e487-4120-b53a-18973e031e36" in namespace "configmap-636" to be "Succeeded or Failed"
    May  9 13:51:44.602: INFO: Pod "pod-configmaps-002031ea-e487-4120-b53a-18973e031e36": Phase="Pending", Reason="", readiness=false. Elapsed: 10.446681ms
    May  9 13:51:46.606: INFO: Pod "pod-configmaps-002031ea-e487-4120-b53a-18973e031e36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015091038s
    May  9 13:51:48.606: INFO: Pod "pod-configmaps-002031ea-e487-4120-b53a-18973e031e36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014404221s
    STEP: Saw pod success 05/09/23 13:51:48.606
    May  9 13:51:48.606: INFO: Pod "pod-configmaps-002031ea-e487-4120-b53a-18973e031e36" satisfied condition "Succeeded or Failed"
    May  9 13:51:48.607: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-configmaps-002031ea-e487-4120-b53a-18973e031e36 container env-test: <nil>
    STEP: delete the pod 05/09/23 13:51:48.615
    May  9 13:51:48.623: INFO: Waiting for pod pod-configmaps-002031ea-e487-4120-b53a-18973e031e36 to disappear
    May  9 13:51:48.625: INFO: Pod pod-configmaps-002031ea-e487-4120-b53a-18973e031e36 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 13:51:48.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-636" for this suite. 05/09/23 13:51:48.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:51:48.633
May  9 13:51:48.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename job 05/09/23 13:51:48.634
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:51:48.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:51:48.647
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 05/09/23 13:51:48.648
STEP: Ensuring active pods == parallelism 05/09/23 13:51:48.652
STEP: delete a job 05/09/23 13:51:50.656
STEP: deleting Job.batch foo in namespace job-8034, will wait for the garbage collector to delete the pods 05/09/23 13:51:50.656
May  9 13:51:50.713: INFO: Deleting Job.batch foo took: 4.328371ms
May  9 13:51:50.814: INFO: Terminating Job.batch foo pods took: 100.411771ms
STEP: Ensuring job was deleted 05/09/23 13:52:23.314
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  9 13:52:23.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8034" for this suite. 05/09/23 13:52:23.321
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":334,"skipped":6314,"failed":0}
------------------------------
• [SLOW TEST] [34.693 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:51:48.633
    May  9 13:51:48.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename job 05/09/23 13:51:48.634
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:51:48.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:51:48.647
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 05/09/23 13:51:48.648
    STEP: Ensuring active pods == parallelism 05/09/23 13:51:48.652
    STEP: delete a job 05/09/23 13:51:50.656
    STEP: deleting Job.batch foo in namespace job-8034, will wait for the garbage collector to delete the pods 05/09/23 13:51:50.656
    May  9 13:51:50.713: INFO: Deleting Job.batch foo took: 4.328371ms
    May  9 13:51:50.814: INFO: Terminating Job.batch foo pods took: 100.411771ms
    STEP: Ensuring job was deleted 05/09/23 13:52:23.314
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  9 13:52:23.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8034" for this suite. 05/09/23 13:52:23.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:52:23.327
May  9 13:52:23.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:52:23.328
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:52:23.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:52:23.341
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2111 05/09/23 13:52:23.342
STEP: changing the ExternalName service to type=ClusterIP 05/09/23 13:52:23.347
STEP: creating replication controller externalname-service in namespace services-2111 05/09/23 13:52:23.365
I0509 13:52:23.370254      24 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2111, replica count: 2
I0509 13:52:26.421415      24 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 13:52:26.421: INFO: Creating new exec pod
May  9 13:52:26.429: INFO: Waiting up to 5m0s for pod "execpod8jr4v" in namespace "services-2111" to be "running"
May  9 13:52:26.432: INFO: Pod "execpod8jr4v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.222542ms
May  9 13:52:28.435: INFO: Pod "execpod8jr4v": Phase="Running", Reason="", readiness=true. Elapsed: 2.005740902s
May  9 13:52:28.435: INFO: Pod "execpod8jr4v" satisfied condition "running"
May  9 13:52:29.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-2111 exec execpod8jr4v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May  9 13:52:29.533: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  9 13:52:29.533: INFO: stdout: "externalname-service-vb9t6"
May  9 13:52:29.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-2111 exec execpod8jr4v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.96.15 80'
May  9 13:52:29.626: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.96.15 80\nConnection to 10.111.96.15 80 port [tcp/http] succeeded!\n"
May  9 13:52:29.626: INFO: stdout: "externalname-service-mptqz"
May  9 13:52:29.626: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:52:29.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2111" for this suite. 05/09/23 13:52:29.648
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":335,"skipped":6346,"failed":0}
------------------------------
• [SLOW TEST] [6.326 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:52:23.327
    May  9 13:52:23.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:52:23.328
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:52:23.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:52:23.341
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2111 05/09/23 13:52:23.342
    STEP: changing the ExternalName service to type=ClusterIP 05/09/23 13:52:23.347
    STEP: creating replication controller externalname-service in namespace services-2111 05/09/23 13:52:23.365
    I0509 13:52:23.370254      24 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2111, replica count: 2
    I0509 13:52:26.421415      24 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 13:52:26.421: INFO: Creating new exec pod
    May  9 13:52:26.429: INFO: Waiting up to 5m0s for pod "execpod8jr4v" in namespace "services-2111" to be "running"
    May  9 13:52:26.432: INFO: Pod "execpod8jr4v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.222542ms
    May  9 13:52:28.435: INFO: Pod "execpod8jr4v": Phase="Running", Reason="", readiness=true. Elapsed: 2.005740902s
    May  9 13:52:28.435: INFO: Pod "execpod8jr4v" satisfied condition "running"
    May  9 13:52:29.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-2111 exec execpod8jr4v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    May  9 13:52:29.533: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May  9 13:52:29.533: INFO: stdout: "externalname-service-vb9t6"
    May  9 13:52:29.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-2111 exec execpod8jr4v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.96.15 80'
    May  9 13:52:29.626: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.96.15 80\nConnection to 10.111.96.15 80 port [tcp/http] succeeded!\n"
    May  9 13:52:29.626: INFO: stdout: "externalname-service-mptqz"
    May  9 13:52:29.626: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:52:29.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2111" for this suite. 05/09/23 13:52:29.648
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:52:29.653
May  9 13:52:29.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename conformance-tests 05/09/23 13:52:29.654
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:52:29.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:52:29.666
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 05/09/23 13:52:29.668
May  9 13:52:29.668: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
May  9 13:52:29.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-2496" for this suite. 05/09/23 13:52:29.675
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":336,"skipped":6351,"failed":0}
------------------------------
• [0.028 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:52:29.653
    May  9 13:52:29.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename conformance-tests 05/09/23 13:52:29.654
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:52:29.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:52:29.666
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 05/09/23 13:52:29.668
    May  9 13:52:29.668: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    May  9 13:52:29.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-2496" for this suite. 05/09/23 13:52:29.675
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:52:29.681
May  9 13:52:29.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:52:29.681
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:52:29.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:52:29.694
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-bb1cc9ef-80d5-4373-83aa-6697ded47e6a 05/09/23 13:52:29.699
STEP: Creating secret with name s-test-opt-upd-106db78d-9299-48dc-8fe9-b76d2948cec6 05/09/23 13:52:29.703
STEP: Creating the pod 05/09/23 13:52:29.706
May  9 13:52:29.714: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f445d060-0ee5-43ef-9b21-ae4bd0ba8ad7" in namespace "projected-5039" to be "running and ready"
May  9 13:52:29.719: INFO: Pod "pod-projected-secrets-f445d060-0ee5-43ef-9b21-ae4bd0ba8ad7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.240313ms
May  9 13:52:29.719: INFO: The phase of Pod pod-projected-secrets-f445d060-0ee5-43ef-9b21-ae4bd0ba8ad7 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:52:31.723: INFO: Pod "pod-projected-secrets-f445d060-0ee5-43ef-9b21-ae4bd0ba8ad7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008802441s
May  9 13:52:31.723: INFO: The phase of Pod pod-projected-secrets-f445d060-0ee5-43ef-9b21-ae4bd0ba8ad7 is Running (Ready = true)
May  9 13:52:31.723: INFO: Pod "pod-projected-secrets-f445d060-0ee5-43ef-9b21-ae4bd0ba8ad7" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-bb1cc9ef-80d5-4373-83aa-6697ded47e6a 05/09/23 13:52:31.745
STEP: Updating secret s-test-opt-upd-106db78d-9299-48dc-8fe9-b76d2948cec6 05/09/23 13:52:31.75
STEP: Creating secret with name s-test-opt-create-c1f5f99c-ec94-4722-85b0-0bff21ecd0a9 05/09/23 13:52:31.753
STEP: waiting to observe update in volume 05/09/23 13:52:31.757
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  9 13:52:35.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5039" for this suite. 05/09/23 13:52:35.784
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":337,"skipped":6352,"failed":0}
------------------------------
• [SLOW TEST] [6.108 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:52:29.681
    May  9 13:52:29.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:52:29.681
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:52:29.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:52:29.694
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-bb1cc9ef-80d5-4373-83aa-6697ded47e6a 05/09/23 13:52:29.699
    STEP: Creating secret with name s-test-opt-upd-106db78d-9299-48dc-8fe9-b76d2948cec6 05/09/23 13:52:29.703
    STEP: Creating the pod 05/09/23 13:52:29.706
    May  9 13:52:29.714: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f445d060-0ee5-43ef-9b21-ae4bd0ba8ad7" in namespace "projected-5039" to be "running and ready"
    May  9 13:52:29.719: INFO: Pod "pod-projected-secrets-f445d060-0ee5-43ef-9b21-ae4bd0ba8ad7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.240313ms
    May  9 13:52:29.719: INFO: The phase of Pod pod-projected-secrets-f445d060-0ee5-43ef-9b21-ae4bd0ba8ad7 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:52:31.723: INFO: Pod "pod-projected-secrets-f445d060-0ee5-43ef-9b21-ae4bd0ba8ad7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008802441s
    May  9 13:52:31.723: INFO: The phase of Pod pod-projected-secrets-f445d060-0ee5-43ef-9b21-ae4bd0ba8ad7 is Running (Ready = true)
    May  9 13:52:31.723: INFO: Pod "pod-projected-secrets-f445d060-0ee5-43ef-9b21-ae4bd0ba8ad7" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-bb1cc9ef-80d5-4373-83aa-6697ded47e6a 05/09/23 13:52:31.745
    STEP: Updating secret s-test-opt-upd-106db78d-9299-48dc-8fe9-b76d2948cec6 05/09/23 13:52:31.75
    STEP: Creating secret with name s-test-opt-create-c1f5f99c-ec94-4722-85b0-0bff21ecd0a9 05/09/23 13:52:31.753
    STEP: waiting to observe update in volume 05/09/23 13:52:31.757
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  9 13:52:35.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5039" for this suite. 05/09/23 13:52:35.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:52:35.79
May  9 13:52:35.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename kubelet-test 05/09/23 13:52:35.791
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:52:35.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:52:35.804
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
May  9 13:52:35.813: INFO: Waiting up to 5m0s for pod "busybox-scheduling-0b34c0d3-e5c4-4abf-97d0-bd34eccada3e" in namespace "kubelet-test-8096" to be "running and ready"
May  9 13:52:35.817: INFO: Pod "busybox-scheduling-0b34c0d3-e5c4-4abf-97d0-bd34eccada3e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.713169ms
May  9 13:52:35.817: INFO: The phase of Pod busybox-scheduling-0b34c0d3-e5c4-4abf-97d0-bd34eccada3e is Pending, waiting for it to be Running (with Ready = true)
May  9 13:52:37.820: INFO: Pod "busybox-scheduling-0b34c0d3-e5c4-4abf-97d0-bd34eccada3e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006826618s
May  9 13:52:37.820: INFO: The phase of Pod busybox-scheduling-0b34c0d3-e5c4-4abf-97d0-bd34eccada3e is Running (Ready = true)
May  9 13:52:37.820: INFO: Pod "busybox-scheduling-0b34c0d3-e5c4-4abf-97d0-bd34eccada3e" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  9 13:52:37.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8096" for this suite. 05/09/23 13:52:37.831
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":338,"skipped":6389,"failed":0}
------------------------------
• [2.045 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:52:35.79
    May  9 13:52:35.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename kubelet-test 05/09/23 13:52:35.791
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:52:35.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:52:35.804
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    May  9 13:52:35.813: INFO: Waiting up to 5m0s for pod "busybox-scheduling-0b34c0d3-e5c4-4abf-97d0-bd34eccada3e" in namespace "kubelet-test-8096" to be "running and ready"
    May  9 13:52:35.817: INFO: Pod "busybox-scheduling-0b34c0d3-e5c4-4abf-97d0-bd34eccada3e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.713169ms
    May  9 13:52:35.817: INFO: The phase of Pod busybox-scheduling-0b34c0d3-e5c4-4abf-97d0-bd34eccada3e is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:52:37.820: INFO: Pod "busybox-scheduling-0b34c0d3-e5c4-4abf-97d0-bd34eccada3e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006826618s
    May  9 13:52:37.820: INFO: The phase of Pod busybox-scheduling-0b34c0d3-e5c4-4abf-97d0-bd34eccada3e is Running (Ready = true)
    May  9 13:52:37.820: INFO: Pod "busybox-scheduling-0b34c0d3-e5c4-4abf-97d0-bd34eccada3e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  9 13:52:37.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8096" for this suite. 05/09/23 13:52:37.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:52:37.836
May  9 13:52:37.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename sched-preemption 05/09/23 13:52:37.837
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:52:37.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:52:37.852
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May  9 13:52:37.864: INFO: Waiting up to 1m0s for all nodes to be ready
May  9 13:53:37.894: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 05/09/23 13:53:37.896
May  9 13:53:37.909: INFO: Created pod: pod0-0-sched-preemption-low-priority
May  9 13:53:37.914: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May  9 13:53:37.926: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May  9 13:53:37.933: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May  9 13:53:37.947: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May  9 13:53:37.954: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/09/23 13:53:37.954
May  9 13:53:37.954: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-471" to be "running"
May  9 13:53:37.961: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.59128ms
May  9 13:53:39.965: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010671634s
May  9 13:53:39.965: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May  9 13:53:39.965: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-471" to be "running"
May  9 13:53:39.967: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.184589ms
May  9 13:53:39.967: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May  9 13:53:39.967: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-471" to be "running"
May  9 13:53:39.969: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.11164ms
May  9 13:53:39.969: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May  9 13:53:39.969: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-471" to be "running"
May  9 13:53:39.972: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.173979ms
May  9 13:53:39.972: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May  9 13:53:39.972: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-471" to be "running"
May  9 13:53:39.974: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.874939ms
May  9 13:53:39.974: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May  9 13:53:39.974: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-471" to be "running"
May  9 13:53:39.976: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.97506ms
May  9 13:53:39.976: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/09/23 13:53:39.976
May  9 13:53:39.981: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-471" to be "running"
May  9 13:53:39.983: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.595472ms
May  9 13:53:41.986: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005887393s
May  9 13:53:43.987: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006201671s
May  9 13:53:43.987: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May  9 13:53:43.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-471" for this suite. 05/09/23 13:53:44.001
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":339,"skipped":6395,"failed":0}
------------------------------
• [SLOW TEST] [66.208 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:52:37.836
    May  9 13:52:37.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename sched-preemption 05/09/23 13:52:37.837
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:52:37.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:52:37.852
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May  9 13:52:37.864: INFO: Waiting up to 1m0s for all nodes to be ready
    May  9 13:53:37.894: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 05/09/23 13:53:37.896
    May  9 13:53:37.909: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May  9 13:53:37.914: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May  9 13:53:37.926: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May  9 13:53:37.933: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May  9 13:53:37.947: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May  9 13:53:37.954: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/09/23 13:53:37.954
    May  9 13:53:37.954: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-471" to be "running"
    May  9 13:53:37.961: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.59128ms
    May  9 13:53:39.965: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010671634s
    May  9 13:53:39.965: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May  9 13:53:39.965: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-471" to be "running"
    May  9 13:53:39.967: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.184589ms
    May  9 13:53:39.967: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May  9 13:53:39.967: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-471" to be "running"
    May  9 13:53:39.969: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.11164ms
    May  9 13:53:39.969: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May  9 13:53:39.969: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-471" to be "running"
    May  9 13:53:39.972: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.173979ms
    May  9 13:53:39.972: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May  9 13:53:39.972: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-471" to be "running"
    May  9 13:53:39.974: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.874939ms
    May  9 13:53:39.974: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May  9 13:53:39.974: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-471" to be "running"
    May  9 13:53:39.976: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.97506ms
    May  9 13:53:39.976: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/09/23 13:53:39.976
    May  9 13:53:39.981: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-471" to be "running"
    May  9 13:53:39.983: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.595472ms
    May  9 13:53:41.986: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005887393s
    May  9 13:53:43.987: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006201671s
    May  9 13:53:43.987: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:53:43.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-471" for this suite. 05/09/23 13:53:44.001
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:53:44.045
May  9 13:53:44.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:53:44.045
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:53:44.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:53:44.057
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
May  9 13:53:44.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/09/23 13:53:46.525
May  9 13:53:46.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 create -f -'
May  9 13:53:47.047: INFO: stderr: ""
May  9 13:53:47.047: INFO: stdout: "e2e-test-crd-publish-openapi-5813-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  9 13:53:47.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 delete e2e-test-crd-publish-openapi-5813-crds test-foo'
May  9 13:53:47.122: INFO: stderr: ""
May  9 13:53:47.122: INFO: stdout: "e2e-test-crd-publish-openapi-5813-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May  9 13:53:47.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 apply -f -'
May  9 13:53:47.551: INFO: stderr: ""
May  9 13:53:47.551: INFO: stdout: "e2e-test-crd-publish-openapi-5813-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  9 13:53:47.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 delete e2e-test-crd-publish-openapi-5813-crds test-foo'
May  9 13:53:47.608: INFO: stderr: ""
May  9 13:53:47.608: INFO: stdout: "e2e-test-crd-publish-openapi-5813-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/09/23 13:53:47.608
May  9 13:53:47.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 create -f -'
May  9 13:53:47.737: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/09/23 13:53:47.737
May  9 13:53:47.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 create -f -'
May  9 13:53:47.869: INFO: rc: 1
May  9 13:53:47.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 apply -f -'
May  9 13:53:48.013: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/09/23 13:53:48.013
May  9 13:53:48.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 create -f -'
May  9 13:53:48.146: INFO: rc: 1
May  9 13:53:48.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 apply -f -'
May  9 13:53:48.281: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 05/09/23 13:53:48.281
May  9 13:53:48.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 explain e2e-test-crd-publish-openapi-5813-crds'
May  9 13:53:48.412: INFO: stderr: ""
May  9 13:53:48.412: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5813-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 05/09/23 13:53:48.412
May  9 13:53:48.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 explain e2e-test-crd-publish-openapi-5813-crds.metadata'
May  9 13:53:48.542: INFO: stderr: ""
May  9 13:53:48.542: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5813-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May  9 13:53:48.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 explain e2e-test-crd-publish-openapi-5813-crds.spec'
May  9 13:53:48.948: INFO: stderr: ""
May  9 13:53:48.948: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5813-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May  9 13:53:48.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 explain e2e-test-crd-publish-openapi-5813-crds.spec.bars'
May  9 13:53:49.075: INFO: stderr: ""
May  9 13:53:49.075: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5813-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/09/23 13:53:49.075
May  9 13:53:49.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 explain e2e-test-crd-publish-openapi-5813-crds.spec.bars2'
May  9 13:53:49.206: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:53:51.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9764" for this suite. 05/09/23 13:53:51.677
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":340,"skipped":6422,"failed":0}
------------------------------
• [SLOW TEST] [7.638 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:53:44.045
    May  9 13:53:44.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 13:53:44.045
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:53:44.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:53:44.057
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    May  9 13:53:44.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/09/23 13:53:46.525
    May  9 13:53:46.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 create -f -'
    May  9 13:53:47.047: INFO: stderr: ""
    May  9 13:53:47.047: INFO: stdout: "e2e-test-crd-publish-openapi-5813-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May  9 13:53:47.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 delete e2e-test-crd-publish-openapi-5813-crds test-foo'
    May  9 13:53:47.122: INFO: stderr: ""
    May  9 13:53:47.122: INFO: stdout: "e2e-test-crd-publish-openapi-5813-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    May  9 13:53:47.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 apply -f -'
    May  9 13:53:47.551: INFO: stderr: ""
    May  9 13:53:47.551: INFO: stdout: "e2e-test-crd-publish-openapi-5813-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May  9 13:53:47.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 delete e2e-test-crd-publish-openapi-5813-crds test-foo'
    May  9 13:53:47.608: INFO: stderr: ""
    May  9 13:53:47.608: INFO: stdout: "e2e-test-crd-publish-openapi-5813-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/09/23 13:53:47.608
    May  9 13:53:47.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 create -f -'
    May  9 13:53:47.737: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/09/23 13:53:47.737
    May  9 13:53:47.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 create -f -'
    May  9 13:53:47.869: INFO: rc: 1
    May  9 13:53:47.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 apply -f -'
    May  9 13:53:48.013: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/09/23 13:53:48.013
    May  9 13:53:48.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 create -f -'
    May  9 13:53:48.146: INFO: rc: 1
    May  9 13:53:48.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 --namespace=crd-publish-openapi-9764 apply -f -'
    May  9 13:53:48.281: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 05/09/23 13:53:48.281
    May  9 13:53:48.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 explain e2e-test-crd-publish-openapi-5813-crds'
    May  9 13:53:48.412: INFO: stderr: ""
    May  9 13:53:48.412: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5813-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 05/09/23 13:53:48.412
    May  9 13:53:48.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 explain e2e-test-crd-publish-openapi-5813-crds.metadata'
    May  9 13:53:48.542: INFO: stderr: ""
    May  9 13:53:48.542: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5813-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    May  9 13:53:48.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 explain e2e-test-crd-publish-openapi-5813-crds.spec'
    May  9 13:53:48.948: INFO: stderr: ""
    May  9 13:53:48.948: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5813-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    May  9 13:53:48.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 explain e2e-test-crd-publish-openapi-5813-crds.spec.bars'
    May  9 13:53:49.075: INFO: stderr: ""
    May  9 13:53:49.075: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5813-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/09/23 13:53:49.075
    May  9 13:53:49.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=crd-publish-openapi-9764 explain e2e-test-crd-publish-openapi-5813-crds.spec.bars2'
    May  9 13:53:49.206: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:53:51.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9764" for this suite. 05/09/23 13:53:51.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:53:51.684
May  9 13:53:51.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename ephemeral-containers-test 05/09/23 13:53:51.685
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:53:51.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:53:51.699
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 05/09/23 13:53:51.701
May  9 13:53:51.715: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-853" to be "running and ready"
May  9 13:53:51.718: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901494ms
May  9 13:53:51.718: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
May  9 13:53:53.722: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007610385s
May  9 13:53:53.722: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
May  9 13:53:53.722: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 05/09/23 13:53:53.725
May  9 13:53:53.737: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-853" to be "container debugger running"
May  9 13:53:53.739: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.245255ms
May  9 13:53:55.743: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005828713s
May  9 13:53:57.743: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006252054s
May  9 13:53:57.743: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 05/09/23 13:53:57.743
May  9 13:53:57.743: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-853 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:53:57.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:53:57.744: INFO: ExecWithOptions: Clientset creation
May  9 13:53:57.744: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-853/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
May  9 13:53:57.788: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
May  9 13:53:57.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-853" for this suite. 05/09/23 13:53:57.803
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":341,"skipped":6433,"failed":0}
------------------------------
• [SLOW TEST] [6.124 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:53:51.684
    May  9 13:53:51.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename ephemeral-containers-test 05/09/23 13:53:51.685
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:53:51.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:53:51.699
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 05/09/23 13:53:51.701
    May  9 13:53:51.715: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-853" to be "running and ready"
    May  9 13:53:51.718: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901494ms
    May  9 13:53:51.718: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:53:53.722: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007610385s
    May  9 13:53:53.722: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    May  9 13:53:53.722: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 05/09/23 13:53:53.725
    May  9 13:53:53.737: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-853" to be "container debugger running"
    May  9 13:53:53.739: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.245255ms
    May  9 13:53:55.743: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005828713s
    May  9 13:53:57.743: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006252054s
    May  9 13:53:57.743: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 05/09/23 13:53:57.743
    May  9 13:53:57.743: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-853 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:53:57.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:53:57.744: INFO: ExecWithOptions: Clientset creation
    May  9 13:53:57.744: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-853/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    May  9 13:53:57.788: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    May  9 13:53:57.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-853" for this suite. 05/09/23 13:53:57.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:53:57.809
May  9 13:53:57.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename configmap 05/09/23 13:53:57.81
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:53:57.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:53:57.825
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-26e34790-b9b3-4a73-b4b1-b2e01aedfdd0 05/09/23 13:53:57.827
STEP: Creating a pod to test consume configMaps 05/09/23 13:53:57.831
May  9 13:53:57.840: INFO: Waiting up to 5m0s for pod "pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988" in namespace "configmap-2144" to be "Succeeded or Failed"
May  9 13:53:57.849: INFO: Pod "pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988": Phase="Pending", Reason="", readiness=false. Elapsed: 9.341775ms
May  9 13:53:59.854: INFO: Pod "pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013732525s
May  9 13:54:01.853: INFO: Pod "pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013318091s
STEP: Saw pod success 05/09/23 13:54:01.853
May  9 13:54:01.854: INFO: Pod "pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988" satisfied condition "Succeeded or Failed"
May  9 13:54:01.857: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 13:54:01.868
May  9 13:54:01.880: INFO: Waiting for pod pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988 to disappear
May  9 13:54:01.883: INFO: Pod pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  9 13:54:01.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2144" for this suite. 05/09/23 13:54:01.887
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":342,"skipped":6440,"failed":0}
------------------------------
• [4.085 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:53:57.809
    May  9 13:53:57.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename configmap 05/09/23 13:53:57.81
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:53:57.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:53:57.825
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-26e34790-b9b3-4a73-b4b1-b2e01aedfdd0 05/09/23 13:53:57.827
    STEP: Creating a pod to test consume configMaps 05/09/23 13:53:57.831
    May  9 13:53:57.840: INFO: Waiting up to 5m0s for pod "pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988" in namespace "configmap-2144" to be "Succeeded or Failed"
    May  9 13:53:57.849: INFO: Pod "pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988": Phase="Pending", Reason="", readiness=false. Elapsed: 9.341775ms
    May  9 13:53:59.854: INFO: Pod "pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013732525s
    May  9 13:54:01.853: INFO: Pod "pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013318091s
    STEP: Saw pod success 05/09/23 13:54:01.853
    May  9 13:54:01.854: INFO: Pod "pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988" satisfied condition "Succeeded or Failed"
    May  9 13:54:01.857: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-879bk pod pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 13:54:01.868
    May  9 13:54:01.880: INFO: Waiting for pod pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988 to disappear
    May  9 13:54:01.883: INFO: Pod pod-configmaps-38ac8c09-1d7e-49b2-941a-cf762fd03988 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  9 13:54:01.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2144" for this suite. 05/09/23 13:54:01.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:54:01.895
May  9 13:54:01.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:54:01.895
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:54:01.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:54:01.912
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-462cfc04-d010-4fa9-9388-bdfa614ccfcf 05/09/23 13:54:01.914
STEP: Creating a pod to test consume secrets 05/09/23 13:54:01.919
May  9 13:54:01.927: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41" in namespace "projected-6714" to be "Succeeded or Failed"
May  9 13:54:01.932: INFO: Pod "pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41": Phase="Pending", Reason="", readiness=false. Elapsed: 5.028513ms
May  9 13:54:03.937: INFO: Pod "pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009947901s
May  9 13:54:05.935: INFO: Pod "pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008621176s
STEP: Saw pod success 05/09/23 13:54:05.935
May  9 13:54:05.935: INFO: Pod "pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41" satisfied condition "Succeeded or Failed"
May  9 13:54:05.938: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/09/23 13:54:05.944
May  9 13:54:05.959: INFO: Waiting for pod pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41 to disappear
May  9 13:54:05.961: INFO: Pod pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  9 13:54:05.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6714" for this suite. 05/09/23 13:54:05.965
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":343,"skipped":6471,"failed":0}
------------------------------
• [4.077 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:54:01.895
    May  9 13:54:01.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:54:01.895
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:54:01.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:54:01.912
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-462cfc04-d010-4fa9-9388-bdfa614ccfcf 05/09/23 13:54:01.914
    STEP: Creating a pod to test consume secrets 05/09/23 13:54:01.919
    May  9 13:54:01.927: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41" in namespace "projected-6714" to be "Succeeded or Failed"
    May  9 13:54:01.932: INFO: Pod "pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41": Phase="Pending", Reason="", readiness=false. Elapsed: 5.028513ms
    May  9 13:54:03.937: INFO: Pod "pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009947901s
    May  9 13:54:05.935: INFO: Pod "pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008621176s
    STEP: Saw pod success 05/09/23 13:54:05.935
    May  9 13:54:05.935: INFO: Pod "pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41" satisfied condition "Succeeded or Failed"
    May  9 13:54:05.938: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 13:54:05.944
    May  9 13:54:05.959: INFO: Waiting for pod pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41 to disappear
    May  9 13:54:05.961: INFO: Pod pod-projected-secrets-165c5bc9-6400-4475-8512-3f7d07f05c41 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  9 13:54:05.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6714" for this suite. 05/09/23 13:54:05.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:54:05.972
May  9 13:54:05.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename resourcequota 05/09/23 13:54:05.973
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:54:05.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:54:05.988
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 05/09/23 13:54:05.99
STEP: Creating a ResourceQuota 05/09/23 13:54:10.993
STEP: Ensuring resource quota status is calculated 05/09/23 13:54:10.999
STEP: Creating a ReplicaSet 05/09/23 13:54:13.003
STEP: Ensuring resource quota status captures replicaset creation 05/09/23 13:54:13.016
STEP: Deleting a ReplicaSet 05/09/23 13:54:15.02
STEP: Ensuring resource quota status released usage 05/09/23 13:54:15.026
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  9 13:54:17.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8550" for this suite. 05/09/23 13:54:17.034
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":344,"skipped":6490,"failed":0}
------------------------------
• [SLOW TEST] [11.070 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:54:05.972
    May  9 13:54:05.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename resourcequota 05/09/23 13:54:05.973
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:54:05.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:54:05.988
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 05/09/23 13:54:05.99
    STEP: Creating a ResourceQuota 05/09/23 13:54:10.993
    STEP: Ensuring resource quota status is calculated 05/09/23 13:54:10.999
    STEP: Creating a ReplicaSet 05/09/23 13:54:13.003
    STEP: Ensuring resource quota status captures replicaset creation 05/09/23 13:54:13.016
    STEP: Deleting a ReplicaSet 05/09/23 13:54:15.02
    STEP: Ensuring resource quota status released usage 05/09/23 13:54:15.026
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  9 13:54:17.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8550" for this suite. 05/09/23 13:54:17.034
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:54:17.043
May  9 13:54:17.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename webhook 05/09/23 13:54:17.044
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:54:17.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:54:17.061
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/09/23 13:54:17.074
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:54:17.777
STEP: Deploying the webhook pod 05/09/23 13:54:17.787
STEP: Wait for the deployment to be ready 05/09/23 13:54:17.799
May  9 13:54:17.804: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/09/23 13:54:19.815
STEP: Verifying the service has paired with the endpoint 05/09/23 13:54:19.829
May  9 13:54:20.829: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 05/09/23 13:54:20.833
STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 13:54:20.847
STEP: Updating a validating webhook configuration's rules to not include the create operation 05/09/23 13:54:20.853
STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 13:54:20.865
STEP: Patching a validating webhook configuration's rules to include the create operation 05/09/23 13:54:20.874
STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 13:54:20.88
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  9 13:54:20.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8347" for this suite. 05/09/23 13:54:20.892
STEP: Destroying namespace "webhook-8347-markers" for this suite. 05/09/23 13:54:20.897
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":345,"skipped":6494,"failed":0}
------------------------------
• [3.903 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:54:17.043
    May  9 13:54:17.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename webhook 05/09/23 13:54:17.044
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:54:17.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:54:17.061
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/09/23 13:54:17.074
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 13:54:17.777
    STEP: Deploying the webhook pod 05/09/23 13:54:17.787
    STEP: Wait for the deployment to be ready 05/09/23 13:54:17.799
    May  9 13:54:17.804: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/09/23 13:54:19.815
    STEP: Verifying the service has paired with the endpoint 05/09/23 13:54:19.829
    May  9 13:54:20.829: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 05/09/23 13:54:20.833
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 13:54:20.847
    STEP: Updating a validating webhook configuration's rules to not include the create operation 05/09/23 13:54:20.853
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 13:54:20.865
    STEP: Patching a validating webhook configuration's rules to include the create operation 05/09/23 13:54:20.874
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 13:54:20.88
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  9 13:54:20.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8347" for this suite. 05/09/23 13:54:20.892
    STEP: Destroying namespace "webhook-8347-markers" for this suite. 05/09/23 13:54:20.897
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:54:20.947
May  9 13:54:20.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename deployment 05/09/23 13:54:20.948
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:54:20.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:54:20.966
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
May  9 13:54:20.978: INFO: Pod name rollover-pod: Found 0 pods out of 1
May  9 13:54:25.984: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/09/23 13:54:25.984
May  9 13:54:25.984: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May  9 13:54:27.988: INFO: Creating deployment "test-rollover-deployment"
May  9 13:54:27.995: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May  9 13:54:30.001: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May  9 13:54:30.007: INFO: Ensure that both replica sets have 1 created replica
May  9 13:54:30.012: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May  9 13:54:30.020: INFO: Updating deployment test-rollover-deployment
May  9 13:54:30.020: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May  9 13:54:32.027: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May  9 13:54:32.033: INFO: Make sure deployment "test-rollover-deployment" is complete
May  9 13:54:32.038: INFO: all replica sets need to contain the pod-template-hash label
May  9 13:54:32.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:54:34.045: INFO: all replica sets need to contain the pod-template-hash label
May  9 13:54:34.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:54:36.045: INFO: all replica sets need to contain the pod-template-hash label
May  9 13:54:36.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:54:38.044: INFO: all replica sets need to contain the pod-template-hash label
May  9 13:54:38.044: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:54:40.045: INFO: all replica sets need to contain the pod-template-hash label
May  9 13:54:40.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 13:54:42.045: INFO: 
May  9 13:54:42.045: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 13:54:42.052: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4582  6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64 52010 2 2023-05-09 13:54:27 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-09 13:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00537faa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 13:54:28 +0000 UTC,LastTransitionTime:2023-05-09 13:54:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-05-09 13:54:41 +0000 UTC,LastTransitionTime:2023-05-09 13:54:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  9 13:54:42.055: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-4582  67787e2a-8b64-425d-acbc-eef35356c687 52000 2 2023-05-09 13:54:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64 0xc000461627 0xc000461628}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:54:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000461f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  9 13:54:42.055: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May  9 13:54:42.055: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4582  e9c421a4-b8e2-4986-9d00-47e031704331 52009 2 2023-05-09 13:54:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64 0xc00537fe37 0xc00537fe38}] [] [{e2e.test Update apps/v1 2023-05-09 13:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:54:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00537fef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 13:54:42.055: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-4582  e5b6a375-43fc-452a-acb8-b05bb75e4b42 51934 2 2023-05-09 13:54:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64 0xc00537ff67 0xc00537ff68}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:54:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000460c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 13:54:42.058: INFO: Pod "test-rollover-deployment-6d45fd857b-s4vhg" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-s4vhg test-rollover-deployment-6d45fd857b- deployment-4582  049d7fa4-99a0-4cef-a035-37ca17097584 51951 0 2023-05-09 13:54:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:3ed9c43f76a60cc501639e814ad03ac969850ae0f8ba93a7cfb57bb8585dc71b cni.projectcalico.org/podIP:172.25.124.215/32 cni.projectcalico.org/podIPs:172.25.124.215/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 67787e2a-8b64-425d-acbc-eef35356c687 0xc00502e547 0xc00502e548}] [] [{calico Update v1 2023-05-09 13:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 13:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67787e2a-8b64-425d-acbc-eef35356c687\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:54:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hpfvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hpfvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:54:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:54:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:54:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:54:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.215,StartTime:2023-05-09 13:54:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:54:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://76c3f41579aa8d301441787711851a0503ed56d1d408ce5125c9161306fd3a4b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  9 13:54:42.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4582" for this suite. 05/09/23 13:54:42.062
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":346,"skipped":6498,"failed":0}
------------------------------
• [SLOW TEST] [21.120 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:54:20.947
    May  9 13:54:20.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename deployment 05/09/23 13:54:20.948
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:54:20.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:54:20.966
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    May  9 13:54:20.978: INFO: Pod name rollover-pod: Found 0 pods out of 1
    May  9 13:54:25.984: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/09/23 13:54:25.984
    May  9 13:54:25.984: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    May  9 13:54:27.988: INFO: Creating deployment "test-rollover-deployment"
    May  9 13:54:27.995: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    May  9 13:54:30.001: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    May  9 13:54:30.007: INFO: Ensure that both replica sets have 1 created replica
    May  9 13:54:30.012: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    May  9 13:54:30.020: INFO: Updating deployment test-rollover-deployment
    May  9 13:54:30.020: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    May  9 13:54:32.027: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    May  9 13:54:32.033: INFO: Make sure deployment "test-rollover-deployment" is complete
    May  9 13:54:32.038: INFO: all replica sets need to contain the pod-template-hash label
    May  9 13:54:32.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:54:34.045: INFO: all replica sets need to contain the pod-template-hash label
    May  9 13:54:34.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:54:36.045: INFO: all replica sets need to contain the pod-template-hash label
    May  9 13:54:36.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:54:38.044: INFO: all replica sets need to contain the pod-template-hash label
    May  9 13:54:38.044: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:54:40.045: INFO: all replica sets need to contain the pod-template-hash label
    May  9 13:54:40.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 13, 54, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 13, 54, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 13:54:42.045: INFO: 
    May  9 13:54:42.045: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 13:54:42.052: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4582  6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64 52010 2 2023-05-09 13:54:27 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-09 13:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00537faa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 13:54:28 +0000 UTC,LastTransitionTime:2023-05-09 13:54:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-05-09 13:54:41 +0000 UTC,LastTransitionTime:2023-05-09 13:54:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  9 13:54:42.055: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-4582  67787e2a-8b64-425d-acbc-eef35356c687 52000 2 2023-05-09 13:54:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64 0xc000461627 0xc000461628}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:54:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000461f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  9 13:54:42.055: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    May  9 13:54:42.055: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4582  e9c421a4-b8e2-4986-9d00-47e031704331 52009 2 2023-05-09 13:54:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64 0xc00537fe37 0xc00537fe38}] [] [{e2e.test Update apps/v1 2023-05-09 13:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:54:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00537fef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 13:54:42.055: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-4582  e5b6a375-43fc-452a-acb8-b05bb75e4b42 51934 2 2023-05-09 13:54:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64 0xc00537ff67 0xc00537ff68}] [] [{kube-controller-manager Update apps/v1 2023-05-09 13:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fd0bcf8-dc90-42ed-bdde-cac0ea90cd64\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 13:54:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000460c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 13:54:42.058: INFO: Pod "test-rollover-deployment-6d45fd857b-s4vhg" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-s4vhg test-rollover-deployment-6d45fd857b- deployment-4582  049d7fa4-99a0-4cef-a035-37ca17097584 51951 0 2023-05-09 13:54:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:3ed9c43f76a60cc501639e814ad03ac969850ae0f8ba93a7cfb57bb8585dc71b cni.projectcalico.org/podIP:172.25.124.215/32 cni.projectcalico.org/podIPs:172.25.124.215/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 67787e2a-8b64-425d-acbc-eef35356c687 0xc00502e547 0xc00502e548}] [] [{calico Update v1 2023-05-09 13:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 13:54:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67787e2a-8b64-425d-acbc-eef35356c687\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 13:54:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.124.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hpfvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hpfvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cl-gks-cncf-ix1-md-0-48ljh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:54:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:54:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:54:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 13:54:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.64,PodIP:172.25.124.215,StartTime:2023-05-09 13:54:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 13:54:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://76c3f41579aa8d301441787711851a0503ed56d1d408ce5125c9161306fd3a4b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.124.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  9 13:54:42.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4582" for this suite. 05/09/23 13:54:42.062
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:54:42.068
May  9 13:54:42.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename statefulset 05/09/23 13:54:42.068
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:54:42.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:54:42.084
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6142 05/09/23 13:54:42.086
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 05/09/23 13:54:42.091
STEP: Creating stateful set ss in namespace statefulset-6142 05/09/23 13:54:42.096
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6142 05/09/23 13:54:42.104
May  9 13:54:42.107: INFO: Found 0 stateful pods, waiting for 1
May  9 13:54:52.112: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/09/23 13:54:52.112
May  9 13:54:52.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 13:54:52.215: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 13:54:52.215: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 13:54:52.215: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 13:54:52.218: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  9 13:55:02.223: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  9 13:55:02.223: INFO: Waiting for statefulset status.replicas updated to 0
May  9 13:55:02.240: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999769s
May  9 13:55:03.243: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995689079s
May  9 13:55:04.247: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992532624s
May  9 13:55:05.250: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988929476s
May  9 13:55:06.256: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984958628s
May  9 13:55:07.260: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979220862s
May  9 13:55:08.264: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975275267s
May  9 13:55:09.267: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.971748396s
May  9 13:55:10.270: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.96861401s
May  9 13:55:11.274: INFO: Verifying statefulset ss doesn't scale past 1 for another 965.383188ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6142 05/09/23 13:55:12.275
May  9 13:55:12.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 13:55:12.376: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 13:55:12.376: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 13:55:12.376: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 13:55:12.379: INFO: Found 1 stateful pods, waiting for 3
May  9 13:55:22.384: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  9 13:55:22.384: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  9 13:55:22.384: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 05/09/23 13:55:22.384
STEP: Scale down will halt with unhealthy stateful pod 05/09/23 13:55:22.384
May  9 13:55:22.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 13:55:22.478: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 13:55:22.478: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 13:55:22.478: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 13:55:22.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 13:55:22.578: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 13:55:22.578: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 13:55:22.578: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 13:55:22.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 13:55:22.683: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 13:55:22.683: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 13:55:22.683: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 13:55:22.683: INFO: Waiting for statefulset status.replicas updated to 0
May  9 13:55:22.686: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May  9 13:55:32.693: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  9 13:55:32.693: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  9 13:55:32.693: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  9 13:55:32.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999789s
May  9 13:55:33.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99643824s
May  9 13:55:34.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992701384s
May  9 13:55:35.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988611152s
May  9 13:55:36.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984634165s
May  9 13:55:37.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979960699s
May  9 13:55:38.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975573998s
May  9 13:55:39.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972017695s
May  9 13:55:40.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967860606s
May  9 13:55:41.742: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.385535ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6142 05/09/23 13:55:42.742
May  9 13:55:42.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 13:55:42.842: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 13:55:42.842: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 13:55:42.842: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 13:55:42.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 13:55:42.942: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 13:55:42.942: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 13:55:42.942: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 13:55:42.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 13:55:43.047: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 13:55:43.047: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 13:55:43.047: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 13:55:43.047: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 05/09/23 13:55:53.06
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  9 13:55:53.060: INFO: Deleting all statefulset in ns statefulset-6142
May  9 13:55:53.063: INFO: Scaling statefulset ss to 0
May  9 13:55:53.071: INFO: Waiting for statefulset status.replicas updated to 0
May  9 13:55:53.073: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  9 13:55:53.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6142" for this suite. 05/09/23 13:55:53.089
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":347,"skipped":6498,"failed":0}
------------------------------
• [SLOW TEST] [71.030 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:54:42.068
    May  9 13:54:42.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename statefulset 05/09/23 13:54:42.068
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:54:42.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:54:42.084
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6142 05/09/23 13:54:42.086
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 05/09/23 13:54:42.091
    STEP: Creating stateful set ss in namespace statefulset-6142 05/09/23 13:54:42.096
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6142 05/09/23 13:54:42.104
    May  9 13:54:42.107: INFO: Found 0 stateful pods, waiting for 1
    May  9 13:54:52.112: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/09/23 13:54:52.112
    May  9 13:54:52.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 13:54:52.215: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 13:54:52.215: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 13:54:52.215: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 13:54:52.218: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May  9 13:55:02.223: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  9 13:55:02.223: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 13:55:02.240: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999769s
    May  9 13:55:03.243: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995689079s
    May  9 13:55:04.247: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992532624s
    May  9 13:55:05.250: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988929476s
    May  9 13:55:06.256: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984958628s
    May  9 13:55:07.260: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979220862s
    May  9 13:55:08.264: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975275267s
    May  9 13:55:09.267: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.971748396s
    May  9 13:55:10.270: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.96861401s
    May  9 13:55:11.274: INFO: Verifying statefulset ss doesn't scale past 1 for another 965.383188ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6142 05/09/23 13:55:12.275
    May  9 13:55:12.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 13:55:12.376: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 13:55:12.376: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 13:55:12.376: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 13:55:12.379: INFO: Found 1 stateful pods, waiting for 3
    May  9 13:55:22.384: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  9 13:55:22.384: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May  9 13:55:22.384: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 05/09/23 13:55:22.384
    STEP: Scale down will halt with unhealthy stateful pod 05/09/23 13:55:22.384
    May  9 13:55:22.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 13:55:22.478: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 13:55:22.478: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 13:55:22.478: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 13:55:22.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 13:55:22.578: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 13:55:22.578: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 13:55:22.578: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 13:55:22.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 13:55:22.683: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 13:55:22.683: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 13:55:22.683: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 13:55:22.683: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 13:55:22.686: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    May  9 13:55:32.693: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  9 13:55:32.693: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May  9 13:55:32.693: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May  9 13:55:32.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999789s
    May  9 13:55:33.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99643824s
    May  9 13:55:34.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992701384s
    May  9 13:55:35.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988611152s
    May  9 13:55:36.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984634165s
    May  9 13:55:37.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979960699s
    May  9 13:55:38.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975573998s
    May  9 13:55:39.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972017695s
    May  9 13:55:40.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967860606s
    May  9 13:55:41.742: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.385535ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6142 05/09/23 13:55:42.742
    May  9 13:55:42.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 13:55:42.842: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 13:55:42.842: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 13:55:42.842: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 13:55:42.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 13:55:42.942: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 13:55:42.942: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 13:55:42.942: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 13:55:42.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=statefulset-6142 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 13:55:43.047: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 13:55:43.047: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 13:55:43.047: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 13:55:43.047: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 05/09/23 13:55:53.06
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  9 13:55:53.060: INFO: Deleting all statefulset in ns statefulset-6142
    May  9 13:55:53.063: INFO: Scaling statefulset ss to 0
    May  9 13:55:53.071: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 13:55:53.073: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  9 13:55:53.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6142" for this suite. 05/09/23 13:55:53.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:55:53.098
May  9 13:55:53.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename var-expansion 05/09/23 13:55:53.099
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:55:53.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:55:53.113
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 05/09/23 13:55:53.115
STEP: waiting for pod running 05/09/23 13:55:53.124
May  9 13:55:53.124: INFO: Waiting up to 2m0s for pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2" in namespace "var-expansion-2127" to be "running"
May  9 13:55:53.130: INFO: Pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.711964ms
May  9 13:55:55.134: INFO: Pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010142018s
May  9 13:55:55.134: INFO: Pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2" satisfied condition "running"
STEP: creating a file in subpath 05/09/23 13:55:55.134
May  9 13:55:55.137: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2127 PodName:var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:55:55.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:55:55.137: INFO: ExecWithOptions: Clientset creation
May  9 13:55:55.137: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2127/pods/var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 05/09/23 13:55:55.187
May  9 13:55:55.190: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2127 PodName:var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:55:55.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:55:55.190: INFO: ExecWithOptions: Clientset creation
May  9 13:55:55.190: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2127/pods/var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 05/09/23 13:55:55.241
May  9 13:55:55.754: INFO: Successfully updated pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2"
STEP: waiting for annotated pod running 05/09/23 13:55:55.754
May  9 13:55:55.754: INFO: Waiting up to 2m0s for pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2" in namespace "var-expansion-2127" to be "running"
May  9 13:55:55.756: INFO: Pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.341788ms
May  9 13:55:55.756: INFO: Pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2" satisfied condition "running"
STEP: deleting the pod gracefully 05/09/23 13:55:55.756
May  9 13:55:55.756: INFO: Deleting pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2" in namespace "var-expansion-2127"
May  9 13:55:55.763: INFO: Wait up to 5m0s for pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  9 13:56:29.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2127" for this suite. 05/09/23 13:56:29.773
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":348,"skipped":6504,"failed":0}
------------------------------
• [SLOW TEST] [36.681 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:55:53.098
    May  9 13:55:53.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename var-expansion 05/09/23 13:55:53.099
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:55:53.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:55:53.113
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 05/09/23 13:55:53.115
    STEP: waiting for pod running 05/09/23 13:55:53.124
    May  9 13:55:53.124: INFO: Waiting up to 2m0s for pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2" in namespace "var-expansion-2127" to be "running"
    May  9 13:55:53.130: INFO: Pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.711964ms
    May  9 13:55:55.134: INFO: Pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010142018s
    May  9 13:55:55.134: INFO: Pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2" satisfied condition "running"
    STEP: creating a file in subpath 05/09/23 13:55:55.134
    May  9 13:55:55.137: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2127 PodName:var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:55:55.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:55:55.137: INFO: ExecWithOptions: Clientset creation
    May  9 13:55:55.137: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2127/pods/var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 05/09/23 13:55:55.187
    May  9 13:55:55.190: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2127 PodName:var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:55:55.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:55:55.190: INFO: ExecWithOptions: Clientset creation
    May  9 13:55:55.190: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2127/pods/var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 05/09/23 13:55:55.241
    May  9 13:55:55.754: INFO: Successfully updated pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2"
    STEP: waiting for annotated pod running 05/09/23 13:55:55.754
    May  9 13:55:55.754: INFO: Waiting up to 2m0s for pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2" in namespace "var-expansion-2127" to be "running"
    May  9 13:55:55.756: INFO: Pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.341788ms
    May  9 13:55:55.756: INFO: Pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2" satisfied condition "running"
    STEP: deleting the pod gracefully 05/09/23 13:55:55.756
    May  9 13:55:55.756: INFO: Deleting pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2" in namespace "var-expansion-2127"
    May  9 13:55:55.763: INFO: Wait up to 5m0s for pod "var-expansion-914a9d18-e3e4-4d48-b6e6-878d309586a2" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  9 13:56:29.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2127" for this suite. 05/09/23 13:56:29.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:56:29.78
May  9 13:56:29.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename projected 05/09/23 13:56:29.78
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:29.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:29.795
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 05/09/23 13:56:29.797
May  9 13:56:29.804: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8" in namespace "projected-5394" to be "Succeeded or Failed"
May  9 13:56:29.809: INFO: Pod "downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.010049ms
May  9 13:56:31.812: INFO: Pod "downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530503s
May  9 13:56:33.813: INFO: Pod "downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009517925s
STEP: Saw pod success 05/09/23 13:56:33.813
May  9 13:56:33.814: INFO: Pod "downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8" satisfied condition "Succeeded or Failed"
May  9 13:56:33.817: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8 container client-container: <nil>
STEP: delete the pod 05/09/23 13:56:33.829
May  9 13:56:33.841: INFO: Waiting for pod downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8 to disappear
May  9 13:56:33.843: INFO: Pod downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  9 13:56:33.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5394" for this suite. 05/09/23 13:56:33.847
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":349,"skipped":6516,"failed":0}
------------------------------
• [4.073 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:56:29.78
    May  9 13:56:29.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename projected 05/09/23 13:56:29.78
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:29.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:29.795
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 05/09/23 13:56:29.797
    May  9 13:56:29.804: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8" in namespace "projected-5394" to be "Succeeded or Failed"
    May  9 13:56:29.809: INFO: Pod "downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.010049ms
    May  9 13:56:31.812: INFO: Pod "downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530503s
    May  9 13:56:33.813: INFO: Pod "downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009517925s
    STEP: Saw pod success 05/09/23 13:56:33.813
    May  9 13:56:33.814: INFO: Pod "downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8" satisfied condition "Succeeded or Failed"
    May  9 13:56:33.817: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8 container client-container: <nil>
    STEP: delete the pod 05/09/23 13:56:33.829
    May  9 13:56:33.841: INFO: Waiting for pod downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8 to disappear
    May  9 13:56:33.843: INFO: Pod downwardapi-volume-58f61d6d-3421-4251-9bcc-0689377f31b8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  9 13:56:33.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5394" for this suite. 05/09/23 13:56:33.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:56:33.853
May  9 13:56:33.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:56:33.854
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:33.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:33.869
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 05/09/23 13:56:33.873
STEP: waiting for available Endpoint 05/09/23 13:56:33.877
STEP: listing all Endpoints 05/09/23 13:56:33.878
STEP: updating the Endpoint 05/09/23 13:56:33.881
STEP: fetching the Endpoint 05/09/23 13:56:33.886
STEP: patching the Endpoint 05/09/23 13:56:33.888
STEP: fetching the Endpoint 05/09/23 13:56:33.895
STEP: deleting the Endpoint by Collection 05/09/23 13:56:33.898
STEP: waiting for Endpoint deletion 05/09/23 13:56:33.905
STEP: fetching the Endpoint 05/09/23 13:56:33.906
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:56:33.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-466" for this suite. 05/09/23 13:56:33.911
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":350,"skipped":6539,"failed":0}
------------------------------
• [0.065 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:56:33.853
    May  9 13:56:33.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:56:33.854
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:33.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:33.869
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 05/09/23 13:56:33.873
    STEP: waiting for available Endpoint 05/09/23 13:56:33.877
    STEP: listing all Endpoints 05/09/23 13:56:33.878
    STEP: updating the Endpoint 05/09/23 13:56:33.881
    STEP: fetching the Endpoint 05/09/23 13:56:33.886
    STEP: patching the Endpoint 05/09/23 13:56:33.888
    STEP: fetching the Endpoint 05/09/23 13:56:33.895
    STEP: deleting the Endpoint by Collection 05/09/23 13:56:33.898
    STEP: waiting for Endpoint deletion 05/09/23 13:56:33.905
    STEP: fetching the Endpoint 05/09/23 13:56:33.906
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:56:33.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-466" for this suite. 05/09/23 13:56:33.911
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:56:33.918
May  9 13:56:33.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename replicaset 05/09/23 13:56:33.919
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:33.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:33.931
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 05/09/23 13:56:33.933
STEP: Verify that the required pods have come up 05/09/23 13:56:33.938
May  9 13:56:33.940: INFO: Pod name sample-pod: Found 0 pods out of 3
May  9 13:56:38.946: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 05/09/23 13:56:38.946
May  9 13:56:38.949: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 05/09/23 13:56:38.949
STEP: DeleteCollection of the ReplicaSets 05/09/23 13:56:38.952
STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/09/23 13:56:38.959
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  9 13:56:38.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3044" for this suite. 05/09/23 13:56:38.966
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":351,"skipped":6541,"failed":0}
------------------------------
• [SLOW TEST] [5.055 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:56:33.918
    May  9 13:56:33.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename replicaset 05/09/23 13:56:33.919
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:33.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:33.931
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 05/09/23 13:56:33.933
    STEP: Verify that the required pods have come up 05/09/23 13:56:33.938
    May  9 13:56:33.940: INFO: Pod name sample-pod: Found 0 pods out of 3
    May  9 13:56:38.946: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 05/09/23 13:56:38.946
    May  9 13:56:38.949: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 05/09/23 13:56:38.949
    STEP: DeleteCollection of the ReplicaSets 05/09/23 13:56:38.952
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/09/23 13:56:38.959
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  9 13:56:38.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3044" for this suite. 05/09/23 13:56:38.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:56:38.973
May  9 13:56:38.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename job 05/09/23 13:56:38.974
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:38.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:38.999
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 05/09/23 13:56:39.003
STEP: Patching the Job 05/09/23 13:56:39.01
STEP: Watching for Job to be patched 05/09/23 13:56:39.026
May  9 13:56:39.028: INFO: Event ADDED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l] and annotations: map[batch.kubernetes.io/job-tracking:]
May  9 13:56:39.028: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l] and annotations: map[batch.kubernetes.io/job-tracking:]
May  9 13:56:39.028: INFO: Event MODIFIED found for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 05/09/23 13:56:39.028
STEP: Watching for Job to be updated 05/09/23 13:56:39.038
May  9 13:56:39.039: INFO: Event MODIFIED found for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:39.039: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 05/09/23 13:56:39.039
May  9 13:56:39.042: INFO: Job: e2e-w2n9l as labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched]
STEP: Waiting for job to complete 05/09/23 13:56:39.042
STEP: Delete a job collection with a labelselector 05/09/23 13:56:49.045
STEP: Watching for Job to be deleted 05/09/23 13:56:49.052
May  9 13:56:49.053: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:49.053: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:49.053: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:49.053: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 13:56:49.054: INFO: Event DELETED found for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 05/09/23 13:56:49.054
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  9 13:56:49.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3839" for this suite. 05/09/23 13:56:49.07
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":352,"skipped":6549,"failed":0}
------------------------------
• [SLOW TEST] [10.102 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:56:38.973
    May  9 13:56:38.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename job 05/09/23 13:56:38.974
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:38.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:38.999
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 05/09/23 13:56:39.003
    STEP: Patching the Job 05/09/23 13:56:39.01
    STEP: Watching for Job to be patched 05/09/23 13:56:39.026
    May  9 13:56:39.028: INFO: Event ADDED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l] and annotations: map[batch.kubernetes.io/job-tracking:]
    May  9 13:56:39.028: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l] and annotations: map[batch.kubernetes.io/job-tracking:]
    May  9 13:56:39.028: INFO: Event MODIFIED found for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 05/09/23 13:56:39.028
    STEP: Watching for Job to be updated 05/09/23 13:56:39.038
    May  9 13:56:39.039: INFO: Event MODIFIED found for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:39.039: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 05/09/23 13:56:39.039
    May  9 13:56:39.042: INFO: Job: e2e-w2n9l as labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched]
    STEP: Waiting for job to complete 05/09/23 13:56:39.042
    STEP: Delete a job collection with a labelselector 05/09/23 13:56:49.045
    STEP: Watching for Job to be deleted 05/09/23 13:56:49.052
    May  9 13:56:49.053: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:49.053: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:49.053: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:49.053: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:49.054: INFO: Event MODIFIED observed for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 13:56:49.054: INFO: Event DELETED found for Job e2e-w2n9l in namespace job-3839 with labels: map[e2e-job-label:e2e-w2n9l e2e-w2n9l:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 05/09/23 13:56:49.054
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  9 13:56:49.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3839" for this suite. 05/09/23 13:56:49.07
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:56:49.076
May  9 13:56:49.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pods 05/09/23 13:56:49.076
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:49.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:49.092
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 05/09/23 13:56:49.094
STEP: submitting the pod to kubernetes 05/09/23 13:56:49.094
May  9 13:56:49.100: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977" in namespace "pods-718" to be "running and ready"
May  9 13:56:49.105: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977": Phase="Pending", Reason="", readiness=false. Elapsed: 4.700628ms
May  9 13:56:49.105: INFO: The phase of Pod pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:56:51.109: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977": Phase="Running", Reason="", readiness=true. Elapsed: 2.008335318s
May  9 13:56:51.109: INFO: The phase of Pod pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977 is Running (Ready = true)
May  9 13:56:51.109: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/09/23 13:56:51.112
STEP: updating the pod 05/09/23 13:56:51.115
May  9 13:56:51.625: INFO: Successfully updated pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977"
May  9 13:56:51.625: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977" in namespace "pods-718" to be "terminated with reason DeadlineExceeded"
May  9 13:56:51.627: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977": Phase="Running", Reason="", readiness=true. Elapsed: 2.501872ms
May  9 13:56:53.632: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977": Phase="Running", Reason="", readiness=true. Elapsed: 2.007210426s
May  9 13:56:55.632: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.006970085s
May  9 13:56:55.632: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  9 13:56:55.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-718" for this suite. 05/09/23 13:56:55.636
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":353,"skipped":6553,"failed":0}
------------------------------
• [SLOW TEST] [6.567 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:56:49.076
    May  9 13:56:49.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pods 05/09/23 13:56:49.076
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:49.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:49.092
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 05/09/23 13:56:49.094
    STEP: submitting the pod to kubernetes 05/09/23 13:56:49.094
    May  9 13:56:49.100: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977" in namespace "pods-718" to be "running and ready"
    May  9 13:56:49.105: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977": Phase="Pending", Reason="", readiness=false. Elapsed: 4.700628ms
    May  9 13:56:49.105: INFO: The phase of Pod pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:56:51.109: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977": Phase="Running", Reason="", readiness=true. Elapsed: 2.008335318s
    May  9 13:56:51.109: INFO: The phase of Pod pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977 is Running (Ready = true)
    May  9 13:56:51.109: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/09/23 13:56:51.112
    STEP: updating the pod 05/09/23 13:56:51.115
    May  9 13:56:51.625: INFO: Successfully updated pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977"
    May  9 13:56:51.625: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977" in namespace "pods-718" to be "terminated with reason DeadlineExceeded"
    May  9 13:56:51.627: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977": Phase="Running", Reason="", readiness=true. Elapsed: 2.501872ms
    May  9 13:56:53.632: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977": Phase="Running", Reason="", readiness=true. Elapsed: 2.007210426s
    May  9 13:56:55.632: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.006970085s
    May  9 13:56:55.632: INFO: Pod "pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  9 13:56:55.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-718" for this suite. 05/09/23 13:56:55.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:56:55.644
May  9 13:56:55.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename sched-pred 05/09/23 13:56:55.645
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:55.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:55.661
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May  9 13:56:55.663: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  9 13:56:55.670: INFO: Waiting for terminating namespaces to be deleted...
May  9 13:56:55.672: INFO: 
Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-48ljh before test
May  9 13:56:55.678: INFO: calico-node-fdkdq from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.678: INFO: 	Container calico-node ready: true, restart count 0
May  9 13:56:55.678: INFO: csi-cinder-nodeplugin-f688q from kube-system started at 2023-05-09 11:59:12 +0000 UTC (3 container statuses recorded)
May  9 13:56:55.678: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May  9 13:56:55.678: INFO: 	Container liveness-probe ready: true, restart count 0
May  9 13:56:55.678: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  9 13:56:55.678: INFO: kube-proxy-lbgl4 from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.678: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 13:56:55.678: INFO: node-local-dns-vg5zf from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.678: INFO: 	Container node-cache ready: true, restart count 0
May  9 13:56:55.678: INFO: pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977 from pods-718 started at 2023-05-09 13:56:49 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.678: INFO: 	Container pause ready: false, restart count 0
May  9 13:56:55.678: INFO: sonobuoy from sonobuoy started at 2023-05-09 12:27:44 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.678: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  9 13:56:55.678: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-nvk78 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:56:55.678: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:56:55.678: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 13:56:55.678: INFO: 
Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-879bk before test
May  9 13:56:55.684: INFO: calico-node-gdlh8 from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.684: INFO: 	Container calico-node ready: true, restart count 0
May  9 13:56:55.684: INFO: csi-cinder-nodeplugin-hzf44 from kube-system started at 2023-05-09 11:59:09 +0000 UTC (3 container statuses recorded)
May  9 13:56:55.684: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May  9 13:56:55.684: INFO: 	Container liveness-probe ready: true, restart count 0
May  9 13:56:55.684: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  9 13:56:55.684: INFO: kube-proxy-pcp2c from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.684: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 13:56:55.684: INFO: node-local-dns-hvfcl from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.684: INFO: 	Container node-cache ready: true, restart count 0
May  9 13:56:55.684: INFO: sonobuoy-e2e-job-588b06ca4a824bd1 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:56:55.684: INFO: 	Container e2e ready: true, restart count 0
May  9 13:56:55.684: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:56:55.684: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-6xh5z from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:56:55.684: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:56:55.684: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 13:56:55.684: INFO: 
Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-skqqr before test
May  9 13:56:55.691: INFO: calico-kube-controllers-578488968b-2nchl from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.692: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  9 13:56:55.692: INFO: calico-node-fm8kr from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.692: INFO: 	Container calico-node ready: true, restart count 0
May  9 13:56:55.692: INFO: coredns-677bf479b7-t4l9l from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.692: INFO: 	Container coredns ready: true, restart count 0
May  9 13:56:55.692: INFO: coredns-677bf479b7-t7hjx from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.692: INFO: 	Container coredns ready: true, restart count 0
May  9 13:56:55.692: INFO: csi-cinder-nodeplugin-f82jf from kube-system started at 2023-05-09 11:59:06 +0000 UTC (3 container statuses recorded)
May  9 13:56:55.692: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May  9 13:56:55.692: INFO: 	Container liveness-probe ready: true, restart count 0
May  9 13:56:55.692: INFO: 	Container node-driver-registrar ready: true, restart count 0
May  9 13:56:55.692: INFO: kube-proxy-d9lqj from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.692: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 13:56:55.692: INFO: metrics-server-5974658d45-zx6c9 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.692: INFO: 	Container metrics-server ready: true, restart count 0
May  9 13:56:55.692: INFO: node-local-dns-hfgs9 from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.692: INFO: 	Container node-cache ready: true, restart count 0
May  9 13:56:55.692: INFO: snapshot-controller-677c67cc64-78dwg from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.692: INFO: 	Container snapshot-controller ready: true, restart count 0
May  9 13:56:55.692: INFO: snapshot-controller-677c67cc64-jrp67 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
May  9 13:56:55.692: INFO: 	Container snapshot-controller ready: true, restart count 0
May  9 13:56:55.692: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-67f2c from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
May  9 13:56:55.692: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 13:56:55.692: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 05/09/23 13:56:55.692
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.175d7e383ce9706e], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 05/09/23 13:56:55.719
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May  9 13:56:56.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9335" for this suite. 05/09/23 13:56:56.722
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":354,"skipped":6567,"failed":0}
------------------------------
• [1.085 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:56:55.644
    May  9 13:56:55.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename sched-pred 05/09/23 13:56:55.645
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:55.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:55.661
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May  9 13:56:55.663: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  9 13:56:55.670: INFO: Waiting for terminating namespaces to be deleted...
    May  9 13:56:55.672: INFO: 
    Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-48ljh before test
    May  9 13:56:55.678: INFO: calico-node-fdkdq from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.678: INFO: 	Container calico-node ready: true, restart count 0
    May  9 13:56:55.678: INFO: csi-cinder-nodeplugin-f688q from kube-system started at 2023-05-09 11:59:12 +0000 UTC (3 container statuses recorded)
    May  9 13:56:55.678: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    May  9 13:56:55.678: INFO: 	Container liveness-probe ready: true, restart count 0
    May  9 13:56:55.678: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May  9 13:56:55.678: INFO: kube-proxy-lbgl4 from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.678: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 13:56:55.678: INFO: node-local-dns-vg5zf from kube-system started at 2023-05-09 11:59:12 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.678: INFO: 	Container node-cache ready: true, restart count 0
    May  9 13:56:55.678: INFO: pod-update-activedeadlineseconds-03271e2e-4009-4647-a740-caff72456977 from pods-718 started at 2023-05-09 13:56:49 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.678: INFO: 	Container pause ready: false, restart count 0
    May  9 13:56:55.678: INFO: sonobuoy from sonobuoy started at 2023-05-09 12:27:44 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.678: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  9 13:56:55.678: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-nvk78 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:56:55.678: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:56:55.678: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 13:56:55.678: INFO: 
    Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-879bk before test
    May  9 13:56:55.684: INFO: calico-node-gdlh8 from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.684: INFO: 	Container calico-node ready: true, restart count 0
    May  9 13:56:55.684: INFO: csi-cinder-nodeplugin-hzf44 from kube-system started at 2023-05-09 11:59:09 +0000 UTC (3 container statuses recorded)
    May  9 13:56:55.684: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    May  9 13:56:55.684: INFO: 	Container liveness-probe ready: true, restart count 0
    May  9 13:56:55.684: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May  9 13:56:55.684: INFO: kube-proxy-pcp2c from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.684: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 13:56:55.684: INFO: node-local-dns-hvfcl from kube-system started at 2023-05-09 11:59:09 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.684: INFO: 	Container node-cache ready: true, restart count 0
    May  9 13:56:55.684: INFO: sonobuoy-e2e-job-588b06ca4a824bd1 from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:56:55.684: INFO: 	Container e2e ready: true, restart count 0
    May  9 13:56:55.684: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:56:55.684: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-6xh5z from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:56:55.684: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:56:55.684: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 13:56:55.684: INFO: 
    Logging pods the apiserver thinks is on node cl-gks-cncf-ix1-md-0-skqqr before test
    May  9 13:56:55.691: INFO: calico-kube-controllers-578488968b-2nchl from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.692: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  9 13:56:55.692: INFO: calico-node-fm8kr from kube-system started at 2023-05-09 11:59:47 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.692: INFO: 	Container calico-node ready: true, restart count 0
    May  9 13:56:55.692: INFO: coredns-677bf479b7-t4l9l from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.692: INFO: 	Container coredns ready: true, restart count 0
    May  9 13:56:55.692: INFO: coredns-677bf479b7-t7hjx from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.692: INFO: 	Container coredns ready: true, restart count 0
    May  9 13:56:55.692: INFO: csi-cinder-nodeplugin-f82jf from kube-system started at 2023-05-09 11:59:06 +0000 UTC (3 container statuses recorded)
    May  9 13:56:55.692: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    May  9 13:56:55.692: INFO: 	Container liveness-probe ready: true, restart count 0
    May  9 13:56:55.692: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May  9 13:56:55.692: INFO: kube-proxy-d9lqj from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.692: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 13:56:55.692: INFO: metrics-server-5974658d45-zx6c9 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.692: INFO: 	Container metrics-server ready: true, restart count 0
    May  9 13:56:55.692: INFO: node-local-dns-hfgs9 from kube-system started at 2023-05-09 11:59:06 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.692: INFO: 	Container node-cache ready: true, restart count 0
    May  9 13:56:55.692: INFO: snapshot-controller-677c67cc64-78dwg from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.692: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  9 13:56:55.692: INFO: snapshot-controller-677c67cc64-jrp67 from kube-system started at 2023-05-09 12:00:19 +0000 UTC (1 container statuses recorded)
    May  9 13:56:55.692: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  9 13:56:55.692: INFO: sonobuoy-systemd-logs-daemon-set-fffd9343e81f4fd5-67f2c from sonobuoy started at 2023-05-09 12:27:48 +0000 UTC (2 container statuses recorded)
    May  9 13:56:55.692: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 13:56:55.692: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 05/09/23 13:56:55.692
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.175d7e383ce9706e], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 05/09/23 13:56:55.719
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:56:56.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9335" for this suite. 05/09/23 13:56:56.722
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:56:56.729
May  9 13:56:56.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename disruption 05/09/23 13:56:56.73
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:56.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:56.746
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 05/09/23 13:56:56.752
STEP: Updating PodDisruptionBudget status 05/09/23 13:56:58.761
STEP: Waiting for all pods to be running 05/09/23 13:56:58.768
May  9 13:56:58.774: INFO: running pods: 0 < 1
STEP: locating a running pod 05/09/23 13:57:00.778
STEP: Waiting for the pdb to be processed 05/09/23 13:57:00.789
STEP: Patching PodDisruptionBudget status 05/09/23 13:57:00.799
STEP: Waiting for the pdb to be processed 05/09/23 13:57:00.808
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  9 13:57:00.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8006" for this suite. 05/09/23 13:57:00.816
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":355,"skipped":6583,"failed":0}
------------------------------
• [4.092 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:56:56.729
    May  9 13:56:56.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename disruption 05/09/23 13:56:56.73
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:56:56.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:56:56.746
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 05/09/23 13:56:56.752
    STEP: Updating PodDisruptionBudget status 05/09/23 13:56:58.761
    STEP: Waiting for all pods to be running 05/09/23 13:56:58.768
    May  9 13:56:58.774: INFO: running pods: 0 < 1
    STEP: locating a running pod 05/09/23 13:57:00.778
    STEP: Waiting for the pdb to be processed 05/09/23 13:57:00.789
    STEP: Patching PodDisruptionBudget status 05/09/23 13:57:00.799
    STEP: Waiting for the pdb to be processed 05/09/23 13:57:00.808
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  9 13:57:00.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8006" for this suite. 05/09/23 13:57:00.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:57:00.823
May  9 13:57:00.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename emptydir 05/09/23 13:57:00.823
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:00.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:00.84
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/09/23 13:57:00.842
May  9 13:57:00.850: INFO: Waiting up to 5m0s for pod "pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16" in namespace "emptydir-9516" to be "Succeeded or Failed"
May  9 13:57:00.854: INFO: Pod "pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.579668ms
May  9 13:57:02.858: INFO: Pod "pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008774041s
May  9 13:57:04.860: INFO: Pod "pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009999289s
STEP: Saw pod success 05/09/23 13:57:04.86
May  9 13:57:04.860: INFO: Pod "pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16" satisfied condition "Succeeded or Failed"
May  9 13:57:04.862: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16 container test-container: <nil>
STEP: delete the pod 05/09/23 13:57:04.87
May  9 13:57:04.880: INFO: Waiting for pod pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16 to disappear
May  9 13:57:04.883: INFO: Pod pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  9 13:57:04.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9516" for this suite. 05/09/23 13:57:04.886
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":356,"skipped":6641,"failed":0}
------------------------------
• [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:57:00.823
    May  9 13:57:00.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename emptydir 05/09/23 13:57:00.823
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:00.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:00.84
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/09/23 13:57:00.842
    May  9 13:57:00.850: INFO: Waiting up to 5m0s for pod "pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16" in namespace "emptydir-9516" to be "Succeeded or Failed"
    May  9 13:57:00.854: INFO: Pod "pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.579668ms
    May  9 13:57:02.858: INFO: Pod "pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008774041s
    May  9 13:57:04.860: INFO: Pod "pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009999289s
    STEP: Saw pod success 05/09/23 13:57:04.86
    May  9 13:57:04.860: INFO: Pod "pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16" satisfied condition "Succeeded or Failed"
    May  9 13:57:04.862: INFO: Trying to get logs from node cl-gks-cncf-ix1-md-0-48ljh pod pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16 container test-container: <nil>
    STEP: delete the pod 05/09/23 13:57:04.87
    May  9 13:57:04.880: INFO: Waiting for pod pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16 to disappear
    May  9 13:57:04.883: INFO: Pod pod-c82acc47-bdc9-4ca0-b4d4-3566d3d83d16 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  9 13:57:04.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9516" for this suite. 05/09/23 13:57:04.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:57:04.892
May  9 13:57:04.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename namespaces 05/09/23 13:57:04.893
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:04.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:04.909
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 05/09/23 13:57:04.911
May  9 13:57:04.913: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 05/09/23 13:57:04.914
May  9 13:57:04.918: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 05/09/23 13:57:04.918
May  9 13:57:04.930: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May  9 13:57:04.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-501" for this suite. 05/09/23 13:57:04.934
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":357,"skipped":6648,"failed":0}
------------------------------
• [0.047 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:57:04.892
    May  9 13:57:04.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename namespaces 05/09/23 13:57:04.893
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:04.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:04.909
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 05/09/23 13:57:04.911
    May  9 13:57:04.913: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 05/09/23 13:57:04.914
    May  9 13:57:04.918: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 05/09/23 13:57:04.918
    May  9 13:57:04.930: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May  9 13:57:04.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-501" for this suite. 05/09/23 13:57:04.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:57:04.94
May  9 13:57:04.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename services 05/09/23 13:57:04.94
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:04.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:04.954
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-1919 05/09/23 13:57:04.956
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1919 to expose endpoints map[] 05/09/23 13:57:04.968
May  9 13:57:04.978: INFO: successfully validated that service endpoint-test2 in namespace services-1919 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1919 05/09/23 13:57:04.978
May  9 13:57:04.985: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1919" to be "running and ready"
May  9 13:57:04.990: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.099057ms
May  9 13:57:04.990: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:57:06.994: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009121309s
May  9 13:57:06.994: INFO: The phase of Pod pod1 is Running (Ready = true)
May  9 13:57:06.994: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1919 to expose endpoints map[pod1:[80]] 05/09/23 13:57:06.997
May  9 13:57:07.005: INFO: successfully validated that service endpoint-test2 in namespace services-1919 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 05/09/23 13:57:07.005
May  9 13:57:07.006: INFO: Creating new exec pod
May  9 13:57:07.011: INFO: Waiting up to 5m0s for pod "execpodfwgv5" in namespace "services-1919" to be "running"
May  9 13:57:07.015: INFO: Pod "execpodfwgv5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.373276ms
May  9 13:57:09.019: INFO: Pod "execpodfwgv5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008355174s
May  9 13:57:09.019: INFO: Pod "execpodfwgv5" satisfied condition "running"
May  9 13:57:10.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1919 exec execpodfwgv5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May  9 13:57:11.129: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  9 13:57:11.129: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:57:11.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1919 exec execpodfwgv5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.120.106 80'
May  9 13:57:11.224: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.120.106 80\nConnection to 10.103.120.106 80 port [tcp/http] succeeded!\n"
May  9 13:57:11.224: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-1919 05/09/23 13:57:11.224
May  9 13:57:11.231: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1919" to be "running and ready"
May  9 13:57:11.234: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.238546ms
May  9 13:57:11.234: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:57:13.239: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00780499s
May  9 13:57:13.239: INFO: The phase of Pod pod2 is Running (Ready = true)
May  9 13:57:13.239: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1919 to expose endpoints map[pod1:[80] pod2:[80]] 05/09/23 13:57:13.241
May  9 13:57:13.252: INFO: successfully validated that service endpoint-test2 in namespace services-1919 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 05/09/23 13:57:13.252
May  9 13:57:14.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1919 exec execpodfwgv5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May  9 13:57:14.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  9 13:57:14.357: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:57:14.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1919 exec execpodfwgv5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.120.106 80'
May  9 13:57:14.463: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.120.106 80\nConnection to 10.103.120.106 80 port [tcp/http] succeeded!\n"
May  9 13:57:14.463: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1919 05/09/23 13:57:14.463
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1919 to expose endpoints map[pod2:[80]] 05/09/23 13:57:14.475
May  9 13:57:14.485: INFO: successfully validated that service endpoint-test2 in namespace services-1919 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 05/09/23 13:57:14.485
May  9 13:57:15.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1919 exec execpodfwgv5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May  9 13:57:15.596: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  9 13:57:15.596: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  9 13:57:15.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1919 exec execpodfwgv5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.120.106 80'
May  9 13:57:15.692: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.120.106 80\nConnection to 10.103.120.106 80 port [tcp/http] succeeded!\n"
May  9 13:57:15.692: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-1919 05/09/23 13:57:15.692
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1919 to expose endpoints map[] 05/09/23 13:57:15.704
May  9 13:57:15.712: INFO: successfully validated that service endpoint-test2 in namespace services-1919 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  9 13:57:15.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1919" for this suite. 05/09/23 13:57:15.742
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":358,"skipped":6674,"failed":0}
------------------------------
• [SLOW TEST] [10.808 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:57:04.94
    May  9 13:57:04.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename services 05/09/23 13:57:04.94
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:04.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:04.954
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-1919 05/09/23 13:57:04.956
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1919 to expose endpoints map[] 05/09/23 13:57:04.968
    May  9 13:57:04.978: INFO: successfully validated that service endpoint-test2 in namespace services-1919 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1919 05/09/23 13:57:04.978
    May  9 13:57:04.985: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1919" to be "running and ready"
    May  9 13:57:04.990: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.099057ms
    May  9 13:57:04.990: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:57:06.994: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009121309s
    May  9 13:57:06.994: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  9 13:57:06.994: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1919 to expose endpoints map[pod1:[80]] 05/09/23 13:57:06.997
    May  9 13:57:07.005: INFO: successfully validated that service endpoint-test2 in namespace services-1919 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 05/09/23 13:57:07.005
    May  9 13:57:07.006: INFO: Creating new exec pod
    May  9 13:57:07.011: INFO: Waiting up to 5m0s for pod "execpodfwgv5" in namespace "services-1919" to be "running"
    May  9 13:57:07.015: INFO: Pod "execpodfwgv5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.373276ms
    May  9 13:57:09.019: INFO: Pod "execpodfwgv5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008355174s
    May  9 13:57:09.019: INFO: Pod "execpodfwgv5" satisfied condition "running"
    May  9 13:57:10.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1919 exec execpodfwgv5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    May  9 13:57:11.129: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  9 13:57:11.129: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:57:11.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1919 exec execpodfwgv5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.120.106 80'
    May  9 13:57:11.224: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.120.106 80\nConnection to 10.103.120.106 80 port [tcp/http] succeeded!\n"
    May  9 13:57:11.224: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-1919 05/09/23 13:57:11.224
    May  9 13:57:11.231: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1919" to be "running and ready"
    May  9 13:57:11.234: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.238546ms
    May  9 13:57:11.234: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:57:13.239: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00780499s
    May  9 13:57:13.239: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  9 13:57:13.239: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1919 to expose endpoints map[pod1:[80] pod2:[80]] 05/09/23 13:57:13.241
    May  9 13:57:13.252: INFO: successfully validated that service endpoint-test2 in namespace services-1919 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 05/09/23 13:57:13.252
    May  9 13:57:14.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1919 exec execpodfwgv5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    May  9 13:57:14.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  9 13:57:14.357: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:57:14.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1919 exec execpodfwgv5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.120.106 80'
    May  9 13:57:14.463: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.120.106 80\nConnection to 10.103.120.106 80 port [tcp/http] succeeded!\n"
    May  9 13:57:14.463: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-1919 05/09/23 13:57:14.463
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1919 to expose endpoints map[pod2:[80]] 05/09/23 13:57:14.475
    May  9 13:57:14.485: INFO: successfully validated that service endpoint-test2 in namespace services-1919 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 05/09/23 13:57:14.485
    May  9 13:57:15.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1919 exec execpodfwgv5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    May  9 13:57:15.596: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  9 13:57:15.596: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  9 13:57:15.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2342994826 --namespace=services-1919 exec execpodfwgv5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.120.106 80'
    May  9 13:57:15.692: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.120.106 80\nConnection to 10.103.120.106 80 port [tcp/http] succeeded!\n"
    May  9 13:57:15.692: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-1919 05/09/23 13:57:15.692
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1919 to expose endpoints map[] 05/09/23 13:57:15.704
    May  9 13:57:15.712: INFO: successfully validated that service endpoint-test2 in namespace services-1919 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  9 13:57:15.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1919" for this suite. 05/09/23 13:57:15.742
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:57:15.748
May  9 13:57:15.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename pod-network-test 05/09/23 13:57:15.749
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:15.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:15.766
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-3773 05/09/23 13:57:15.768
STEP: creating a selector 05/09/23 13:57:15.768
STEP: Creating the service pods in kubernetes 05/09/23 13:57:15.768
May  9 13:57:15.768: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  9 13:57:15.795: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3773" to be "running and ready"
May  9 13:57:15.800: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.553097ms
May  9 13:57:15.800: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  9 13:57:17.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008727517s
May  9 13:57:17.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:57:19.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008982509s
May  9 13:57:19.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:57:21.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008534176s
May  9 13:57:21.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:57:23.805: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00915398s
May  9 13:57:23.805: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:57:25.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008454499s
May  9 13:57:25.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 13:57:27.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.008648428s
May  9 13:57:27.804: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  9 13:57:27.804: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  9 13:57:27.807: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3773" to be "running and ready"
May  9 13:57:27.809: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.556747ms
May  9 13:57:27.809: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  9 13:57:27.809: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  9 13:57:27.812: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3773" to be "running and ready"
May  9 13:57:27.814: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.236147ms
May  9 13:57:27.814: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  9 13:57:27.814: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/09/23 13:57:27.817
May  9 13:57:27.827: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3773" to be "running"
May  9 13:57:27.830: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.54493ms
May  9 13:57:29.834: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006929973s
May  9 13:57:29.834: INFO: Pod "test-container-pod" satisfied condition "running"
May  9 13:57:29.836: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3773" to be "running"
May  9 13:57:29.838: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.282435ms
May  9 13:57:29.838: INFO: Pod "host-test-container-pod" satisfied condition "running"
May  9 13:57:29.841: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  9 13:57:29.841: INFO: Going to poll 172.25.124.224 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May  9 13:57:29.843: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.124.224:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3773 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:57:29.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:57:29.843: INFO: ExecWithOptions: Clientset creation
May  9 13:57:29.843: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3773/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.124.224%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 13:57:29.891: INFO: Found all 1 expected endpoints: [netserver-0]
May  9 13:57:29.891: INFO: Going to poll 172.25.53.184 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May  9 13:57:29.894: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.53.184:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3773 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:57:29.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:57:29.894: INFO: ExecWithOptions: Clientset creation
May  9 13:57:29.894: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3773/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.53.184%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 13:57:29.939: INFO: Found all 1 expected endpoints: [netserver-1]
May  9 13:57:29.939: INFO: Going to poll 172.25.72.200 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May  9 13:57:29.942: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.72.200:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3773 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 13:57:29.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
May  9 13:57:29.942: INFO: ExecWithOptions: Clientset creation
May  9 13:57:29.942: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3773/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.72.200%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 13:57:29.989: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May  9 13:57:29.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3773" for this suite. 05/09/23 13:57:29.993
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":359,"skipped":6676,"failed":0}
------------------------------
• [SLOW TEST] [14.250 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:57:15.748
    May  9 13:57:15.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename pod-network-test 05/09/23 13:57:15.749
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:15.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:15.766
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-3773 05/09/23 13:57:15.768
    STEP: creating a selector 05/09/23 13:57:15.768
    STEP: Creating the service pods in kubernetes 05/09/23 13:57:15.768
    May  9 13:57:15.768: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  9 13:57:15.795: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3773" to be "running and ready"
    May  9 13:57:15.800: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.553097ms
    May  9 13:57:15.800: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  9 13:57:17.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008727517s
    May  9 13:57:17.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:57:19.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008982509s
    May  9 13:57:19.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:57:21.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008534176s
    May  9 13:57:21.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:57:23.805: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00915398s
    May  9 13:57:23.805: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:57:25.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008454499s
    May  9 13:57:25.804: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 13:57:27.804: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.008648428s
    May  9 13:57:27.804: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  9 13:57:27.804: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  9 13:57:27.807: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3773" to be "running and ready"
    May  9 13:57:27.809: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.556747ms
    May  9 13:57:27.809: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  9 13:57:27.809: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  9 13:57:27.812: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3773" to be "running and ready"
    May  9 13:57:27.814: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.236147ms
    May  9 13:57:27.814: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  9 13:57:27.814: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/09/23 13:57:27.817
    May  9 13:57:27.827: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3773" to be "running"
    May  9 13:57:27.830: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.54493ms
    May  9 13:57:29.834: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006929973s
    May  9 13:57:29.834: INFO: Pod "test-container-pod" satisfied condition "running"
    May  9 13:57:29.836: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3773" to be "running"
    May  9 13:57:29.838: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.282435ms
    May  9 13:57:29.838: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May  9 13:57:29.841: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May  9 13:57:29.841: INFO: Going to poll 172.25.124.224 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May  9 13:57:29.843: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.124.224:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3773 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:57:29.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:57:29.843: INFO: ExecWithOptions: Clientset creation
    May  9 13:57:29.843: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3773/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.124.224%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 13:57:29.891: INFO: Found all 1 expected endpoints: [netserver-0]
    May  9 13:57:29.891: INFO: Going to poll 172.25.53.184 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May  9 13:57:29.894: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.53.184:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3773 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:57:29.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:57:29.894: INFO: ExecWithOptions: Clientset creation
    May  9 13:57:29.894: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3773/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.53.184%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 13:57:29.939: INFO: Found all 1 expected endpoints: [netserver-1]
    May  9 13:57:29.939: INFO: Going to poll 172.25.72.200 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May  9 13:57:29.942: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.72.200:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3773 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 13:57:29.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    May  9 13:57:29.942: INFO: ExecWithOptions: Clientset creation
    May  9 13:57:29.942: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3773/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.72.200%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 13:57:29.989: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May  9 13:57:29.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3773" for this suite. 05/09/23 13:57:29.993
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:57:29.999
May  9 13:57:29.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename resourcequota 05/09/23 13:57:29.999
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:30.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:30.013
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 05/09/23 13:57:30.015
STEP: Creating a ResourceQuota 05/09/23 13:57:35.021
STEP: Ensuring resource quota status is calculated 05/09/23 13:57:35.027
STEP: Creating a Pod that fits quota 05/09/23 13:57:37.031
STEP: Ensuring ResourceQuota status captures the pod usage 05/09/23 13:57:37.044
STEP: Not allowing a pod to be created that exceeds remaining quota 05/09/23 13:57:39.048
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/09/23 13:57:39.05
STEP: Ensuring a pod cannot update its resource requirements 05/09/23 13:57:39.051
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/09/23 13:57:39.056
STEP: Deleting the pod 05/09/23 13:57:41.059
STEP: Ensuring resource quota status released the pod usage 05/09/23 13:57:41.068
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  9 13:57:43.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-861" for this suite. 05/09/23 13:57:43.077
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":360,"skipped":6677,"failed":0}
------------------------------
• [SLOW TEST] [13.083 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:57:29.999
    May  9 13:57:29.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename resourcequota 05/09/23 13:57:29.999
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:30.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:30.013
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 05/09/23 13:57:30.015
    STEP: Creating a ResourceQuota 05/09/23 13:57:35.021
    STEP: Ensuring resource quota status is calculated 05/09/23 13:57:35.027
    STEP: Creating a Pod that fits quota 05/09/23 13:57:37.031
    STEP: Ensuring ResourceQuota status captures the pod usage 05/09/23 13:57:37.044
    STEP: Not allowing a pod to be created that exceeds remaining quota 05/09/23 13:57:39.048
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/09/23 13:57:39.05
    STEP: Ensuring a pod cannot update its resource requirements 05/09/23 13:57:39.051
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/09/23 13:57:39.056
    STEP: Deleting the pod 05/09/23 13:57:41.059
    STEP: Ensuring resource quota status released the pod usage 05/09/23 13:57:41.068
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  9 13:57:43.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-861" for this suite. 05/09/23 13:57:43.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:57:43.083
May  9 13:57:43.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename sysctl 05/09/23 13:57:43.083
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:43.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:43.099
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 05/09/23 13:57:43.101
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
May  9 13:57:43.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2249" for this suite. 05/09/23 13:57:43.107
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":361,"skipped":6685,"failed":0}
------------------------------
• [0.030 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:57:43.083
    May  9 13:57:43.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename sysctl 05/09/23 13:57:43.083
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:43.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:43.099
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 05/09/23 13:57:43.101
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    May  9 13:57:43.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-2249" for this suite. 05/09/23 13:57:43.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/09/23 13:57:43.113
May  9 13:57:43.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
STEP: Building a namespace api object, basename dns 05/09/23 13:57:43.113
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:43.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:43.128
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 05/09/23 13:57:43.13
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-874.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-874.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 05/09/23 13:57:43.134
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-874.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-874.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 05/09/23 13:57:43.134
STEP: creating a pod to probe DNS 05/09/23 13:57:43.134
STEP: submitting the pod to kubernetes 05/09/23 13:57:43.134
May  9 13:57:43.144: INFO: Waiting up to 15m0s for pod "dns-test-08d4ac5d-56be-42a9-9e3b-1d2380405c76" in namespace "dns-874" to be "running"
May  9 13:57:43.146: INFO: Pod "dns-test-08d4ac5d-56be-42a9-9e3b-1d2380405c76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.386443ms
May  9 13:57:45.150: INFO: Pod "dns-test-08d4ac5d-56be-42a9-9e3b-1d2380405c76": Phase="Running", Reason="", readiness=true. Elapsed: 2.006316536s
May  9 13:57:45.150: INFO: Pod "dns-test-08d4ac5d-56be-42a9-9e3b-1d2380405c76" satisfied condition "running"
STEP: retrieving the pod 05/09/23 13:57:45.15
STEP: looking for the results for each expected name from probers 05/09/23 13:57:45.153
May  9 13:57:45.167: INFO: DNS probes using dns-874/dns-test-08d4ac5d-56be-42a9-9e3b-1d2380405c76 succeeded

STEP: deleting the pod 05/09/23 13:57:45.167
STEP: deleting the test headless service 05/09/23 13:57:45.186
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  9 13:57:45.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-874" for this suite. 05/09/23 13:57:45.206
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":362,"skipped":6694,"failed":0}
------------------------------
• [2.100 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/09/23 13:57:43.113
    May  9 13:57:43.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2342994826
    STEP: Building a namespace api object, basename dns 05/09/23 13:57:43.113
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 13:57:43.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 13:57:43.128
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 05/09/23 13:57:43.13
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-874.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-874.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     05/09/23 13:57:43.134
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-874.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-874.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     05/09/23 13:57:43.134
    STEP: creating a pod to probe DNS 05/09/23 13:57:43.134
    STEP: submitting the pod to kubernetes 05/09/23 13:57:43.134
    May  9 13:57:43.144: INFO: Waiting up to 15m0s for pod "dns-test-08d4ac5d-56be-42a9-9e3b-1d2380405c76" in namespace "dns-874" to be "running"
    May  9 13:57:43.146: INFO: Pod "dns-test-08d4ac5d-56be-42a9-9e3b-1d2380405c76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.386443ms
    May  9 13:57:45.150: INFO: Pod "dns-test-08d4ac5d-56be-42a9-9e3b-1d2380405c76": Phase="Running", Reason="", readiness=true. Elapsed: 2.006316536s
    May  9 13:57:45.150: INFO: Pod "dns-test-08d4ac5d-56be-42a9-9e3b-1d2380405c76" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 13:57:45.15
    STEP: looking for the results for each expected name from probers 05/09/23 13:57:45.153
    May  9 13:57:45.167: INFO: DNS probes using dns-874/dns-test-08d4ac5d-56be-42a9-9e3b-1d2380405c76 succeeded

    STEP: deleting the pod 05/09/23 13:57:45.167
    STEP: deleting the test headless service 05/09/23 13:57:45.186
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  9 13:57:45.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-874" for this suite. 05/09/23 13:57:45.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
May  9 13:57:45.214: INFO: Running AfterSuite actions on all nodes
May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
May  9 13:57:45.214: INFO: Running AfterSuite actions on node 1
May  9 13:57:45.214: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    May  9 13:57:45.214: INFO: Running AfterSuite actions on all nodes
    May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    May  9 13:57:45.214: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    May  9 13:57:45.214: INFO: Running AfterSuite actions on node 1
    May  9 13:57:45.214: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.049 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 5383.298 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h29m43.482041504s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

