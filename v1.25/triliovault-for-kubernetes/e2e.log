I0517 05:36:15.703117      23 e2e.go:116] Starting e2e run "01622574-b831-410b-947c-38c822a8716f" on Ginkgo node 1
May 17 05:36:15.709: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1684301775 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
May 17 05:36:15.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 05:36:15.768: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0517 05:36:15.769205      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0517 05:36:15.769205      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
May 17 05:36:15.807: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 17 05:36:15.841: INFO: 26 / 26 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 17 05:36:15.841: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
May 17 05:36:15.841: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 17 05:36:15.852: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ama-logs' (0 seconds elapsed)
May 17 05:36:15.852: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'ama-logs-windows' (0 seconds elapsed)
May 17 05:36:15.852: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'azure-ip-masq-agent' (0 seconds elapsed)
May 17 05:36:15.852: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-node-manager' (0 seconds elapsed)
May 17 05:36:15.852: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'cloud-node-manager-windows' (0 seconds elapsed)
May 17 05:36:15.852: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-azuredisk-node' (0 seconds elapsed)
May 17 05:36:15.852: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-azuredisk-node-win' (0 seconds elapsed)
May 17 05:36:15.852: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-azurefile-node' (0 seconds elapsed)
May 17 05:36:15.852: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-azurefile-node-win' (0 seconds elapsed)
May 17 05:36:15.852: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 17 05:36:15.852: INFO: e2e test version: v1.25.6
May 17 05:36:15.853: INFO: kube-apiserver version: v1.25.6
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
May 17 05:36:15.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 05:36:15.858: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.090 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    May 17 05:36:15.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 05:36:15.768: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0517 05:36:15.769205      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    May 17 05:36:15.807: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    May 17 05:36:15.841: INFO: 26 / 26 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    May 17 05:36:15.841: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
    May 17 05:36:15.841: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    May 17 05:36:15.852: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ama-logs' (0 seconds elapsed)
    May 17 05:36:15.852: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'ama-logs-windows' (0 seconds elapsed)
    May 17 05:36:15.852: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'azure-ip-masq-agent' (0 seconds elapsed)
    May 17 05:36:15.852: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-node-manager' (0 seconds elapsed)
    May 17 05:36:15.852: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'cloud-node-manager-windows' (0 seconds elapsed)
    May 17 05:36:15.852: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-azuredisk-node' (0 seconds elapsed)
    May 17 05:36:15.852: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-azuredisk-node-win' (0 seconds elapsed)
    May 17 05:36:15.852: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-azurefile-node' (0 seconds elapsed)
    May 17 05:36:15.852: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-azurefile-node-win' (0 seconds elapsed)
    May 17 05:36:15.852: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    May 17 05:36:15.852: INFO: e2e test version: v1.25.6
    May 17 05:36:15.853: INFO: kube-apiserver version: v1.25.6
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    May 17 05:36:15.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 05:36:15.858: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:36:15.877
May 17 05:36:15.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename events 05/17/23 05:36:15.878
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:36:15.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:36:15.895
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 05/17/23 05:36:15.898
STEP: listing events in all namespaces 05/17/23 05:36:15.911
STEP: listing events in test namespace 05/17/23 05:36:15.918
STEP: listing events with field selection filtering on source 05/17/23 05:36:15.925
STEP: listing events with field selection filtering on reportingController 05/17/23 05:36:15.933
STEP: getting the test event 05/17/23 05:36:15.944
STEP: patching the test event 05/17/23 05:36:15.949
STEP: getting the test event 05/17/23 05:36:15.965
STEP: updating the test event 05/17/23 05:36:15.972
STEP: getting the test event 05/17/23 05:36:15.988
STEP: deleting the test event 05/17/23 05:36:15.993
STEP: listing events in all namespaces 05/17/23 05:36:16.008
STEP: listing events in test namespace 05/17/23 05:36:16.016
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
May 17 05:36:16.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6120" for this suite. 05/17/23 05:36:16.03
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":1,"skipped":16,"failed":0}
------------------------------
â€¢ [0.159 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:36:15.877
    May 17 05:36:15.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename events 05/17/23 05:36:15.878
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:36:15.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:36:15.895
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 05/17/23 05:36:15.898
    STEP: listing events in all namespaces 05/17/23 05:36:15.911
    STEP: listing events in test namespace 05/17/23 05:36:15.918
    STEP: listing events with field selection filtering on source 05/17/23 05:36:15.925
    STEP: listing events with field selection filtering on reportingController 05/17/23 05:36:15.933
    STEP: getting the test event 05/17/23 05:36:15.944
    STEP: patching the test event 05/17/23 05:36:15.949
    STEP: getting the test event 05/17/23 05:36:15.965
    STEP: updating the test event 05/17/23 05:36:15.972
    STEP: getting the test event 05/17/23 05:36:15.988
    STEP: deleting the test event 05/17/23 05:36:15.993
    STEP: listing events in all namespaces 05/17/23 05:36:16.008
    STEP: listing events in test namespace 05/17/23 05:36:16.016
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    May 17 05:36:16.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6120" for this suite. 05/17/23 05:36:16.03
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:36:16.037
May 17 05:36:16.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
E0517 05:36:16.037724      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0517 05:36:16.037724      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
STEP: Building a namespace api object, basename daemonsets 05/17/23 05:36:16.038
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:36:16.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:36:16.054
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 05/17/23 05:36:16.092
STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 05:36:16.099
May 17 05:36:16.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 05:36:16.114: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 05:36:17.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 05:36:17.128: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 05:36:18.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 05:36:18.126: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 05:36:19.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 05:36:19.126: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 05:36:20.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 17 05:36:20.127: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 05/17/23 05:36:20.131
May 17 05:36:20.153: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 05:36:20.153: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 05:36:21.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 05:36:21.177: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 05:36:22.171: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 05:36:22.171: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 05:36:23.165: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 05:36:23.165: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 05:36:24.165: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 05:36:24.165: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 05:36:25.167: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 05:36:25.167: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 05:36:26.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 05:36:26.168: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 05:36:27.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 05:36:27.166: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 05:36:28.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 05:36:28.166: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 05:36:29.165: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 17 05:36:29.165: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/17/23 05:36:29.169
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3328, will wait for the garbage collector to delete the pods 05/17/23 05:36:29.169
May 17 05:36:29.230: INFO: Deleting DaemonSet.extensions daemon-set took: 7.766688ms
May 17 05:36:29.331: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.821738ms
May 17 05:36:36.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 05:36:36.537: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 17 05:36:36.542: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1430999"},"items":null}

May 17 05:36:36.546: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1430999"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May 17 05:36:36.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3328" for this suite. 05/17/23 05:36:36.572
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":2,"skipped":20,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.542 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:36:16.037
    May 17 05:36:16.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    E0517 05:36:16.037724      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    STEP: Building a namespace api object, basename daemonsets 05/17/23 05:36:16.038
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:36:16.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:36:16.054
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 05/17/23 05:36:16.092
    STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 05:36:16.099
    May 17 05:36:16.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 05:36:16.114: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 05:36:17.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 05:36:17.128: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 05:36:18.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 05:36:18.126: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 05:36:19.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 05:36:19.126: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 05:36:20.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 17 05:36:20.127: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 05/17/23 05:36:20.131
    May 17 05:36:20.153: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 05:36:20.153: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 05:36:21.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 05:36:21.177: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 05:36:22.171: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 05:36:22.171: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 05:36:23.165: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 05:36:23.165: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 05:36:24.165: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 05:36:24.165: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 05:36:25.167: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 05:36:25.167: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 05:36:26.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 05:36:26.168: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 05:36:27.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 05:36:27.166: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 05:36:28.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 05:36:28.166: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 05:36:29.165: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 17 05:36:29.165: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/17/23 05:36:29.169
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3328, will wait for the garbage collector to delete the pods 05/17/23 05:36:29.169
    May 17 05:36:29.230: INFO: Deleting DaemonSet.extensions daemon-set took: 7.766688ms
    May 17 05:36:29.331: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.821738ms
    May 17 05:36:36.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 05:36:36.537: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 17 05:36:36.542: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1430999"},"items":null}

    May 17 05:36:36.546: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1430999"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May 17 05:36:36.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3328" for this suite. 05/17/23 05:36:36.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:36:36.58
May 17 05:36:36.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-probe 05/17/23 05:36:36.581
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:36:36.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:36:36.597
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c in namespace container-probe-3842 05/17/23 05:36:36.601
May 17 05:36:36.611: INFO: Waiting up to 5m0s for pod "liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c" in namespace "container-probe-3842" to be "not pending"
May 17 05:36:36.615: INFO: Pod "liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.556156ms
May 17 05:36:38.620: INFO: Pod "liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008633512s
May 17 05:36:40.620: INFO: Pod "liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c": Phase="Running", Reason="", readiness=true. Elapsed: 4.008076123s
May 17 05:36:40.620: INFO: Pod "liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c" satisfied condition "not pending"
May 17 05:36:40.620: INFO: Started pod liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c in namespace container-probe-3842
STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 05:36:40.62
May 17 05:36:40.623: INFO: Initial restart count of pod liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c is 0
May 17 05:37:00.687: INFO: Restart count of pod container-probe-3842/liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c is now 1 (20.063382721s elapsed)
STEP: deleting the pod 05/17/23 05:37:00.687
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May 17 05:37:00.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3842" for this suite. 05/17/23 05:37:00.706
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":3,"skipped":38,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.133 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:36:36.58
    May 17 05:36:36.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-probe 05/17/23 05:36:36.581
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:36:36.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:36:36.597
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c in namespace container-probe-3842 05/17/23 05:36:36.601
    May 17 05:36:36.611: INFO: Waiting up to 5m0s for pod "liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c" in namespace "container-probe-3842" to be "not pending"
    May 17 05:36:36.615: INFO: Pod "liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.556156ms
    May 17 05:36:38.620: INFO: Pod "liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008633512s
    May 17 05:36:40.620: INFO: Pod "liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c": Phase="Running", Reason="", readiness=true. Elapsed: 4.008076123s
    May 17 05:36:40.620: INFO: Pod "liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c" satisfied condition "not pending"
    May 17 05:36:40.620: INFO: Started pod liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c in namespace container-probe-3842
    STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 05:36:40.62
    May 17 05:36:40.623: INFO: Initial restart count of pod liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c is 0
    May 17 05:37:00.687: INFO: Restart count of pod container-probe-3842/liveness-b37b8790-2f37-41ad-9699-1b0674d7c43c is now 1 (20.063382721s elapsed)
    STEP: deleting the pod 05/17/23 05:37:00.687
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May 17 05:37:00.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3842" for this suite. 05/17/23 05:37:00.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:37:00.714
May 17 05:37:00.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename resourcequota 05/17/23 05:37:00.715
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:37:00.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:37:00.731
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 05/17/23 05:37:00.734
STEP: Creating a ResourceQuota 05/17/23 05:37:05.739
STEP: Ensuring resource quota status is calculated 05/17/23 05:37:05.744
STEP: Creating a Service 05/17/23 05:37:07.749
STEP: Creating a NodePort Service 05/17/23 05:37:07.773
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/17/23 05:37:07.795
STEP: Ensuring resource quota status captures service creation 05/17/23 05:37:07.816
STEP: Deleting Services 05/17/23 05:37:09.821
STEP: Ensuring resource quota status released usage 05/17/23 05:37:09.855
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May 17 05:37:11.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9173" for this suite. 05/17/23 05:37:11.871
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":4,"skipped":48,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.165 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:37:00.714
    May 17 05:37:00.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename resourcequota 05/17/23 05:37:00.715
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:37:00.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:37:00.731
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 05/17/23 05:37:00.734
    STEP: Creating a ResourceQuota 05/17/23 05:37:05.739
    STEP: Ensuring resource quota status is calculated 05/17/23 05:37:05.744
    STEP: Creating a Service 05/17/23 05:37:07.749
    STEP: Creating a NodePort Service 05/17/23 05:37:07.773
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/17/23 05:37:07.795
    STEP: Ensuring resource quota status captures service creation 05/17/23 05:37:07.816
    STEP: Deleting Services 05/17/23 05:37:09.821
    STEP: Ensuring resource quota status released usage 05/17/23 05:37:09.855
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May 17 05:37:11.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9173" for this suite. 05/17/23 05:37:11.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:37:11.879
May 17 05:37:11.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 05:37:11.879
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:37:11.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:37:11.897
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 05:37:11.914
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 05:37:12.17
STEP: Deploying the webhook pod 05/17/23 05:37:12.179
STEP: Wait for the deployment to be ready 05/17/23 05:37:12.19
May 17 05:37:12.199: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/17/23 05:37:14.211
STEP: Verifying the service has paired with the endpoint 05/17/23 05:37:14.223
May 17 05:37:15.223: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 05/17/23 05:37:15.233
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/17/23 05:37:15.239
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/17/23 05:37:15.239
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/17/23 05:37:15.239
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/17/23 05:37:15.24
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/17/23 05:37:15.24
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/17/23 05:37:15.242
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 05:37:15.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7437" for this suite. 05/17/23 05:37:15.25
STEP: Destroying namespace "webhook-7437-markers" for this suite. 05/17/23 05:37:15.256
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":5,"skipped":54,"failed":0}
------------------------------
â€¢ [3.426 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:37:11.879
    May 17 05:37:11.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 05:37:11.879
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:37:11.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:37:11.897
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 05:37:11.914
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 05:37:12.17
    STEP: Deploying the webhook pod 05/17/23 05:37:12.179
    STEP: Wait for the deployment to be ready 05/17/23 05:37:12.19
    May 17 05:37:12.199: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/17/23 05:37:14.211
    STEP: Verifying the service has paired with the endpoint 05/17/23 05:37:14.223
    May 17 05:37:15.223: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 05/17/23 05:37:15.233
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/17/23 05:37:15.239
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/17/23 05:37:15.239
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/17/23 05:37:15.239
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/17/23 05:37:15.24
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/17/23 05:37:15.24
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/17/23 05:37:15.242
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 05:37:15.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7437" for this suite. 05/17/23 05:37:15.25
    STEP: Destroying namespace "webhook-7437-markers" for this suite. 05/17/23 05:37:15.256
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:37:15.306
May 17 05:37:15.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename sched-preemption 05/17/23 05:37:15.307
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:37:15.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:37:15.322
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May 17 05:37:15.345: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 05:38:15.397: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:38:15.401
May 17 05:38:15.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename sched-preemption-path 05/17/23 05:38:15.402
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:38:15.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:38:15.429
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
May 17 05:38:15.447: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
May 17 05:38:15.452: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
May 17 05:38:15.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3643" for this suite. 05/17/23 05:38:15.479
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May 17 05:38:15.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5522" for this suite. 05/17/23 05:38:15.503
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":6,"skipped":62,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.292 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:37:15.306
    May 17 05:37:15.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename sched-preemption 05/17/23 05:37:15.307
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:37:15.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:37:15.322
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May 17 05:37:15.345: INFO: Waiting up to 1m0s for all nodes to be ready
    May 17 05:38:15.397: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:38:15.401
    May 17 05:38:15.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename sched-preemption-path 05/17/23 05:38:15.402
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:38:15.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:38:15.429
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    May 17 05:38:15.447: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    May 17 05:38:15.452: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    May 17 05:38:15.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-3643" for this suite. 05/17/23 05:38:15.479
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May 17 05:38:15.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5522" for this suite. 05/17/23 05:38:15.503
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:38:15.599
May 17 05:38:15.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename podtemplate 05/17/23 05:38:15.6
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:38:15.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:38:15.62
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 05/17/23 05:38:15.624
May 17 05:38:15.629: INFO: created test-podtemplate-1
May 17 05:38:15.635: INFO: created test-podtemplate-2
May 17 05:38:15.639: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 05/17/23 05:38:15.639
STEP: delete collection of pod templates 05/17/23 05:38:15.643
May 17 05:38:15.643: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 05/17/23 05:38:15.653
May 17 05:38:15.653: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
May 17 05:38:15.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3675" for this suite. 05/17/23 05:38:15.662
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":7,"skipped":86,"failed":0}
------------------------------
â€¢ [0.068 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:38:15.599
    May 17 05:38:15.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename podtemplate 05/17/23 05:38:15.6
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:38:15.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:38:15.62
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 05/17/23 05:38:15.624
    May 17 05:38:15.629: INFO: created test-podtemplate-1
    May 17 05:38:15.635: INFO: created test-podtemplate-2
    May 17 05:38:15.639: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 05/17/23 05:38:15.639
    STEP: delete collection of pod templates 05/17/23 05:38:15.643
    May 17 05:38:15.643: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 05/17/23 05:38:15.653
    May 17 05:38:15.653: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    May 17 05:38:15.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3675" for this suite. 05/17/23 05:38:15.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:38:15.669
May 17 05:38:15.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 05:38:15.67
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:38:15.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:38:15.695
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-9e0a9465-945a-4e01-b3ac-910386f618f9 05/17/23 05:38:15.698
STEP: Creating a pod to test consume secrets 05/17/23 05:38:15.702
May 17 05:38:15.712: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f" in namespace "projected-3531" to be "Succeeded or Failed"
May 17 05:38:15.716: INFO: Pod "pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.666515ms
May 17 05:38:17.722: INFO: Pod "pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009023444s
May 17 05:38:19.722: INFO: Pod "pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009611842s
STEP: Saw pod success 05/17/23 05:38:19.722
May 17 05:38:19.722: INFO: Pod "pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f" satisfied condition "Succeeded or Failed"
May 17 05:38:19.727: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f container projected-secret-volume-test: <nil>
STEP: delete the pod 05/17/23 05:38:19.737
May 17 05:38:19.749: INFO: Waiting for pod pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f to disappear
May 17 05:38:19.753: INFO: Pod pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May 17 05:38:19.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3531" for this suite. 05/17/23 05:38:19.76
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":8,"skipped":121,"failed":0}
------------------------------
â€¢ [4.098 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:38:15.669
    May 17 05:38:15.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 05:38:15.67
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:38:15.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:38:15.695
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-9e0a9465-945a-4e01-b3ac-910386f618f9 05/17/23 05:38:15.698
    STEP: Creating a pod to test consume secrets 05/17/23 05:38:15.702
    May 17 05:38:15.712: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f" in namespace "projected-3531" to be "Succeeded or Failed"
    May 17 05:38:15.716: INFO: Pod "pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.666515ms
    May 17 05:38:17.722: INFO: Pod "pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009023444s
    May 17 05:38:19.722: INFO: Pod "pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009611842s
    STEP: Saw pod success 05/17/23 05:38:19.722
    May 17 05:38:19.722: INFO: Pod "pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f" satisfied condition "Succeeded or Failed"
    May 17 05:38:19.727: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 05:38:19.737
    May 17 05:38:19.749: INFO: Waiting for pod pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f to disappear
    May 17 05:38:19.753: INFO: Pod pod-projected-secrets-c25b2861-ed4a-4f05-8e81-d1d8fc71b72f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May 17 05:38:19.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3531" for this suite. 05/17/23 05:38:19.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:38:19.769
May 17 05:38:19.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pod-network-test 05/17/23 05:38:19.769
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:38:19.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:38:19.791
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-3774 05/17/23 05:38:19.794
STEP: creating a selector 05/17/23 05:38:19.794
STEP: Creating the service pods in kubernetes 05/17/23 05:38:19.794
May 17 05:38:19.794: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 17 05:38:19.828: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3774" to be "running and ready"
May 17 05:38:19.831: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.394597ms
May 17 05:38:19.831: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 05:38:21.836: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00881174s
May 17 05:38:21.836: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 05:38:23.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010122314s
May 17 05:38:23.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 05:38:25.839: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011205625s
May 17 05:38:25.839: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 05:38:27.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010240765s
May 17 05:38:27.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 05:38:29.836: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008725126s
May 17 05:38:29.836: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 05:38:31.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008950747s
May 17 05:38:31.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 05:38:33.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007733114s
May 17 05:38:33.835: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 05:38:35.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009215508s
May 17 05:38:35.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 05:38:37.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009857283s
May 17 05:38:37.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 05:38:39.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009004775s
May 17 05:38:39.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 05:38:41.836: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008656565s
May 17 05:38:41.836: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May 17 05:38:41.836: INFO: Pod "netserver-0" satisfied condition "running and ready"
May 17 05:38:41.841: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3774" to be "running and ready"
May 17 05:38:41.845: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.162842ms
May 17 05:38:41.845: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May 17 05:38:41.845: INFO: Pod "netserver-1" satisfied condition "running and ready"
May 17 05:38:41.849: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3774" to be "running and ready"
May 17 05:38:41.853: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.66427ms
May 17 05:38:41.853: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May 17 05:38:41.853: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/17/23 05:38:41.857
May 17 05:38:41.864: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3774" to be "running"
May 17 05:38:41.868: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.768225ms
May 17 05:38:43.874: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009187686s
May 17 05:38:43.874: INFO: Pod "test-container-pod" satisfied condition "running"
May 17 05:38:43.877: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 17 05:38:43.877: INFO: Breadth first check of 10.244.0.38 on host 10.224.0.4...
May 17 05:38:43.881: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.14:9080/dial?request=hostname&protocol=http&host=10.244.0.38&port=8083&tries=1'] Namespace:pod-network-test-3774 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 05:38:43.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 05:38:43.882: INFO: ExecWithOptions: Clientset creation
May 17 05:38:43.882: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-3774/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.14%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.38%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 17 05:38:44.031: INFO: Waiting for responses: map[]
May 17 05:38:44.031: INFO: reached 10.244.0.38 after 0/1 tries
May 17 05:38:44.031: INFO: Breadth first check of 10.244.2.13 on host 10.224.0.6...
May 17 05:38:44.036: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.14:9080/dial?request=hostname&protocol=http&host=10.244.2.13&port=8083&tries=1'] Namespace:pod-network-test-3774 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 05:38:44.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 05:38:44.037: INFO: ExecWithOptions: Clientset creation
May 17 05:38:44.037: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-3774/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.14%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.13%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 17 05:38:44.163: INFO: Waiting for responses: map[]
May 17 05:38:44.163: INFO: reached 10.244.2.13 after 0/1 tries
May 17 05:38:44.163: INFO: Breadth first check of 10.244.1.7 on host 10.224.0.5...
May 17 05:38:44.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.14:9080/dial?request=hostname&protocol=http&host=10.244.1.7&port=8083&tries=1'] Namespace:pod-network-test-3774 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 05:38:44.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 05:38:44.167: INFO: ExecWithOptions: Clientset creation
May 17 05:38:44.167: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-3774/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.14%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.7%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 17 05:38:44.280: INFO: Waiting for responses: map[]
May 17 05:38:44.280: INFO: reached 10.244.1.7 after 0/1 tries
May 17 05:38:44.280: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May 17 05:38:44.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3774" for this suite. 05/17/23 05:38:44.286
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":9,"skipped":160,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.524 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:38:19.769
    May 17 05:38:19.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pod-network-test 05/17/23 05:38:19.769
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:38:19.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:38:19.791
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-3774 05/17/23 05:38:19.794
    STEP: creating a selector 05/17/23 05:38:19.794
    STEP: Creating the service pods in kubernetes 05/17/23 05:38:19.794
    May 17 05:38:19.794: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May 17 05:38:19.828: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3774" to be "running and ready"
    May 17 05:38:19.831: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.394597ms
    May 17 05:38:19.831: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 17 05:38:21.836: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00881174s
    May 17 05:38:21.836: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 17 05:38:23.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010122314s
    May 17 05:38:23.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 05:38:25.839: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011205625s
    May 17 05:38:25.839: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 05:38:27.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010240765s
    May 17 05:38:27.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 05:38:29.836: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008725126s
    May 17 05:38:29.836: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 05:38:31.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008950747s
    May 17 05:38:31.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 05:38:33.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007733114s
    May 17 05:38:33.835: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 05:38:35.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009215508s
    May 17 05:38:35.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 05:38:37.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009857283s
    May 17 05:38:37.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 05:38:39.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009004775s
    May 17 05:38:39.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 05:38:41.836: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008656565s
    May 17 05:38:41.836: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May 17 05:38:41.836: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May 17 05:38:41.841: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3774" to be "running and ready"
    May 17 05:38:41.845: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.162842ms
    May 17 05:38:41.845: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May 17 05:38:41.845: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May 17 05:38:41.849: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3774" to be "running and ready"
    May 17 05:38:41.853: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.66427ms
    May 17 05:38:41.853: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May 17 05:38:41.853: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/17/23 05:38:41.857
    May 17 05:38:41.864: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3774" to be "running"
    May 17 05:38:41.868: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.768225ms
    May 17 05:38:43.874: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009187686s
    May 17 05:38:43.874: INFO: Pod "test-container-pod" satisfied condition "running"
    May 17 05:38:43.877: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May 17 05:38:43.877: INFO: Breadth first check of 10.244.0.38 on host 10.224.0.4...
    May 17 05:38:43.881: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.14:9080/dial?request=hostname&protocol=http&host=10.244.0.38&port=8083&tries=1'] Namespace:pod-network-test-3774 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 05:38:43.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 05:38:43.882: INFO: ExecWithOptions: Clientset creation
    May 17 05:38:43.882: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-3774/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.14%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.38%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 17 05:38:44.031: INFO: Waiting for responses: map[]
    May 17 05:38:44.031: INFO: reached 10.244.0.38 after 0/1 tries
    May 17 05:38:44.031: INFO: Breadth first check of 10.244.2.13 on host 10.224.0.6...
    May 17 05:38:44.036: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.14:9080/dial?request=hostname&protocol=http&host=10.244.2.13&port=8083&tries=1'] Namespace:pod-network-test-3774 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 05:38:44.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 05:38:44.037: INFO: ExecWithOptions: Clientset creation
    May 17 05:38:44.037: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-3774/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.14%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.13%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 17 05:38:44.163: INFO: Waiting for responses: map[]
    May 17 05:38:44.163: INFO: reached 10.244.2.13 after 0/1 tries
    May 17 05:38:44.163: INFO: Breadth first check of 10.244.1.7 on host 10.224.0.5...
    May 17 05:38:44.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.14:9080/dial?request=hostname&protocol=http&host=10.244.1.7&port=8083&tries=1'] Namespace:pod-network-test-3774 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 05:38:44.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 05:38:44.167: INFO: ExecWithOptions: Clientset creation
    May 17 05:38:44.167: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-3774/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.14%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.7%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 17 05:38:44.280: INFO: Waiting for responses: map[]
    May 17 05:38:44.280: INFO: reached 10.244.1.7 after 0/1 tries
    May 17 05:38:44.280: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May 17 05:38:44.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3774" for this suite. 05/17/23 05:38:44.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:38:44.294
May 17 05:38:44.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-probe 05/17/23 05:38:44.295
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:38:44.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:38:44.316
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-52397296-d1fc-433b-b4ed-454814c4500a in namespace container-probe-7553 05/17/23 05:38:44.32
May 17 05:38:44.333: INFO: Waiting up to 5m0s for pod "liveness-52397296-d1fc-433b-b4ed-454814c4500a" in namespace "container-probe-7553" to be "not pending"
May 17 05:38:44.337: INFO: Pod "liveness-52397296-d1fc-433b-b4ed-454814c4500a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068276ms
May 17 05:38:46.341: INFO: Pod "liveness-52397296-d1fc-433b-b4ed-454814c4500a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00872233s
May 17 05:38:46.341: INFO: Pod "liveness-52397296-d1fc-433b-b4ed-454814c4500a" satisfied condition "not pending"
May 17 05:38:46.341: INFO: Started pod liveness-52397296-d1fc-433b-b4ed-454814c4500a in namespace container-probe-7553
STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 05:38:46.341
May 17 05:38:46.346: INFO: Initial restart count of pod liveness-52397296-d1fc-433b-b4ed-454814c4500a is 0
STEP: deleting the pod 05/17/23 05:42:46.993
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May 17 05:42:47.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7553" for this suite. 05/17/23 05:42:47.015
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":10,"skipped":172,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.733 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:38:44.294
    May 17 05:38:44.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-probe 05/17/23 05:38:44.295
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:38:44.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:38:44.316
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-52397296-d1fc-433b-b4ed-454814c4500a in namespace container-probe-7553 05/17/23 05:38:44.32
    May 17 05:38:44.333: INFO: Waiting up to 5m0s for pod "liveness-52397296-d1fc-433b-b4ed-454814c4500a" in namespace "container-probe-7553" to be "not pending"
    May 17 05:38:44.337: INFO: Pod "liveness-52397296-d1fc-433b-b4ed-454814c4500a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068276ms
    May 17 05:38:46.341: INFO: Pod "liveness-52397296-d1fc-433b-b4ed-454814c4500a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00872233s
    May 17 05:38:46.341: INFO: Pod "liveness-52397296-d1fc-433b-b4ed-454814c4500a" satisfied condition "not pending"
    May 17 05:38:46.341: INFO: Started pod liveness-52397296-d1fc-433b-b4ed-454814c4500a in namespace container-probe-7553
    STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 05:38:46.341
    May 17 05:38:46.346: INFO: Initial restart count of pod liveness-52397296-d1fc-433b-b4ed-454814c4500a is 0
    STEP: deleting the pod 05/17/23 05:42:46.993
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May 17 05:42:47.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7553" for this suite. 05/17/23 05:42:47.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:42:47.027
May 17 05:42:47.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename resourcequota 05/17/23 05:42:47.028
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:42:47.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:42:47.048
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 05/17/23 05:42:47.051
STEP: Creating a ResourceQuota 05/17/23 05:42:52.057
STEP: Ensuring resource quota status is calculated 05/17/23 05:42:52.063
STEP: Creating a ReplicaSet 05/17/23 05:42:54.071
STEP: Ensuring resource quota status captures replicaset creation 05/17/23 05:42:54.085
STEP: Deleting a ReplicaSet 05/17/23 05:42:56.09
STEP: Ensuring resource quota status released usage 05/17/23 05:42:56.098
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May 17 05:42:58.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8144" for this suite. 05/17/23 05:42:58.118
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":11,"skipped":179,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.099 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:42:47.027
    May 17 05:42:47.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename resourcequota 05/17/23 05:42:47.028
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:42:47.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:42:47.048
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 05/17/23 05:42:47.051
    STEP: Creating a ResourceQuota 05/17/23 05:42:52.057
    STEP: Ensuring resource quota status is calculated 05/17/23 05:42:52.063
    STEP: Creating a ReplicaSet 05/17/23 05:42:54.071
    STEP: Ensuring resource quota status captures replicaset creation 05/17/23 05:42:54.085
    STEP: Deleting a ReplicaSet 05/17/23 05:42:56.09
    STEP: Ensuring resource quota status released usage 05/17/23 05:42:56.098
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May 17 05:42:58.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8144" for this suite. 05/17/23 05:42:58.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:42:58.127
May 17 05:42:58.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pods 05/17/23 05:42:58.128
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:42:58.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:42:58.148
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 05/17/23 05:42:58.151
May 17 05:42:58.165: INFO: created test-pod-1
May 17 05:42:58.172: INFO: created test-pod-2
May 17 05:42:58.178: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 05/17/23 05:42:58.178
May 17 05:42:58.178: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4760' to be running and ready
May 17 05:42:58.190: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May 17 05:42:58.190: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May 17 05:42:58.190: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May 17 05:42:58.190: INFO: 0 / 3 pods in namespace 'pods-4760' are running and ready (0 seconds elapsed)
May 17 05:42:58.190: INFO: expected 0 pod replicas in namespace 'pods-4760', 0 are Running and Ready.
May 17 05:42:58.190: INFO: POD         NODE                               PHASE    GRACE  CONDITIONS
May 17 05:42:58.190: INFO: test-pod-1  aks-agentpool-72615086-vmss00000c  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:42:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:42:58 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:42:58 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:42:58 +0000 UTC  }]
May 17 05:42:58.190: INFO: test-pod-2  aks-agentpool-72615086-vmss00000c  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:42:58 +0000 UTC  }]
May 17 05:42:58.190: INFO: test-pod-3  aks-agentpool-72615086-vmss00000d  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:42:58 +0000 UTC  }]
May 17 05:42:58.190: INFO: 
May 17 05:43:00.204: INFO: 3 / 3 pods in namespace 'pods-4760' are running and ready (2 seconds elapsed)
May 17 05:43:00.204: INFO: expected 0 pod replicas in namespace 'pods-4760', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 05/17/23 05:43:00.216
May 17 05:43:00.220: INFO: Pod quantity 3 is different from expected quantity 0
May 17 05:43:01.226: INFO: Pod quantity 3 is different from expected quantity 0
May 17 05:43:02.225: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May 17 05:43:03.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4760" for this suite. 05/17/23 05:43:03.232
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":12,"skipped":217,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.112 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:42:58.127
    May 17 05:42:58.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pods 05/17/23 05:42:58.128
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:42:58.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:42:58.148
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 05/17/23 05:42:58.151
    May 17 05:42:58.165: INFO: created test-pod-1
    May 17 05:42:58.172: INFO: created test-pod-2
    May 17 05:42:58.178: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 05/17/23 05:42:58.178
    May 17 05:42:58.178: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4760' to be running and ready
    May 17 05:42:58.190: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May 17 05:42:58.190: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May 17 05:42:58.190: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May 17 05:42:58.190: INFO: 0 / 3 pods in namespace 'pods-4760' are running and ready (0 seconds elapsed)
    May 17 05:42:58.190: INFO: expected 0 pod replicas in namespace 'pods-4760', 0 are Running and Ready.
    May 17 05:42:58.190: INFO: POD         NODE                               PHASE    GRACE  CONDITIONS
    May 17 05:42:58.190: INFO: test-pod-1  aks-agentpool-72615086-vmss00000c  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:42:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:42:58 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:42:58 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:42:58 +0000 UTC  }]
    May 17 05:42:58.190: INFO: test-pod-2  aks-agentpool-72615086-vmss00000c  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:42:58 +0000 UTC  }]
    May 17 05:42:58.190: INFO: test-pod-3  aks-agentpool-72615086-vmss00000d  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:42:58 +0000 UTC  }]
    May 17 05:42:58.190: INFO: 
    May 17 05:43:00.204: INFO: 3 / 3 pods in namespace 'pods-4760' are running and ready (2 seconds elapsed)
    May 17 05:43:00.204: INFO: expected 0 pod replicas in namespace 'pods-4760', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 05/17/23 05:43:00.216
    May 17 05:43:00.220: INFO: Pod quantity 3 is different from expected quantity 0
    May 17 05:43:01.226: INFO: Pod quantity 3 is different from expected quantity 0
    May 17 05:43:02.225: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May 17 05:43:03.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4760" for this suite. 05/17/23 05:43:03.232
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:43:03.239
May 17 05:43:03.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 05:43:03.24
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:03.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:03.258
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/17/23 05:43:03.266
May 17 05:43:03.276: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8959" to be "running and ready"
May 17 05:43:03.279: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.439781ms
May 17 05:43:03.279: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 17 05:43:05.285: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009029496s
May 17 05:43:05.285: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May 17 05:43:05.285: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 05/17/23 05:43:05.289
May 17 05:43:05.296: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8959" to be "running and ready"
May 17 05:43:05.300: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.472431ms
May 17 05:43:05.300: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 17 05:43:07.306: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009625899s
May 17 05:43:07.306: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
May 17 05:43:07.306: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/17/23 05:43:07.309
May 17 05:43:07.317: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 17 05:43:07.321: INFO: Pod pod-with-prestop-http-hook still exists
May 17 05:43:09.323: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 17 05:43:09.327: INFO: Pod pod-with-prestop-http-hook still exists
May 17 05:43:11.323: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 17 05:43:11.328: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 05/17/23 05:43:11.328
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May 17 05:43:11.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8959" for this suite. 05/17/23 05:43:11.35
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":13,"skipped":217,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.118 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:43:03.239
    May 17 05:43:03.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 05:43:03.24
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:03.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:03.258
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/17/23 05:43:03.266
    May 17 05:43:03.276: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8959" to be "running and ready"
    May 17 05:43:03.279: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.439781ms
    May 17 05:43:03.279: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May 17 05:43:05.285: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009029496s
    May 17 05:43:05.285: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May 17 05:43:05.285: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 05/17/23 05:43:05.289
    May 17 05:43:05.296: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8959" to be "running and ready"
    May 17 05:43:05.300: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.472431ms
    May 17 05:43:05.300: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May 17 05:43:07.306: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009625899s
    May 17 05:43:07.306: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    May 17 05:43:07.306: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/17/23 05:43:07.309
    May 17 05:43:07.317: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May 17 05:43:07.321: INFO: Pod pod-with-prestop-http-hook still exists
    May 17 05:43:09.323: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May 17 05:43:09.327: INFO: Pod pod-with-prestop-http-hook still exists
    May 17 05:43:11.323: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May 17 05:43:11.328: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 05/17/23 05:43:11.328
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May 17 05:43:11.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8959" for this suite. 05/17/23 05:43:11.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:43:11.358
May 17 05:43:11.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename watch 05/17/23 05:43:11.359
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:11.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:11.38
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 05/17/23 05:43:11.383
STEP: creating a watch on configmaps with label B 05/17/23 05:43:11.384
STEP: creating a watch on configmaps with label A or B 05/17/23 05:43:11.386
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/17/23 05:43:11.387
May 17 05:43:11.392: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435362 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 05:43:11.392: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435362 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/17/23 05:43:11.392
May 17 05:43:11.400: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435363 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 05:43:11.400: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435363 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/17/23 05:43:11.4
May 17 05:43:11.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435364 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 05:43:11.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435364 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/17/23 05:43:11.409
May 17 05:43:11.415: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435365 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 05:43:11.415: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435365 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/17/23 05:43:11.416
May 17 05:43:11.420: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7431  e653b4a8-de06-4bfa-9edd-80a5522c1c93 1435366 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 05:43:11.420: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7431  e653b4a8-de06-4bfa-9edd-80a5522c1c93 1435366 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/17/23 05:43:21.421
May 17 05:43:21.429: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7431  e653b4a8-de06-4bfa-9edd-80a5522c1c93 1435479 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 05:43:21.429: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7431  e653b4a8-de06-4bfa-9edd-80a5522c1c93 1435479 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May 17 05:43:31.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7431" for this suite. 05/17/23 05:43:31.437
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":14,"skipped":233,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.087 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:43:11.358
    May 17 05:43:11.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename watch 05/17/23 05:43:11.359
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:11.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:11.38
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 05/17/23 05:43:11.383
    STEP: creating a watch on configmaps with label B 05/17/23 05:43:11.384
    STEP: creating a watch on configmaps with label A or B 05/17/23 05:43:11.386
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/17/23 05:43:11.387
    May 17 05:43:11.392: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435362 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 05:43:11.392: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435362 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/17/23 05:43:11.392
    May 17 05:43:11.400: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435363 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 05:43:11.400: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435363 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/17/23 05:43:11.4
    May 17 05:43:11.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435364 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 05:43:11.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435364 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/17/23 05:43:11.409
    May 17 05:43:11.415: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435365 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 05:43:11.415: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7431  cc4b195b-0eb9-4b99-9566-406b77352faa 1435365 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/17/23 05:43:11.416
    May 17 05:43:11.420: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7431  e653b4a8-de06-4bfa-9edd-80a5522c1c93 1435366 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 05:43:11.420: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7431  e653b4a8-de06-4bfa-9edd-80a5522c1c93 1435366 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/17/23 05:43:21.421
    May 17 05:43:21.429: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7431  e653b4a8-de06-4bfa-9edd-80a5522c1c93 1435479 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 05:43:21.429: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7431  e653b4a8-de06-4bfa-9edd-80a5522c1c93 1435479 0 2023-05-17 05:43:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-17 05:43:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May 17 05:43:31.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7431" for this suite. 05/17/23 05:43:31.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:43:31.446
May 17 05:43:31.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pods 05/17/23 05:43:31.447
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:31.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:31.493
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 05/17/23 05:43:31.497
May 17 05:43:31.507: INFO: Waiting up to 5m0s for pod "pod-bd6sl" in namespace "pods-2068" to be "running"
May 17 05:43:31.511: INFO: Pod "pod-bd6sl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.570968ms
May 17 05:43:33.517: INFO: Pod "pod-bd6sl": Phase="Running", Reason="", readiness=true. Elapsed: 2.009018958s
May 17 05:43:33.517: INFO: Pod "pod-bd6sl" satisfied condition "running"
STEP: patching /status 05/17/23 05:43:33.517
May 17 05:43:33.527: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May 17 05:43:33.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2068" for this suite. 05/17/23 05:43:33.534
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":15,"skipped":272,"failed":0}
------------------------------
â€¢ [2.095 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:43:31.446
    May 17 05:43:31.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pods 05/17/23 05:43:31.447
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:31.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:31.493
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 05/17/23 05:43:31.497
    May 17 05:43:31.507: INFO: Waiting up to 5m0s for pod "pod-bd6sl" in namespace "pods-2068" to be "running"
    May 17 05:43:31.511: INFO: Pod "pod-bd6sl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.570968ms
    May 17 05:43:33.517: INFO: Pod "pod-bd6sl": Phase="Running", Reason="", readiness=true. Elapsed: 2.009018958s
    May 17 05:43:33.517: INFO: Pod "pod-bd6sl" satisfied condition "running"
    STEP: patching /status 05/17/23 05:43:33.517
    May 17 05:43:33.527: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May 17 05:43:33.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2068" for this suite. 05/17/23 05:43:33.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:43:33.542
May 17 05:43:33.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 05:43:33.543
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:33.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:33.559
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/17/23 05:43:33.562
May 17 05:43:33.573: INFO: Waiting up to 5m0s for pod "pod-23563f31-c1e9-46f8-9468-51095339f82b" in namespace "emptydir-8662" to be "Succeeded or Failed"
May 17 05:43:33.577: INFO: Pod "pod-23563f31-c1e9-46f8-9468-51095339f82b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.638402ms
May 17 05:43:35.582: INFO: Pod "pod-23563f31-c1e9-46f8-9468-51095339f82b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008729792s
May 17 05:43:37.584: INFO: Pod "pod-23563f31-c1e9-46f8-9468-51095339f82b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010956209s
STEP: Saw pod success 05/17/23 05:43:37.584
May 17 05:43:37.584: INFO: Pod "pod-23563f31-c1e9-46f8-9468-51095339f82b" satisfied condition "Succeeded or Failed"
May 17 05:43:37.588: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-23563f31-c1e9-46f8-9468-51095339f82b container test-container: <nil>
STEP: delete the pod 05/17/23 05:43:37.598
May 17 05:43:37.609: INFO: Waiting for pod pod-23563f31-c1e9-46f8-9468-51095339f82b to disappear
May 17 05:43:37.613: INFO: Pod pod-23563f31-c1e9-46f8-9468-51095339f82b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 05:43:37.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8662" for this suite. 05/17/23 05:43:37.619
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":16,"skipped":291,"failed":0}
------------------------------
â€¢ [4.084 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:43:33.542
    May 17 05:43:33.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 05:43:33.543
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:33.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:33.559
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/17/23 05:43:33.562
    May 17 05:43:33.573: INFO: Waiting up to 5m0s for pod "pod-23563f31-c1e9-46f8-9468-51095339f82b" in namespace "emptydir-8662" to be "Succeeded or Failed"
    May 17 05:43:33.577: INFO: Pod "pod-23563f31-c1e9-46f8-9468-51095339f82b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.638402ms
    May 17 05:43:35.582: INFO: Pod "pod-23563f31-c1e9-46f8-9468-51095339f82b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008729792s
    May 17 05:43:37.584: INFO: Pod "pod-23563f31-c1e9-46f8-9468-51095339f82b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010956209s
    STEP: Saw pod success 05/17/23 05:43:37.584
    May 17 05:43:37.584: INFO: Pod "pod-23563f31-c1e9-46f8-9468-51095339f82b" satisfied condition "Succeeded or Failed"
    May 17 05:43:37.588: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-23563f31-c1e9-46f8-9468-51095339f82b container test-container: <nil>
    STEP: delete the pod 05/17/23 05:43:37.598
    May 17 05:43:37.609: INFO: Waiting for pod pod-23563f31-c1e9-46f8-9468-51095339f82b to disappear
    May 17 05:43:37.613: INFO: Pod pod-23563f31-c1e9-46f8-9468-51095339f82b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 05:43:37.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8662" for this suite. 05/17/23 05:43:37.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:43:37.627
May 17 05:43:37.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir-wrapper 05/17/23 05:43:37.628
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:37.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:37.643
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
May 17 05:43:37.666: INFO: Waiting up to 5m0s for pod "pod-secrets-aa9e4a8e-ef66-4a1f-80fe-3b9394c25ada" in namespace "emptydir-wrapper-6247" to be "running and ready"
May 17 05:43:37.669: INFO: Pod "pod-secrets-aa9e4a8e-ef66-4a1f-80fe-3b9394c25ada": Phase="Pending", Reason="", readiness=false. Elapsed: 3.367304ms
May 17 05:43:37.669: INFO: The phase of Pod pod-secrets-aa9e4a8e-ef66-4a1f-80fe-3b9394c25ada is Pending, waiting for it to be Running (with Ready = true)
May 17 05:43:39.675: INFO: Pod "pod-secrets-aa9e4a8e-ef66-4a1f-80fe-3b9394c25ada": Phase="Running", Reason="", readiness=true. Elapsed: 2.00888915s
May 17 05:43:39.675: INFO: The phase of Pod pod-secrets-aa9e4a8e-ef66-4a1f-80fe-3b9394c25ada is Running (Ready = true)
May 17 05:43:39.675: INFO: Pod "pod-secrets-aa9e4a8e-ef66-4a1f-80fe-3b9394c25ada" satisfied condition "running and ready"
STEP: Cleaning up the secret 05/17/23 05:43:39.679
STEP: Cleaning up the configmap 05/17/23 05:43:39.685
STEP: Cleaning up the pod 05/17/23 05:43:39.692
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
May 17 05:43:39.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6247" for this suite. 05/17/23 05:43:39.711
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":17,"skipped":312,"failed":0}
------------------------------
â€¢ [2.090 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:43:37.627
    May 17 05:43:37.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir-wrapper 05/17/23 05:43:37.628
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:37.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:37.643
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    May 17 05:43:37.666: INFO: Waiting up to 5m0s for pod "pod-secrets-aa9e4a8e-ef66-4a1f-80fe-3b9394c25ada" in namespace "emptydir-wrapper-6247" to be "running and ready"
    May 17 05:43:37.669: INFO: Pod "pod-secrets-aa9e4a8e-ef66-4a1f-80fe-3b9394c25ada": Phase="Pending", Reason="", readiness=false. Elapsed: 3.367304ms
    May 17 05:43:37.669: INFO: The phase of Pod pod-secrets-aa9e4a8e-ef66-4a1f-80fe-3b9394c25ada is Pending, waiting for it to be Running (with Ready = true)
    May 17 05:43:39.675: INFO: Pod "pod-secrets-aa9e4a8e-ef66-4a1f-80fe-3b9394c25ada": Phase="Running", Reason="", readiness=true. Elapsed: 2.00888915s
    May 17 05:43:39.675: INFO: The phase of Pod pod-secrets-aa9e4a8e-ef66-4a1f-80fe-3b9394c25ada is Running (Ready = true)
    May 17 05:43:39.675: INFO: Pod "pod-secrets-aa9e4a8e-ef66-4a1f-80fe-3b9394c25ada" satisfied condition "running and ready"
    STEP: Cleaning up the secret 05/17/23 05:43:39.679
    STEP: Cleaning up the configmap 05/17/23 05:43:39.685
    STEP: Cleaning up the pod 05/17/23 05:43:39.692
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    May 17 05:43:39.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-6247" for this suite. 05/17/23 05:43:39.711
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:43:39.718
May 17 05:43:39.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 05:43:39.718
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:39.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:39.734
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 05:43:39.751
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 05:43:40.116
STEP: Deploying the webhook pod 05/17/23 05:43:40.125
STEP: Wait for the deployment to be ready 05/17/23 05:43:40.137
May 17 05:43:40.145: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 05:43:42.161
STEP: Verifying the service has paired with the endpoint 05/17/23 05:43:42.172
May 17 05:43:43.173: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 05/17/23 05:43:43.176
STEP: create a pod 05/17/23 05:43:43.195
May 17 05:43:43.203: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6020" to be "running"
May 17 05:43:43.210: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.348369ms
May 17 05:43:45.214: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011126486s
May 17 05:43:45.214: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 05/17/23 05:43:45.214
May 17 05:43:45.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=webhook-6020 attach --namespace=webhook-6020 to-be-attached-pod -i -c=container1'
May 17 05:43:45.329: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 05:43:45.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6020" for this suite. 05/17/23 05:43:45.342
STEP: Destroying namespace "webhook-6020-markers" for this suite. 05/17/23 05:43:45.349
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":18,"skipped":316,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.673 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:43:39.718
    May 17 05:43:39.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 05:43:39.718
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:39.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:39.734
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 05:43:39.751
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 05:43:40.116
    STEP: Deploying the webhook pod 05/17/23 05:43:40.125
    STEP: Wait for the deployment to be ready 05/17/23 05:43:40.137
    May 17 05:43:40.145: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 05:43:42.161
    STEP: Verifying the service has paired with the endpoint 05/17/23 05:43:42.172
    May 17 05:43:43.173: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 05/17/23 05:43:43.176
    STEP: create a pod 05/17/23 05:43:43.195
    May 17 05:43:43.203: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6020" to be "running"
    May 17 05:43:43.210: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.348369ms
    May 17 05:43:45.214: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011126486s
    May 17 05:43:45.214: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 05/17/23 05:43:45.214
    May 17 05:43:45.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=webhook-6020 attach --namespace=webhook-6020 to-be-attached-pod -i -c=container1'
    May 17 05:43:45.329: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 05:43:45.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6020" for this suite. 05/17/23 05:43:45.342
    STEP: Destroying namespace "webhook-6020-markers" for this suite. 05/17/23 05:43:45.349
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:43:45.391
May 17 05:43:45.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename subpath 05/17/23 05:43:45.392
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:45.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:45.41
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/17/23 05:43:45.414
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-z54m 05/17/23 05:43:45.423
STEP: Creating a pod to test atomic-volume-subpath 05/17/23 05:43:45.423
May 17 05:43:45.433: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-z54m" in namespace "subpath-7376" to be "Succeeded or Failed"
May 17 05:43:45.437: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Pending", Reason="", readiness=false. Elapsed: 3.675392ms
May 17 05:43:47.446: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 2.012576825s
May 17 05:43:49.442: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 4.008975655s
May 17 05:43:51.442: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 6.008517707s
May 17 05:43:53.444: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 8.010550284s
May 17 05:43:55.442: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 10.008745618s
May 17 05:43:57.443: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 12.009492269s
May 17 05:43:59.442: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 14.008310182s
May 17 05:44:01.442: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 16.009231204s
May 17 05:44:03.443: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 18.010104836s
May 17 05:44:05.444: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 20.010318818s
May 17 05:44:07.443: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=false. Elapsed: 22.009388462s
May 17 05:44:09.443: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009810491s
STEP: Saw pod success 05/17/23 05:44:09.443
May 17 05:44:09.443: INFO: Pod "pod-subpath-test-configmap-z54m" satisfied condition "Succeeded or Failed"
May 17 05:44:09.448: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000d pod pod-subpath-test-configmap-z54m container test-container-subpath-configmap-z54m: <nil>
STEP: delete the pod 05/17/23 05:44:09.457
May 17 05:44:09.469: INFO: Waiting for pod pod-subpath-test-configmap-z54m to disappear
May 17 05:44:09.473: INFO: Pod pod-subpath-test-configmap-z54m no longer exists
STEP: Deleting pod pod-subpath-test-configmap-z54m 05/17/23 05:44:09.473
May 17 05:44:09.473: INFO: Deleting pod "pod-subpath-test-configmap-z54m" in namespace "subpath-7376"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May 17 05:44:09.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7376" for this suite. 05/17/23 05:44:09.482
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":19,"skipped":317,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.098 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:43:45.391
    May 17 05:43:45.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename subpath 05/17/23 05:43:45.392
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:43:45.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:43:45.41
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/17/23 05:43:45.414
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-z54m 05/17/23 05:43:45.423
    STEP: Creating a pod to test atomic-volume-subpath 05/17/23 05:43:45.423
    May 17 05:43:45.433: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-z54m" in namespace "subpath-7376" to be "Succeeded or Failed"
    May 17 05:43:45.437: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Pending", Reason="", readiness=false. Elapsed: 3.675392ms
    May 17 05:43:47.446: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 2.012576825s
    May 17 05:43:49.442: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 4.008975655s
    May 17 05:43:51.442: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 6.008517707s
    May 17 05:43:53.444: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 8.010550284s
    May 17 05:43:55.442: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 10.008745618s
    May 17 05:43:57.443: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 12.009492269s
    May 17 05:43:59.442: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 14.008310182s
    May 17 05:44:01.442: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 16.009231204s
    May 17 05:44:03.443: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 18.010104836s
    May 17 05:44:05.444: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=true. Elapsed: 20.010318818s
    May 17 05:44:07.443: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Running", Reason="", readiness=false. Elapsed: 22.009388462s
    May 17 05:44:09.443: INFO: Pod "pod-subpath-test-configmap-z54m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009810491s
    STEP: Saw pod success 05/17/23 05:44:09.443
    May 17 05:44:09.443: INFO: Pod "pod-subpath-test-configmap-z54m" satisfied condition "Succeeded or Failed"
    May 17 05:44:09.448: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000d pod pod-subpath-test-configmap-z54m container test-container-subpath-configmap-z54m: <nil>
    STEP: delete the pod 05/17/23 05:44:09.457
    May 17 05:44:09.469: INFO: Waiting for pod pod-subpath-test-configmap-z54m to disappear
    May 17 05:44:09.473: INFO: Pod pod-subpath-test-configmap-z54m no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-z54m 05/17/23 05:44:09.473
    May 17 05:44:09.473: INFO: Deleting pod "pod-subpath-test-configmap-z54m" in namespace "subpath-7376"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May 17 05:44:09.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7376" for this suite. 05/17/23 05:44:09.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:44:09.491
May 17 05:44:09.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 05:44:09.492
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:09.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:09.51
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-f7d8bee5-0ffa-4d0f-ab25-aaa84b50c2c0 05/17/23 05:44:09.513
STEP: Creating a pod to test consume configMaps 05/17/23 05:44:09.518
May 17 05:44:09.529: INFO: Waiting up to 5m0s for pod "pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5" in namespace "configmap-7816" to be "Succeeded or Failed"
May 17 05:44:09.533: INFO: Pod "pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.855274ms
May 17 05:44:11.538: INFO: Pod "pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008332194s
May 17 05:44:13.539: INFO: Pod "pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00976983s
STEP: Saw pod success 05/17/23 05:44:13.539
May 17 05:44:13.539: INFO: Pod "pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5" satisfied condition "Succeeded or Failed"
May 17 05:44:13.543: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 05:44:13.553
May 17 05:44:13.563: INFO: Waiting for pod pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5 to disappear
May 17 05:44:13.567: INFO: Pod pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May 17 05:44:13.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7816" for this suite. 05/17/23 05:44:13.574
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":20,"skipped":345,"failed":0}
------------------------------
â€¢ [4.089 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:44:09.491
    May 17 05:44:09.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 05:44:09.492
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:09.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:09.51
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-f7d8bee5-0ffa-4d0f-ab25-aaa84b50c2c0 05/17/23 05:44:09.513
    STEP: Creating a pod to test consume configMaps 05/17/23 05:44:09.518
    May 17 05:44:09.529: INFO: Waiting up to 5m0s for pod "pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5" in namespace "configmap-7816" to be "Succeeded or Failed"
    May 17 05:44:09.533: INFO: Pod "pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.855274ms
    May 17 05:44:11.538: INFO: Pod "pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008332194s
    May 17 05:44:13.539: INFO: Pod "pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00976983s
    STEP: Saw pod success 05/17/23 05:44:13.539
    May 17 05:44:13.539: INFO: Pod "pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5" satisfied condition "Succeeded or Failed"
    May 17 05:44:13.543: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 05:44:13.553
    May 17 05:44:13.563: INFO: Waiting for pod pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5 to disappear
    May 17 05:44:13.567: INFO: Pod pod-configmaps-0995292d-9b23-488b-b339-9808fb5fd7c5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 05:44:13.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7816" for this suite. 05/17/23 05:44:13.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:44:13.581
May 17 05:44:13.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 05:44:13.582
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:13.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:13.605
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 05:44:13.622
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 05:44:13.833
STEP: Deploying the webhook pod 05/17/23 05:44:13.842
STEP: Wait for the deployment to be ready 05/17/23 05:44:13.853
May 17 05:44:13.860: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/17/23 05:44:15.873
STEP: Verifying the service has paired with the endpoint 05/17/23 05:44:15.884
May 17 05:44:16.884: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 05/17/23 05:44:16.889
STEP: Creating a custom resource definition that should be denied by the webhook 05/17/23 05:44:16.909
May 17 05:44:16.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 05:44:16.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-232" for this suite. 05/17/23 05:44:16.938
STEP: Destroying namespace "webhook-232-markers" for this suite. 05/17/23 05:44:16.945
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":21,"skipped":373,"failed":0}
------------------------------
â€¢ [3.405 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:44:13.581
    May 17 05:44:13.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 05:44:13.582
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:13.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:13.605
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 05:44:13.622
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 05:44:13.833
    STEP: Deploying the webhook pod 05/17/23 05:44:13.842
    STEP: Wait for the deployment to be ready 05/17/23 05:44:13.853
    May 17 05:44:13.860: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/17/23 05:44:15.873
    STEP: Verifying the service has paired with the endpoint 05/17/23 05:44:15.884
    May 17 05:44:16.884: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 05/17/23 05:44:16.889
    STEP: Creating a custom resource definition that should be denied by the webhook 05/17/23 05:44:16.909
    May 17 05:44:16.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 05:44:16.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-232" for this suite. 05/17/23 05:44:16.938
    STEP: Destroying namespace "webhook-232-markers" for this suite. 05/17/23 05:44:16.945
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:44:16.987
May 17 05:44:16.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pods 05/17/23 05:44:16.988
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:17.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:17.015
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 05/17/23 05:44:17.033
STEP: watching for Pod to be ready 05/17/23 05:44:17.043
May 17 05:44:17.045: INFO: observed Pod pod-test in namespace pods-1164 in phase Pending with labels: map[test-pod-static:true] & conditions []
May 17 05:44:17.050: INFO: observed Pod pod-test in namespace pods-1164 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC  }]
May 17 05:44:17.063: INFO: observed Pod pod-test in namespace pods-1164 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC  }]
May 17 05:44:18.596: INFO: Found Pod pod-test in namespace pods-1164 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 05/17/23 05:44:18.6
STEP: getting the Pod and ensuring that it's patched 05/17/23 05:44:18.61
STEP: replacing the Pod's status Ready condition to False 05/17/23 05:44:18.614
STEP: check the Pod again to ensure its Ready conditions are False 05/17/23 05:44:18.624
STEP: deleting the Pod via a Collection with a LabelSelector 05/17/23 05:44:18.624
STEP: watching for the Pod to be deleted 05/17/23 05:44:18.634
May 17 05:44:18.636: INFO: observed event type MODIFIED
May 17 05:44:20.602: INFO: observed event type MODIFIED
May 17 05:44:21.601: INFO: observed event type MODIFIED
May 17 05:44:21.609: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May 17 05:44:21.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1164" for this suite. 05/17/23 05:44:21.625
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":22,"skipped":381,"failed":0}
------------------------------
â€¢ [4.645 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:44:16.987
    May 17 05:44:16.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pods 05/17/23 05:44:16.988
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:17.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:17.015
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 05/17/23 05:44:17.033
    STEP: watching for Pod to be ready 05/17/23 05:44:17.043
    May 17 05:44:17.045: INFO: observed Pod pod-test in namespace pods-1164 in phase Pending with labels: map[test-pod-static:true] & conditions []
    May 17 05:44:17.050: INFO: observed Pod pod-test in namespace pods-1164 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC  }]
    May 17 05:44:17.063: INFO: observed Pod pod-test in namespace pods-1164 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC  }]
    May 17 05:44:18.596: INFO: Found Pod pod-test in namespace pods-1164 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 05:44:17 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 05/17/23 05:44:18.6
    STEP: getting the Pod and ensuring that it's patched 05/17/23 05:44:18.61
    STEP: replacing the Pod's status Ready condition to False 05/17/23 05:44:18.614
    STEP: check the Pod again to ensure its Ready conditions are False 05/17/23 05:44:18.624
    STEP: deleting the Pod via a Collection with a LabelSelector 05/17/23 05:44:18.624
    STEP: watching for the Pod to be deleted 05/17/23 05:44:18.634
    May 17 05:44:18.636: INFO: observed event type MODIFIED
    May 17 05:44:20.602: INFO: observed event type MODIFIED
    May 17 05:44:21.601: INFO: observed event type MODIFIED
    May 17 05:44:21.609: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May 17 05:44:21.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1164" for this suite. 05/17/23 05:44:21.625
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:44:21.632
May 17 05:44:21.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename replicaset 05/17/23 05:44:21.633
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:21.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:21.653
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
May 17 05:44:21.657: INFO: Creating ReplicaSet my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802
May 17 05:44:21.667: INFO: Pod name my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802: Found 0 pods out of 1
May 17 05:44:26.672: INFO: Pod name my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802: Found 1 pods out of 1
May 17 05:44:26.672: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802" is running
May 17 05:44:26.672: INFO: Waiting up to 5m0s for pod "my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802-zfp8t" in namespace "replicaset-999" to be "running"
May 17 05:44:26.687: INFO: Pod "my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802-zfp8t": Phase="Running", Reason="", readiness=true. Elapsed: 15.17842ms
May 17 05:44:26.687: INFO: Pod "my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802-zfp8t" satisfied condition "running"
May 17 05:44:26.687: INFO: Pod "my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802-zfp8t" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 05:44:21 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 05:44:22 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 05:44:22 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 05:44:21 +0000 UTC Reason: Message:}])
May 17 05:44:26.687: INFO: Trying to dial the pod
May 17 05:44:31.708: INFO: Controller my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802: Got expected result from replica 1 [my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802-zfp8t]: "my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802-zfp8t", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May 17 05:44:31.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-999" for this suite. 05/17/23 05:44:31.715
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":23,"skipped":382,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.089 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:44:21.632
    May 17 05:44:21.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename replicaset 05/17/23 05:44:21.633
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:21.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:21.653
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    May 17 05:44:21.657: INFO: Creating ReplicaSet my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802
    May 17 05:44:21.667: INFO: Pod name my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802: Found 0 pods out of 1
    May 17 05:44:26.672: INFO: Pod name my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802: Found 1 pods out of 1
    May 17 05:44:26.672: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802" is running
    May 17 05:44:26.672: INFO: Waiting up to 5m0s for pod "my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802-zfp8t" in namespace "replicaset-999" to be "running"
    May 17 05:44:26.687: INFO: Pod "my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802-zfp8t": Phase="Running", Reason="", readiness=true. Elapsed: 15.17842ms
    May 17 05:44:26.687: INFO: Pod "my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802-zfp8t" satisfied condition "running"
    May 17 05:44:26.687: INFO: Pod "my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802-zfp8t" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 05:44:21 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 05:44:22 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 05:44:22 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 05:44:21 +0000 UTC Reason: Message:}])
    May 17 05:44:26.687: INFO: Trying to dial the pod
    May 17 05:44:31.708: INFO: Controller my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802: Got expected result from replica 1 [my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802-zfp8t]: "my-hostname-basic-8553c892-9b3a-4b56-b272-3a9a9ccb6802-zfp8t", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May 17 05:44:31.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-999" for this suite. 05/17/23 05:44:31.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:44:31.722
May 17 05:44:31.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename security-context 05/17/23 05:44:31.723
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:31.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:31.743
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/17/23 05:44:31.746
May 17 05:44:31.756: INFO: Waiting up to 5m0s for pod "security-context-3ae0484d-be29-4b10-8626-bcc46dea259e" in namespace "security-context-2438" to be "Succeeded or Failed"
May 17 05:44:31.760: INFO: Pod "security-context-3ae0484d-be29-4b10-8626-bcc46dea259e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.810777ms
May 17 05:44:33.765: INFO: Pod "security-context-3ae0484d-be29-4b10-8626-bcc46dea259e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009084878s
May 17 05:44:35.765: INFO: Pod "security-context-3ae0484d-be29-4b10-8626-bcc46dea259e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008848285s
STEP: Saw pod success 05/17/23 05:44:35.765
May 17 05:44:35.765: INFO: Pod "security-context-3ae0484d-be29-4b10-8626-bcc46dea259e" satisfied condition "Succeeded or Failed"
May 17 05:44:35.769: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod security-context-3ae0484d-be29-4b10-8626-bcc46dea259e container test-container: <nil>
STEP: delete the pod 05/17/23 05:44:35.779
May 17 05:44:35.789: INFO: Waiting for pod security-context-3ae0484d-be29-4b10-8626-bcc46dea259e to disappear
May 17 05:44:35.793: INFO: Pod security-context-3ae0484d-be29-4b10-8626-bcc46dea259e no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May 17 05:44:35.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2438" for this suite. 05/17/23 05:44:35.8
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":24,"skipped":400,"failed":0}
------------------------------
â€¢ [4.085 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:44:31.722
    May 17 05:44:31.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename security-context 05/17/23 05:44:31.723
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:31.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:31.743
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/17/23 05:44:31.746
    May 17 05:44:31.756: INFO: Waiting up to 5m0s for pod "security-context-3ae0484d-be29-4b10-8626-bcc46dea259e" in namespace "security-context-2438" to be "Succeeded or Failed"
    May 17 05:44:31.760: INFO: Pod "security-context-3ae0484d-be29-4b10-8626-bcc46dea259e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.810777ms
    May 17 05:44:33.765: INFO: Pod "security-context-3ae0484d-be29-4b10-8626-bcc46dea259e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009084878s
    May 17 05:44:35.765: INFO: Pod "security-context-3ae0484d-be29-4b10-8626-bcc46dea259e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008848285s
    STEP: Saw pod success 05/17/23 05:44:35.765
    May 17 05:44:35.765: INFO: Pod "security-context-3ae0484d-be29-4b10-8626-bcc46dea259e" satisfied condition "Succeeded or Failed"
    May 17 05:44:35.769: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod security-context-3ae0484d-be29-4b10-8626-bcc46dea259e container test-container: <nil>
    STEP: delete the pod 05/17/23 05:44:35.779
    May 17 05:44:35.789: INFO: Waiting for pod security-context-3ae0484d-be29-4b10-8626-bcc46dea259e to disappear
    May 17 05:44:35.793: INFO: Pod security-context-3ae0484d-be29-4b10-8626-bcc46dea259e no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May 17 05:44:35.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-2438" for this suite. 05/17/23 05:44:35.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:44:35.808
May 17 05:44:35.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename resourcequota 05/17/23 05:44:35.808
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:35.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:35.834
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 05/17/23 05:44:35.837
STEP: Getting a ResourceQuota 05/17/23 05:44:35.842
STEP: Listing all ResourceQuotas with LabelSelector 05/17/23 05:44:35.847
STEP: Patching the ResourceQuota 05/17/23 05:44:35.85
STEP: Deleting a Collection of ResourceQuotas 05/17/23 05:44:35.858
STEP: Verifying the deleted ResourceQuota 05/17/23 05:44:35.867
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May 17 05:44:35.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-938" for this suite. 05/17/23 05:44:35.877
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":25,"skipped":410,"failed":0}
------------------------------
â€¢ [0.077 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:44:35.808
    May 17 05:44:35.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename resourcequota 05/17/23 05:44:35.808
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:35.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:35.834
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 05/17/23 05:44:35.837
    STEP: Getting a ResourceQuota 05/17/23 05:44:35.842
    STEP: Listing all ResourceQuotas with LabelSelector 05/17/23 05:44:35.847
    STEP: Patching the ResourceQuota 05/17/23 05:44:35.85
    STEP: Deleting a Collection of ResourceQuotas 05/17/23 05:44:35.858
    STEP: Verifying the deleted ResourceQuota 05/17/23 05:44:35.867
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May 17 05:44:35.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-938" for this suite. 05/17/23 05:44:35.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:44:35.886
May 17 05:44:35.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename statefulset 05/17/23 05:44:35.886
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:35.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:35.905
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3673 05/17/23 05:44:35.909
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-3673 05/17/23 05:44:35.92
May 17 05:44:35.933: INFO: Found 0 stateful pods, waiting for 1
May 17 05:44:45.939: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 05/17/23 05:44:45.952
STEP: Getting /status 05/17/23 05:44:45.963
May 17 05:44:45.968: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 05/17/23 05:44:45.968
May 17 05:44:45.988: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 05/17/23 05:44:45.989
May 17 05:44:45.991: INFO: Observed &StatefulSet event: ADDED
May 17 05:44:45.991: INFO: Found Statefulset ss in namespace statefulset-3673 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 17 05:44:45.991: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 05/17/23 05:44:45.991
May 17 05:44:45.991: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 17 05:44:46.009: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 05/17/23 05:44:46.009
May 17 05:44:46.011: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May 17 05:44:46.011: INFO: Deleting all statefulset in ns statefulset-3673
May 17 05:44:46.016: INFO: Scaling statefulset ss to 0
May 17 05:44:56.037: INFO: Waiting for statefulset status.replicas updated to 0
May 17 05:44:56.041: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May 17 05:44:56.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3673" for this suite. 05/17/23 05:44:56.067
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":26,"skipped":421,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.191 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:44:35.886
    May 17 05:44:35.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename statefulset 05/17/23 05:44:35.886
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:35.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:35.905
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3673 05/17/23 05:44:35.909
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-3673 05/17/23 05:44:35.92
    May 17 05:44:35.933: INFO: Found 0 stateful pods, waiting for 1
    May 17 05:44:45.939: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 05/17/23 05:44:45.952
    STEP: Getting /status 05/17/23 05:44:45.963
    May 17 05:44:45.968: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 05/17/23 05:44:45.968
    May 17 05:44:45.988: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 05/17/23 05:44:45.989
    May 17 05:44:45.991: INFO: Observed &StatefulSet event: ADDED
    May 17 05:44:45.991: INFO: Found Statefulset ss in namespace statefulset-3673 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 17 05:44:45.991: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 05/17/23 05:44:45.991
    May 17 05:44:45.991: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May 17 05:44:46.009: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 05/17/23 05:44:46.009
    May 17 05:44:46.011: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May 17 05:44:46.011: INFO: Deleting all statefulset in ns statefulset-3673
    May 17 05:44:46.016: INFO: Scaling statefulset ss to 0
    May 17 05:44:56.037: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 05:44:56.041: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May 17 05:44:56.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3673" for this suite. 05/17/23 05:44:56.067
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:44:56.077
May 17 05:44:56.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-runtime 05/17/23 05:44:56.078
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:56.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:56.098
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 05/17/23 05:44:56.101
STEP: wait for the container to reach Succeeded 05/17/23 05:44:56.111
STEP: get the container status 05/17/23 05:45:00.134
STEP: the container should be terminated 05/17/23 05:45:00.139
STEP: the termination message should be set 05/17/23 05:45:00.139
May 17 05:45:00.139: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 05/17/23 05:45:00.139
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May 17 05:45:00.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5581" for this suite. 05/17/23 05:45:00.169
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":27,"skipped":425,"failed":0}
------------------------------
â€¢ [4.100 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:44:56.077
    May 17 05:44:56.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-runtime 05/17/23 05:44:56.078
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:44:56.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:44:56.098
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 05/17/23 05:44:56.101
    STEP: wait for the container to reach Succeeded 05/17/23 05:44:56.111
    STEP: get the container status 05/17/23 05:45:00.134
    STEP: the container should be terminated 05/17/23 05:45:00.139
    STEP: the termination message should be set 05/17/23 05:45:00.139
    May 17 05:45:00.139: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 05/17/23 05:45:00.139
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May 17 05:45:00.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5581" for this suite. 05/17/23 05:45:00.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:00.178
May 17 05:45:00.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 05:45:00.179
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:00.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:00.202
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 05/17/23 05:45:00.205
May 17 05:45:00.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59" in namespace "downward-api-826" to be "Succeeded or Failed"
May 17 05:45:00.221: INFO: Pod "downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506854ms
May 17 05:45:02.226: INFO: Pod "downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008802576s
May 17 05:45:04.226: INFO: Pod "downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008282423s
STEP: Saw pod success 05/17/23 05:45:04.226
May 17 05:45:04.226: INFO: Pod "downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59" satisfied condition "Succeeded or Failed"
May 17 05:45:04.231: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59 container client-container: <nil>
STEP: delete the pod 05/17/23 05:45:04.24
May 17 05:45:04.254: INFO: Waiting for pod downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59 to disappear
May 17 05:45:04.257: INFO: Pod downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May 17 05:45:04.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-826" for this suite. 05/17/23 05:45:04.264
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":28,"skipped":430,"failed":0}
------------------------------
â€¢ [4.093 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:00.178
    May 17 05:45:00.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 05:45:00.179
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:00.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:00.202
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 05/17/23 05:45:00.205
    May 17 05:45:00.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59" in namespace "downward-api-826" to be "Succeeded or Failed"
    May 17 05:45:00.221: INFO: Pod "downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506854ms
    May 17 05:45:02.226: INFO: Pod "downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008802576s
    May 17 05:45:04.226: INFO: Pod "downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008282423s
    STEP: Saw pod success 05/17/23 05:45:04.226
    May 17 05:45:04.226: INFO: Pod "downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59" satisfied condition "Succeeded or Failed"
    May 17 05:45:04.231: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59 container client-container: <nil>
    STEP: delete the pod 05/17/23 05:45:04.24
    May 17 05:45:04.254: INFO: Waiting for pod downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59 to disappear
    May 17 05:45:04.257: INFO: Pod downwardapi-volume-81313ec1-0769-4fa2-b55f-2bb9e993ba59 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May 17 05:45:04.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-826" for this suite. 05/17/23 05:45:04.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:04.271
May 17 05:45:04.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 05:45:04.272
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:04.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:04.291
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 05/17/23 05:45:04.294
May 17 05:45:04.304: INFO: Waiting up to 5m0s for pod "downward-api-246534da-6d06-41ce-9187-1e6bfee6b854" in namespace "downward-api-7641" to be "Succeeded or Failed"
May 17 05:45:04.307: INFO: Pod "downward-api-246534da-6d06-41ce-9187-1e6bfee6b854": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38077ms
May 17 05:45:06.313: INFO: Pod "downward-api-246534da-6d06-41ce-9187-1e6bfee6b854": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008843732s
May 17 05:45:08.313: INFO: Pod "downward-api-246534da-6d06-41ce-9187-1e6bfee6b854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009082808s
STEP: Saw pod success 05/17/23 05:45:08.313
May 17 05:45:08.313: INFO: Pod "downward-api-246534da-6d06-41ce-9187-1e6bfee6b854" satisfied condition "Succeeded or Failed"
May 17 05:45:08.317: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downward-api-246534da-6d06-41ce-9187-1e6bfee6b854 container dapi-container: <nil>
STEP: delete the pod 05/17/23 05:45:08.335
May 17 05:45:08.350: INFO: Waiting for pod downward-api-246534da-6d06-41ce-9187-1e6bfee6b854 to disappear
May 17 05:45:08.354: INFO: Pod downward-api-246534da-6d06-41ce-9187-1e6bfee6b854 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May 17 05:45:08.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7641" for this suite. 05/17/23 05:45:08.361
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":29,"skipped":443,"failed":0}
------------------------------
â€¢ [4.098 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:04.271
    May 17 05:45:04.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 05:45:04.272
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:04.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:04.291
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 05/17/23 05:45:04.294
    May 17 05:45:04.304: INFO: Waiting up to 5m0s for pod "downward-api-246534da-6d06-41ce-9187-1e6bfee6b854" in namespace "downward-api-7641" to be "Succeeded or Failed"
    May 17 05:45:04.307: INFO: Pod "downward-api-246534da-6d06-41ce-9187-1e6bfee6b854": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38077ms
    May 17 05:45:06.313: INFO: Pod "downward-api-246534da-6d06-41ce-9187-1e6bfee6b854": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008843732s
    May 17 05:45:08.313: INFO: Pod "downward-api-246534da-6d06-41ce-9187-1e6bfee6b854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009082808s
    STEP: Saw pod success 05/17/23 05:45:08.313
    May 17 05:45:08.313: INFO: Pod "downward-api-246534da-6d06-41ce-9187-1e6bfee6b854" satisfied condition "Succeeded or Failed"
    May 17 05:45:08.317: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downward-api-246534da-6d06-41ce-9187-1e6bfee6b854 container dapi-container: <nil>
    STEP: delete the pod 05/17/23 05:45:08.335
    May 17 05:45:08.350: INFO: Waiting for pod downward-api-246534da-6d06-41ce-9187-1e6bfee6b854 to disappear
    May 17 05:45:08.354: INFO: Pod downward-api-246534da-6d06-41ce-9187-1e6bfee6b854 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May 17 05:45:08.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7641" for this suite. 05/17/23 05:45:08.361
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:08.369
May 17 05:45:08.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename dns 05/17/23 05:45:08.37
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:08.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:08.389
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 05/17/23 05:45:08.393
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2776.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2776.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 05/17/23 05:45:08.399
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2776.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2776.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 05/17/23 05:45:08.399
STEP: creating a pod to probe DNS 05/17/23 05:45:08.399
STEP: submitting the pod to kubernetes 05/17/23 05:45:08.399
May 17 05:45:08.412: INFO: Waiting up to 15m0s for pod "dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe" in namespace "dns-2776" to be "running"
May 17 05:45:08.416: INFO: Pod "dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.855758ms
May 17 05:45:10.420: INFO: Pod "dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008382364s
May 17 05:45:12.423: INFO: Pod "dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010597458s
May 17 05:45:14.421: INFO: Pod "dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe": Phase="Running", Reason="", readiness=true. Elapsed: 6.009265134s
May 17 05:45:14.421: INFO: Pod "dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe" satisfied condition "running"
STEP: retrieving the pod 05/17/23 05:45:14.421
STEP: looking for the results for each expected name from probers 05/17/23 05:45:14.425
May 17 05:45:14.456: INFO: DNS probes using dns-2776/dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe succeeded

STEP: deleting the pod 05/17/23 05:45:14.456
STEP: deleting the test headless service 05/17/23 05:45:14.469
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May 17 05:45:14.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2776" for this suite. 05/17/23 05:45:14.488
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":30,"skipped":443,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.126 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:08.369
    May 17 05:45:08.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename dns 05/17/23 05:45:08.37
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:08.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:08.389
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 05/17/23 05:45:08.393
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2776.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2776.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     05/17/23 05:45:08.399
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2776.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2776.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     05/17/23 05:45:08.399
    STEP: creating a pod to probe DNS 05/17/23 05:45:08.399
    STEP: submitting the pod to kubernetes 05/17/23 05:45:08.399
    May 17 05:45:08.412: INFO: Waiting up to 15m0s for pod "dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe" in namespace "dns-2776" to be "running"
    May 17 05:45:08.416: INFO: Pod "dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.855758ms
    May 17 05:45:10.420: INFO: Pod "dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008382364s
    May 17 05:45:12.423: INFO: Pod "dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010597458s
    May 17 05:45:14.421: INFO: Pod "dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe": Phase="Running", Reason="", readiness=true. Elapsed: 6.009265134s
    May 17 05:45:14.421: INFO: Pod "dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 05:45:14.421
    STEP: looking for the results for each expected name from probers 05/17/23 05:45:14.425
    May 17 05:45:14.456: INFO: DNS probes using dns-2776/dns-test-cedfaa8d-5104-4ff6-942a-b69f6786ddbe succeeded

    STEP: deleting the pod 05/17/23 05:45:14.456
    STEP: deleting the test headless service 05/17/23 05:45:14.469
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May 17 05:45:14.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2776" for this suite. 05/17/23 05:45:14.488
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:14.495
May 17 05:45:14.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 05:45:14.496
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:14.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:14.512
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-3454/secret-test-2ce1218f-5192-4197-bc7b-e7b31f3ba56c 05/17/23 05:45:14.515
STEP: Creating a pod to test consume secrets 05/17/23 05:45:14.52
May 17 05:45:14.532: INFO: Waiting up to 5m0s for pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f" in namespace "secrets-3454" to be "Succeeded or Failed"
May 17 05:45:14.535: INFO: Pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.542993ms
May 17 05:45:16.542: INFO: Pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009653599s
May 17 05:45:18.540: INFO: Pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f": Phase="Running", Reason="", readiness=true. Elapsed: 4.008163217s
May 17 05:45:20.541: INFO: Pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f": Phase="Running", Reason="", readiness=false. Elapsed: 6.009045624s
May 17 05:45:22.541: INFO: Pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009179869s
STEP: Saw pod success 05/17/23 05:45:22.541
May 17 05:45:22.541: INFO: Pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f" satisfied condition "Succeeded or Failed"
May 17 05:45:22.545: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f container env-test: <nil>
STEP: delete the pod 05/17/23 05:45:22.556
May 17 05:45:22.568: INFO: Waiting for pod pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f to disappear
May 17 05:45:22.571: INFO: Pod pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May 17 05:45:22.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3454" for this suite. 05/17/23 05:45:22.578
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":31,"skipped":444,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.089 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:14.495
    May 17 05:45:14.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 05:45:14.496
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:14.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:14.512
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-3454/secret-test-2ce1218f-5192-4197-bc7b-e7b31f3ba56c 05/17/23 05:45:14.515
    STEP: Creating a pod to test consume secrets 05/17/23 05:45:14.52
    May 17 05:45:14.532: INFO: Waiting up to 5m0s for pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f" in namespace "secrets-3454" to be "Succeeded or Failed"
    May 17 05:45:14.535: INFO: Pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.542993ms
    May 17 05:45:16.542: INFO: Pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009653599s
    May 17 05:45:18.540: INFO: Pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f": Phase="Running", Reason="", readiness=true. Elapsed: 4.008163217s
    May 17 05:45:20.541: INFO: Pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f": Phase="Running", Reason="", readiness=false. Elapsed: 6.009045624s
    May 17 05:45:22.541: INFO: Pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009179869s
    STEP: Saw pod success 05/17/23 05:45:22.541
    May 17 05:45:22.541: INFO: Pod "pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f" satisfied condition "Succeeded or Failed"
    May 17 05:45:22.545: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f container env-test: <nil>
    STEP: delete the pod 05/17/23 05:45:22.556
    May 17 05:45:22.568: INFO: Waiting for pod pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f to disappear
    May 17 05:45:22.571: INFO: Pod pod-configmaps-b8974a71-cce5-4526-bbe4-b7c580e40b8f no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May 17 05:45:22.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3454" for this suite. 05/17/23 05:45:22.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:22.586
May 17 05:45:22.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubelet-test 05/17/23 05:45:22.586
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:22.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:22.601
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 05/17/23 05:45:22.62
May 17 05:45:22.620: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesbbc5d05e-ee65-4b6e-92bf-0726b9e5e232" in namespace "kubelet-test-7591" to be "completed"
May 17 05:45:22.624: INFO: Pod "agnhost-host-aliasesbbc5d05e-ee65-4b6e-92bf-0726b9e5e232": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075709ms
May 17 05:45:24.629: INFO: Pod "agnhost-host-aliasesbbc5d05e-ee65-4b6e-92bf-0726b9e5e232": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008934683s
May 17 05:45:26.631: INFO: Pod "agnhost-host-aliasesbbc5d05e-ee65-4b6e-92bf-0726b9e5e232": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011149493s
May 17 05:45:26.631: INFO: Pod "agnhost-host-aliasesbbc5d05e-ee65-4b6e-92bf-0726b9e5e232" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May 17 05:45:26.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7591" for this suite. 05/17/23 05:45:26.648
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":32,"skipped":459,"failed":0}
------------------------------
â€¢ [4.070 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:22.586
    May 17 05:45:22.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubelet-test 05/17/23 05:45:22.586
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:22.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:22.601
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 05/17/23 05:45:22.62
    May 17 05:45:22.620: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesbbc5d05e-ee65-4b6e-92bf-0726b9e5e232" in namespace "kubelet-test-7591" to be "completed"
    May 17 05:45:22.624: INFO: Pod "agnhost-host-aliasesbbc5d05e-ee65-4b6e-92bf-0726b9e5e232": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075709ms
    May 17 05:45:24.629: INFO: Pod "agnhost-host-aliasesbbc5d05e-ee65-4b6e-92bf-0726b9e5e232": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008934683s
    May 17 05:45:26.631: INFO: Pod "agnhost-host-aliasesbbc5d05e-ee65-4b6e-92bf-0726b9e5e232": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011149493s
    May 17 05:45:26.631: INFO: Pod "agnhost-host-aliasesbbc5d05e-ee65-4b6e-92bf-0726b9e5e232" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May 17 05:45:26.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7591" for this suite. 05/17/23 05:45:26.648
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:26.655
May 17 05:45:26.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 05:45:26.656
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:26.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:26.676
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-ad3c8ec0-c380-4c21-9963-fdb72ba000b3 05/17/23 05:45:26.679
STEP: Creating a pod to test consume secrets 05/17/23 05:45:26.686
May 17 05:45:26.697: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f" in namespace "projected-7877" to be "Succeeded or Failed"
May 17 05:45:26.701: INFO: Pod "pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.878008ms
May 17 05:45:28.705: INFO: Pod "pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00837588s
May 17 05:45:30.706: INFO: Pod "pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008899715s
STEP: Saw pod success 05/17/23 05:45:30.706
May 17 05:45:30.706: INFO: Pod "pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f" satisfied condition "Succeeded or Failed"
May 17 05:45:30.710: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f container projected-secret-volume-test: <nil>
STEP: delete the pod 05/17/23 05:45:30.72
May 17 05:45:30.732: INFO: Waiting for pod pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f to disappear
May 17 05:45:30.737: INFO: Pod pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May 17 05:45:30.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7877" for this suite. 05/17/23 05:45:30.743
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":33,"skipped":461,"failed":0}
------------------------------
â€¢ [4.096 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:26.655
    May 17 05:45:26.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 05:45:26.656
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:26.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:26.676
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-ad3c8ec0-c380-4c21-9963-fdb72ba000b3 05/17/23 05:45:26.679
    STEP: Creating a pod to test consume secrets 05/17/23 05:45:26.686
    May 17 05:45:26.697: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f" in namespace "projected-7877" to be "Succeeded or Failed"
    May 17 05:45:26.701: INFO: Pod "pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.878008ms
    May 17 05:45:28.705: INFO: Pod "pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00837588s
    May 17 05:45:30.706: INFO: Pod "pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008899715s
    STEP: Saw pod success 05/17/23 05:45:30.706
    May 17 05:45:30.706: INFO: Pod "pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f" satisfied condition "Succeeded or Failed"
    May 17 05:45:30.710: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 05:45:30.72
    May 17 05:45:30.732: INFO: Waiting for pod pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f to disappear
    May 17 05:45:30.737: INFO: Pod pod-projected-secrets-65c156b0-197e-4d15-a8b5-97b91cca021f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May 17 05:45:30.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7877" for this suite. 05/17/23 05:45:30.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:30.753
May 17 05:45:30.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 05:45:30.754
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:30.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:30.776
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-a7a35662-d1bb-4eac-9535-484eccda27c8 05/17/23 05:45:30.78
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May 17 05:45:30.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-734" for this suite. 05/17/23 05:45:30.791
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":34,"skipped":503,"failed":0}
------------------------------
â€¢ [0.045 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:30.753
    May 17 05:45:30.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 05:45:30.754
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:30.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:30.776
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-a7a35662-d1bb-4eac-9535-484eccda27c8 05/17/23 05:45:30.78
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May 17 05:45:30.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-734" for this suite. 05/17/23 05:45:30.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:30.798
May 17 05:45:30.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 05:45:30.799
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:30.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:30.82
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-315ab3d8-8a21-46e1-98cc-76c107736dd5 05/17/23 05:45:30.824
STEP: Creating a pod to test consume secrets 05/17/23 05:45:30.831
May 17 05:45:30.845: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8" in namespace "projected-4269" to be "Succeeded or Failed"
May 17 05:45:30.854: INFO: Pod "pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.433433ms
May 17 05:45:32.858: INFO: Pod "pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013110278s
May 17 05:45:34.859: INFO: Pod "pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013628271s
STEP: Saw pod success 05/17/23 05:45:34.859
May 17 05:45:34.859: INFO: Pod "pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8" satisfied condition "Succeeded or Failed"
May 17 05:45:34.863: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/17/23 05:45:34.88
May 17 05:45:34.892: INFO: Waiting for pod pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8 to disappear
May 17 05:45:34.895: INFO: Pod pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May 17 05:45:34.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4269" for this suite. 05/17/23 05:45:34.902
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":35,"skipped":512,"failed":0}
------------------------------
â€¢ [4.111 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:30.798
    May 17 05:45:30.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 05:45:30.799
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:30.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:30.82
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-315ab3d8-8a21-46e1-98cc-76c107736dd5 05/17/23 05:45:30.824
    STEP: Creating a pod to test consume secrets 05/17/23 05:45:30.831
    May 17 05:45:30.845: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8" in namespace "projected-4269" to be "Succeeded or Failed"
    May 17 05:45:30.854: INFO: Pod "pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.433433ms
    May 17 05:45:32.858: INFO: Pod "pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013110278s
    May 17 05:45:34.859: INFO: Pod "pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013628271s
    STEP: Saw pod success 05/17/23 05:45:34.859
    May 17 05:45:34.859: INFO: Pod "pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8" satisfied condition "Succeeded or Failed"
    May 17 05:45:34.863: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 05:45:34.88
    May 17 05:45:34.892: INFO: Waiting for pod pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8 to disappear
    May 17 05:45:34.895: INFO: Pod pod-projected-secrets-af6cf1d8-5f40-4c47-bf0c-4500bb0485a8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May 17 05:45:34.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4269" for this suite. 05/17/23 05:45:34.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:34.91
May 17 05:45:34.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename proxy 05/17/23 05:45:34.911
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:34.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:34.93
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
May 17 05:45:34.933: INFO: Creating pod...
May 17 05:45:34.945: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1328" to be "running"
May 17 05:45:34.948: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.440811ms
May 17 05:45:36.954: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008950448s
May 17 05:45:36.954: INFO: Pod "agnhost" satisfied condition "running"
May 17 05:45:36.954: INFO: Creating service...
May 17 05:45:36.964: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=DELETE
May 17 05:45:36.973: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 17 05:45:36.973: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=OPTIONS
May 17 05:45:36.979: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 17 05:45:36.979: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=PATCH
May 17 05:45:36.984: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 17 05:45:36.984: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=POST
May 17 05:45:36.990: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 17 05:45:36.990: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=PUT
May 17 05:45:36.995: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 17 05:45:36.995: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=DELETE
May 17 05:45:37.004: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 17 05:45:37.004: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=OPTIONS
May 17 05:45:37.011: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 17 05:45:37.011: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=PATCH
May 17 05:45:37.018: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 17 05:45:37.018: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=POST
May 17 05:45:37.026: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 17 05:45:37.026: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=PUT
May 17 05:45:37.033: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 17 05:45:37.033: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=GET
May 17 05:45:37.038: INFO: http.Client request:GET StatusCode:301
May 17 05:45:37.038: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=GET
May 17 05:45:37.043: INFO: http.Client request:GET StatusCode:301
May 17 05:45:37.043: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=HEAD
May 17 05:45:37.047: INFO: http.Client request:HEAD StatusCode:301
May 17 05:45:37.047: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=HEAD
May 17 05:45:37.053: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
May 17 05:45:37.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1328" for this suite. 05/17/23 05:45:37.059
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":36,"skipped":533,"failed":0}
------------------------------
â€¢ [2.157 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:34.91
    May 17 05:45:34.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename proxy 05/17/23 05:45:34.911
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:34.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:34.93
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    May 17 05:45:34.933: INFO: Creating pod...
    May 17 05:45:34.945: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1328" to be "running"
    May 17 05:45:34.948: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.440811ms
    May 17 05:45:36.954: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008950448s
    May 17 05:45:36.954: INFO: Pod "agnhost" satisfied condition "running"
    May 17 05:45:36.954: INFO: Creating service...
    May 17 05:45:36.964: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=DELETE
    May 17 05:45:36.973: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May 17 05:45:36.973: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=OPTIONS
    May 17 05:45:36.979: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May 17 05:45:36.979: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=PATCH
    May 17 05:45:36.984: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May 17 05:45:36.984: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=POST
    May 17 05:45:36.990: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May 17 05:45:36.990: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=PUT
    May 17 05:45:36.995: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May 17 05:45:36.995: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=DELETE
    May 17 05:45:37.004: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May 17 05:45:37.004: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=OPTIONS
    May 17 05:45:37.011: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May 17 05:45:37.011: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=PATCH
    May 17 05:45:37.018: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May 17 05:45:37.018: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=POST
    May 17 05:45:37.026: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May 17 05:45:37.026: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=PUT
    May 17 05:45:37.033: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May 17 05:45:37.033: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=GET
    May 17 05:45:37.038: INFO: http.Client request:GET StatusCode:301
    May 17 05:45:37.038: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=GET
    May 17 05:45:37.043: INFO: http.Client request:GET StatusCode:301
    May 17 05:45:37.043: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/pods/agnhost/proxy?method=HEAD
    May 17 05:45:37.047: INFO: http.Client request:HEAD StatusCode:301
    May 17 05:45:37.047: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-1328/services/e2e-proxy-test-service/proxy?method=HEAD
    May 17 05:45:37.053: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    May 17 05:45:37.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-1328" for this suite. 05/17/23 05:45:37.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:37.068
May 17 05:45:37.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 05:45:37.069
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:37.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:37.091
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-a92e7589-14e3-4afd-b88a-1363799e2a0f 05/17/23 05:45:37.094
STEP: Creating a pod to test consume secrets 05/17/23 05:45:37.098
May 17 05:45:37.108: INFO: Waiting up to 5m0s for pod "pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31" in namespace "secrets-9121" to be "Succeeded or Failed"
May 17 05:45:37.111: INFO: Pod "pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31": Phase="Pending", Reason="", readiness=false. Elapsed: 3.374966ms
May 17 05:45:39.116: INFO: Pod "pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008510197s
May 17 05:45:41.116: INFO: Pod "pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008499626s
STEP: Saw pod success 05/17/23 05:45:41.116
May 17 05:45:41.117: INFO: Pod "pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31" satisfied condition "Succeeded or Failed"
May 17 05:45:41.121: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31 container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 05:45:41.131
May 17 05:45:41.145: INFO: Waiting for pod pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31 to disappear
May 17 05:45:41.149: INFO: Pod pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May 17 05:45:41.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9121" for this suite. 05/17/23 05:45:41.156
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":37,"skipped":551,"failed":0}
------------------------------
â€¢ [4.094 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:37.068
    May 17 05:45:37.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 05:45:37.069
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:37.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:37.091
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-a92e7589-14e3-4afd-b88a-1363799e2a0f 05/17/23 05:45:37.094
    STEP: Creating a pod to test consume secrets 05/17/23 05:45:37.098
    May 17 05:45:37.108: INFO: Waiting up to 5m0s for pod "pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31" in namespace "secrets-9121" to be "Succeeded or Failed"
    May 17 05:45:37.111: INFO: Pod "pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31": Phase="Pending", Reason="", readiness=false. Elapsed: 3.374966ms
    May 17 05:45:39.116: INFO: Pod "pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008510197s
    May 17 05:45:41.116: INFO: Pod "pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008499626s
    STEP: Saw pod success 05/17/23 05:45:41.116
    May 17 05:45:41.117: INFO: Pod "pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31" satisfied condition "Succeeded or Failed"
    May 17 05:45:41.121: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31 container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 05:45:41.131
    May 17 05:45:41.145: INFO: Waiting for pod pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31 to disappear
    May 17 05:45:41.149: INFO: Pod pod-secrets-c3cb3ae3-b5ed-4946-89a3-c68181341f31 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May 17 05:45:41.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9121" for this suite. 05/17/23 05:45:41.156
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:41.163
May 17 05:45:41.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename sched-pred 05/17/23 05:45:41.164
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:41.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:41.186
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May 17 05:45:41.189: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 05:45:41.202: INFO: Waiting for terminating namespaces to be deleted...
May 17 05:45:41.206: INFO: 
Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000b before test
May 17 05:45:41.224: INFO: cdi-apiserver-656bdddf7b-t4q89 from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container cdi-apiserver ready: true, restart count 0
May 17 05:45:41.224: INFO: cdi-deployment-59b94f4bcf-6b8js from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container cdi-controller ready: true, restart count 0
May 17 05:45:41.224: INFO: cdi-operator-7f58c444b8-cnf5v from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container cdi-operator ready: true, restart count 0
May 17 05:45:41.224: INFO: cdi-uploadproxy-7787fffb9b-g7dzg from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
May 17 05:45:41.224: INFO: cert-manager-96f7799d6-csgfn from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container cert-manager-controller ready: true, restart count 0
May 17 05:45:41.224: INFO: cert-manager-cainjector-6f79489644-w5ggx from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
May 17 05:45:41.224: INFO: cert-manager-webhook-7dfbf7bff6-zk657 from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container cert-manager-webhook ready: true, restart count 0
May 17 05:45:41.224: INFO: cluster-management-agent-lite-758b6c7957-74gwr from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container cluster-management-agent-lite ready: true, restart count 0
May 17 05:45:41.224: INFO: palette-lite-controller-manager-6cd9fb7bd-h4j4p from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container manager ready: true, restart count 0
May 17 05:45:41.224: INFO: terraform-controller-848679f679-6zmxw from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container terraform-controller ready: true, restart count 0
May 17 05:45:41.224: INFO: ama-logs-87zkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (2 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container ama-logs ready: true, restart count 0
May 17 05:45:41.224: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
May 17 05:45:41.224: INFO: ama-logs-rs-68c99469fb-bzwjp from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container ama-logs ready: true, restart count 0
May 17 05:45:41.224: INFO: azure-ip-masq-agent-928nn from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 17 05:45:41.224: INFO: cloud-node-manager-x7fjk from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container cloud-node-manager ready: true, restart count 0
May 17 05:45:41.224: INFO: coredns-75bbfcbc66-mkbkm from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container coredns ready: true, restart count 0
May 17 05:45:41.224: INFO: coredns-75bbfcbc66-vjv2g from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container coredns ready: true, restart count 0
May 17 05:45:41.224: INFO: coredns-autoscaler-7879846967-wvdn8 from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container autoscaler ready: true, restart count 0
May 17 05:45:41.224: INFO: csi-azuredisk-node-pccm7 from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container azuredisk ready: true, restart count 0
May 17 05:45:41.224: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 05:45:41.224: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 05:45:41.224: INFO: csi-azurefile-node-bzf7d from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container azurefile ready: true, restart count 0
May 17 05:45:41.224: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 05:45:41.224: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 05:45:41.224: INFO: konnectivity-agent-6859749db4-r746q from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container konnectivity-agent ready: true, restart count 0
May 17 05:45:41.224: INFO: konnectivity-agent-6859749db4-vbj9c from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container konnectivity-agent ready: true, restart count 0
May 17 05:45:41.224: INFO: kube-proxy-mzbkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 05:45:41.224: INFO: metrics-server-85c9977f87-k2zzk from kube-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container metrics-server ready: true, restart count 0
May 17 05:45:41.224: INFO: 	Container metrics-server-vpa ready: true, restart count 0
May 17 05:45:41.224: INFO: metrics-server-85c9977f87-slkjl from kube-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container metrics-server ready: true, restart count 0
May 17 05:45:41.224: INFO: 	Container metrics-server-vpa ready: true, restart count 0
May 17 05:45:41.224: INFO: virt-api-6c4f849c9d-2jwc2 from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container virt-api ready: true, restart count 0
May 17 05:45:41.224: INFO: virt-api-6c4f849c9d-2x4kv from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container virt-api ready: true, restart count 0
May 17 05:45:41.224: INFO: virt-controller-67b95d99d5-2cnzp from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container virt-controller ready: true, restart count 0
May 17 05:45:41.224: INFO: virt-controller-67b95d99d5-8wv5s from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container virt-controller ready: true, restart count 0
May 17 05:45:41.224: INFO: virt-handler-hnwhl from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container virt-handler ready: true, restart count 0
May 17 05:45:41.224: INFO: virt-operator-798f64bdf6-gx4sg from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container virt-operator ready: true, restart count 0
May 17 05:45:41.224: INFO: virt-operator-798f64bdf6-x5z2b from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container virt-operator ready: true, restart count 0
May 17 05:45:41.224: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-5jttt from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 05:45:41.224: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 05:45:41.224: INFO: k8s-triliovault-admission-webhook-77b46b96cd-9lksn from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container triliovault-admission-webhook ready: true, restart count 0
May 17 05:45:41.224: INFO: k8s-triliovault-control-plane-6fcbd76b99-8kgx9 from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container triliovault-analyzer ready: true, restart count 0
May 17 05:45:41.224: INFO: 	Container triliovault-control-plane ready: true, restart count 0
May 17 05:45:41.224: INFO: k8s-triliovault-exporter-59c8c8dcdc-kwbxw from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container triliovault-exporter ready: true, restart count 0
May 17 05:45:41.224: INFO: k8s-triliovault-ingress-nginx-controller-76b7885d96-rr66q from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container controller ready: true, restart count 0
May 17 05:45:41.224: INFO: k8s-triliovault-operator-66747bd477-pmljl from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container k8s-triliovault-operator ready: true, restart count 0
May 17 05:45:41.224: INFO: k8s-triliovault-web-75bdc67d7d-l8t7z from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container triliovault-web ready: true, restart count 0
May 17 05:45:41.224: INFO: k8s-triliovault-web-backend-54b448979f-98rfz from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.224: INFO: 	Container triliovault-web-backend ready: true, restart count 0
May 17 05:45:41.224: INFO: 
Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000c before test
May 17 05:45:41.236: INFO: virt-launcher-cirros-vm-n6f7b from default started at 2023-05-17 04:35:13 +0000 UTC (2 container statuses recorded)
May 17 05:45:41.236: INFO: 	Container compute ready: true, restart count 0
May 17 05:45:41.236: INFO: 	Container volumecontainerdisk ready: true, restart count 0
May 17 05:45:41.236: INFO: ama-logs-xntrk from kube-system started at 2023-05-17 04:32:30 +0000 UTC (2 container statuses recorded)
May 17 05:45:41.236: INFO: 	Container ama-logs ready: true, restart count 0
May 17 05:45:41.236: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
May 17 05:45:41.236: INFO: azure-ip-masq-agent-ns2hc from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.236: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 17 05:45:41.236: INFO: cloud-node-manager-hhrc6 from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.236: INFO: 	Container cloud-node-manager ready: true, restart count 0
May 17 05:45:41.236: INFO: csi-azuredisk-node-dms8v from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
May 17 05:45:41.236: INFO: 	Container azuredisk ready: true, restart count 0
May 17 05:45:41.236: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 05:45:41.236: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 05:45:41.236: INFO: csi-azurefile-node-whbbj from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
May 17 05:45:41.236: INFO: 	Container azurefile ready: true, restart count 0
May 17 05:45:41.236: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 05:45:41.236: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 05:45:41.236: INFO: kube-proxy-lpnpx from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.236: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 05:45:41.236: INFO: virt-handler-bzx8t from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.236: INFO: 	Container virt-handler ready: true, restart count 0
May 17 05:45:41.236: INFO: agnhost from proxy-1328 started at 2023-05-17 05:45:34 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.236: INFO: 	Container agnhost ready: true, restart count 0
May 17 05:45:41.236: INFO: sonobuoy from sonobuoy started at 2023-05-17 05:36:06 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.236: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 05:45:41.236: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-6hdss from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 05:45:41.236: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 05:45:41.236: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 05:45:41.236: INFO: k8s-triliovault-resource-cleaner-28071690-kxt5w from trilio-system started at 2023-05-17 05:30:00 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.236: INFO: 	Container resource-cleaner ready: false, restart count 0
May 17 05:45:41.236: INFO: 
Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000d before test
May 17 05:45:41.250: INFO: ama-logs-bmxmm from kube-system started at 2023-05-17 04:32:24 +0000 UTC (2 container statuses recorded)
May 17 05:45:41.250: INFO: 	Container ama-logs ready: true, restart count 0
May 17 05:45:41.250: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
May 17 05:45:41.250: INFO: azure-ip-masq-agent-vm9n2 from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.250: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 17 05:45:41.250: INFO: cloud-node-manager-6mv9z from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.250: INFO: 	Container cloud-node-manager ready: true, restart count 0
May 17 05:45:41.250: INFO: csi-azuredisk-node-rg26q from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
May 17 05:45:41.250: INFO: 	Container azuredisk ready: true, restart count 0
May 17 05:45:41.250: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 05:45:41.250: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 05:45:41.250: INFO: csi-azurefile-node-zlpwj from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
May 17 05:45:41.250: INFO: 	Container azurefile ready: true, restart count 0
May 17 05:45:41.250: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 05:45:41.250: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 05:45:41.250: INFO: kube-proxy-nsx8v from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.250: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 05:45:41.250: INFO: virt-handler-9qq9z from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.250: INFO: 	Container virt-handler ready: true, restart count 0
May 17 05:45:41.250: INFO: mysql-qa-64f5b55869-877ll from mysql started at 2023-05-17 04:33:14 +0000 UTC (1 container statuses recorded)
May 17 05:45:41.250: INFO: 	Container mysql-qa ready: true, restart count 0
May 17 05:45:41.250: INFO: sonobuoy-e2e-job-a0b627fa1b44421a from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 05:45:41.250: INFO: 	Container e2e ready: true, restart count 0
May 17 05:45:41.250: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 05:45:41.250: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-tnssv from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 05:45:41.250: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 05:45:41.250: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/17/23 05:45:41.25
May 17 05:45:41.261: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6631" to be "running"
May 17 05:45:41.264: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.672346ms
May 17 05:45:43.269: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008870137s
May 17 05:45:43.269: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/17/23 05:45:43.273
STEP: Trying to apply a random label on the found node. 05/17/23 05:45:43.284
STEP: verifying the node has the label kubernetes.io/e2e-95ca2a67-82de-4a8f-b6b8-7428fc275c11 42 05/17/23 05:45:43.316
STEP: Trying to relaunch the pod, now with labels. 05/17/23 05:45:43.32
May 17 05:45:43.328: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6631" to be "not pending"
May 17 05:45:43.331: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.631525ms
May 17 05:45:45.336: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008690924s
May 17 05:45:45.336: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-95ca2a67-82de-4a8f-b6b8-7428fc275c11 off the node aks-agentpool-72615086-vmss00000c 05/17/23 05:45:45.34
STEP: verifying the node doesn't have the label kubernetes.io/e2e-95ca2a67-82de-4a8f-b6b8-7428fc275c11 05/17/23 05:45:45.371
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May 17 05:45:45.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6631" for this suite. 05/17/23 05:45:45.381
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":38,"skipped":552,"failed":0}
------------------------------
â€¢ [4.225 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:41.163
    May 17 05:45:41.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename sched-pred 05/17/23 05:45:41.164
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:41.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:41.186
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May 17 05:45:41.189: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May 17 05:45:41.202: INFO: Waiting for terminating namespaces to be deleted...
    May 17 05:45:41.206: INFO: 
    Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000b before test
    May 17 05:45:41.224: INFO: cdi-apiserver-656bdddf7b-t4q89 from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container cdi-apiserver ready: true, restart count 0
    May 17 05:45:41.224: INFO: cdi-deployment-59b94f4bcf-6b8js from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container cdi-controller ready: true, restart count 0
    May 17 05:45:41.224: INFO: cdi-operator-7f58c444b8-cnf5v from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container cdi-operator ready: true, restart count 0
    May 17 05:45:41.224: INFO: cdi-uploadproxy-7787fffb9b-g7dzg from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
    May 17 05:45:41.224: INFO: cert-manager-96f7799d6-csgfn from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container cert-manager-controller ready: true, restart count 0
    May 17 05:45:41.224: INFO: cert-manager-cainjector-6f79489644-w5ggx from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
    May 17 05:45:41.224: INFO: cert-manager-webhook-7dfbf7bff6-zk657 from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container cert-manager-webhook ready: true, restart count 0
    May 17 05:45:41.224: INFO: cluster-management-agent-lite-758b6c7957-74gwr from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container cluster-management-agent-lite ready: true, restart count 0
    May 17 05:45:41.224: INFO: palette-lite-controller-manager-6cd9fb7bd-h4j4p from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container manager ready: true, restart count 0
    May 17 05:45:41.224: INFO: terraform-controller-848679f679-6zmxw from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container terraform-controller ready: true, restart count 0
    May 17 05:45:41.224: INFO: ama-logs-87zkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (2 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 05:45:41.224: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
    May 17 05:45:41.224: INFO: ama-logs-rs-68c99469fb-bzwjp from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 05:45:41.224: INFO: azure-ip-masq-agent-928nn from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    May 17 05:45:41.224: INFO: cloud-node-manager-x7fjk from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container cloud-node-manager ready: true, restart count 0
    May 17 05:45:41.224: INFO: coredns-75bbfcbc66-mkbkm from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container coredns ready: true, restart count 0
    May 17 05:45:41.224: INFO: coredns-75bbfcbc66-vjv2g from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container coredns ready: true, restart count 0
    May 17 05:45:41.224: INFO: coredns-autoscaler-7879846967-wvdn8 from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container autoscaler ready: true, restart count 0
    May 17 05:45:41.224: INFO: csi-azuredisk-node-pccm7 from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container azuredisk ready: true, restart count 0
    May 17 05:45:41.224: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 05:45:41.224: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 05:45:41.224: INFO: csi-azurefile-node-bzf7d from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container azurefile ready: true, restart count 0
    May 17 05:45:41.224: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 05:45:41.224: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 05:45:41.224: INFO: konnectivity-agent-6859749db4-r746q from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container konnectivity-agent ready: true, restart count 0
    May 17 05:45:41.224: INFO: konnectivity-agent-6859749db4-vbj9c from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container konnectivity-agent ready: true, restart count 0
    May 17 05:45:41.224: INFO: kube-proxy-mzbkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 05:45:41.224: INFO: metrics-server-85c9977f87-k2zzk from kube-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container metrics-server ready: true, restart count 0
    May 17 05:45:41.224: INFO: 	Container metrics-server-vpa ready: true, restart count 0
    May 17 05:45:41.224: INFO: metrics-server-85c9977f87-slkjl from kube-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container metrics-server ready: true, restart count 0
    May 17 05:45:41.224: INFO: 	Container metrics-server-vpa ready: true, restart count 0
    May 17 05:45:41.224: INFO: virt-api-6c4f849c9d-2jwc2 from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container virt-api ready: true, restart count 0
    May 17 05:45:41.224: INFO: virt-api-6c4f849c9d-2x4kv from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container virt-api ready: true, restart count 0
    May 17 05:45:41.224: INFO: virt-controller-67b95d99d5-2cnzp from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container virt-controller ready: true, restart count 0
    May 17 05:45:41.224: INFO: virt-controller-67b95d99d5-8wv5s from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container virt-controller ready: true, restart count 0
    May 17 05:45:41.224: INFO: virt-handler-hnwhl from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container virt-handler ready: true, restart count 0
    May 17 05:45:41.224: INFO: virt-operator-798f64bdf6-gx4sg from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container virt-operator ready: true, restart count 0
    May 17 05:45:41.224: INFO: virt-operator-798f64bdf6-x5z2b from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container virt-operator ready: true, restart count 0
    May 17 05:45:41.224: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-5jttt from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 05:45:41.224: INFO: 	Container systemd-logs ready: true, restart count 0
    May 17 05:45:41.224: INFO: k8s-triliovault-admission-webhook-77b46b96cd-9lksn from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container triliovault-admission-webhook ready: true, restart count 0
    May 17 05:45:41.224: INFO: k8s-triliovault-control-plane-6fcbd76b99-8kgx9 from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container triliovault-analyzer ready: true, restart count 0
    May 17 05:45:41.224: INFO: 	Container triliovault-control-plane ready: true, restart count 0
    May 17 05:45:41.224: INFO: k8s-triliovault-exporter-59c8c8dcdc-kwbxw from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container triliovault-exporter ready: true, restart count 0
    May 17 05:45:41.224: INFO: k8s-triliovault-ingress-nginx-controller-76b7885d96-rr66q from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container controller ready: true, restart count 0
    May 17 05:45:41.224: INFO: k8s-triliovault-operator-66747bd477-pmljl from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container k8s-triliovault-operator ready: true, restart count 0
    May 17 05:45:41.224: INFO: k8s-triliovault-web-75bdc67d7d-l8t7z from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container triliovault-web ready: true, restart count 0
    May 17 05:45:41.224: INFO: k8s-triliovault-web-backend-54b448979f-98rfz from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.224: INFO: 	Container triliovault-web-backend ready: true, restart count 0
    May 17 05:45:41.224: INFO: 
    Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000c before test
    May 17 05:45:41.236: INFO: virt-launcher-cirros-vm-n6f7b from default started at 2023-05-17 04:35:13 +0000 UTC (2 container statuses recorded)
    May 17 05:45:41.236: INFO: 	Container compute ready: true, restart count 0
    May 17 05:45:41.236: INFO: 	Container volumecontainerdisk ready: true, restart count 0
    May 17 05:45:41.236: INFO: ama-logs-xntrk from kube-system started at 2023-05-17 04:32:30 +0000 UTC (2 container statuses recorded)
    May 17 05:45:41.236: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 05:45:41.236: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
    May 17 05:45:41.236: INFO: azure-ip-masq-agent-ns2hc from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.236: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    May 17 05:45:41.236: INFO: cloud-node-manager-hhrc6 from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.236: INFO: 	Container cloud-node-manager ready: true, restart count 0
    May 17 05:45:41.236: INFO: csi-azuredisk-node-dms8v from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
    May 17 05:45:41.236: INFO: 	Container azuredisk ready: true, restart count 0
    May 17 05:45:41.236: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 05:45:41.236: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 05:45:41.236: INFO: csi-azurefile-node-whbbj from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
    May 17 05:45:41.236: INFO: 	Container azurefile ready: true, restart count 0
    May 17 05:45:41.236: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 05:45:41.236: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 05:45:41.236: INFO: kube-proxy-lpnpx from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.236: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 05:45:41.236: INFO: virt-handler-bzx8t from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.236: INFO: 	Container virt-handler ready: true, restart count 0
    May 17 05:45:41.236: INFO: agnhost from proxy-1328 started at 2023-05-17 05:45:34 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.236: INFO: 	Container agnhost ready: true, restart count 0
    May 17 05:45:41.236: INFO: sonobuoy from sonobuoy started at 2023-05-17 05:36:06 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.236: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May 17 05:45:41.236: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-6hdss from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 05:45:41.236: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 05:45:41.236: INFO: 	Container systemd-logs ready: true, restart count 0
    May 17 05:45:41.236: INFO: k8s-triliovault-resource-cleaner-28071690-kxt5w from trilio-system started at 2023-05-17 05:30:00 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.236: INFO: 	Container resource-cleaner ready: false, restart count 0
    May 17 05:45:41.236: INFO: 
    Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000d before test
    May 17 05:45:41.250: INFO: ama-logs-bmxmm from kube-system started at 2023-05-17 04:32:24 +0000 UTC (2 container statuses recorded)
    May 17 05:45:41.250: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 05:45:41.250: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
    May 17 05:45:41.250: INFO: azure-ip-masq-agent-vm9n2 from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.250: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    May 17 05:45:41.250: INFO: cloud-node-manager-6mv9z from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.250: INFO: 	Container cloud-node-manager ready: true, restart count 0
    May 17 05:45:41.250: INFO: csi-azuredisk-node-rg26q from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
    May 17 05:45:41.250: INFO: 	Container azuredisk ready: true, restart count 0
    May 17 05:45:41.250: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 05:45:41.250: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 05:45:41.250: INFO: csi-azurefile-node-zlpwj from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
    May 17 05:45:41.250: INFO: 	Container azurefile ready: true, restart count 0
    May 17 05:45:41.250: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 05:45:41.250: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 05:45:41.250: INFO: kube-proxy-nsx8v from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.250: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 05:45:41.250: INFO: virt-handler-9qq9z from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.250: INFO: 	Container virt-handler ready: true, restart count 0
    May 17 05:45:41.250: INFO: mysql-qa-64f5b55869-877ll from mysql started at 2023-05-17 04:33:14 +0000 UTC (1 container statuses recorded)
    May 17 05:45:41.250: INFO: 	Container mysql-qa ready: true, restart count 0
    May 17 05:45:41.250: INFO: sonobuoy-e2e-job-a0b627fa1b44421a from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 05:45:41.250: INFO: 	Container e2e ready: true, restart count 0
    May 17 05:45:41.250: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 05:45:41.250: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-tnssv from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 05:45:41.250: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 05:45:41.250: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/17/23 05:45:41.25
    May 17 05:45:41.261: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6631" to be "running"
    May 17 05:45:41.264: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.672346ms
    May 17 05:45:43.269: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008870137s
    May 17 05:45:43.269: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/17/23 05:45:43.273
    STEP: Trying to apply a random label on the found node. 05/17/23 05:45:43.284
    STEP: verifying the node has the label kubernetes.io/e2e-95ca2a67-82de-4a8f-b6b8-7428fc275c11 42 05/17/23 05:45:43.316
    STEP: Trying to relaunch the pod, now with labels. 05/17/23 05:45:43.32
    May 17 05:45:43.328: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6631" to be "not pending"
    May 17 05:45:43.331: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.631525ms
    May 17 05:45:45.336: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008690924s
    May 17 05:45:45.336: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-95ca2a67-82de-4a8f-b6b8-7428fc275c11 off the node aks-agentpool-72615086-vmss00000c 05/17/23 05:45:45.34
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-95ca2a67-82de-4a8f-b6b8-7428fc275c11 05/17/23 05:45:45.371
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May 17 05:45:45.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6631" for this suite. 05/17/23 05:45:45.381
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:45.389
May 17 05:45:45.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename svcaccounts 05/17/23 05:45:45.39
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:45.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:45.437
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  05/17/23 05:45:45.44
May 17 05:45:45.451: INFO: Waiting up to 5m0s for pod "test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8" in namespace "svcaccounts-8747" to be "Succeeded or Failed"
May 17 05:45:45.454: INFO: Pod "test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.592705ms
May 17 05:45:47.459: INFO: Pod "test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008568597s
May 17 05:45:49.466: INFO: Pod "test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015374489s
STEP: Saw pod success 05/17/23 05:45:49.466
May 17 05:45:49.466: INFO: Pod "test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8" satisfied condition "Succeeded or Failed"
May 17 05:45:49.470: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 05:45:49.48
May 17 05:45:49.492: INFO: Waiting for pod test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8 to disappear
May 17 05:45:49.496: INFO: Pod test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May 17 05:45:49.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8747" for this suite. 05/17/23 05:45:49.502
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":39,"skipped":568,"failed":0}
------------------------------
â€¢ [4.122 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:45.389
    May 17 05:45:45.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 05:45:45.39
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:45.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:45.437
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  05/17/23 05:45:45.44
    May 17 05:45:45.451: INFO: Waiting up to 5m0s for pod "test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8" in namespace "svcaccounts-8747" to be "Succeeded or Failed"
    May 17 05:45:45.454: INFO: Pod "test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.592705ms
    May 17 05:45:47.459: INFO: Pod "test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008568597s
    May 17 05:45:49.466: INFO: Pod "test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015374489s
    STEP: Saw pod success 05/17/23 05:45:49.466
    May 17 05:45:49.466: INFO: Pod "test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8" satisfied condition "Succeeded or Failed"
    May 17 05:45:49.470: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 05:45:49.48
    May 17 05:45:49.492: INFO: Waiting for pod test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8 to disappear
    May 17 05:45:49.496: INFO: Pod test-pod-2bad5aad-927c-4ce7-9bcb-003c87ba49c8 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May 17 05:45:49.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8747" for this suite. 05/17/23 05:45:49.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:49.513
May 17 05:45:49.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename replicaset 05/17/23 05:45:49.513
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:49.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:49.535
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/17/23 05:45:49.539
May 17 05:45:49.549: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9393" to be "running and ready"
May 17 05:45:49.553: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.779995ms
May 17 05:45:49.553: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May 17 05:45:51.558: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.008602124s
May 17 05:45:51.558: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
May 17 05:45:51.558: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 05/17/23 05:45:51.562
STEP: Then the orphan pod is adopted 05/17/23 05:45:51.568
STEP: When the matched label of one of its pods change 05/17/23 05:45:52.576
May 17 05:45:52.580: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 05/17/23 05:45:52.59
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May 17 05:45:53.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9393" for this suite. 05/17/23 05:45:53.604
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":40,"skipped":596,"failed":0}
------------------------------
â€¢ [4.098 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:49.513
    May 17 05:45:49.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename replicaset 05/17/23 05:45:49.513
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:49.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:49.535
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/17/23 05:45:49.539
    May 17 05:45:49.549: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9393" to be "running and ready"
    May 17 05:45:49.553: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.779995ms
    May 17 05:45:49.553: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    May 17 05:45:51.558: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.008602124s
    May 17 05:45:51.558: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    May 17 05:45:51.558: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 05/17/23 05:45:51.562
    STEP: Then the orphan pod is adopted 05/17/23 05:45:51.568
    STEP: When the matched label of one of its pods change 05/17/23 05:45:52.576
    May 17 05:45:52.580: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/17/23 05:45:52.59
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May 17 05:45:53.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9393" for this suite. 05/17/23 05:45:53.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:53.613
May 17 05:45:53.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename disruption 05/17/23 05:45:53.613
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:53.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:53.629
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 05/17/23 05:45:53.638
STEP: Waiting for all pods to be running 05/17/23 05:45:55.679
May 17 05:45:55.684: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May 17 05:45:57.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8120" for this suite. 05/17/23 05:45:57.701
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":41,"skipped":663,"failed":0}
------------------------------
â€¢ [4.095 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:53.613
    May 17 05:45:53.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename disruption 05/17/23 05:45:53.613
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:53.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:53.629
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 05/17/23 05:45:53.638
    STEP: Waiting for all pods to be running 05/17/23 05:45:55.679
    May 17 05:45:55.684: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May 17 05:45:57.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8120" for this suite. 05/17/23 05:45:57.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:45:57.708
May 17 05:45:57.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename statefulset 05/17/23 05:45:57.709
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:57.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:57.724
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9524 05/17/23 05:45:57.727
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
May 17 05:45:57.742: INFO: Found 0 stateful pods, waiting for 1
May 17 05:46:07.746: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 05/17/23 05:46:07.755
W0517 05:46:07.762903      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May 17 05:46:07.770: INFO: Found 1 stateful pods, waiting for 2
May 17 05:46:17.776: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 05:46:17.776: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 05/17/23 05:46:17.787
STEP: Delete all of the StatefulSets 05/17/23 05:46:17.792
STEP: Verify that StatefulSets have been deleted 05/17/23 05:46:17.804
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May 17 05:46:17.809: INFO: Deleting all statefulset in ns statefulset-9524
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May 17 05:46:17.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9524" for this suite. 05/17/23 05:46:17.827
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":42,"skipped":674,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.126 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:45:57.708
    May 17 05:45:57.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename statefulset 05/17/23 05:45:57.709
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:45:57.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:45:57.724
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9524 05/17/23 05:45:57.727
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    May 17 05:45:57.742: INFO: Found 0 stateful pods, waiting for 1
    May 17 05:46:07.746: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 05/17/23 05:46:07.755
    W0517 05:46:07.762903      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May 17 05:46:07.770: INFO: Found 1 stateful pods, waiting for 2
    May 17 05:46:17.776: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May 17 05:46:17.776: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 05/17/23 05:46:17.787
    STEP: Delete all of the StatefulSets 05/17/23 05:46:17.792
    STEP: Verify that StatefulSets have been deleted 05/17/23 05:46:17.804
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May 17 05:46:17.809: INFO: Deleting all statefulset in ns statefulset-9524
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May 17 05:46:17.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9524" for this suite. 05/17/23 05:46:17.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:46:17.839
May 17 05:46:17.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubelet-test 05/17/23 05:46:17.839
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:46:17.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:46:17.855
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May 17 05:46:17.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4034" for this suite. 05/17/23 05:46:17.905
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":43,"skipped":790,"failed":0}
------------------------------
â€¢ [0.074 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:46:17.839
    May 17 05:46:17.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubelet-test 05/17/23 05:46:17.839
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:46:17.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:46:17.855
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May 17 05:46:17.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4034" for this suite. 05/17/23 05:46:17.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:46:17.914
May 17 05:46:17.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 05:46:17.915
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:46:17.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:46:17.933
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
May 17 05:46:17.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 05:46:18.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2368" for this suite. 05/17/23 05:46:18.969
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":44,"skipped":838,"failed":0}
------------------------------
â€¢ [1.062 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:46:17.914
    May 17 05:46:17.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 05:46:17.915
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:46:17.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:46:17.933
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    May 17 05:46:17.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 05:46:18.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2368" for this suite. 05/17/23 05:46:18.969
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:46:18.977
May 17 05:46:18.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename proxy 05/17/23 05:46:18.977
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:46:18.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:46:19.003
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
May 17 05:46:19.006: INFO: Creating pod...
May 17 05:46:19.017: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8958" to be "running"
May 17 05:46:19.021: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.627479ms
May 17 05:46:21.026: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008435545s
May 17 05:46:21.026: INFO: Pod "agnhost" satisfied condition "running"
May 17 05:46:21.026: INFO: Creating service...
May 17 05:46:21.036: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/DELETE
May 17 05:46:21.045: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 17 05:46:21.045: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/GET
May 17 05:46:21.051: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 17 05:46:21.051: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/HEAD
May 17 05:46:21.057: INFO: http.Client request:HEAD | StatusCode:200
May 17 05:46:21.057: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/OPTIONS
May 17 05:46:21.062: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 17 05:46:21.062: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/PATCH
May 17 05:46:21.069: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 17 05:46:21.069: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/POST
May 17 05:46:21.075: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 17 05:46:21.075: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/PUT
May 17 05:46:21.082: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 17 05:46:21.082: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/DELETE
May 17 05:46:21.090: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 17 05:46:21.090: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/GET
May 17 05:46:21.098: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 17 05:46:21.098: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/HEAD
May 17 05:46:21.106: INFO: http.Client request:HEAD | StatusCode:200
May 17 05:46:21.106: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/OPTIONS
May 17 05:46:21.114: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 17 05:46:21.114: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/PATCH
May 17 05:46:21.123: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 17 05:46:21.123: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/POST
May 17 05:46:21.131: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 17 05:46:21.131: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/PUT
May 17 05:46:21.138: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
May 17 05:46:21.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8958" for this suite. 05/17/23 05:46:21.145
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":45,"skipped":840,"failed":0}
------------------------------
â€¢ [2.175 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:46:18.977
    May 17 05:46:18.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename proxy 05/17/23 05:46:18.977
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:46:18.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:46:19.003
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    May 17 05:46:19.006: INFO: Creating pod...
    May 17 05:46:19.017: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8958" to be "running"
    May 17 05:46:19.021: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.627479ms
    May 17 05:46:21.026: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008435545s
    May 17 05:46:21.026: INFO: Pod "agnhost" satisfied condition "running"
    May 17 05:46:21.026: INFO: Creating service...
    May 17 05:46:21.036: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/DELETE
    May 17 05:46:21.045: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May 17 05:46:21.045: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/GET
    May 17 05:46:21.051: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May 17 05:46:21.051: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/HEAD
    May 17 05:46:21.057: INFO: http.Client request:HEAD | StatusCode:200
    May 17 05:46:21.057: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/OPTIONS
    May 17 05:46:21.062: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May 17 05:46:21.062: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/PATCH
    May 17 05:46:21.069: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May 17 05:46:21.069: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/POST
    May 17 05:46:21.075: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May 17 05:46:21.075: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/pods/agnhost/proxy/some/path/with/PUT
    May 17 05:46:21.082: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May 17 05:46:21.082: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/DELETE
    May 17 05:46:21.090: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May 17 05:46:21.090: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/GET
    May 17 05:46:21.098: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May 17 05:46:21.098: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/HEAD
    May 17 05:46:21.106: INFO: http.Client request:HEAD | StatusCode:200
    May 17 05:46:21.106: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/OPTIONS
    May 17 05:46:21.114: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May 17 05:46:21.114: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/PATCH
    May 17 05:46:21.123: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May 17 05:46:21.123: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/POST
    May 17 05:46:21.131: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May 17 05:46:21.131: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-8958/services/test-service/proxy/some/path/with/PUT
    May 17 05:46:21.138: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    May 17 05:46:21.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-8958" for this suite. 05/17/23 05:46:21.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:46:21.154
May 17 05:46:21.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename taint-single-pod 05/17/23 05:46:21.155
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:46:21.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:46:21.172
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
May 17 05:46:21.175: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 05:47:21.221: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
May 17 05:47:21.227: INFO: Starting informer...
STEP: Starting pod... 05/17/23 05:47:21.227
May 17 05:47:21.447: INFO: Pod is running on aks-agentpool-72615086-vmss00000c. Tainting Node
STEP: Trying to apply a taint on the Node 05/17/23 05:47:21.447
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 05:47:21.476
STEP: Waiting short time to make sure Pod is queued for deletion 05/17/23 05:47:21.481
May 17 05:47:21.481: INFO: Pod wasn't evicted. Proceeding
May 17 05:47:21.481: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 05:47:21.521
STEP: Waiting some time to make sure that toleration time passed. 05/17/23 05:47:21.526
May 17 05:48:36.527: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
May 17 05:48:36.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4846" for this suite. 05/17/23 05:48:36.535
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":46,"skipped":908,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.387 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:46:21.154
    May 17 05:46:21.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename taint-single-pod 05/17/23 05:46:21.155
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:46:21.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:46:21.172
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    May 17 05:46:21.175: INFO: Waiting up to 1m0s for all nodes to be ready
    May 17 05:47:21.221: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    May 17 05:47:21.227: INFO: Starting informer...
    STEP: Starting pod... 05/17/23 05:47:21.227
    May 17 05:47:21.447: INFO: Pod is running on aks-agentpool-72615086-vmss00000c. Tainting Node
    STEP: Trying to apply a taint on the Node 05/17/23 05:47:21.447
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 05:47:21.476
    STEP: Waiting short time to make sure Pod is queued for deletion 05/17/23 05:47:21.481
    May 17 05:47:21.481: INFO: Pod wasn't evicted. Proceeding
    May 17 05:47:21.481: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 05:47:21.521
    STEP: Waiting some time to make sure that toleration time passed. 05/17/23 05:47:21.526
    May 17 05:48:36.527: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    May 17 05:48:36.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-4846" for this suite. 05/17/23 05:48:36.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:48:36.542
May 17 05:48:36.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 05:48:36.543
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:48:36.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:48:36.591
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 05:48:36.607
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 05:48:36.922
STEP: Deploying the webhook pod 05/17/23 05:48:36.931
STEP: Wait for the deployment to be ready 05/17/23 05:48:36.943
May 17 05:48:36.950: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/17/23 05:48:38.964
STEP: Verifying the service has paired with the endpoint 05/17/23 05:48:38.975
May 17 05:48:39.975: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/17/23 05:48:39.979
STEP: create a namespace for the webhook 05/17/23 05:48:39.997
STEP: create a configmap should be unconditionally rejected by the webhook 05/17/23 05:48:40.007
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 05:48:40.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9080" for this suite. 05/17/23 05:48:40.047
STEP: Destroying namespace "webhook-9080-markers" for this suite. 05/17/23 05:48:40.054
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":47,"skipped":923,"failed":0}
------------------------------
â€¢ [3.560 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:48:36.542
    May 17 05:48:36.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 05:48:36.543
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:48:36.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:48:36.591
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 05:48:36.607
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 05:48:36.922
    STEP: Deploying the webhook pod 05/17/23 05:48:36.931
    STEP: Wait for the deployment to be ready 05/17/23 05:48:36.943
    May 17 05:48:36.950: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/17/23 05:48:38.964
    STEP: Verifying the service has paired with the endpoint 05/17/23 05:48:38.975
    May 17 05:48:39.975: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/17/23 05:48:39.979
    STEP: create a namespace for the webhook 05/17/23 05:48:39.997
    STEP: create a configmap should be unconditionally rejected by the webhook 05/17/23 05:48:40.007
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 05:48:40.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9080" for this suite. 05/17/23 05:48:40.047
    STEP: Destroying namespace "webhook-9080-markers" for this suite. 05/17/23 05:48:40.054
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:48:40.102
May 17 05:48:40.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename containers 05/17/23 05:48:40.102
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:48:40.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:48:40.127
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 05/17/23 05:48:40.131
May 17 05:48:40.143: INFO: Waiting up to 5m0s for pod "client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99" in namespace "containers-4150" to be "Succeeded or Failed"
May 17 05:48:40.146: INFO: Pod "client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99": Phase="Pending", Reason="", readiness=false. Elapsed: 3.55805ms
May 17 05:48:42.151: INFO: Pod "client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008746545s
May 17 05:48:44.152: INFO: Pod "client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00970233s
STEP: Saw pod success 05/17/23 05:48:44.152
May 17 05:48:44.153: INFO: Pod "client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99" satisfied condition "Succeeded or Failed"
May 17 05:48:44.156: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 05:48:44.167
May 17 05:48:44.184: INFO: Waiting for pod client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99 to disappear
May 17 05:48:44.187: INFO: Pod client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May 17 05:48:44.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4150" for this suite. 05/17/23 05:48:44.194
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":48,"skipped":923,"failed":0}
------------------------------
â€¢ [4.099 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:48:40.102
    May 17 05:48:40.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename containers 05/17/23 05:48:40.102
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:48:40.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:48:40.127
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 05/17/23 05:48:40.131
    May 17 05:48:40.143: INFO: Waiting up to 5m0s for pod "client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99" in namespace "containers-4150" to be "Succeeded or Failed"
    May 17 05:48:40.146: INFO: Pod "client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99": Phase="Pending", Reason="", readiness=false. Elapsed: 3.55805ms
    May 17 05:48:42.151: INFO: Pod "client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008746545s
    May 17 05:48:44.152: INFO: Pod "client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00970233s
    STEP: Saw pod success 05/17/23 05:48:44.152
    May 17 05:48:44.153: INFO: Pod "client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99" satisfied condition "Succeeded or Failed"
    May 17 05:48:44.156: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 05:48:44.167
    May 17 05:48:44.184: INFO: Waiting for pod client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99 to disappear
    May 17 05:48:44.187: INFO: Pod client-containers-9ff9b524-49ad-4672-a542-e4f138c05b99 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May 17 05:48:44.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-4150" for this suite. 05/17/23 05:48:44.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:48:44.202
May 17 05:48:44.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename conformance-tests 05/17/23 05:48:44.203
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:48:44.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:48:44.222
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 05/17/23 05:48:44.226
May 17 05:48:44.226: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
May 17 05:48:44.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-1932" for this suite. 05/17/23 05:48:44.239
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":49,"skipped":986,"failed":0}
------------------------------
â€¢ [0.043 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:48:44.202
    May 17 05:48:44.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename conformance-tests 05/17/23 05:48:44.203
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:48:44.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:48:44.222
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 05/17/23 05:48:44.226
    May 17 05:48:44.226: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    May 17 05:48:44.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-1932" for this suite. 05/17/23 05:48:44.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:48:44.246
May 17 05:48:44.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename cronjob 05/17/23 05:48:44.247
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:48:44.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:48:44.268
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 05/17/23 05:48:44.272
STEP: Ensuring more than one job is running at a time 05/17/23 05:48:44.278
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/17/23 05:50:00.284
STEP: Removing cronjob 05/17/23 05:50:00.288
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May 17 05:50:00.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8448" for this suite. 05/17/23 05:50:00.302
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":50,"skipped":999,"failed":0}
------------------------------
â€¢ [SLOW TEST] [76.062 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:48:44.246
    May 17 05:48:44.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename cronjob 05/17/23 05:48:44.247
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:48:44.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:48:44.268
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 05/17/23 05:48:44.272
    STEP: Ensuring more than one job is running at a time 05/17/23 05:48:44.278
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/17/23 05:50:00.284
    STEP: Removing cronjob 05/17/23 05:50:00.288
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May 17 05:50:00.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8448" for this suite. 05/17/23 05:50:00.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:50:00.309
May 17 05:50:00.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename var-expansion 05/17/23 05:50:00.31
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:00.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:00.328
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 05/17/23 05:50:00.332
May 17 05:50:00.344: INFO: Waiting up to 5m0s for pod "var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c" in namespace "var-expansion-7395" to be "Succeeded or Failed"
May 17 05:50:00.347: INFO: Pod "var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.637048ms
May 17 05:50:02.353: INFO: Pod "var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009243725s
May 17 05:50:04.353: INFO: Pod "var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009783275s
STEP: Saw pod success 05/17/23 05:50:04.353
May 17 05:50:04.353: INFO: Pod "var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c" satisfied condition "Succeeded or Failed"
May 17 05:50:04.358: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c container dapi-container: <nil>
STEP: delete the pod 05/17/23 05:50:04.368
May 17 05:50:04.387: INFO: Waiting for pod var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c to disappear
May 17 05:50:04.392: INFO: Pod var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May 17 05:50:04.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7395" for this suite. 05/17/23 05:50:04.399
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":51,"skipped":1021,"failed":0}
------------------------------
â€¢ [4.100 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:50:00.309
    May 17 05:50:00.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename var-expansion 05/17/23 05:50:00.31
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:00.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:00.328
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 05/17/23 05:50:00.332
    May 17 05:50:00.344: INFO: Waiting up to 5m0s for pod "var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c" in namespace "var-expansion-7395" to be "Succeeded or Failed"
    May 17 05:50:00.347: INFO: Pod "var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.637048ms
    May 17 05:50:02.353: INFO: Pod "var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009243725s
    May 17 05:50:04.353: INFO: Pod "var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009783275s
    STEP: Saw pod success 05/17/23 05:50:04.353
    May 17 05:50:04.353: INFO: Pod "var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c" satisfied condition "Succeeded or Failed"
    May 17 05:50:04.358: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c container dapi-container: <nil>
    STEP: delete the pod 05/17/23 05:50:04.368
    May 17 05:50:04.387: INFO: Waiting for pod var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c to disappear
    May 17 05:50:04.392: INFO: Pod var-expansion-d87317f2-1aa8-4439-aaee-e8cb4b3e714c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May 17 05:50:04.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7395" for this suite. 05/17/23 05:50:04.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:50:04.41
May 17 05:50:04.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename disruption 05/17/23 05:50:04.41
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:04.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:04.428
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 05/17/23 05:50:04.431
STEP: Waiting for the pdb to be processed 05/17/23 05:50:04.435
STEP: updating the pdb 05/17/23 05:50:06.444
STEP: Waiting for the pdb to be processed 05/17/23 05:50:06.46
STEP: patching the pdb 05/17/23 05:50:08.47
STEP: Waiting for the pdb to be processed 05/17/23 05:50:08.48
STEP: Waiting for the pdb to be deleted 05/17/23 05:50:10.496
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May 17 05:50:10.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-209" for this suite. 05/17/23 05:50:10.507
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":52,"skipped":1028,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.103 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:50:04.41
    May 17 05:50:04.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename disruption 05/17/23 05:50:04.41
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:04.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:04.428
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 05/17/23 05:50:04.431
    STEP: Waiting for the pdb to be processed 05/17/23 05:50:04.435
    STEP: updating the pdb 05/17/23 05:50:06.444
    STEP: Waiting for the pdb to be processed 05/17/23 05:50:06.46
    STEP: patching the pdb 05/17/23 05:50:08.47
    STEP: Waiting for the pdb to be processed 05/17/23 05:50:08.48
    STEP: Waiting for the pdb to be deleted 05/17/23 05:50:10.496
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May 17 05:50:10.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-209" for this suite. 05/17/23 05:50:10.507
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:50:10.514
May 17 05:50:10.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename controllerrevisions 05/17/23 05:50:10.514
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:10.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:10.53
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-qz5ct-daemon-set" 05/17/23 05:50:10.556
STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 05:50:10.562
May 17 05:50:10.582: INFO: Number of nodes with available pods controlled by daemonset e2e-qz5ct-daemon-set: 0
May 17 05:50:10.582: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 05:50:11.593: INFO: Number of nodes with available pods controlled by daemonset e2e-qz5ct-daemon-set: 1
May 17 05:50:11.593: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 05:50:12.594: INFO: Number of nodes with available pods controlled by daemonset e2e-qz5ct-daemon-set: 3
May 17 05:50:12.594: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-qz5ct-daemon-set
STEP: Confirm DaemonSet "e2e-qz5ct-daemon-set" successfully created with "daemonset-name=e2e-qz5ct-daemon-set" label 05/17/23 05:50:12.597
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-qz5ct-daemon-set" 05/17/23 05:50:12.607
May 17 05:50:12.613: INFO: Located ControllerRevision: "e2e-qz5ct-daemon-set-69db89d5cd"
STEP: Patching ControllerRevision "e2e-qz5ct-daemon-set-69db89d5cd" 05/17/23 05:50:12.616
May 17 05:50:12.622: INFO: e2e-qz5ct-daemon-set-69db89d5cd has been patched
STEP: Create a new ControllerRevision 05/17/23 05:50:12.622
May 17 05:50:12.628: INFO: Created ControllerRevision: e2e-qz5ct-daemon-set-69bc78d57b
STEP: Confirm that there are two ControllerRevisions 05/17/23 05:50:12.628
May 17 05:50:12.628: INFO: Requesting list of ControllerRevisions to confirm quantity
May 17 05:50:12.632: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-qz5ct-daemon-set-69db89d5cd" 05/17/23 05:50:12.632
STEP: Confirm that there is only one ControllerRevision 05/17/23 05:50:12.638
May 17 05:50:12.638: INFO: Requesting list of ControllerRevisions to confirm quantity
May 17 05:50:12.641: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-qz5ct-daemon-set-69bc78d57b" 05/17/23 05:50:12.645
May 17 05:50:12.654: INFO: e2e-qz5ct-daemon-set-69bc78d57b has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 05/17/23 05:50:12.654
W0517 05:50:12.661186      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 05/17/23 05:50:12.661
May 17 05:50:12.661: INFO: Requesting list of ControllerRevisions to confirm quantity
May 17 05:50:13.664: INFO: Requesting list of ControllerRevisions to confirm quantity
May 17 05:50:13.669: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-qz5ct-daemon-set-69bc78d57b=updated" 05/17/23 05:50:13.669
STEP: Confirm that there is only one ControllerRevision 05/17/23 05:50:13.678
May 17 05:50:13.678: INFO: Requesting list of ControllerRevisions to confirm quantity
May 17 05:50:13.682: INFO: Found 1 ControllerRevisions
May 17 05:50:13.685: INFO: ControllerRevision "e2e-qz5ct-daemon-set-7654d46b9" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-qz5ct-daemon-set" 05/17/23 05:50:13.689
STEP: deleting DaemonSet.extensions e2e-qz5ct-daemon-set in namespace controllerrevisions-6391, will wait for the garbage collector to delete the pods 05/17/23 05:50:13.689
May 17 05:50:13.751: INFO: Deleting DaemonSet.extensions e2e-qz5ct-daemon-set took: 8.255497ms
May 17 05:50:13.852: INFO: Terminating DaemonSet.extensions e2e-qz5ct-daemon-set pods took: 101.088918ms
May 17 05:50:14.857: INFO: Number of nodes with available pods controlled by daemonset e2e-qz5ct-daemon-set: 0
May 17 05:50:14.857: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-qz5ct-daemon-set
May 17 05:50:14.861: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1440767"},"items":null}

May 17 05:50:14.865: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1440767"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
May 17 05:50:14.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-6391" for this suite. 05/17/23 05:50:14.892
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":53,"skipped":1028,"failed":0}
------------------------------
â€¢ [4.386 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:50:10.514
    May 17 05:50:10.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename controllerrevisions 05/17/23 05:50:10.514
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:10.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:10.53
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-qz5ct-daemon-set" 05/17/23 05:50:10.556
    STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 05:50:10.562
    May 17 05:50:10.582: INFO: Number of nodes with available pods controlled by daemonset e2e-qz5ct-daemon-set: 0
    May 17 05:50:10.582: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 05:50:11.593: INFO: Number of nodes with available pods controlled by daemonset e2e-qz5ct-daemon-set: 1
    May 17 05:50:11.593: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 05:50:12.594: INFO: Number of nodes with available pods controlled by daemonset e2e-qz5ct-daemon-set: 3
    May 17 05:50:12.594: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-qz5ct-daemon-set
    STEP: Confirm DaemonSet "e2e-qz5ct-daemon-set" successfully created with "daemonset-name=e2e-qz5ct-daemon-set" label 05/17/23 05:50:12.597
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-qz5ct-daemon-set" 05/17/23 05:50:12.607
    May 17 05:50:12.613: INFO: Located ControllerRevision: "e2e-qz5ct-daemon-set-69db89d5cd"
    STEP: Patching ControllerRevision "e2e-qz5ct-daemon-set-69db89d5cd" 05/17/23 05:50:12.616
    May 17 05:50:12.622: INFO: e2e-qz5ct-daemon-set-69db89d5cd has been patched
    STEP: Create a new ControllerRevision 05/17/23 05:50:12.622
    May 17 05:50:12.628: INFO: Created ControllerRevision: e2e-qz5ct-daemon-set-69bc78d57b
    STEP: Confirm that there are two ControllerRevisions 05/17/23 05:50:12.628
    May 17 05:50:12.628: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 17 05:50:12.632: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-qz5ct-daemon-set-69db89d5cd" 05/17/23 05:50:12.632
    STEP: Confirm that there is only one ControllerRevision 05/17/23 05:50:12.638
    May 17 05:50:12.638: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 17 05:50:12.641: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-qz5ct-daemon-set-69bc78d57b" 05/17/23 05:50:12.645
    May 17 05:50:12.654: INFO: e2e-qz5ct-daemon-set-69bc78d57b has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 05/17/23 05:50:12.654
    W0517 05:50:12.661186      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 05/17/23 05:50:12.661
    May 17 05:50:12.661: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 17 05:50:13.664: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 17 05:50:13.669: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-qz5ct-daemon-set-69bc78d57b=updated" 05/17/23 05:50:13.669
    STEP: Confirm that there is only one ControllerRevision 05/17/23 05:50:13.678
    May 17 05:50:13.678: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 17 05:50:13.682: INFO: Found 1 ControllerRevisions
    May 17 05:50:13.685: INFO: ControllerRevision "e2e-qz5ct-daemon-set-7654d46b9" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-qz5ct-daemon-set" 05/17/23 05:50:13.689
    STEP: deleting DaemonSet.extensions e2e-qz5ct-daemon-set in namespace controllerrevisions-6391, will wait for the garbage collector to delete the pods 05/17/23 05:50:13.689
    May 17 05:50:13.751: INFO: Deleting DaemonSet.extensions e2e-qz5ct-daemon-set took: 8.255497ms
    May 17 05:50:13.852: INFO: Terminating DaemonSet.extensions e2e-qz5ct-daemon-set pods took: 101.088918ms
    May 17 05:50:14.857: INFO: Number of nodes with available pods controlled by daemonset e2e-qz5ct-daemon-set: 0
    May 17 05:50:14.857: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-qz5ct-daemon-set
    May 17 05:50:14.861: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1440767"},"items":null}

    May 17 05:50:14.865: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1440767"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    May 17 05:50:14.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-6391" for this suite. 05/17/23 05:50:14.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:50:14.901
May 17 05:50:14.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-probe 05/17/23 05:50:14.901
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:14.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:14.917
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
May 17 05:50:14.931: INFO: Waiting up to 5m0s for pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92" in namespace "container-probe-2756" to be "running and ready"
May 17 05:50:14.936: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.476906ms
May 17 05:50:14.936: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Pending, waiting for it to be Running (with Ready = true)
May 17 05:50:16.943: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 2.011786087s
May 17 05:50:16.943: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
May 17 05:50:18.941: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 4.009404647s
May 17 05:50:18.941: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
May 17 05:50:20.940: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 6.008946168s
May 17 05:50:20.940: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
May 17 05:50:22.940: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 8.009145048s
May 17 05:50:22.940: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
May 17 05:50:24.941: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 10.010032249s
May 17 05:50:24.941: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
May 17 05:50:26.940: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 12.009342388s
May 17 05:50:26.940: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
May 17 05:50:28.942: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 14.010685944s
May 17 05:50:28.942: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
May 17 05:50:30.941: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 16.009395724s
May 17 05:50:30.941: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
May 17 05:50:32.941: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 18.01006665s
May 17 05:50:32.941: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
May 17 05:50:34.942: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 20.010688723s
May 17 05:50:34.942: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
May 17 05:50:36.941: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=true. Elapsed: 22.009911149s
May 17 05:50:36.941: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = true)
May 17 05:50:36.941: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92" satisfied condition "running and ready"
May 17 05:50:36.945: INFO: Container started at 2023-05-17 05:50:15 +0000 UTC, pod became ready at 2023-05-17 05:50:35 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May 17 05:50:36.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2756" for this suite. 05/17/23 05:50:36.952
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":54,"skipped":1042,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.059 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:50:14.901
    May 17 05:50:14.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-probe 05/17/23 05:50:14.901
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:14.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:14.917
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    May 17 05:50:14.931: INFO: Waiting up to 5m0s for pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92" in namespace "container-probe-2756" to be "running and ready"
    May 17 05:50:14.936: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.476906ms
    May 17 05:50:14.936: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Pending, waiting for it to be Running (with Ready = true)
    May 17 05:50:16.943: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 2.011786087s
    May 17 05:50:16.943: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
    May 17 05:50:18.941: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 4.009404647s
    May 17 05:50:18.941: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
    May 17 05:50:20.940: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 6.008946168s
    May 17 05:50:20.940: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
    May 17 05:50:22.940: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 8.009145048s
    May 17 05:50:22.940: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
    May 17 05:50:24.941: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 10.010032249s
    May 17 05:50:24.941: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
    May 17 05:50:26.940: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 12.009342388s
    May 17 05:50:26.940: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
    May 17 05:50:28.942: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 14.010685944s
    May 17 05:50:28.942: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
    May 17 05:50:30.941: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 16.009395724s
    May 17 05:50:30.941: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
    May 17 05:50:32.941: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 18.01006665s
    May 17 05:50:32.941: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
    May 17 05:50:34.942: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=false. Elapsed: 20.010688723s
    May 17 05:50:34.942: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = false)
    May 17 05:50:36.941: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92": Phase="Running", Reason="", readiness=true. Elapsed: 22.009911149s
    May 17 05:50:36.941: INFO: The phase of Pod test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92 is Running (Ready = true)
    May 17 05:50:36.941: INFO: Pod "test-webserver-443167ee-07a7-418e-a19a-9cff64b92b92" satisfied condition "running and ready"
    May 17 05:50:36.945: INFO: Container started at 2023-05-17 05:50:15 +0000 UTC, pod became ready at 2023-05-17 05:50:35 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May 17 05:50:36.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2756" for this suite. 05/17/23 05:50:36.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:50:36.961
May 17 05:50:36.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename proxy 05/17/23 05:50:36.962
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:36.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:36.978
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 05/17/23 05:50:36.991
STEP: creating replication controller proxy-service-x9556 in namespace proxy-2448 05/17/23 05:50:36.991
I0517 05:50:36.997890      23 runners.go:193] Created replication controller with name: proxy-service-x9556, namespace: proxy-2448, replica count: 1
I0517 05:50:38.049354      23 runners.go:193] proxy-service-x9556 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0517 05:50:39.049574      23 runners.go:193] proxy-service-x9556 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 05:50:39.054: INFO: setup took 2.072260047s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/17/23 05:50:39.054
May 17 05:50:39.066: INFO: (0) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 11.707098ms)
May 17 05:50:39.066: INFO: (0) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 11.764372ms)
May 17 05:50:39.066: INFO: (0) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 11.534487ms)
May 17 05:50:39.067: INFO: (0) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 12.278006ms)
May 17 05:50:39.067: INFO: (0) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 12.535405ms)
May 17 05:50:39.068: INFO: (0) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 13.924413ms)
May 17 05:50:39.069: INFO: (0) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 14.31968ms)
May 17 05:50:39.069: INFO: (0) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 14.333103ms)
May 17 05:50:39.069: INFO: (0) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 14.692457ms)
May 17 05:50:39.069: INFO: (0) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 14.77022ms)
May 17 05:50:39.070: INFO: (0) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 15.242682ms)
May 17 05:50:39.071: INFO: (0) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 16.613255ms)
May 17 05:50:39.071: INFO: (0) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 16.541141ms)
May 17 05:50:39.073: INFO: (0) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 18.131985ms)
May 17 05:50:39.073: INFO: (0) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 18.095226ms)
May 17 05:50:39.073: INFO: (0) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 18.577484ms)
May 17 05:50:39.079: INFO: (1) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 5.948497ms)
May 17 05:50:39.080: INFO: (1) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 6.989754ms)
May 17 05:50:39.080: INFO: (1) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 7.02624ms)
May 17 05:50:39.080: INFO: (1) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 7.126898ms)
May 17 05:50:39.081: INFO: (1) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 8.132145ms)
May 17 05:50:39.081: INFO: (1) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.167463ms)
May 17 05:50:39.081: INFO: (1) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 8.157756ms)
May 17 05:50:39.082: INFO: (1) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 9.18313ms)
May 17 05:50:39.082: INFO: (1) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 9.225541ms)
May 17 05:50:39.082: INFO: (1) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 9.178721ms)
May 17 05:50:39.083: INFO: (1) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 9.471322ms)
May 17 05:50:39.084: INFO: (1) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 10.848583ms)
May 17 05:50:39.084: INFO: (1) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 10.746054ms)
May 17 05:50:39.084: INFO: (1) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 10.851709ms)
May 17 05:50:39.084: INFO: (1) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 10.809758ms)
May 17 05:50:39.084: INFO: (1) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 10.903305ms)
May 17 05:50:39.092: INFO: (2) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.359209ms)
May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 8.434551ms)
May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 8.477199ms)
May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 8.601882ms)
May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.90869ms)
May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 9.012583ms)
May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.226134ms)
May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 9.247ms)
May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 9.222194ms)
May 17 05:50:39.094: INFO: (2) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 9.405508ms)
May 17 05:50:39.095: INFO: (2) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 10.622982ms)
May 17 05:50:39.096: INFO: (2) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 11.699706ms)
May 17 05:50:39.096: INFO: (2) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 11.814899ms)
May 17 05:50:39.096: INFO: (2) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 12.197644ms)
May 17 05:50:39.097: INFO: (2) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 12.894807ms)
May 17 05:50:39.099: INFO: (2) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 14.418533ms)
May 17 05:50:39.108: INFO: (3) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 8.928976ms)
May 17 05:50:39.108: INFO: (3) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 8.992917ms)
May 17 05:50:39.108: INFO: (3) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.044792ms)
May 17 05:50:39.108: INFO: (3) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 9.616332ms)
May 17 05:50:39.108: INFO: (3) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 9.699233ms)
May 17 05:50:39.108: INFO: (3) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 9.577576ms)
May 17 05:50:39.109: INFO: (3) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.961353ms)
May 17 05:50:39.109: INFO: (3) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.755771ms)
May 17 05:50:39.109: INFO: (3) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 10.126193ms)
May 17 05:50:39.109: INFO: (3) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 10.501675ms)
May 17 05:50:39.110: INFO: (3) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 10.67589ms)
May 17 05:50:39.110: INFO: (3) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 11.172557ms)
May 17 05:50:39.111: INFO: (3) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 11.886112ms)
May 17 05:50:39.111: INFO: (3) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 12.503626ms)
May 17 05:50:39.112: INFO: (3) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 13.477295ms)
May 17 05:50:39.112: INFO: (3) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 13.739866ms)
May 17 05:50:39.119: INFO: (4) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 6.42648ms)
May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 9.065092ms)
May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.044642ms)
May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 8.943534ms)
May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 8.998897ms)
May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.107992ms)
May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.025999ms)
May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 9.048449ms)
May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 9.200423ms)
May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 9.162081ms)
May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 9.244043ms)
May 17 05:50:39.124: INFO: (4) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 11.676682ms)
May 17 05:50:39.125: INFO: (4) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 12.788058ms)
May 17 05:50:39.125: INFO: (4) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 12.819176ms)
May 17 05:50:39.125: INFO: (4) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 12.8373ms)
May 17 05:50:39.126: INFO: (4) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 12.868047ms)
May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.226011ms)
May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.489125ms)
May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 9.66426ms)
May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 9.736474ms)
May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 9.591672ms)
May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 9.721877ms)
May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.730352ms)
May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 9.770751ms)
May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 9.679919ms)
May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.85674ms)
May 17 05:50:39.137: INFO: (5) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 11.717765ms)
May 17 05:50:39.138: INFO: (5) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 11.982991ms)
May 17 05:50:39.138: INFO: (5) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 11.996809ms)
May 17 05:50:39.138: INFO: (5) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 12.142081ms)
May 17 05:50:39.138: INFO: (5) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 12.265671ms)
May 17 05:50:39.140: INFO: (5) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 14.153429ms)
May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 7.066247ms)
May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 7.051858ms)
May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 7.242985ms)
May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 7.237124ms)
May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 7.109704ms)
May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 7.139662ms)
May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 7.590002ms)
May 17 05:50:39.148: INFO: (6) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 7.735766ms)
May 17 05:50:39.148: INFO: (6) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 8.029597ms)
May 17 05:50:39.148: INFO: (6) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 8.351715ms)
May 17 05:50:39.148: INFO: (6) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 8.541159ms)
May 17 05:50:39.149: INFO: (6) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 9.475499ms)
May 17 05:50:39.150: INFO: (6) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 9.862661ms)
May 17 05:50:39.150: INFO: (6) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 9.913035ms)
May 17 05:50:39.150: INFO: (6) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 10.165338ms)
May 17 05:50:39.151: INFO: (6) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 10.668539ms)
May 17 05:50:39.159: INFO: (7) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 8.493302ms)
May 17 05:50:39.161: INFO: (7) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 10.654702ms)
May 17 05:50:39.161: INFO: (7) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 10.762683ms)
May 17 05:50:39.162: INFO: (7) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 10.892976ms)
May 17 05:50:39.162: INFO: (7) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 11.360379ms)
May 17 05:50:39.163: INFO: (7) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 12.036583ms)
May 17 05:50:39.163: INFO: (7) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 12.537489ms)
May 17 05:50:39.163: INFO: (7) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 12.567986ms)
May 17 05:50:39.163: INFO: (7) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 12.419268ms)
May 17 05:50:39.164: INFO: (7) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 12.595909ms)
May 17 05:50:39.164: INFO: (7) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 13.381577ms)
May 17 05:50:39.165: INFO: (7) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 14.002878ms)
May 17 05:50:39.166: INFO: (7) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 15.012806ms)
May 17 05:50:39.166: INFO: (7) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 15.073158ms)
May 17 05:50:39.167: INFO: (7) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 16.020919ms)
May 17 05:50:39.171: INFO: (7) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 20.095042ms)
May 17 05:50:39.179: INFO: (8) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 7.823713ms)
May 17 05:50:39.179: INFO: (8) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 8.114123ms)
May 17 05:50:39.179: INFO: (8) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 8.270195ms)
May 17 05:50:39.179: INFO: (8) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 8.308546ms)
May 17 05:50:39.179: INFO: (8) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.287126ms)
May 17 05:50:39.179: INFO: (8) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 8.226104ms)
May 17 05:50:39.180: INFO: (8) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 8.768488ms)
May 17 05:50:39.180: INFO: (8) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.799592ms)
May 17 05:50:39.180: INFO: (8) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 9.376923ms)
May 17 05:50:39.180: INFO: (8) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 9.155847ms)
May 17 05:50:39.180: INFO: (8) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.176448ms)
May 17 05:50:39.181: INFO: (8) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 9.904068ms)
May 17 05:50:39.182: INFO: (8) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 10.545284ms)
May 17 05:50:39.182: INFO: (8) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 10.517352ms)
May 17 05:50:39.182: INFO: (8) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 11.397769ms)
May 17 05:50:39.192: INFO: (8) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 20.366168ms)
May 17 05:50:39.198: INFO: (9) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 6.775273ms)
May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 11.562195ms)
May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 11.488932ms)
May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 11.547479ms)
May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 11.542239ms)
May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 11.574511ms)
May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 11.608924ms)
May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 11.528224ms)
May 17 05:50:39.204: INFO: (9) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 11.866493ms)
May 17 05:50:39.204: INFO: (9) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 11.741622ms)
May 17 05:50:39.205: INFO: (9) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 13.289053ms)
May 17 05:50:39.206: INFO: (9) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 14.562462ms)
May 17 05:50:39.206: INFO: (9) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 14.498305ms)
May 17 05:50:39.206: INFO: (9) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 14.419829ms)
May 17 05:50:39.206: INFO: (9) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 14.589816ms)
May 17 05:50:39.206: INFO: (9) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 14.536675ms)
May 17 05:50:39.218: INFO: (10) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 11.33367ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 18.84791ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 18.839066ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 18.77743ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 18.987984ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 18.837229ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 18.892496ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 18.802275ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 18.791194ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 18.84695ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 18.84786ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 18.898877ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 18.907493ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 18.861519ms)
May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 19.020162ms)
May 17 05:50:39.226: INFO: (10) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 19.762829ms)
May 17 05:50:39.239: INFO: (11) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 12.06775ms)
May 17 05:50:39.239: INFO: (11) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 12.144436ms)
May 17 05:50:39.239: INFO: (11) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 12.164632ms)
May 17 05:50:39.239: INFO: (11) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 12.293134ms)
May 17 05:50:39.239: INFO: (11) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 12.10433ms)
May 17 05:50:39.239: INFO: (11) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 12.096784ms)
May 17 05:50:39.242: INFO: (11) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 16.113ms)
May 17 05:50:39.242: INFO: (11) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 16.254265ms)
May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 16.168696ms)
May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 16.250226ms)
May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 16.077995ms)
May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 16.792691ms)
May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 16.94879ms)
May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 17.013692ms)
May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 16.977927ms)
May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 17.066873ms)
May 17 05:50:39.255: INFO: (12) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 11.803899ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 15.092423ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 15.005203ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 15.144749ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 15.190277ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 15.105426ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 15.134532ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 15.064832ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 15.099195ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 15.132677ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 15.163335ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 15.183275ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 15.335907ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 15.381115ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 15.784309ms)
May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 15.936982ms)
May 17 05:50:39.268: INFO: (13) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 8.569272ms)
May 17 05:50:39.268: INFO: (13) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.43496ms)
May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 15.136504ms)
May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 15.014288ms)
May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 14.946511ms)
May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 15.00072ms)
May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 14.702898ms)
May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 14.706119ms)
May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 14.96354ms)
May 17 05:50:39.276: INFO: (13) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 15.602335ms)
May 17 05:50:39.277: INFO: (13) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 17.61612ms)
May 17 05:50:39.278: INFO: (13) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 17.63793ms)
May 17 05:50:39.278: INFO: (13) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 17.581876ms)
May 17 05:50:39.281: INFO: (13) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 20.726612ms)
May 17 05:50:39.281: INFO: (13) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 20.882391ms)
May 17 05:50:39.290: INFO: (13) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 29.701166ms)
May 17 05:50:39.298: INFO: (14) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 8.091662ms)
May 17 05:50:39.298: INFO: (14) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.063829ms)
May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 12.595109ms)
May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 12.640812ms)
May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 12.759243ms)
May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 12.712217ms)
May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 12.70958ms)
May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 12.753162ms)
May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 12.692369ms)
May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 12.879097ms)
May 17 05:50:39.307: INFO: (14) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 16.561461ms)
May 17 05:50:39.307: INFO: (14) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 16.626553ms)
May 17 05:50:39.307: INFO: (14) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 16.613867ms)
May 17 05:50:39.307: INFO: (14) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 16.602827ms)
May 17 05:50:39.307: INFO: (14) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 16.749652ms)
May 17 05:50:39.307: INFO: (14) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 16.857934ms)
May 17 05:50:39.323: INFO: (15) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 16.408972ms)
May 17 05:50:39.323: INFO: (15) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 16.137815ms)
May 17 05:50:39.323: INFO: (15) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 16.259397ms)
May 17 05:50:39.324: INFO: (15) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 17.450518ms)
May 17 05:50:39.324: INFO: (15) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 17.388904ms)
May 17 05:50:39.326: INFO: (15) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 19.112426ms)
May 17 05:50:39.326: INFO: (15) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 19.14023ms)
May 17 05:50:39.326: INFO: (15) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 19.323068ms)
May 17 05:50:39.326: INFO: (15) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 19.479191ms)
May 17 05:50:39.326: INFO: (15) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 19.525139ms)
May 17 05:50:39.327: INFO: (15) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 19.853513ms)
May 17 05:50:39.327: INFO: (15) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 19.769582ms)
May 17 05:50:39.327: INFO: (15) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 19.964059ms)
May 17 05:50:39.327: INFO: (15) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 19.713727ms)
May 17 05:50:39.327: INFO: (15) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 20.558316ms)
May 17 05:50:39.329: INFO: (15) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 22.145433ms)
May 17 05:50:39.335: INFO: (16) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 6.369271ms)
May 17 05:50:39.335: INFO: (16) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 6.33999ms)
May 17 05:50:39.336: INFO: (16) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 6.864277ms)
May 17 05:50:39.337: INFO: (16) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 7.994309ms)
May 17 05:50:39.338: INFO: (16) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 8.588401ms)
May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 9.575925ms)
May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.498109ms)
May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.571368ms)
May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.530811ms)
May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 9.551217ms)
May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 9.585202ms)
May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 10.138426ms)
May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 10.399973ms)
May 17 05:50:39.341: INFO: (16) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 12.074356ms)
May 17 05:50:39.341: INFO: (16) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 12.333288ms)
May 17 05:50:39.342: INFO: (16) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 13.054104ms)
May 17 05:50:39.348: INFO: (17) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 5.738602ms)
May 17 05:50:39.356: INFO: (17) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 13.522052ms)
May 17 05:50:39.356: INFO: (17) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 13.797493ms)
May 17 05:50:39.356: INFO: (17) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 13.614001ms)
May 17 05:50:39.356: INFO: (17) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 13.941613ms)
May 17 05:50:39.357: INFO: (17) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 14.987428ms)
May 17 05:50:39.357: INFO: (17) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 14.944317ms)
May 17 05:50:39.357: INFO: (17) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 15.081221ms)
May 17 05:50:39.357: INFO: (17) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 14.969586ms)
May 17 05:50:39.357: INFO: (17) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 15.001692ms)
May 17 05:50:39.357: INFO: (17) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 15.247513ms)
May 17 05:50:39.358: INFO: (17) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 15.381206ms)
May 17 05:50:39.358: INFO: (17) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 15.925158ms)
May 17 05:50:39.358: INFO: (17) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 15.876678ms)
May 17 05:50:39.360: INFO: (17) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 17.41299ms)
May 17 05:50:39.360: INFO: (17) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 17.41212ms)
May 17 05:50:39.368: INFO: (18) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 8.211252ms)
May 17 05:50:39.368: INFO: (18) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 8.437888ms)
May 17 05:50:39.369: INFO: (18) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 8.618924ms)
May 17 05:50:39.370: INFO: (18) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 10.366112ms)
May 17 05:50:39.370: INFO: (18) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 10.417038ms)
May 17 05:50:39.370: INFO: (18) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 10.375241ms)
May 17 05:50:39.370: INFO: (18) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 10.411909ms)
May 17 05:50:39.371: INFO: (18) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 11.346612ms)
May 17 05:50:39.371: INFO: (18) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 11.362796ms)
May 17 05:50:39.373: INFO: (18) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 12.978916ms)
May 17 05:50:39.373: INFO: (18) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 13.375275ms)
May 17 05:50:39.375: INFO: (18) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 15.113874ms)
May 17 05:50:39.375: INFO: (18) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 14.899703ms)
May 17 05:50:39.375: INFO: (18) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 15.312125ms)
May 17 05:50:39.375: INFO: (18) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 15.425925ms)
May 17 05:50:39.376: INFO: (18) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 15.521654ms)
May 17 05:50:39.383: INFO: (19) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 7.243659ms)
May 17 05:50:39.383: INFO: (19) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 7.826606ms)
May 17 05:50:39.385: INFO: (19) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 9.34881ms)
May 17 05:50:39.385: INFO: (19) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.406921ms)
May 17 05:50:39.388: INFO: (19) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 11.658667ms)
May 17 05:50:39.388: INFO: (19) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 11.607063ms)
May 17 05:50:39.388: INFO: (19) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 11.576114ms)
May 17 05:50:39.389: INFO: (19) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 12.540627ms)
May 17 05:50:39.389: INFO: (19) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 12.936897ms)
May 17 05:50:39.389: INFO: (19) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 12.829908ms)
May 17 05:50:39.389: INFO: (19) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 13.055018ms)
May 17 05:50:39.389: INFO: (19) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 13.279468ms)
May 17 05:50:39.389: INFO: (19) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 13.067181ms)
May 17 05:50:39.391: INFO: (19) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 15.584443ms)
May 17 05:50:39.392: INFO: (19) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 16.037911ms)
May 17 05:50:39.394: INFO: (19) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 18.377173ms)
STEP: deleting ReplicationController proxy-service-x9556 in namespace proxy-2448, will wait for the garbage collector to delete the pods 05/17/23 05:50:39.394
May 17 05:50:39.455: INFO: Deleting ReplicationController proxy-service-x9556 took: 6.77988ms
May 17 05:50:39.556: INFO: Terminating ReplicationController proxy-service-x9556 pods took: 100.506689ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
May 17 05:50:42.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2448" for this suite. 05/17/23 05:50:42.464
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":55,"skipped":1068,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.509 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:50:36.961
    May 17 05:50:36.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename proxy 05/17/23 05:50:36.962
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:36.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:36.978
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 05/17/23 05:50:36.991
    STEP: creating replication controller proxy-service-x9556 in namespace proxy-2448 05/17/23 05:50:36.991
    I0517 05:50:36.997890      23 runners.go:193] Created replication controller with name: proxy-service-x9556, namespace: proxy-2448, replica count: 1
    I0517 05:50:38.049354      23 runners.go:193] proxy-service-x9556 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0517 05:50:39.049574      23 runners.go:193] proxy-service-x9556 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 05:50:39.054: INFO: setup took 2.072260047s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/17/23 05:50:39.054
    May 17 05:50:39.066: INFO: (0) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 11.707098ms)
    May 17 05:50:39.066: INFO: (0) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 11.764372ms)
    May 17 05:50:39.066: INFO: (0) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 11.534487ms)
    May 17 05:50:39.067: INFO: (0) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 12.278006ms)
    May 17 05:50:39.067: INFO: (0) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 12.535405ms)
    May 17 05:50:39.068: INFO: (0) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 13.924413ms)
    May 17 05:50:39.069: INFO: (0) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 14.31968ms)
    May 17 05:50:39.069: INFO: (0) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 14.333103ms)
    May 17 05:50:39.069: INFO: (0) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 14.692457ms)
    May 17 05:50:39.069: INFO: (0) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 14.77022ms)
    May 17 05:50:39.070: INFO: (0) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 15.242682ms)
    May 17 05:50:39.071: INFO: (0) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 16.613255ms)
    May 17 05:50:39.071: INFO: (0) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 16.541141ms)
    May 17 05:50:39.073: INFO: (0) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 18.131985ms)
    May 17 05:50:39.073: INFO: (0) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 18.095226ms)
    May 17 05:50:39.073: INFO: (0) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 18.577484ms)
    May 17 05:50:39.079: INFO: (1) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 5.948497ms)
    May 17 05:50:39.080: INFO: (1) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 6.989754ms)
    May 17 05:50:39.080: INFO: (1) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 7.02624ms)
    May 17 05:50:39.080: INFO: (1) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 7.126898ms)
    May 17 05:50:39.081: INFO: (1) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 8.132145ms)
    May 17 05:50:39.081: INFO: (1) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.167463ms)
    May 17 05:50:39.081: INFO: (1) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 8.157756ms)
    May 17 05:50:39.082: INFO: (1) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 9.18313ms)
    May 17 05:50:39.082: INFO: (1) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 9.225541ms)
    May 17 05:50:39.082: INFO: (1) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 9.178721ms)
    May 17 05:50:39.083: INFO: (1) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 9.471322ms)
    May 17 05:50:39.084: INFO: (1) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 10.848583ms)
    May 17 05:50:39.084: INFO: (1) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 10.746054ms)
    May 17 05:50:39.084: INFO: (1) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 10.851709ms)
    May 17 05:50:39.084: INFO: (1) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 10.809758ms)
    May 17 05:50:39.084: INFO: (1) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 10.903305ms)
    May 17 05:50:39.092: INFO: (2) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.359209ms)
    May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 8.434551ms)
    May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 8.477199ms)
    May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 8.601882ms)
    May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.90869ms)
    May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 9.012583ms)
    May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.226134ms)
    May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 9.247ms)
    May 17 05:50:39.093: INFO: (2) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 9.222194ms)
    May 17 05:50:39.094: INFO: (2) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 9.405508ms)
    May 17 05:50:39.095: INFO: (2) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 10.622982ms)
    May 17 05:50:39.096: INFO: (2) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 11.699706ms)
    May 17 05:50:39.096: INFO: (2) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 11.814899ms)
    May 17 05:50:39.096: INFO: (2) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 12.197644ms)
    May 17 05:50:39.097: INFO: (2) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 12.894807ms)
    May 17 05:50:39.099: INFO: (2) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 14.418533ms)
    May 17 05:50:39.108: INFO: (3) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 8.928976ms)
    May 17 05:50:39.108: INFO: (3) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 8.992917ms)
    May 17 05:50:39.108: INFO: (3) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.044792ms)
    May 17 05:50:39.108: INFO: (3) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 9.616332ms)
    May 17 05:50:39.108: INFO: (3) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 9.699233ms)
    May 17 05:50:39.108: INFO: (3) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 9.577576ms)
    May 17 05:50:39.109: INFO: (3) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.961353ms)
    May 17 05:50:39.109: INFO: (3) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.755771ms)
    May 17 05:50:39.109: INFO: (3) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 10.126193ms)
    May 17 05:50:39.109: INFO: (3) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 10.501675ms)
    May 17 05:50:39.110: INFO: (3) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 10.67589ms)
    May 17 05:50:39.110: INFO: (3) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 11.172557ms)
    May 17 05:50:39.111: INFO: (3) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 11.886112ms)
    May 17 05:50:39.111: INFO: (3) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 12.503626ms)
    May 17 05:50:39.112: INFO: (3) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 13.477295ms)
    May 17 05:50:39.112: INFO: (3) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 13.739866ms)
    May 17 05:50:39.119: INFO: (4) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 6.42648ms)
    May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 9.065092ms)
    May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.044642ms)
    May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 8.943534ms)
    May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 8.998897ms)
    May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.107992ms)
    May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.025999ms)
    May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 9.048449ms)
    May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 9.200423ms)
    May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 9.162081ms)
    May 17 05:50:39.122: INFO: (4) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 9.244043ms)
    May 17 05:50:39.124: INFO: (4) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 11.676682ms)
    May 17 05:50:39.125: INFO: (4) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 12.788058ms)
    May 17 05:50:39.125: INFO: (4) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 12.819176ms)
    May 17 05:50:39.125: INFO: (4) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 12.8373ms)
    May 17 05:50:39.126: INFO: (4) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 12.868047ms)
    May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.226011ms)
    May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.489125ms)
    May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 9.66426ms)
    May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 9.736474ms)
    May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 9.591672ms)
    May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 9.721877ms)
    May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.730352ms)
    May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 9.770751ms)
    May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 9.679919ms)
    May 17 05:50:39.135: INFO: (5) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.85674ms)
    May 17 05:50:39.137: INFO: (5) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 11.717765ms)
    May 17 05:50:39.138: INFO: (5) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 11.982991ms)
    May 17 05:50:39.138: INFO: (5) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 11.996809ms)
    May 17 05:50:39.138: INFO: (5) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 12.142081ms)
    May 17 05:50:39.138: INFO: (5) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 12.265671ms)
    May 17 05:50:39.140: INFO: (5) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 14.153429ms)
    May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 7.066247ms)
    May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 7.051858ms)
    May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 7.242985ms)
    May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 7.237124ms)
    May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 7.109704ms)
    May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 7.139662ms)
    May 17 05:50:39.147: INFO: (6) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 7.590002ms)
    May 17 05:50:39.148: INFO: (6) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 7.735766ms)
    May 17 05:50:39.148: INFO: (6) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 8.029597ms)
    May 17 05:50:39.148: INFO: (6) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 8.351715ms)
    May 17 05:50:39.148: INFO: (6) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 8.541159ms)
    May 17 05:50:39.149: INFO: (6) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 9.475499ms)
    May 17 05:50:39.150: INFO: (6) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 9.862661ms)
    May 17 05:50:39.150: INFO: (6) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 9.913035ms)
    May 17 05:50:39.150: INFO: (6) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 10.165338ms)
    May 17 05:50:39.151: INFO: (6) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 10.668539ms)
    May 17 05:50:39.159: INFO: (7) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 8.493302ms)
    May 17 05:50:39.161: INFO: (7) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 10.654702ms)
    May 17 05:50:39.161: INFO: (7) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 10.762683ms)
    May 17 05:50:39.162: INFO: (7) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 10.892976ms)
    May 17 05:50:39.162: INFO: (7) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 11.360379ms)
    May 17 05:50:39.163: INFO: (7) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 12.036583ms)
    May 17 05:50:39.163: INFO: (7) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 12.537489ms)
    May 17 05:50:39.163: INFO: (7) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 12.567986ms)
    May 17 05:50:39.163: INFO: (7) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 12.419268ms)
    May 17 05:50:39.164: INFO: (7) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 12.595909ms)
    May 17 05:50:39.164: INFO: (7) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 13.381577ms)
    May 17 05:50:39.165: INFO: (7) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 14.002878ms)
    May 17 05:50:39.166: INFO: (7) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 15.012806ms)
    May 17 05:50:39.166: INFO: (7) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 15.073158ms)
    May 17 05:50:39.167: INFO: (7) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 16.020919ms)
    May 17 05:50:39.171: INFO: (7) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 20.095042ms)
    May 17 05:50:39.179: INFO: (8) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 7.823713ms)
    May 17 05:50:39.179: INFO: (8) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 8.114123ms)
    May 17 05:50:39.179: INFO: (8) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 8.270195ms)
    May 17 05:50:39.179: INFO: (8) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 8.308546ms)
    May 17 05:50:39.179: INFO: (8) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.287126ms)
    May 17 05:50:39.179: INFO: (8) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 8.226104ms)
    May 17 05:50:39.180: INFO: (8) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 8.768488ms)
    May 17 05:50:39.180: INFO: (8) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.799592ms)
    May 17 05:50:39.180: INFO: (8) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 9.376923ms)
    May 17 05:50:39.180: INFO: (8) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 9.155847ms)
    May 17 05:50:39.180: INFO: (8) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.176448ms)
    May 17 05:50:39.181: INFO: (8) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 9.904068ms)
    May 17 05:50:39.182: INFO: (8) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 10.545284ms)
    May 17 05:50:39.182: INFO: (8) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 10.517352ms)
    May 17 05:50:39.182: INFO: (8) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 11.397769ms)
    May 17 05:50:39.192: INFO: (8) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 20.366168ms)
    May 17 05:50:39.198: INFO: (9) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 6.775273ms)
    May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 11.562195ms)
    May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 11.488932ms)
    May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 11.547479ms)
    May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 11.542239ms)
    May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 11.574511ms)
    May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 11.608924ms)
    May 17 05:50:39.203: INFO: (9) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 11.528224ms)
    May 17 05:50:39.204: INFO: (9) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 11.866493ms)
    May 17 05:50:39.204: INFO: (9) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 11.741622ms)
    May 17 05:50:39.205: INFO: (9) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 13.289053ms)
    May 17 05:50:39.206: INFO: (9) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 14.562462ms)
    May 17 05:50:39.206: INFO: (9) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 14.498305ms)
    May 17 05:50:39.206: INFO: (9) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 14.419829ms)
    May 17 05:50:39.206: INFO: (9) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 14.589816ms)
    May 17 05:50:39.206: INFO: (9) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 14.536675ms)
    May 17 05:50:39.218: INFO: (10) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 11.33367ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 18.84791ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 18.839066ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 18.77743ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 18.987984ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 18.837229ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 18.892496ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 18.802275ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 18.791194ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 18.84695ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 18.84786ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 18.898877ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 18.907493ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 18.861519ms)
    May 17 05:50:39.225: INFO: (10) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 19.020162ms)
    May 17 05:50:39.226: INFO: (10) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 19.762829ms)
    May 17 05:50:39.239: INFO: (11) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 12.06775ms)
    May 17 05:50:39.239: INFO: (11) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 12.144436ms)
    May 17 05:50:39.239: INFO: (11) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 12.164632ms)
    May 17 05:50:39.239: INFO: (11) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 12.293134ms)
    May 17 05:50:39.239: INFO: (11) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 12.10433ms)
    May 17 05:50:39.239: INFO: (11) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 12.096784ms)
    May 17 05:50:39.242: INFO: (11) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 16.113ms)
    May 17 05:50:39.242: INFO: (11) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 16.254265ms)
    May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 16.168696ms)
    May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 16.250226ms)
    May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 16.077995ms)
    May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 16.792691ms)
    May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 16.94879ms)
    May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 17.013692ms)
    May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 16.977927ms)
    May 17 05:50:39.243: INFO: (11) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 17.066873ms)
    May 17 05:50:39.255: INFO: (12) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 11.803899ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 15.092423ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 15.005203ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 15.144749ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 15.190277ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 15.105426ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 15.134532ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 15.064832ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 15.099195ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 15.132677ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 15.163335ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 15.183275ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 15.335907ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 15.381115ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 15.784309ms)
    May 17 05:50:39.259: INFO: (12) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 15.936982ms)
    May 17 05:50:39.268: INFO: (13) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 8.569272ms)
    May 17 05:50:39.268: INFO: (13) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.43496ms)
    May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 15.136504ms)
    May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 15.014288ms)
    May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 14.946511ms)
    May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 15.00072ms)
    May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 14.702898ms)
    May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 14.706119ms)
    May 17 05:50:39.275: INFO: (13) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 14.96354ms)
    May 17 05:50:39.276: INFO: (13) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 15.602335ms)
    May 17 05:50:39.277: INFO: (13) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 17.61612ms)
    May 17 05:50:39.278: INFO: (13) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 17.63793ms)
    May 17 05:50:39.278: INFO: (13) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 17.581876ms)
    May 17 05:50:39.281: INFO: (13) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 20.726612ms)
    May 17 05:50:39.281: INFO: (13) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 20.882391ms)
    May 17 05:50:39.290: INFO: (13) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 29.701166ms)
    May 17 05:50:39.298: INFO: (14) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 8.091662ms)
    May 17 05:50:39.298: INFO: (14) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 8.063829ms)
    May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 12.595109ms)
    May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 12.640812ms)
    May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 12.759243ms)
    May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 12.712217ms)
    May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 12.70958ms)
    May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 12.753162ms)
    May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 12.692369ms)
    May 17 05:50:39.303: INFO: (14) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 12.879097ms)
    May 17 05:50:39.307: INFO: (14) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 16.561461ms)
    May 17 05:50:39.307: INFO: (14) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 16.626553ms)
    May 17 05:50:39.307: INFO: (14) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 16.613867ms)
    May 17 05:50:39.307: INFO: (14) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 16.602827ms)
    May 17 05:50:39.307: INFO: (14) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 16.749652ms)
    May 17 05:50:39.307: INFO: (14) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 16.857934ms)
    May 17 05:50:39.323: INFO: (15) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 16.408972ms)
    May 17 05:50:39.323: INFO: (15) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 16.137815ms)
    May 17 05:50:39.323: INFO: (15) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 16.259397ms)
    May 17 05:50:39.324: INFO: (15) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 17.450518ms)
    May 17 05:50:39.324: INFO: (15) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 17.388904ms)
    May 17 05:50:39.326: INFO: (15) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 19.112426ms)
    May 17 05:50:39.326: INFO: (15) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 19.14023ms)
    May 17 05:50:39.326: INFO: (15) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 19.323068ms)
    May 17 05:50:39.326: INFO: (15) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 19.479191ms)
    May 17 05:50:39.326: INFO: (15) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 19.525139ms)
    May 17 05:50:39.327: INFO: (15) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 19.853513ms)
    May 17 05:50:39.327: INFO: (15) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 19.769582ms)
    May 17 05:50:39.327: INFO: (15) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 19.964059ms)
    May 17 05:50:39.327: INFO: (15) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 19.713727ms)
    May 17 05:50:39.327: INFO: (15) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 20.558316ms)
    May 17 05:50:39.329: INFO: (15) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 22.145433ms)
    May 17 05:50:39.335: INFO: (16) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 6.369271ms)
    May 17 05:50:39.335: INFO: (16) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 6.33999ms)
    May 17 05:50:39.336: INFO: (16) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 6.864277ms)
    May 17 05:50:39.337: INFO: (16) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 7.994309ms)
    May 17 05:50:39.338: INFO: (16) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 8.588401ms)
    May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 9.575925ms)
    May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.498109ms)
    May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.571368ms)
    May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 9.530811ms)
    May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 9.551217ms)
    May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 9.585202ms)
    May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 10.138426ms)
    May 17 05:50:39.339: INFO: (16) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 10.399973ms)
    May 17 05:50:39.341: INFO: (16) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 12.074356ms)
    May 17 05:50:39.341: INFO: (16) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 12.333288ms)
    May 17 05:50:39.342: INFO: (16) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 13.054104ms)
    May 17 05:50:39.348: INFO: (17) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 5.738602ms)
    May 17 05:50:39.356: INFO: (17) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 13.522052ms)
    May 17 05:50:39.356: INFO: (17) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 13.797493ms)
    May 17 05:50:39.356: INFO: (17) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 13.614001ms)
    May 17 05:50:39.356: INFO: (17) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 13.941613ms)
    May 17 05:50:39.357: INFO: (17) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 14.987428ms)
    May 17 05:50:39.357: INFO: (17) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 14.944317ms)
    May 17 05:50:39.357: INFO: (17) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 15.081221ms)
    May 17 05:50:39.357: INFO: (17) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 14.969586ms)
    May 17 05:50:39.357: INFO: (17) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 15.001692ms)
    May 17 05:50:39.357: INFO: (17) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 15.247513ms)
    May 17 05:50:39.358: INFO: (17) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 15.381206ms)
    May 17 05:50:39.358: INFO: (17) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 15.925158ms)
    May 17 05:50:39.358: INFO: (17) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 15.876678ms)
    May 17 05:50:39.360: INFO: (17) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 17.41299ms)
    May 17 05:50:39.360: INFO: (17) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 17.41212ms)
    May 17 05:50:39.368: INFO: (18) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 8.211252ms)
    May 17 05:50:39.368: INFO: (18) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 8.437888ms)
    May 17 05:50:39.369: INFO: (18) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 8.618924ms)
    May 17 05:50:39.370: INFO: (18) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 10.366112ms)
    May 17 05:50:39.370: INFO: (18) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 10.417038ms)
    May 17 05:50:39.370: INFO: (18) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 10.375241ms)
    May 17 05:50:39.370: INFO: (18) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 10.411909ms)
    May 17 05:50:39.371: INFO: (18) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 11.346612ms)
    May 17 05:50:39.371: INFO: (18) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 11.362796ms)
    May 17 05:50:39.373: INFO: (18) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 12.978916ms)
    May 17 05:50:39.373: INFO: (18) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 13.375275ms)
    May 17 05:50:39.375: INFO: (18) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 15.113874ms)
    May 17 05:50:39.375: INFO: (18) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 14.899703ms)
    May 17 05:50:39.375: INFO: (18) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 15.312125ms)
    May 17 05:50:39.375: INFO: (18) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 15.425925ms)
    May 17 05:50:39.376: INFO: (18) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 15.521654ms)
    May 17 05:50:39.383: INFO: (19) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:462/proxy/: tls qux (200; 7.243659ms)
    May 17 05:50:39.383: INFO: (19) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w/proxy/rewriteme">test</a> (200; 7.826606ms)
    May 17 05:50:39.385: INFO: (19) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:443/proxy/tlsrewritem... (200; 9.34881ms)
    May 17 05:50:39.385: INFO: (19) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:162/proxy/: bar (200; 9.406921ms)
    May 17 05:50:39.388: INFO: (19) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:160/proxy/: foo (200; 11.658667ms)
    May 17 05:50:39.388: INFO: (19) /api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/proxy-service-x9556-gfg2w:1080/proxy/rewriteme">test<... (200; 11.607063ms)
    May 17 05:50:39.388: INFO: (19) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/: <a href="/api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:1080/proxy/rewriteme">... (200; 11.576114ms)
    May 17 05:50:39.389: INFO: (19) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname1/proxy/: foo (200; 12.540627ms)
    May 17 05:50:39.389: INFO: (19) /api/v1/namespaces/proxy-2448/pods/https:proxy-service-x9556-gfg2w:460/proxy/: tls baz (200; 12.936897ms)
    May 17 05:50:39.389: INFO: (19) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:160/proxy/: foo (200; 12.829908ms)
    May 17 05:50:39.389: INFO: (19) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname1/proxy/: tls baz (200; 13.055018ms)
    May 17 05:50:39.389: INFO: (19) /api/v1/namespaces/proxy-2448/services/https:proxy-service-x9556:tlsportname2/proxy/: tls qux (200; 13.279468ms)
    May 17 05:50:39.389: INFO: (19) /api/v1/namespaces/proxy-2448/pods/http:proxy-service-x9556-gfg2w:162/proxy/: bar (200; 13.067181ms)
    May 17 05:50:39.391: INFO: (19) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname1/proxy/: foo (200; 15.584443ms)
    May 17 05:50:39.392: INFO: (19) /api/v1/namespaces/proxy-2448/services/http:proxy-service-x9556:portname2/proxy/: bar (200; 16.037911ms)
    May 17 05:50:39.394: INFO: (19) /api/v1/namespaces/proxy-2448/services/proxy-service-x9556:portname2/proxy/: bar (200; 18.377173ms)
    STEP: deleting ReplicationController proxy-service-x9556 in namespace proxy-2448, will wait for the garbage collector to delete the pods 05/17/23 05:50:39.394
    May 17 05:50:39.455: INFO: Deleting ReplicationController proxy-service-x9556 took: 6.77988ms
    May 17 05:50:39.556: INFO: Terminating ReplicationController proxy-service-x9556 pods took: 100.506689ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    May 17 05:50:42.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2448" for this suite. 05/17/23 05:50:42.464
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:50:42.473
May 17 05:50:42.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 05:50:42.474
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:42.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:42.488
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 05/17/23 05:50:42.492
May 17 05:50:42.501: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203" in namespace "projected-325" to be "Succeeded or Failed"
May 17 05:50:42.504: INFO: Pod "downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.457141ms
May 17 05:50:44.510: INFO: Pod "downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009145451s
May 17 05:50:46.509: INFO: Pod "downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007827346s
STEP: Saw pod success 05/17/23 05:50:46.509
May 17 05:50:46.509: INFO: Pod "downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203" satisfied condition "Succeeded or Failed"
May 17 05:50:46.512: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203 container client-container: <nil>
STEP: delete the pod 05/17/23 05:50:46.522
May 17 05:50:46.535: INFO: Waiting for pod downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203 to disappear
May 17 05:50:46.539: INFO: Pod downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May 17 05:50:46.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-325" for this suite. 05/17/23 05:50:46.551
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":56,"skipped":1069,"failed":0}
------------------------------
â€¢ [4.085 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:50:42.473
    May 17 05:50:42.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 05:50:42.474
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:42.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:42.488
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 05/17/23 05:50:42.492
    May 17 05:50:42.501: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203" in namespace "projected-325" to be "Succeeded or Failed"
    May 17 05:50:42.504: INFO: Pod "downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.457141ms
    May 17 05:50:44.510: INFO: Pod "downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009145451s
    May 17 05:50:46.509: INFO: Pod "downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007827346s
    STEP: Saw pod success 05/17/23 05:50:46.509
    May 17 05:50:46.509: INFO: Pod "downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203" satisfied condition "Succeeded or Failed"
    May 17 05:50:46.512: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203 container client-container: <nil>
    STEP: delete the pod 05/17/23 05:50:46.522
    May 17 05:50:46.535: INFO: Waiting for pod downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203 to disappear
    May 17 05:50:46.539: INFO: Pod downwardapi-volume-dda6f48e-9016-4f59-8e6a-4ec6899e1203 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May 17 05:50:46.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-325" for this suite. 05/17/23 05:50:46.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:50:46.559
May 17 05:50:46.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename var-expansion 05/17/23 05:50:46.559
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:46.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:46.581
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 05/17/23 05:50:46.585
May 17 05:50:46.599: INFO: Waiting up to 5m0s for pod "var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f" in namespace "var-expansion-5017" to be "Succeeded or Failed"
May 17 05:50:46.602: INFO: Pod "var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.674616ms
May 17 05:50:48.608: INFO: Pod "var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00893349s
May 17 05:50:50.612: INFO: Pod "var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013295792s
STEP: Saw pod success 05/17/23 05:50:50.612
May 17 05:50:50.612: INFO: Pod "var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f" satisfied condition "Succeeded or Failed"
May 17 05:50:50.616: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f container dapi-container: <nil>
STEP: delete the pod 05/17/23 05:50:50.627
May 17 05:50:50.639: INFO: Waiting for pod var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f to disappear
May 17 05:50:50.642: INFO: Pod var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May 17 05:50:50.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5017" for this suite. 05/17/23 05:50:50.648
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":57,"skipped":1083,"failed":0}
------------------------------
â€¢ [4.096 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:50:46.559
    May 17 05:50:46.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename var-expansion 05/17/23 05:50:46.559
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:46.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:46.581
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 05/17/23 05:50:46.585
    May 17 05:50:46.599: INFO: Waiting up to 5m0s for pod "var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f" in namespace "var-expansion-5017" to be "Succeeded or Failed"
    May 17 05:50:46.602: INFO: Pod "var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.674616ms
    May 17 05:50:48.608: INFO: Pod "var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00893349s
    May 17 05:50:50.612: INFO: Pod "var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013295792s
    STEP: Saw pod success 05/17/23 05:50:50.612
    May 17 05:50:50.612: INFO: Pod "var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f" satisfied condition "Succeeded or Failed"
    May 17 05:50:50.616: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f container dapi-container: <nil>
    STEP: delete the pod 05/17/23 05:50:50.627
    May 17 05:50:50.639: INFO: Waiting for pod var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f to disappear
    May 17 05:50:50.642: INFO: Pod var-expansion-fdf444c9-d95e-4dbf-ae93-88219d9ed23f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May 17 05:50:50.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5017" for this suite. 05/17/23 05:50:50.648
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:50:50.655
May 17 05:50:50.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 05:50:50.656
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:50.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:50.675
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-4aebbd30-6d53-4bb0-b814-ec3693ba2ffa 05/17/23 05:50:50.697
STEP: Creating a pod to test consume secrets 05/17/23 05:50:50.702
May 17 05:50:50.714: INFO: Waiting up to 5m0s for pod "pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1" in namespace "secrets-7318" to be "Succeeded or Failed"
May 17 05:50:50.719: INFO: Pod "pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.040679ms
May 17 05:50:52.724: INFO: Pod "pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1": Phase="Running", Reason="", readiness=false. Elapsed: 2.009818255s
May 17 05:50:54.725: INFO: Pod "pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011271406s
STEP: Saw pod success 05/17/23 05:50:54.725
May 17 05:50:54.725: INFO: Pod "pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1" satisfied condition "Succeeded or Failed"
May 17 05:50:54.729: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1 container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 05:50:54.742
May 17 05:50:54.756: INFO: Waiting for pod pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1 to disappear
May 17 05:50:54.760: INFO: Pod pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May 17 05:50:54.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7318" for this suite. 05/17/23 05:50:54.766
STEP: Destroying namespace "secret-namespace-3135" for this suite. 05/17/23 05:50:54.773
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":58,"skipped":1087,"failed":0}
------------------------------
â€¢ [4.126 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:50:50.655
    May 17 05:50:50.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 05:50:50.656
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:50.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:50.675
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-4aebbd30-6d53-4bb0-b814-ec3693ba2ffa 05/17/23 05:50:50.697
    STEP: Creating a pod to test consume secrets 05/17/23 05:50:50.702
    May 17 05:50:50.714: INFO: Waiting up to 5m0s for pod "pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1" in namespace "secrets-7318" to be "Succeeded or Failed"
    May 17 05:50:50.719: INFO: Pod "pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.040679ms
    May 17 05:50:52.724: INFO: Pod "pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1": Phase="Running", Reason="", readiness=false. Elapsed: 2.009818255s
    May 17 05:50:54.725: INFO: Pod "pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011271406s
    STEP: Saw pod success 05/17/23 05:50:54.725
    May 17 05:50:54.725: INFO: Pod "pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1" satisfied condition "Succeeded or Failed"
    May 17 05:50:54.729: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1 container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 05:50:54.742
    May 17 05:50:54.756: INFO: Waiting for pod pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1 to disappear
    May 17 05:50:54.760: INFO: Pod pod-secrets-1a6a5160-df93-43b1-a728-6715cb1740c1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May 17 05:50:54.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7318" for this suite. 05/17/23 05:50:54.766
    STEP: Destroying namespace "secret-namespace-3135" for this suite. 05/17/23 05:50:54.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:50:54.782
May 17 05:50:54.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 05:50:54.782
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:54.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:54.805
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-ea725440-a42d-4685-a11c-ab605ae319c1 05/17/23 05:50:54.808
STEP: Creating a pod to test consume secrets 05/17/23 05:50:54.814
May 17 05:50:54.826: INFO: Waiting up to 5m0s for pod "pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab" in namespace "secrets-8723" to be "Succeeded or Failed"
May 17 05:50:54.830: INFO: Pod "pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.917523ms
May 17 05:50:56.836: INFO: Pod "pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009587222s
May 17 05:50:58.836: INFO: Pod "pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009898606s
STEP: Saw pod success 05/17/23 05:50:58.836
May 17 05:50:58.836: INFO: Pod "pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab" satisfied condition "Succeeded or Failed"
May 17 05:50:58.840: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 05:50:58.85
May 17 05:50:58.863: INFO: Waiting for pod pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab to disappear
May 17 05:50:58.867: INFO: Pod pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May 17 05:50:58.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8723" for this suite. 05/17/23 05:50:58.874
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":59,"skipped":1109,"failed":0}
------------------------------
â€¢ [4.099 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:50:54.782
    May 17 05:50:54.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 05:50:54.782
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:54.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:54.805
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-ea725440-a42d-4685-a11c-ab605ae319c1 05/17/23 05:50:54.808
    STEP: Creating a pod to test consume secrets 05/17/23 05:50:54.814
    May 17 05:50:54.826: INFO: Waiting up to 5m0s for pod "pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab" in namespace "secrets-8723" to be "Succeeded or Failed"
    May 17 05:50:54.830: INFO: Pod "pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.917523ms
    May 17 05:50:56.836: INFO: Pod "pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009587222s
    May 17 05:50:58.836: INFO: Pod "pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009898606s
    STEP: Saw pod success 05/17/23 05:50:58.836
    May 17 05:50:58.836: INFO: Pod "pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab" satisfied condition "Succeeded or Failed"
    May 17 05:50:58.840: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 05:50:58.85
    May 17 05:50:58.863: INFO: Waiting for pod pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab to disappear
    May 17 05:50:58.867: INFO: Pod pod-secrets-145cc78e-f776-4246-b15b-75eb620476ab no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May 17 05:50:58.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8723" for this suite. 05/17/23 05:50:58.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:50:58.881
May 17 05:50:58.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename sysctl 05/17/23 05:50:58.882
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:58.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:58.902
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/17/23 05:50:58.905
STEP: Watching for error events or started pod 05/17/23 05:50:58.921
STEP: Waiting for pod completion 05/17/23 05:51:00.929
May 17 05:51:00.929: INFO: Waiting up to 3m0s for pod "sysctl-98902d8c-fb33-4c84-8bc3-3e72e102759b" in namespace "sysctl-8405" to be "completed"
May 17 05:51:00.933: INFO: Pod "sysctl-98902d8c-fb33-4c84-8bc3-3e72e102759b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282541ms
May 17 05:51:02.939: INFO: Pod "sysctl-98902d8c-fb33-4c84-8bc3-3e72e102759b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010247686s
May 17 05:51:02.939: INFO: Pod "sysctl-98902d8c-fb33-4c84-8bc3-3e72e102759b" satisfied condition "completed"
STEP: Checking that the pod succeeded 05/17/23 05:51:02.943
STEP: Getting logs from the pod 05/17/23 05:51:02.943
STEP: Checking that the sysctl is actually updated 05/17/23 05:51:02.952
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
May 17 05:51:02.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8405" for this suite. 05/17/23 05:51:02.958
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":60,"skipped":1123,"failed":0}
------------------------------
â€¢ [4.083 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:50:58.881
    May 17 05:50:58.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename sysctl 05/17/23 05:50:58.882
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:50:58.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:50:58.902
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/17/23 05:50:58.905
    STEP: Watching for error events or started pod 05/17/23 05:50:58.921
    STEP: Waiting for pod completion 05/17/23 05:51:00.929
    May 17 05:51:00.929: INFO: Waiting up to 3m0s for pod "sysctl-98902d8c-fb33-4c84-8bc3-3e72e102759b" in namespace "sysctl-8405" to be "completed"
    May 17 05:51:00.933: INFO: Pod "sysctl-98902d8c-fb33-4c84-8bc3-3e72e102759b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282541ms
    May 17 05:51:02.939: INFO: Pod "sysctl-98902d8c-fb33-4c84-8bc3-3e72e102759b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010247686s
    May 17 05:51:02.939: INFO: Pod "sysctl-98902d8c-fb33-4c84-8bc3-3e72e102759b" satisfied condition "completed"
    STEP: Checking that the pod succeeded 05/17/23 05:51:02.943
    STEP: Getting logs from the pod 05/17/23 05:51:02.943
    STEP: Checking that the sysctl is actually updated 05/17/23 05:51:02.952
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    May 17 05:51:02.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-8405" for this suite. 05/17/23 05:51:02.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:51:02.965
May 17 05:51:02.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 05:51:02.965
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:51:02.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:51:02.986
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
May 17 05:51:02.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/17/23 05:51:12.867
May 17 05:51:12.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1517 --namespace=crd-publish-openapi-1517 create -f -'
May 17 05:51:14.275: INFO: stderr: ""
May 17 05:51:14.275: INFO: stdout: "e2e-test-crd-publish-openapi-2503-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 17 05:51:14.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1517 --namespace=crd-publish-openapi-1517 delete e2e-test-crd-publish-openapi-2503-crds test-cr'
May 17 05:51:14.418: INFO: stderr: ""
May 17 05:51:14.418: INFO: stdout: "e2e-test-crd-publish-openapi-2503-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 17 05:51:14.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1517 --namespace=crd-publish-openapi-1517 apply -f -'
May 17 05:51:15.692: INFO: stderr: ""
May 17 05:51:15.692: INFO: stdout: "e2e-test-crd-publish-openapi-2503-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 17 05:51:15.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1517 --namespace=crd-publish-openapi-1517 delete e2e-test-crd-publish-openapi-2503-crds test-cr'
May 17 05:51:15.762: INFO: stderr: ""
May 17 05:51:15.762: INFO: stdout: "e2e-test-crd-publish-openapi-2503-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/17/23 05:51:15.762
May 17 05:51:15.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1517 explain e2e-test-crd-publish-openapi-2503-crds'
May 17 05:51:17.181: INFO: stderr: ""
May 17 05:51:17.181: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2503-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 05:51:26.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1517" for this suite. 05/17/23 05:51:26.698
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":61,"skipped":1129,"failed":0}
------------------------------
â€¢ [SLOW TEST] [23.748 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:51:02.965
    May 17 05:51:02.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 05:51:02.965
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:51:02.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:51:02.986
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    May 17 05:51:02.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/17/23 05:51:12.867
    May 17 05:51:12.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1517 --namespace=crd-publish-openapi-1517 create -f -'
    May 17 05:51:14.275: INFO: stderr: ""
    May 17 05:51:14.275: INFO: stdout: "e2e-test-crd-publish-openapi-2503-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May 17 05:51:14.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1517 --namespace=crd-publish-openapi-1517 delete e2e-test-crd-publish-openapi-2503-crds test-cr'
    May 17 05:51:14.418: INFO: stderr: ""
    May 17 05:51:14.418: INFO: stdout: "e2e-test-crd-publish-openapi-2503-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    May 17 05:51:14.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1517 --namespace=crd-publish-openapi-1517 apply -f -'
    May 17 05:51:15.692: INFO: stderr: ""
    May 17 05:51:15.692: INFO: stdout: "e2e-test-crd-publish-openapi-2503-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May 17 05:51:15.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1517 --namespace=crd-publish-openapi-1517 delete e2e-test-crd-publish-openapi-2503-crds test-cr'
    May 17 05:51:15.762: INFO: stderr: ""
    May 17 05:51:15.762: INFO: stdout: "e2e-test-crd-publish-openapi-2503-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/17/23 05:51:15.762
    May 17 05:51:15.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1517 explain e2e-test-crd-publish-openapi-2503-crds'
    May 17 05:51:17.181: INFO: stderr: ""
    May 17 05:51:17.181: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2503-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 05:51:26.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1517" for this suite. 05/17/23 05:51:26.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:51:26.714
May 17 05:51:26.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 05:51:26.715
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:51:26.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:51:26.756
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-658234f8-ce3f-4a89-9ac5-e939bc82ef96 05/17/23 05:51:26.767
STEP: Creating the pod 05/17/23 05:51:26.774
May 17 05:51:26.793: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7ec7b21-6d7a-4e65-95c3-470acbbb2e9f" in namespace "configmap-7051" to be "running"
May 17 05:51:26.799: INFO: Pod "pod-configmaps-a7ec7b21-6d7a-4e65-95c3-470acbbb2e9f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055225ms
May 17 05:51:28.807: INFO: Pod "pod-configmaps-a7ec7b21-6d7a-4e65-95c3-470acbbb2e9f": Phase="Running", Reason="", readiness=false. Elapsed: 2.013910121s
May 17 05:51:28.807: INFO: Pod "pod-configmaps-a7ec7b21-6d7a-4e65-95c3-470acbbb2e9f" satisfied condition "running"
STEP: Waiting for pod with text data 05/17/23 05:51:28.807
STEP: Waiting for pod with binary data 05/17/23 05:51:28.82
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May 17 05:51:28.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7051" for this suite. 05/17/23 05:51:28.841
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":62,"skipped":1157,"failed":0}
------------------------------
â€¢ [2.135 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:51:26.714
    May 17 05:51:26.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 05:51:26.715
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:51:26.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:51:26.756
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-658234f8-ce3f-4a89-9ac5-e939bc82ef96 05/17/23 05:51:26.767
    STEP: Creating the pod 05/17/23 05:51:26.774
    May 17 05:51:26.793: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7ec7b21-6d7a-4e65-95c3-470acbbb2e9f" in namespace "configmap-7051" to be "running"
    May 17 05:51:26.799: INFO: Pod "pod-configmaps-a7ec7b21-6d7a-4e65-95c3-470acbbb2e9f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055225ms
    May 17 05:51:28.807: INFO: Pod "pod-configmaps-a7ec7b21-6d7a-4e65-95c3-470acbbb2e9f": Phase="Running", Reason="", readiness=false. Elapsed: 2.013910121s
    May 17 05:51:28.807: INFO: Pod "pod-configmaps-a7ec7b21-6d7a-4e65-95c3-470acbbb2e9f" satisfied condition "running"
    STEP: Waiting for pod with text data 05/17/23 05:51:28.807
    STEP: Waiting for pod with binary data 05/17/23 05:51:28.82
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 05:51:28.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7051" for this suite. 05/17/23 05:51:28.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:51:28.85
May 17 05:51:28.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename svcaccounts 05/17/23 05:51:28.851
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:51:28.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:51:28.875
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
May 17 05:51:28.902: INFO: Waiting up to 5m0s for pod "pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216" in namespace "svcaccounts-1463" to be "running"
May 17 05:51:28.908: INFO: Pod "pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216": Phase="Pending", Reason="", readiness=false. Elapsed: 6.203304ms
May 17 05:51:30.914: INFO: Pod "pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216": Phase="Running", Reason="", readiness=true. Elapsed: 2.012040166s
May 17 05:51:30.914: INFO: Pod "pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216" satisfied condition "running"
STEP: reading a file in the container 05/17/23 05:51:30.914
May 17 05:51:30.914: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1463 pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 05/17/23 05:51:31.091
May 17 05:51:31.091: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1463 pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 05/17/23 05:51:31.283
May 17 05:51:31.283: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1463 pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
May 17 05:51:31.448: INFO: Got root ca configmap in namespace "svcaccounts-1463"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May 17 05:51:31.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1463" for this suite. 05/17/23 05:51:31.465
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":63,"skipped":1166,"failed":0}
------------------------------
â€¢ [2.623 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:51:28.85
    May 17 05:51:28.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 05:51:28.851
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:51:28.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:51:28.875
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    May 17 05:51:28.902: INFO: Waiting up to 5m0s for pod "pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216" in namespace "svcaccounts-1463" to be "running"
    May 17 05:51:28.908: INFO: Pod "pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216": Phase="Pending", Reason="", readiness=false. Elapsed: 6.203304ms
    May 17 05:51:30.914: INFO: Pod "pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216": Phase="Running", Reason="", readiness=true. Elapsed: 2.012040166s
    May 17 05:51:30.914: INFO: Pod "pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216" satisfied condition "running"
    STEP: reading a file in the container 05/17/23 05:51:30.914
    May 17 05:51:30.914: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1463 pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 05/17/23 05:51:31.091
    May 17 05:51:31.091: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1463 pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 05/17/23 05:51:31.283
    May 17 05:51:31.283: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1463 pod-service-account-b6ccce67-ba87-416d-ad75-721c96f46216 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    May 17 05:51:31.448: INFO: Got root ca configmap in namespace "svcaccounts-1463"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May 17 05:51:31.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1463" for this suite. 05/17/23 05:51:31.465
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:51:31.474
May 17 05:51:31.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 05:51:31.474
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:51:31.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:51:31.503
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/17/23 05:51:31.507
May 17 05:51:31.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 05:51:41.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 05:52:13.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1987" for this suite. 05/17/23 05:52:13.237
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":64,"skipped":1166,"failed":0}
------------------------------
â€¢ [SLOW TEST] [41.770 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:51:31.474
    May 17 05:51:31.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 05:51:31.474
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:51:31.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:51:31.503
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/17/23 05:51:31.507
    May 17 05:51:31.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 05:51:41.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 05:52:13.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1987" for this suite. 05/17/23 05:52:13.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:52:13.244
May 17 05:52:13.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 05:52:13.245
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:13.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:13.262
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/17/23 05:52:13.266
May 17 05:52:13.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-899 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
May 17 05:52:13.338: INFO: stderr: ""
May 17 05:52:13.338: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 05/17/23 05:52:13.338
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
May 17 05:52:13.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-899 delete pods e2e-test-httpd-pod'
May 17 05:52:15.524: INFO: stderr: ""
May 17 05:52:15.524: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 05:52:15.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-899" for this suite. 05/17/23 05:52:15.531
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":65,"skipped":1173,"failed":0}
------------------------------
â€¢ [2.293 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:52:13.244
    May 17 05:52:13.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 05:52:13.245
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:13.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:13.262
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/17/23 05:52:13.266
    May 17 05:52:13.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-899 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    May 17 05:52:13.338: INFO: stderr: ""
    May 17 05:52:13.338: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 05/17/23 05:52:13.338
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    May 17 05:52:13.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-899 delete pods e2e-test-httpd-pod'
    May 17 05:52:15.524: INFO: stderr: ""
    May 17 05:52:15.524: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 05:52:15.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-899" for this suite. 05/17/23 05:52:15.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:52:15.538
May 17 05:52:15.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubelet-test 05/17/23 05:52:15.538
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:15.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:15.554
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
May 17 05:52:15.568: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs2b085ed5-3eba-47f6-9f93-d6652319dac0" in namespace "kubelet-test-6977" to be "running and ready"
May 17 05:52:15.572: INFO: Pod "busybox-readonly-fs2b085ed5-3eba-47f6-9f93-d6652319dac0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.407326ms
May 17 05:52:15.572: INFO: The phase of Pod busybox-readonly-fs2b085ed5-3eba-47f6-9f93-d6652319dac0 is Pending, waiting for it to be Running (with Ready = true)
May 17 05:52:17.589: INFO: Pod "busybox-readonly-fs2b085ed5-3eba-47f6-9f93-d6652319dac0": Phase="Running", Reason="", readiness=true. Elapsed: 2.02136165s
May 17 05:52:17.589: INFO: The phase of Pod busybox-readonly-fs2b085ed5-3eba-47f6-9f93-d6652319dac0 is Running (Ready = true)
May 17 05:52:17.589: INFO: Pod "busybox-readonly-fs2b085ed5-3eba-47f6-9f93-d6652319dac0" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May 17 05:52:17.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6977" for this suite. 05/17/23 05:52:17.609
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":66,"skipped":1179,"failed":0}
------------------------------
â€¢ [2.091 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:52:15.538
    May 17 05:52:15.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubelet-test 05/17/23 05:52:15.538
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:15.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:15.554
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    May 17 05:52:15.568: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs2b085ed5-3eba-47f6-9f93-d6652319dac0" in namespace "kubelet-test-6977" to be "running and ready"
    May 17 05:52:15.572: INFO: Pod "busybox-readonly-fs2b085ed5-3eba-47f6-9f93-d6652319dac0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.407326ms
    May 17 05:52:15.572: INFO: The phase of Pod busybox-readonly-fs2b085ed5-3eba-47f6-9f93-d6652319dac0 is Pending, waiting for it to be Running (with Ready = true)
    May 17 05:52:17.589: INFO: Pod "busybox-readonly-fs2b085ed5-3eba-47f6-9f93-d6652319dac0": Phase="Running", Reason="", readiness=true. Elapsed: 2.02136165s
    May 17 05:52:17.589: INFO: The phase of Pod busybox-readonly-fs2b085ed5-3eba-47f6-9f93-d6652319dac0 is Running (Ready = true)
    May 17 05:52:17.589: INFO: Pod "busybox-readonly-fs2b085ed5-3eba-47f6-9f93-d6652319dac0" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May 17 05:52:17.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6977" for this suite. 05/17/23 05:52:17.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:52:17.63
May 17 05:52:17.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename gc 05/17/23 05:52:17.63
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:17.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:17.645
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 05/17/23 05:52:17.649
STEP: Wait for the Deployment to create new ReplicaSet 05/17/23 05:52:17.654
STEP: delete the deployment 05/17/23 05:52:18.162
STEP: wait for all rs to be garbage collected 05/17/23 05:52:18.168
STEP: expected 0 rs, got 1 rs 05/17/23 05:52:18.174
STEP: expected 0 pods, got 2 pods 05/17/23 05:52:18.178
STEP: Gathering metrics 05/17/23 05:52:18.689
W0517 05:52:18.700898      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 17 05:52:18.700: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May 17 05:52:18.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8692" for this suite. 05/17/23 05:52:18.707
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":67,"skipped":1205,"failed":0}
------------------------------
â€¢ [1.084 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:52:17.63
    May 17 05:52:17.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename gc 05/17/23 05:52:17.63
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:17.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:17.645
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 05/17/23 05:52:17.649
    STEP: Wait for the Deployment to create new ReplicaSet 05/17/23 05:52:17.654
    STEP: delete the deployment 05/17/23 05:52:18.162
    STEP: wait for all rs to be garbage collected 05/17/23 05:52:18.168
    STEP: expected 0 rs, got 1 rs 05/17/23 05:52:18.174
    STEP: expected 0 pods, got 2 pods 05/17/23 05:52:18.178
    STEP: Gathering metrics 05/17/23 05:52:18.689
    W0517 05:52:18.700898      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May 17 05:52:18.700: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May 17 05:52:18.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8692" for this suite. 05/17/23 05:52:18.707
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:52:18.714
May 17 05:52:18.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename namespaces 05/17/23 05:52:18.714
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:18.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:18.73
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 05/17/23 05:52:18.733
May 17 05:52:18.739: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 05/17/23 05:52:18.739
May 17 05:52:18.747: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 05/17/23 05:52:18.747
May 17 05:52:18.756: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May 17 05:52:18.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1415" for this suite. 05/17/23 05:52:18.762
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":68,"skipped":1209,"failed":0}
------------------------------
â€¢ [0.054 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:52:18.714
    May 17 05:52:18.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename namespaces 05/17/23 05:52:18.714
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:18.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:18.73
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 05/17/23 05:52:18.733
    May 17 05:52:18.739: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 05/17/23 05:52:18.739
    May 17 05:52:18.747: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 05/17/23 05:52:18.747
    May 17 05:52:18.756: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May 17 05:52:18.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1415" for this suite. 05/17/23 05:52:18.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:52:18.768
May 17 05:52:18.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename deployment 05/17/23 05:52:18.769
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:18.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:18.783
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
May 17 05:52:18.787: INFO: Creating deployment "test-recreate-deployment"
May 17 05:52:18.792: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 17 05:52:18.799: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 17 05:52:20.805: INFO: Waiting deployment "test-recreate-deployment" to complete
May 17 05:52:20.808: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 17 05:52:20.818: INFO: Updating deployment test-recreate-deployment
May 17 05:52:20.818: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 05:52:20.890: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3082  42b18812-67c7-4036-8323-87c07d32dfa2 1442392 2 2023-05-17 05:52:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a97658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-17 05:52:20 +0000 UTC,LastTransitionTime:2023-05-17 05:52:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-05-17 05:52:20 +0000 UTC,LastTransitionTime:2023-05-17 05:52:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 17 05:52:20.894: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-3082  05266c51-e253-4406-9662-9c9f4652acff 1442389 1 2023-05-17 05:52:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 42b18812-67c7-4036-8323-87c07d32dfa2 0xc00826f7d0 0xc00826f7d1}] [] [{kube-controller-manager Update apps/v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"42b18812-67c7-4036-8323-87c07d32dfa2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00826f878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 05:52:20.894: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 17 05:52:20.894: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-3082  ce526249-9cbb-4dd0-bdcf-080ed40d30f2 1442383 2 2023-05-17 05:52:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 42b18812-67c7-4036-8323-87c07d32dfa2 0xc00826f687 0xc00826f688}] [] [{kube-controller-manager Update apps/v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"42b18812-67c7-4036-8323-87c07d32dfa2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00826f758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 05:52:20.898: INFO: Pod "test-recreate-deployment-9d58999df-dnjjj" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-dnjjj test-recreate-deployment-9d58999df- deployment-3082  3f233881-3591-4a18-9af7-6ed436f9ab81 1442391 0 2023-05-17 05:52:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 05266c51-e253-4406-9662-9c9f4652acff 0xc00826fd80 0xc00826fd81}] [] [{kube-controller-manager Update v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05266c51-e253-4406-9662-9c9f4652acff\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnhfx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnhfx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 05:52:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 05:52:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 05:52:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 05:52:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:,StartTime:2023-05-17 05:52:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May 17 05:52:20.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3082" for this suite. 05/17/23 05:52:20.905
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":69,"skipped":1215,"failed":0}
------------------------------
â€¢ [2.142 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:52:18.768
    May 17 05:52:18.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename deployment 05/17/23 05:52:18.769
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:18.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:18.783
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    May 17 05:52:18.787: INFO: Creating deployment "test-recreate-deployment"
    May 17 05:52:18.792: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    May 17 05:52:18.799: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    May 17 05:52:20.805: INFO: Waiting deployment "test-recreate-deployment" to complete
    May 17 05:52:20.808: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    May 17 05:52:20.818: INFO: Updating deployment test-recreate-deployment
    May 17 05:52:20.818: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 05:52:20.890: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3082  42b18812-67c7-4036-8323-87c07d32dfa2 1442392 2 2023-05-17 05:52:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a97658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-17 05:52:20 +0000 UTC,LastTransitionTime:2023-05-17 05:52:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-05-17 05:52:20 +0000 UTC,LastTransitionTime:2023-05-17 05:52:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    May 17 05:52:20.894: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-3082  05266c51-e253-4406-9662-9c9f4652acff 1442389 1 2023-05-17 05:52:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 42b18812-67c7-4036-8323-87c07d32dfa2 0xc00826f7d0 0xc00826f7d1}] [] [{kube-controller-manager Update apps/v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"42b18812-67c7-4036-8323-87c07d32dfa2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00826f878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 17 05:52:20.894: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    May 17 05:52:20.894: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-3082  ce526249-9cbb-4dd0-bdcf-080ed40d30f2 1442383 2 2023-05-17 05:52:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 42b18812-67c7-4036-8323-87c07d32dfa2 0xc00826f687 0xc00826f688}] [] [{kube-controller-manager Update apps/v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"42b18812-67c7-4036-8323-87c07d32dfa2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00826f758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 17 05:52:20.898: INFO: Pod "test-recreate-deployment-9d58999df-dnjjj" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-dnjjj test-recreate-deployment-9d58999df- deployment-3082  3f233881-3591-4a18-9af7-6ed436f9ab81 1442391 0 2023-05-17 05:52:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 05266c51-e253-4406-9662-9c9f4652acff 0xc00826fd80 0xc00826fd81}] [] [{kube-controller-manager Update v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05266c51-e253-4406-9662-9c9f4652acff\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 05:52:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnhfx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnhfx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 05:52:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 05:52:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 05:52:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 05:52:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:,StartTime:2023-05-17 05:52:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May 17 05:52:20.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3082" for this suite. 05/17/23 05:52:20.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:52:20.912
May 17 05:52:20.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 05:52:20.912
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:20.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:20.926
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-ff3a07c3-a02f-43f6-b22c-01e5d335af0a 05/17/23 05:52:20.93
STEP: Creating a pod to test consume configMaps 05/17/23 05:52:20.934
May 17 05:52:20.943: INFO: Waiting up to 5m0s for pod "pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2" in namespace "configmap-6729" to be "Succeeded or Failed"
May 17 05:52:20.946: INFO: Pod "pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.34428ms
May 17 05:52:22.951: INFO: Pod "pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008613358s
May 17 05:52:24.952: INFO: Pod "pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009377235s
STEP: Saw pod success 05/17/23 05:52:24.952
May 17 05:52:24.952: INFO: Pod "pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2" satisfied condition "Succeeded or Failed"
May 17 05:52:24.956: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 05:52:24.964
May 17 05:52:24.973: INFO: Waiting for pod pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2 to disappear
May 17 05:52:24.977: INFO: Pod pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May 17 05:52:24.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6729" for this suite. 05/17/23 05:52:24.984
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":70,"skipped":1242,"failed":0}
------------------------------
â€¢ [4.079 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:52:20.912
    May 17 05:52:20.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 05:52:20.912
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:20.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:20.926
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-ff3a07c3-a02f-43f6-b22c-01e5d335af0a 05/17/23 05:52:20.93
    STEP: Creating a pod to test consume configMaps 05/17/23 05:52:20.934
    May 17 05:52:20.943: INFO: Waiting up to 5m0s for pod "pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2" in namespace "configmap-6729" to be "Succeeded or Failed"
    May 17 05:52:20.946: INFO: Pod "pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.34428ms
    May 17 05:52:22.951: INFO: Pod "pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008613358s
    May 17 05:52:24.952: INFO: Pod "pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009377235s
    STEP: Saw pod success 05/17/23 05:52:24.952
    May 17 05:52:24.952: INFO: Pod "pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2" satisfied condition "Succeeded or Failed"
    May 17 05:52:24.956: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 05:52:24.964
    May 17 05:52:24.973: INFO: Waiting for pod pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2 to disappear
    May 17 05:52:24.977: INFO: Pod pod-configmaps-5dc95cfa-fb1e-478e-ac0a-c146d9b6d7a2 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 05:52:24.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6729" for this suite. 05/17/23 05:52:24.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:52:24.992
May 17 05:52:24.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename prestop 05/17/23 05:52:24.993
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:25.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:25.016
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9724 05/17/23 05:52:25.02
STEP: Waiting for pods to come up. 05/17/23 05:52:25.029
May 17 05:52:25.029: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9724" to be "running"
May 17 05:52:25.032: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.508587ms
May 17 05:52:27.036: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.007492969s
May 17 05:52:27.036: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9724 05/17/23 05:52:27.04
May 17 05:52:27.057: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9724" to be "running"
May 17 05:52:27.060: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369219ms
May 17 05:52:29.065: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.008069268s
May 17 05:52:29.065: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 05/17/23 05:52:29.065
May 17 05:52:34.088: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 05/17/23 05:52:34.088
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
May 17 05:52:34.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9724" for this suite. 05/17/23 05:52:34.107
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":71,"skipped":1266,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.120 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:52:24.992
    May 17 05:52:24.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename prestop 05/17/23 05:52:24.993
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:25.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:25.016
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9724 05/17/23 05:52:25.02
    STEP: Waiting for pods to come up. 05/17/23 05:52:25.029
    May 17 05:52:25.029: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9724" to be "running"
    May 17 05:52:25.032: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.508587ms
    May 17 05:52:27.036: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.007492969s
    May 17 05:52:27.036: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9724 05/17/23 05:52:27.04
    May 17 05:52:27.057: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9724" to be "running"
    May 17 05:52:27.060: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369219ms
    May 17 05:52:29.065: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.008069268s
    May 17 05:52:29.065: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 05/17/23 05:52:29.065
    May 17 05:52:34.088: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 05/17/23 05:52:34.088
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    May 17 05:52:34.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-9724" for this suite. 05/17/23 05:52:34.107
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:52:34.113
May 17 05:52:34.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename subpath 05/17/23 05:52:34.114
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:34.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:34.134
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/17/23 05:52:34.137
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-kv4x 05/17/23 05:52:34.146
STEP: Creating a pod to test atomic-volume-subpath 05/17/23 05:52:34.146
May 17 05:52:34.157: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-kv4x" in namespace "subpath-7976" to be "Succeeded or Failed"
May 17 05:52:34.160: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Pending", Reason="", readiness=false. Elapsed: 3.339321ms
May 17 05:52:36.165: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 2.008829638s
May 17 05:52:38.164: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 4.007751963s
May 17 05:52:40.166: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 6.009140956s
May 17 05:52:42.165: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 8.00841065s
May 17 05:52:44.165: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 10.008361606s
May 17 05:52:46.165: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 12.008272767s
May 17 05:52:48.167: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 14.010113244s
May 17 05:52:50.166: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 16.009323975s
May 17 05:52:52.165: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 18.008281472s
May 17 05:52:54.167: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 20.009931247s
May 17 05:52:56.165: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=false. Elapsed: 22.008176341s
May 17 05:52:58.166: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009098962s
STEP: Saw pod success 05/17/23 05:52:58.166
May 17 05:52:58.166: INFO: Pod "pod-subpath-test-projected-kv4x" satisfied condition "Succeeded or Failed"
May 17 05:52:58.171: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-subpath-test-projected-kv4x container test-container-subpath-projected-kv4x: <nil>
STEP: delete the pod 05/17/23 05:52:58.18
May 17 05:52:58.193: INFO: Waiting for pod pod-subpath-test-projected-kv4x to disappear
May 17 05:52:58.196: INFO: Pod pod-subpath-test-projected-kv4x no longer exists
STEP: Deleting pod pod-subpath-test-projected-kv4x 05/17/23 05:52:58.197
May 17 05:52:58.197: INFO: Deleting pod "pod-subpath-test-projected-kv4x" in namespace "subpath-7976"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May 17 05:52:58.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7976" for this suite. 05/17/23 05:52:58.207
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":72,"skipped":1267,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.100 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:52:34.113
    May 17 05:52:34.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename subpath 05/17/23 05:52:34.114
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:34.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:34.134
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/17/23 05:52:34.137
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-kv4x 05/17/23 05:52:34.146
    STEP: Creating a pod to test atomic-volume-subpath 05/17/23 05:52:34.146
    May 17 05:52:34.157: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-kv4x" in namespace "subpath-7976" to be "Succeeded or Failed"
    May 17 05:52:34.160: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Pending", Reason="", readiness=false. Elapsed: 3.339321ms
    May 17 05:52:36.165: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 2.008829638s
    May 17 05:52:38.164: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 4.007751963s
    May 17 05:52:40.166: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 6.009140956s
    May 17 05:52:42.165: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 8.00841065s
    May 17 05:52:44.165: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 10.008361606s
    May 17 05:52:46.165: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 12.008272767s
    May 17 05:52:48.167: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 14.010113244s
    May 17 05:52:50.166: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 16.009323975s
    May 17 05:52:52.165: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 18.008281472s
    May 17 05:52:54.167: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=true. Elapsed: 20.009931247s
    May 17 05:52:56.165: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Running", Reason="", readiness=false. Elapsed: 22.008176341s
    May 17 05:52:58.166: INFO: Pod "pod-subpath-test-projected-kv4x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009098962s
    STEP: Saw pod success 05/17/23 05:52:58.166
    May 17 05:52:58.166: INFO: Pod "pod-subpath-test-projected-kv4x" satisfied condition "Succeeded or Failed"
    May 17 05:52:58.171: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-subpath-test-projected-kv4x container test-container-subpath-projected-kv4x: <nil>
    STEP: delete the pod 05/17/23 05:52:58.18
    May 17 05:52:58.193: INFO: Waiting for pod pod-subpath-test-projected-kv4x to disappear
    May 17 05:52:58.196: INFO: Pod pod-subpath-test-projected-kv4x no longer exists
    STEP: Deleting pod pod-subpath-test-projected-kv4x 05/17/23 05:52:58.197
    May 17 05:52:58.197: INFO: Deleting pod "pod-subpath-test-projected-kv4x" in namespace "subpath-7976"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May 17 05:52:58.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7976" for this suite. 05/17/23 05:52:58.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:52:58.213
May 17 05:52:58.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 05:52:58.214
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:58.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:58.233
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May 17 05:52:58.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9333" for this suite. 05/17/23 05:52:58.281
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":73,"skipped":1275,"failed":0}
------------------------------
â€¢ [0.075 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:52:58.213
    May 17 05:52:58.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 05:52:58.214
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:58.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:58.233
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 05:52:58.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9333" for this suite. 05/17/23 05:52:58.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:52:58.289
May 17 05:52:58.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename job 05/17/23 05:52:58.289
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:58.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:58.307
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 05/17/23 05:52:58.31
STEP: Ensure pods equal to paralellism count is attached to the job 05/17/23 05:52:58.317
STEP: patching /status 05/17/23 05:53:00.324
STEP: updating /status 05/17/23 05:53:00.331
STEP: get /status 05/17/23 05:53:00.361
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May 17 05:53:00.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7216" for this suite. 05/17/23 05:53:00.378
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":74,"skipped":1282,"failed":0}
------------------------------
â€¢ [2.096 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:52:58.289
    May 17 05:52:58.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename job 05/17/23 05:52:58.289
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:52:58.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:52:58.307
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 05/17/23 05:52:58.31
    STEP: Ensure pods equal to paralellism count is attached to the job 05/17/23 05:52:58.317
    STEP: patching /status 05/17/23 05:53:00.324
    STEP: updating /status 05/17/23 05:53:00.331
    STEP: get /status 05/17/23 05:53:00.361
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May 17 05:53:00.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7216" for this suite. 05/17/23 05:53:00.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:53:00.386
May 17 05:53:00.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 05:53:00.386
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:53:00.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:53:00.405
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 05/17/23 05:53:00.408
May 17 05:53:00.418: INFO: Waiting up to 5m0s for pod "downward-api-736e9796-42f0-4672-b005-493ad8083639" in namespace "downward-api-826" to be "Succeeded or Failed"
May 17 05:53:00.421: INFO: Pod "downward-api-736e9796-42f0-4672-b005-493ad8083639": Phase="Pending", Reason="", readiness=false. Elapsed: 3.528075ms
May 17 05:53:02.425: INFO: Pod "downward-api-736e9796-42f0-4672-b005-493ad8083639": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007398958s
May 17 05:53:04.427: INFO: Pod "downward-api-736e9796-42f0-4672-b005-493ad8083639": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009751999s
STEP: Saw pod success 05/17/23 05:53:04.427
May 17 05:53:04.428: INFO: Pod "downward-api-736e9796-42f0-4672-b005-493ad8083639" satisfied condition "Succeeded or Failed"
May 17 05:53:04.431: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downward-api-736e9796-42f0-4672-b005-493ad8083639 container dapi-container: <nil>
STEP: delete the pod 05/17/23 05:53:04.441
May 17 05:53:04.452: INFO: Waiting for pod downward-api-736e9796-42f0-4672-b005-493ad8083639 to disappear
May 17 05:53:04.456: INFO: Pod downward-api-736e9796-42f0-4672-b005-493ad8083639 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May 17 05:53:04.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-826" for this suite. 05/17/23 05:53:04.462
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":75,"skipped":1297,"failed":0}
------------------------------
â€¢ [4.082 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:53:00.386
    May 17 05:53:00.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 05:53:00.386
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:53:00.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:53:00.405
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 05/17/23 05:53:00.408
    May 17 05:53:00.418: INFO: Waiting up to 5m0s for pod "downward-api-736e9796-42f0-4672-b005-493ad8083639" in namespace "downward-api-826" to be "Succeeded or Failed"
    May 17 05:53:00.421: INFO: Pod "downward-api-736e9796-42f0-4672-b005-493ad8083639": Phase="Pending", Reason="", readiness=false. Elapsed: 3.528075ms
    May 17 05:53:02.425: INFO: Pod "downward-api-736e9796-42f0-4672-b005-493ad8083639": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007398958s
    May 17 05:53:04.427: INFO: Pod "downward-api-736e9796-42f0-4672-b005-493ad8083639": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009751999s
    STEP: Saw pod success 05/17/23 05:53:04.427
    May 17 05:53:04.428: INFO: Pod "downward-api-736e9796-42f0-4672-b005-493ad8083639" satisfied condition "Succeeded or Failed"
    May 17 05:53:04.431: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downward-api-736e9796-42f0-4672-b005-493ad8083639 container dapi-container: <nil>
    STEP: delete the pod 05/17/23 05:53:04.441
    May 17 05:53:04.452: INFO: Waiting for pod downward-api-736e9796-42f0-4672-b005-493ad8083639 to disappear
    May 17 05:53:04.456: INFO: Pod downward-api-736e9796-42f0-4672-b005-493ad8083639 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May 17 05:53:04.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-826" for this suite. 05/17/23 05:53:04.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:53:04.469
May 17 05:53:04.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 05:53:04.47
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:53:04.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:53:04.492
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 05:53:04.515
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 05:53:04.924
STEP: Deploying the webhook pod 05/17/23 05:53:04.932
STEP: Wait for the deployment to be ready 05/17/23 05:53:04.942
May 17 05:53:04.949: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/17/23 05:53:06.961
STEP: Verifying the service has paired with the endpoint 05/17/23 05:53:06.97
May 17 05:53:07.970: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 05/17/23 05:53:08.029
STEP: Creating a configMap that should be mutated 05/17/23 05:53:08.048
STEP: Deleting the collection of validation webhooks 05/17/23 05:53:08.094
STEP: Creating a configMap that should not be mutated 05/17/23 05:53:08.115
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 05:53:08.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5031" for this suite. 05/17/23 05:53:08.13
STEP: Destroying namespace "webhook-5031-markers" for this suite. 05/17/23 05:53:08.137
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":76,"skipped":1349,"failed":0}
------------------------------
â€¢ [3.720 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:53:04.469
    May 17 05:53:04.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 05:53:04.47
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:53:04.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:53:04.492
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 05:53:04.515
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 05:53:04.924
    STEP: Deploying the webhook pod 05/17/23 05:53:04.932
    STEP: Wait for the deployment to be ready 05/17/23 05:53:04.942
    May 17 05:53:04.949: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/17/23 05:53:06.961
    STEP: Verifying the service has paired with the endpoint 05/17/23 05:53:06.97
    May 17 05:53:07.970: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 05/17/23 05:53:08.029
    STEP: Creating a configMap that should be mutated 05/17/23 05:53:08.048
    STEP: Deleting the collection of validation webhooks 05/17/23 05:53:08.094
    STEP: Creating a configMap that should not be mutated 05/17/23 05:53:08.115
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 05:53:08.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5031" for this suite. 05/17/23 05:53:08.13
    STEP: Destroying namespace "webhook-5031-markers" for this suite. 05/17/23 05:53:08.137
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:53:08.191
May 17 05:53:08.191: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename lease-test 05/17/23 05:53:08.191
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:53:08.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:53:08.21
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
May 17 05:53:08.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-338" for this suite. 05/17/23 05:53:08.276
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":77,"skipped":1371,"failed":0}
------------------------------
â€¢ [0.092 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:53:08.191
    May 17 05:53:08.191: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename lease-test 05/17/23 05:53:08.191
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:53:08.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:53:08.21
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    May 17 05:53:08.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-338" for this suite. 05/17/23 05:53:08.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:53:08.283
May 17 05:53:08.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename dns 05/17/23 05:53:08.283
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:53:08.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:53:08.302
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7730.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7730.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 05/17/23 05:53:08.306
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7730.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7730.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 05/17/23 05:53:08.306
STEP: creating a pod to probe /etc/hosts 05/17/23 05:53:08.306
STEP: submitting the pod to kubernetes 05/17/23 05:53:08.306
May 17 05:53:08.316: INFO: Waiting up to 15m0s for pod "dns-test-af076dd5-2c3c-44a6-9792-4e07e248ff07" in namespace "dns-7730" to be "running"
May 17 05:53:08.319: INFO: Pod "dns-test-af076dd5-2c3c-44a6-9792-4e07e248ff07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.230176ms
May 17 05:53:10.325: INFO: Pod "dns-test-af076dd5-2c3c-44a6-9792-4e07e248ff07": Phase="Running", Reason="", readiness=true. Elapsed: 2.008419343s
May 17 05:53:10.325: INFO: Pod "dns-test-af076dd5-2c3c-44a6-9792-4e07e248ff07" satisfied condition "running"
STEP: retrieving the pod 05/17/23 05:53:10.325
STEP: looking for the results for each expected name from probers 05/17/23 05:53:10.328
May 17 05:53:10.354: INFO: DNS probes using dns-7730/dns-test-af076dd5-2c3c-44a6-9792-4e07e248ff07 succeeded

STEP: deleting the pod 05/17/23 05:53:10.354
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May 17 05:53:10.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7730" for this suite. 05/17/23 05:53:10.372
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":78,"skipped":1383,"failed":0}
------------------------------
â€¢ [2.095 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:53:08.283
    May 17 05:53:08.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename dns 05/17/23 05:53:08.283
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:53:08.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:53:08.302
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7730.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7730.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     05/17/23 05:53:08.306
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7730.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7730.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     05/17/23 05:53:08.306
    STEP: creating a pod to probe /etc/hosts 05/17/23 05:53:08.306
    STEP: submitting the pod to kubernetes 05/17/23 05:53:08.306
    May 17 05:53:08.316: INFO: Waiting up to 15m0s for pod "dns-test-af076dd5-2c3c-44a6-9792-4e07e248ff07" in namespace "dns-7730" to be "running"
    May 17 05:53:08.319: INFO: Pod "dns-test-af076dd5-2c3c-44a6-9792-4e07e248ff07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.230176ms
    May 17 05:53:10.325: INFO: Pod "dns-test-af076dd5-2c3c-44a6-9792-4e07e248ff07": Phase="Running", Reason="", readiness=true. Elapsed: 2.008419343s
    May 17 05:53:10.325: INFO: Pod "dns-test-af076dd5-2c3c-44a6-9792-4e07e248ff07" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 05:53:10.325
    STEP: looking for the results for each expected name from probers 05/17/23 05:53:10.328
    May 17 05:53:10.354: INFO: DNS probes using dns-7730/dns-test-af076dd5-2c3c-44a6-9792-4e07e248ff07 succeeded

    STEP: deleting the pod 05/17/23 05:53:10.354
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May 17 05:53:10.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7730" for this suite. 05/17/23 05:53:10.372
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:53:10.379
May 17 05:53:10.379: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 05:53:10.379
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:53:10.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:53:10.398
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/17/23 05:53:10.401
May 17 05:53:10.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4065 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May 17 05:53:10.476: INFO: stderr: ""
May 17 05:53:10.476: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 05/17/23 05:53:10.476
STEP: verifying the pod e2e-test-httpd-pod was created 05/17/23 05:53:15.528
May 17 05:53:15.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4065 get pod e2e-test-httpd-pod -o json'
May 17 05:53:15.592: INFO: stderr: ""
May 17 05:53:15.592: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-05-17T05:53:10Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4065\",\n        \"resourceVersion\": \"1443167\",\n        \"uid\": \"b9d0463d-f310-4029-99b8-4b521cb79fb0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5bcx7\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"aks-agentpool-72615086-vmss00000c\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5bcx7\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T05:53:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T05:53:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T05:53:11Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T05:53:10Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://37a3639050dab18df9ee28aff5ac99dce8cea26b04349e05cdcf82861f21f4aa\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-17T05:53:11Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.224.0.6\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.2.78\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.2.78\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-17T05:53:10Z\"\n    }\n}\n"
STEP: replace the image in the pod 05/17/23 05:53:15.592
May 17 05:53:15.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4065 replace -f -'
May 17 05:53:16.972: INFO: stderr: ""
May 17 05:53:16.972: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 05/17/23 05:53:16.972
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
May 17 05:53:16.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4065 delete pods e2e-test-httpd-pod'
May 17 05:53:18.698: INFO: stderr: ""
May 17 05:53:18.698: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 05:53:18.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4065" for this suite. 05/17/23 05:53:18.705
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":79,"skipped":1387,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.333 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:53:10.379
    May 17 05:53:10.379: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 05:53:10.379
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:53:10.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:53:10.398
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/17/23 05:53:10.401
    May 17 05:53:10.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4065 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May 17 05:53:10.476: INFO: stderr: ""
    May 17 05:53:10.476: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 05/17/23 05:53:10.476
    STEP: verifying the pod e2e-test-httpd-pod was created 05/17/23 05:53:15.528
    May 17 05:53:15.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4065 get pod e2e-test-httpd-pod -o json'
    May 17 05:53:15.592: INFO: stderr: ""
    May 17 05:53:15.592: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-05-17T05:53:10Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4065\",\n        \"resourceVersion\": \"1443167\",\n        \"uid\": \"b9d0463d-f310-4029-99b8-4b521cb79fb0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5bcx7\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"aks-agentpool-72615086-vmss00000c\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5bcx7\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T05:53:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T05:53:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T05:53:11Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-17T05:53:10Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://37a3639050dab18df9ee28aff5ac99dce8cea26b04349e05cdcf82861f21f4aa\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-17T05:53:11Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.224.0.6\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.2.78\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.2.78\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-17T05:53:10Z\"\n    }\n}\n"
    STEP: replace the image in the pod 05/17/23 05:53:15.592
    May 17 05:53:15.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4065 replace -f -'
    May 17 05:53:16.972: INFO: stderr: ""
    May 17 05:53:16.972: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 05/17/23 05:53:16.972
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    May 17 05:53:16.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4065 delete pods e2e-test-httpd-pod'
    May 17 05:53:18.698: INFO: stderr: ""
    May 17 05:53:18.698: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 05:53:18.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4065" for this suite. 05/17/23 05:53:18.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:53:18.712
May 17 05:53:18.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename sched-pred 05/17/23 05:53:18.713
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:53:18.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:53:18.731
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May 17 05:53:18.735: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 05:53:18.745: INFO: Waiting for terminating namespaces to be deleted...
May 17 05:53:18.749: INFO: 
Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000b before test
May 17 05:53:18.768: INFO: cdi-apiserver-656bdddf7b-t4q89 from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container cdi-apiserver ready: true, restart count 0
May 17 05:53:18.768: INFO: cdi-deployment-59b94f4bcf-6b8js from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container cdi-controller ready: true, restart count 0
May 17 05:53:18.768: INFO: cdi-operator-7f58c444b8-cnf5v from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container cdi-operator ready: true, restart count 0
May 17 05:53:18.768: INFO: cdi-uploadproxy-7787fffb9b-g7dzg from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
May 17 05:53:18.768: INFO: cert-manager-96f7799d6-csgfn from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container cert-manager-controller ready: true, restart count 0
May 17 05:53:18.768: INFO: cert-manager-cainjector-6f79489644-w5ggx from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
May 17 05:53:18.768: INFO: cert-manager-webhook-7dfbf7bff6-zk657 from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container cert-manager-webhook ready: true, restart count 0
May 17 05:53:18.768: INFO: cluster-management-agent-lite-758b6c7957-74gwr from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container cluster-management-agent-lite ready: true, restart count 0
May 17 05:53:18.768: INFO: palette-lite-controller-manager-6cd9fb7bd-h4j4p from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container manager ready: true, restart count 0
May 17 05:53:18.768: INFO: terraform-controller-848679f679-6zmxw from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container terraform-controller ready: true, restart count 0
May 17 05:53:18.768: INFO: ama-logs-87zkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (2 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container ama-logs ready: true, restart count 0
May 17 05:53:18.768: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
May 17 05:53:18.768: INFO: ama-logs-rs-68c99469fb-bzwjp from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container ama-logs ready: true, restart count 0
May 17 05:53:18.768: INFO: azure-ip-masq-agent-928nn from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 17 05:53:18.768: INFO: cloud-node-manager-x7fjk from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container cloud-node-manager ready: true, restart count 0
May 17 05:53:18.768: INFO: coredns-75bbfcbc66-mkbkm from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container coredns ready: true, restart count 0
May 17 05:53:18.768: INFO: coredns-75bbfcbc66-vjv2g from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container coredns ready: true, restart count 0
May 17 05:53:18.768: INFO: coredns-autoscaler-7879846967-wvdn8 from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container autoscaler ready: true, restart count 0
May 17 05:53:18.768: INFO: csi-azuredisk-node-pccm7 from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container azuredisk ready: true, restart count 0
May 17 05:53:18.768: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 05:53:18.768: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 05:53:18.768: INFO: csi-azurefile-node-bzf7d from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container azurefile ready: true, restart count 0
May 17 05:53:18.768: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 05:53:18.768: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 05:53:18.768: INFO: konnectivity-agent-6859749db4-r746q from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container konnectivity-agent ready: true, restart count 0
May 17 05:53:18.768: INFO: konnectivity-agent-6859749db4-vbj9c from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container konnectivity-agent ready: true, restart count 0
May 17 05:53:18.768: INFO: kube-proxy-mzbkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 05:53:18.768: INFO: metrics-server-85c9977f87-k2zzk from kube-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container metrics-server ready: true, restart count 0
May 17 05:53:18.768: INFO: 	Container metrics-server-vpa ready: true, restart count 0
May 17 05:53:18.768: INFO: metrics-server-85c9977f87-slkjl from kube-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container metrics-server ready: true, restart count 0
May 17 05:53:18.768: INFO: 	Container metrics-server-vpa ready: true, restart count 0
May 17 05:53:18.768: INFO: virt-api-6c4f849c9d-2jwc2 from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container virt-api ready: true, restart count 0
May 17 05:53:18.768: INFO: virt-api-6c4f849c9d-2x4kv from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container virt-api ready: true, restart count 0
May 17 05:53:18.768: INFO: virt-controller-67b95d99d5-2cnzp from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container virt-controller ready: true, restart count 0
May 17 05:53:18.768: INFO: virt-controller-67b95d99d5-8wv5s from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container virt-controller ready: true, restart count 0
May 17 05:53:18.768: INFO: virt-handler-hnwhl from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container virt-handler ready: true, restart count 0
May 17 05:53:18.768: INFO: virt-operator-798f64bdf6-gx4sg from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container virt-operator ready: true, restart count 0
May 17 05:53:18.768: INFO: virt-operator-798f64bdf6-x5z2b from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container virt-operator ready: true, restart count 0
May 17 05:53:18.768: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-5jttt from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 05:53:18.768: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 05:53:18.768: INFO: k8s-triliovault-admission-webhook-77b46b96cd-9lksn from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container triliovault-admission-webhook ready: true, restart count 0
May 17 05:53:18.768: INFO: k8s-triliovault-control-plane-6fcbd76b99-8kgx9 from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container triliovault-analyzer ready: true, restart count 0
May 17 05:53:18.768: INFO: 	Container triliovault-control-plane ready: true, restart count 0
May 17 05:53:18.768: INFO: k8s-triliovault-exporter-59c8c8dcdc-kwbxw from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container triliovault-exporter ready: true, restart count 0
May 17 05:53:18.768: INFO: k8s-triliovault-ingress-nginx-controller-76b7885d96-rr66q from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container controller ready: true, restart count 0
May 17 05:53:18.768: INFO: k8s-triliovault-operator-66747bd477-pmljl from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container k8s-triliovault-operator ready: true, restart count 0
May 17 05:53:18.768: INFO: k8s-triliovault-web-75bdc67d7d-l8t7z from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container triliovault-web ready: true, restart count 0
May 17 05:53:18.768: INFO: k8s-triliovault-web-backend-54b448979f-98rfz from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.768: INFO: 	Container triliovault-web-backend ready: true, restart count 0
May 17 05:53:18.768: INFO: 
Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000c before test
May 17 05:53:18.782: INFO: suspend-false-to-true-7b8x5 from job-7216 started at 2023-05-17 05:52:58 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.782: INFO: 	Container c ready: true, restart count 0
May 17 05:53:18.782: INFO: suspend-false-to-true-mh7th from job-7216 started at 2023-05-17 05:52:58 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.782: INFO: 	Container c ready: true, restart count 0
May 17 05:53:18.782: INFO: ama-logs-xntrk from kube-system started at 2023-05-17 04:32:30 +0000 UTC (2 container statuses recorded)
May 17 05:53:18.782: INFO: 	Container ama-logs ready: true, restart count 0
May 17 05:53:18.782: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
May 17 05:53:18.782: INFO: azure-ip-masq-agent-ns2hc from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.782: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 17 05:53:18.782: INFO: cloud-node-manager-hhrc6 from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.782: INFO: 	Container cloud-node-manager ready: true, restart count 0
May 17 05:53:18.782: INFO: csi-azuredisk-node-dms8v from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
May 17 05:53:18.782: INFO: 	Container azuredisk ready: true, restart count 0
May 17 05:53:18.782: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 05:53:18.782: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 05:53:18.782: INFO: csi-azurefile-node-whbbj from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
May 17 05:53:18.782: INFO: 	Container azurefile ready: true, restart count 0
May 17 05:53:18.782: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 05:53:18.782: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 05:53:18.782: INFO: kube-proxy-lpnpx from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.782: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 05:53:18.782: INFO: virt-handler-gxb46 from kubevirt started at 2023-05-17 05:47:22 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.782: INFO: 	Container virt-handler ready: true, restart count 0
May 17 05:53:18.782: INFO: sonobuoy from sonobuoy started at 2023-05-17 05:36:06 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.782: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 05:53:18.782: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-6hdss from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 05:53:18.782: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 05:53:18.782: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 05:53:18.782: INFO: 
Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000d before test
May 17 05:53:18.794: INFO: virt-launcher-cirros-vm-x6gh2 from default started at 2023-05-17 05:47:31 +0000 UTC (2 container statuses recorded)
May 17 05:53:18.794: INFO: 	Container compute ready: true, restart count 0
May 17 05:53:18.794: INFO: 	Container volumecontainerdisk ready: true, restart count 0
May 17 05:53:18.794: INFO: ama-logs-bmxmm from kube-system started at 2023-05-17 04:32:24 +0000 UTC (2 container statuses recorded)
May 17 05:53:18.794: INFO: 	Container ama-logs ready: true, restart count 0
May 17 05:53:18.794: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
May 17 05:53:18.794: INFO: azure-ip-masq-agent-vm9n2 from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.794: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 17 05:53:18.794: INFO: cloud-node-manager-6mv9z from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.794: INFO: 	Container cloud-node-manager ready: true, restart count 0
May 17 05:53:18.794: INFO: csi-azuredisk-node-rg26q from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
May 17 05:53:18.794: INFO: 	Container azuredisk ready: true, restart count 0
May 17 05:53:18.794: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 05:53:18.794: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 05:53:18.794: INFO: csi-azurefile-node-zlpwj from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
May 17 05:53:18.794: INFO: 	Container azurefile ready: true, restart count 0
May 17 05:53:18.794: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 05:53:18.794: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 05:53:18.794: INFO: kube-proxy-nsx8v from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.794: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 05:53:18.794: INFO: virt-handler-9qq9z from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.794: INFO: 	Container virt-handler ready: true, restart count 0
May 17 05:53:18.794: INFO: mysql-qa-64f5b55869-877ll from mysql started at 2023-05-17 04:33:14 +0000 UTC (1 container statuses recorded)
May 17 05:53:18.794: INFO: 	Container mysql-qa ready: true, restart count 0
May 17 05:53:18.794: INFO: sonobuoy-e2e-job-a0b627fa1b44421a from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 05:53:18.794: INFO: 	Container e2e ready: true, restart count 0
May 17 05:53:18.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 05:53:18.794: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-tnssv from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 05:53:18.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 05:53:18.794: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/17/23 05:53:18.794
May 17 05:53:18.804: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6547" to be "running"
May 17 05:53:18.808: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.971924ms
May 17 05:53:20.814: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009411089s
May 17 05:53:20.814: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/17/23 05:53:20.817
STEP: Trying to apply a random label on the found node. 05/17/23 05:53:20.833
STEP: verifying the node has the label kubernetes.io/e2e-5d5ff0da-2f0d-4b4f-8cd4-8deac6f6696f 95 05/17/23 05:53:20.862
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/17/23 05:53:20.866
May 17 05:53:20.874: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6547" to be "not pending"
May 17 05:53:20.878: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036244ms
May 17 05:53:22.884: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010100193s
May 17 05:53:22.884: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.224.0.6 on the node which pod4 resides and expect not scheduled 05/17/23 05:53:22.884
May 17 05:53:22.891: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6547" to be "not pending"
May 17 05:53:22.895: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.301472ms
May 17 05:53:24.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008587133s
May 17 05:53:26.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007965979s
May 17 05:53:28.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008256943s
May 17 05:53:30.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007933575s
May 17 05:53:32.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00875991s
May 17 05:53:34.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00875208s
May 17 05:53:36.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007783153s
May 17 05:53:38.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00941032s
May 17 05:53:40.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007422295s
May 17 05:53:42.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009240019s
May 17 05:53:44.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008315581s
May 17 05:53:46.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.0085778s
May 17 05:53:48.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008446065s
May 17 05:53:50.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008284297s
May 17 05:53:52.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007731737s
May 17 05:53:54.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008634801s
May 17 05:53:56.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008274519s
May 17 05:53:58.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008610476s
May 17 05:54:00.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007697603s
May 17 05:54:02.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009579183s
May 17 05:54:04.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008943958s
May 17 05:54:06.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008817548s
May 17 05:54:08.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008300386s
May 17 05:54:10.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007284564s
May 17 05:54:12.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009012557s
May 17 05:54:14.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009343172s
May 17 05:54:16.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008023631s
May 17 05:54:18.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009357044s
May 17 05:54:20.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007313137s
May 17 05:54:22.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00891274s
May 17 05:54:24.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009184296s
May 17 05:54:26.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008983895s
May 17 05:54:28.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008913565s
May 17 05:54:30.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007960485s
May 17 05:54:32.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007600696s
May 17 05:54:34.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009256543s
May 17 05:54:36.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.00884354s
May 17 05:54:38.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009760331s
May 17 05:54:40.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008024623s
May 17 05:54:42.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008920706s
May 17 05:54:44.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007810982s
May 17 05:54:46.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.00795397s
May 17 05:54:48.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008243135s
May 17 05:54:50.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008009949s
May 17 05:54:52.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008873926s
May 17 05:54:54.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009471489s
May 17 05:54:56.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007416429s
May 17 05:54:58.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008570916s
May 17 05:55:00.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.007835573s
May 17 05:55:02.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009116843s
May 17 05:55:04.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009672118s
May 17 05:55:06.905: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.013765653s
May 17 05:55:08.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009104329s
May 17 05:55:10.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007975309s
May 17 05:55:12.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007631754s
May 17 05:55:14.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008953266s
May 17 05:55:16.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007951553s
May 17 05:55:18.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.007183643s
May 17 05:55:20.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007325533s
May 17 05:55:22.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006988686s
May 17 05:55:24.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.007865378s
May 17 05:55:26.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008555714s
May 17 05:55:28.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.007808808s
May 17 05:55:30.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.008053611s
May 17 05:55:32.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.007135547s
May 17 05:55:34.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.007885477s
May 17 05:55:36.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008714964s
May 17 05:55:38.902: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.010240828s
May 17 05:55:40.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008800619s
May 17 05:55:42.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.007522462s
May 17 05:55:44.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008760135s
May 17 05:55:46.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.00807873s
May 17 05:55:48.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.00806986s
May 17 05:55:50.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.00932462s
May 17 05:55:52.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.008700598s
May 17 05:55:54.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009238017s
May 17 05:55:56.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008438786s
May 17 05:55:58.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007826178s
May 17 05:56:00.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.008094124s
May 17 05:56:02.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009561812s
May 17 05:56:04.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.007999305s
May 17 05:56:06.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007490244s
May 17 05:56:08.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007716306s
May 17 05:56:10.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.007612687s
May 17 05:56:12.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.00933654s
May 17 05:56:14.902: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.01022227s
May 17 05:56:16.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.0086219s
May 17 05:56:18.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.008592273s
May 17 05:56:20.906: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.01421188s
May 17 05:56:22.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009077825s
May 17 05:56:24.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009260397s
May 17 05:56:26.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009143732s
May 17 05:56:28.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008879281s
May 17 05:56:30.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.007961043s
May 17 05:56:32.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009294657s
May 17 05:56:34.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009622422s
May 17 05:56:36.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007823921s
May 17 05:56:38.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009345765s
May 17 05:56:40.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007533109s
May 17 05:56:42.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008125369s
May 17 05:56:44.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.008782148s
May 17 05:56:46.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008134164s
May 17 05:56:48.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009186325s
May 17 05:56:50.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008670429s
May 17 05:56:52.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008725798s
May 17 05:56:54.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008785909s
May 17 05:56:56.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008631071s
May 17 05:56:58.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.008752882s
May 17 05:57:00.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.008378213s
May 17 05:57:02.902: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.010168635s
May 17 05:57:04.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.00968549s
May 17 05:57:06.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007729188s
May 17 05:57:08.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.008645761s
May 17 05:57:10.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.007604208s
May 17 05:57:12.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00851366s
May 17 05:57:14.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008360474s
May 17 05:57:16.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007656502s
May 17 05:57:18.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007751802s
May 17 05:57:20.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007667791s
May 17 05:57:22.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008957267s
May 17 05:57:24.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009828897s
May 17 05:57:26.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.008328095s
May 17 05:57:28.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009154225s
May 17 05:57:30.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.008347793s
May 17 05:57:32.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008359557s
May 17 05:57:34.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.007674768s
May 17 05:57:36.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.0084819s
May 17 05:57:38.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008571007s
May 17 05:57:40.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007655414s
May 17 05:57:42.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008622182s
May 17 05:57:44.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.008311468s
May 17 05:57:46.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.008977271s
May 17 05:57:48.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009988934s
May 17 05:57:50.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007634491s
May 17 05:57:52.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008506515s
May 17 05:57:54.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009255587s
May 17 05:57:56.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008794273s
May 17 05:57:58.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008877743s
May 17 05:58:00.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00864326s
May 17 05:58:02.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008908478s
May 17 05:58:04.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009719873s
May 17 05:58:06.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007920689s
May 17 05:58:08.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009060637s
May 17 05:58:10.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.008486971s
May 17 05:58:12.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.0079313s
May 17 05:58:14.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.00781706s
May 17 05:58:16.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008238675s
May 17 05:58:18.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009030047s
May 17 05:58:20.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.008077788s
May 17 05:58:22.902: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010096957s
May 17 05:58:22.905: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.013444815s
STEP: removing the label kubernetes.io/e2e-5d5ff0da-2f0d-4b4f-8cd4-8deac6f6696f off the node aks-agentpool-72615086-vmss00000c 05/17/23 05:58:22.905
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5d5ff0da-2f0d-4b4f-8cd4-8deac6f6696f 05/17/23 05:58:22.936
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May 17 05:58:22.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6547" for this suite. 05/17/23 05:58:22.947
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":80,"skipped":1399,"failed":0}
------------------------------
â€¢ [SLOW TEST] [304.241 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:53:18.712
    May 17 05:53:18.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename sched-pred 05/17/23 05:53:18.713
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:53:18.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:53:18.731
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May 17 05:53:18.735: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May 17 05:53:18.745: INFO: Waiting for terminating namespaces to be deleted...
    May 17 05:53:18.749: INFO: 
    Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000b before test
    May 17 05:53:18.768: INFO: cdi-apiserver-656bdddf7b-t4q89 from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container cdi-apiserver ready: true, restart count 0
    May 17 05:53:18.768: INFO: cdi-deployment-59b94f4bcf-6b8js from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container cdi-controller ready: true, restart count 0
    May 17 05:53:18.768: INFO: cdi-operator-7f58c444b8-cnf5v from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container cdi-operator ready: true, restart count 0
    May 17 05:53:18.768: INFO: cdi-uploadproxy-7787fffb9b-g7dzg from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
    May 17 05:53:18.768: INFO: cert-manager-96f7799d6-csgfn from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container cert-manager-controller ready: true, restart count 0
    May 17 05:53:18.768: INFO: cert-manager-cainjector-6f79489644-w5ggx from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
    May 17 05:53:18.768: INFO: cert-manager-webhook-7dfbf7bff6-zk657 from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container cert-manager-webhook ready: true, restart count 0
    May 17 05:53:18.768: INFO: cluster-management-agent-lite-758b6c7957-74gwr from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container cluster-management-agent-lite ready: true, restart count 0
    May 17 05:53:18.768: INFO: palette-lite-controller-manager-6cd9fb7bd-h4j4p from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container manager ready: true, restart count 0
    May 17 05:53:18.768: INFO: terraform-controller-848679f679-6zmxw from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container terraform-controller ready: true, restart count 0
    May 17 05:53:18.768: INFO: ama-logs-87zkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (2 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 05:53:18.768: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
    May 17 05:53:18.768: INFO: ama-logs-rs-68c99469fb-bzwjp from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 05:53:18.768: INFO: azure-ip-masq-agent-928nn from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    May 17 05:53:18.768: INFO: cloud-node-manager-x7fjk from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container cloud-node-manager ready: true, restart count 0
    May 17 05:53:18.768: INFO: coredns-75bbfcbc66-mkbkm from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container coredns ready: true, restart count 0
    May 17 05:53:18.768: INFO: coredns-75bbfcbc66-vjv2g from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container coredns ready: true, restart count 0
    May 17 05:53:18.768: INFO: coredns-autoscaler-7879846967-wvdn8 from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container autoscaler ready: true, restart count 0
    May 17 05:53:18.768: INFO: csi-azuredisk-node-pccm7 from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container azuredisk ready: true, restart count 0
    May 17 05:53:18.768: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 05:53:18.768: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 05:53:18.768: INFO: csi-azurefile-node-bzf7d from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container azurefile ready: true, restart count 0
    May 17 05:53:18.768: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 05:53:18.768: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 05:53:18.768: INFO: konnectivity-agent-6859749db4-r746q from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container konnectivity-agent ready: true, restart count 0
    May 17 05:53:18.768: INFO: konnectivity-agent-6859749db4-vbj9c from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container konnectivity-agent ready: true, restart count 0
    May 17 05:53:18.768: INFO: kube-proxy-mzbkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 05:53:18.768: INFO: metrics-server-85c9977f87-k2zzk from kube-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container metrics-server ready: true, restart count 0
    May 17 05:53:18.768: INFO: 	Container metrics-server-vpa ready: true, restart count 0
    May 17 05:53:18.768: INFO: metrics-server-85c9977f87-slkjl from kube-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container metrics-server ready: true, restart count 0
    May 17 05:53:18.768: INFO: 	Container metrics-server-vpa ready: true, restart count 0
    May 17 05:53:18.768: INFO: virt-api-6c4f849c9d-2jwc2 from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container virt-api ready: true, restart count 0
    May 17 05:53:18.768: INFO: virt-api-6c4f849c9d-2x4kv from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container virt-api ready: true, restart count 0
    May 17 05:53:18.768: INFO: virt-controller-67b95d99d5-2cnzp from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container virt-controller ready: true, restart count 0
    May 17 05:53:18.768: INFO: virt-controller-67b95d99d5-8wv5s from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container virt-controller ready: true, restart count 0
    May 17 05:53:18.768: INFO: virt-handler-hnwhl from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container virt-handler ready: true, restart count 0
    May 17 05:53:18.768: INFO: virt-operator-798f64bdf6-gx4sg from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container virt-operator ready: true, restart count 0
    May 17 05:53:18.768: INFO: virt-operator-798f64bdf6-x5z2b from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container virt-operator ready: true, restart count 0
    May 17 05:53:18.768: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-5jttt from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 05:53:18.768: INFO: 	Container systemd-logs ready: true, restart count 0
    May 17 05:53:18.768: INFO: k8s-triliovault-admission-webhook-77b46b96cd-9lksn from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container triliovault-admission-webhook ready: true, restart count 0
    May 17 05:53:18.768: INFO: k8s-triliovault-control-plane-6fcbd76b99-8kgx9 from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container triliovault-analyzer ready: true, restart count 0
    May 17 05:53:18.768: INFO: 	Container triliovault-control-plane ready: true, restart count 0
    May 17 05:53:18.768: INFO: k8s-triliovault-exporter-59c8c8dcdc-kwbxw from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container triliovault-exporter ready: true, restart count 0
    May 17 05:53:18.768: INFO: k8s-triliovault-ingress-nginx-controller-76b7885d96-rr66q from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container controller ready: true, restart count 0
    May 17 05:53:18.768: INFO: k8s-triliovault-operator-66747bd477-pmljl from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container k8s-triliovault-operator ready: true, restart count 0
    May 17 05:53:18.768: INFO: k8s-triliovault-web-75bdc67d7d-l8t7z from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container triliovault-web ready: true, restart count 0
    May 17 05:53:18.768: INFO: k8s-triliovault-web-backend-54b448979f-98rfz from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.768: INFO: 	Container triliovault-web-backend ready: true, restart count 0
    May 17 05:53:18.768: INFO: 
    Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000c before test
    May 17 05:53:18.782: INFO: suspend-false-to-true-7b8x5 from job-7216 started at 2023-05-17 05:52:58 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.782: INFO: 	Container c ready: true, restart count 0
    May 17 05:53:18.782: INFO: suspend-false-to-true-mh7th from job-7216 started at 2023-05-17 05:52:58 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.782: INFO: 	Container c ready: true, restart count 0
    May 17 05:53:18.782: INFO: ama-logs-xntrk from kube-system started at 2023-05-17 04:32:30 +0000 UTC (2 container statuses recorded)
    May 17 05:53:18.782: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 05:53:18.782: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
    May 17 05:53:18.782: INFO: azure-ip-masq-agent-ns2hc from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.782: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    May 17 05:53:18.782: INFO: cloud-node-manager-hhrc6 from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.782: INFO: 	Container cloud-node-manager ready: true, restart count 0
    May 17 05:53:18.782: INFO: csi-azuredisk-node-dms8v from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
    May 17 05:53:18.782: INFO: 	Container azuredisk ready: true, restart count 0
    May 17 05:53:18.782: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 05:53:18.782: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 05:53:18.782: INFO: csi-azurefile-node-whbbj from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
    May 17 05:53:18.782: INFO: 	Container azurefile ready: true, restart count 0
    May 17 05:53:18.782: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 05:53:18.782: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 05:53:18.782: INFO: kube-proxy-lpnpx from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.782: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 05:53:18.782: INFO: virt-handler-gxb46 from kubevirt started at 2023-05-17 05:47:22 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.782: INFO: 	Container virt-handler ready: true, restart count 0
    May 17 05:53:18.782: INFO: sonobuoy from sonobuoy started at 2023-05-17 05:36:06 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.782: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May 17 05:53:18.782: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-6hdss from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 05:53:18.782: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 05:53:18.782: INFO: 	Container systemd-logs ready: true, restart count 0
    May 17 05:53:18.782: INFO: 
    Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000d before test
    May 17 05:53:18.794: INFO: virt-launcher-cirros-vm-x6gh2 from default started at 2023-05-17 05:47:31 +0000 UTC (2 container statuses recorded)
    May 17 05:53:18.794: INFO: 	Container compute ready: true, restart count 0
    May 17 05:53:18.794: INFO: 	Container volumecontainerdisk ready: true, restart count 0
    May 17 05:53:18.794: INFO: ama-logs-bmxmm from kube-system started at 2023-05-17 04:32:24 +0000 UTC (2 container statuses recorded)
    May 17 05:53:18.794: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 05:53:18.794: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
    May 17 05:53:18.794: INFO: azure-ip-masq-agent-vm9n2 from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.794: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    May 17 05:53:18.794: INFO: cloud-node-manager-6mv9z from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.794: INFO: 	Container cloud-node-manager ready: true, restart count 0
    May 17 05:53:18.794: INFO: csi-azuredisk-node-rg26q from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
    May 17 05:53:18.794: INFO: 	Container azuredisk ready: true, restart count 0
    May 17 05:53:18.794: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 05:53:18.794: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 05:53:18.794: INFO: csi-azurefile-node-zlpwj from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
    May 17 05:53:18.794: INFO: 	Container azurefile ready: true, restart count 0
    May 17 05:53:18.794: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 05:53:18.794: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 05:53:18.794: INFO: kube-proxy-nsx8v from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.794: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 05:53:18.794: INFO: virt-handler-9qq9z from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.794: INFO: 	Container virt-handler ready: true, restart count 0
    May 17 05:53:18.794: INFO: mysql-qa-64f5b55869-877ll from mysql started at 2023-05-17 04:33:14 +0000 UTC (1 container statuses recorded)
    May 17 05:53:18.794: INFO: 	Container mysql-qa ready: true, restart count 0
    May 17 05:53:18.794: INFO: sonobuoy-e2e-job-a0b627fa1b44421a from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 05:53:18.794: INFO: 	Container e2e ready: true, restart count 0
    May 17 05:53:18.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 05:53:18.794: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-tnssv from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 05:53:18.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 05:53:18.794: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/17/23 05:53:18.794
    May 17 05:53:18.804: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6547" to be "running"
    May 17 05:53:18.808: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.971924ms
    May 17 05:53:20.814: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009411089s
    May 17 05:53:20.814: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/17/23 05:53:20.817
    STEP: Trying to apply a random label on the found node. 05/17/23 05:53:20.833
    STEP: verifying the node has the label kubernetes.io/e2e-5d5ff0da-2f0d-4b4f-8cd4-8deac6f6696f 95 05/17/23 05:53:20.862
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/17/23 05:53:20.866
    May 17 05:53:20.874: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6547" to be "not pending"
    May 17 05:53:20.878: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036244ms
    May 17 05:53:22.884: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010100193s
    May 17 05:53:22.884: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.224.0.6 on the node which pod4 resides and expect not scheduled 05/17/23 05:53:22.884
    May 17 05:53:22.891: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6547" to be "not pending"
    May 17 05:53:22.895: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.301472ms
    May 17 05:53:24.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008587133s
    May 17 05:53:26.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007965979s
    May 17 05:53:28.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008256943s
    May 17 05:53:30.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007933575s
    May 17 05:53:32.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00875991s
    May 17 05:53:34.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00875208s
    May 17 05:53:36.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007783153s
    May 17 05:53:38.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00941032s
    May 17 05:53:40.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007422295s
    May 17 05:53:42.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009240019s
    May 17 05:53:44.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008315581s
    May 17 05:53:46.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.0085778s
    May 17 05:53:48.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008446065s
    May 17 05:53:50.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008284297s
    May 17 05:53:52.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007731737s
    May 17 05:53:54.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008634801s
    May 17 05:53:56.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008274519s
    May 17 05:53:58.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008610476s
    May 17 05:54:00.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007697603s
    May 17 05:54:02.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009579183s
    May 17 05:54:04.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008943958s
    May 17 05:54:06.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008817548s
    May 17 05:54:08.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008300386s
    May 17 05:54:10.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007284564s
    May 17 05:54:12.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009012557s
    May 17 05:54:14.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009343172s
    May 17 05:54:16.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008023631s
    May 17 05:54:18.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009357044s
    May 17 05:54:20.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007313137s
    May 17 05:54:22.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00891274s
    May 17 05:54:24.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009184296s
    May 17 05:54:26.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008983895s
    May 17 05:54:28.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008913565s
    May 17 05:54:30.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007960485s
    May 17 05:54:32.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007600696s
    May 17 05:54:34.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009256543s
    May 17 05:54:36.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.00884354s
    May 17 05:54:38.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009760331s
    May 17 05:54:40.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008024623s
    May 17 05:54:42.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008920706s
    May 17 05:54:44.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007810982s
    May 17 05:54:46.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.00795397s
    May 17 05:54:48.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008243135s
    May 17 05:54:50.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008009949s
    May 17 05:54:52.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008873926s
    May 17 05:54:54.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009471489s
    May 17 05:54:56.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007416429s
    May 17 05:54:58.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008570916s
    May 17 05:55:00.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.007835573s
    May 17 05:55:02.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009116843s
    May 17 05:55:04.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009672118s
    May 17 05:55:06.905: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.013765653s
    May 17 05:55:08.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009104329s
    May 17 05:55:10.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007975309s
    May 17 05:55:12.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007631754s
    May 17 05:55:14.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008953266s
    May 17 05:55:16.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007951553s
    May 17 05:55:18.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.007183643s
    May 17 05:55:20.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007325533s
    May 17 05:55:22.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006988686s
    May 17 05:55:24.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.007865378s
    May 17 05:55:26.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008555714s
    May 17 05:55:28.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.007808808s
    May 17 05:55:30.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.008053611s
    May 17 05:55:32.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.007135547s
    May 17 05:55:34.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.007885477s
    May 17 05:55:36.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008714964s
    May 17 05:55:38.902: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.010240828s
    May 17 05:55:40.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008800619s
    May 17 05:55:42.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.007522462s
    May 17 05:55:44.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008760135s
    May 17 05:55:46.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.00807873s
    May 17 05:55:48.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.00806986s
    May 17 05:55:50.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.00932462s
    May 17 05:55:52.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.008700598s
    May 17 05:55:54.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009238017s
    May 17 05:55:56.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008438786s
    May 17 05:55:58.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007826178s
    May 17 05:56:00.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.008094124s
    May 17 05:56:02.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009561812s
    May 17 05:56:04.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.007999305s
    May 17 05:56:06.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007490244s
    May 17 05:56:08.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007716306s
    May 17 05:56:10.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.007612687s
    May 17 05:56:12.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.00933654s
    May 17 05:56:14.902: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.01022227s
    May 17 05:56:16.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.0086219s
    May 17 05:56:18.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.008592273s
    May 17 05:56:20.906: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.01421188s
    May 17 05:56:22.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009077825s
    May 17 05:56:24.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009260397s
    May 17 05:56:26.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009143732s
    May 17 05:56:28.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008879281s
    May 17 05:56:30.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.007961043s
    May 17 05:56:32.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009294657s
    May 17 05:56:34.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009622422s
    May 17 05:56:36.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007823921s
    May 17 05:56:38.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009345765s
    May 17 05:56:40.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007533109s
    May 17 05:56:42.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008125369s
    May 17 05:56:44.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.008782148s
    May 17 05:56:46.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008134164s
    May 17 05:56:48.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009186325s
    May 17 05:56:50.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008670429s
    May 17 05:56:52.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008725798s
    May 17 05:56:54.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008785909s
    May 17 05:56:56.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008631071s
    May 17 05:56:58.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.008752882s
    May 17 05:57:00.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.008378213s
    May 17 05:57:02.902: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.010168635s
    May 17 05:57:04.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.00968549s
    May 17 05:57:06.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007729188s
    May 17 05:57:08.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.008645761s
    May 17 05:57:10.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.007604208s
    May 17 05:57:12.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00851366s
    May 17 05:57:14.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008360474s
    May 17 05:57:16.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007656502s
    May 17 05:57:18.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007751802s
    May 17 05:57:20.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007667791s
    May 17 05:57:22.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008957267s
    May 17 05:57:24.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009828897s
    May 17 05:57:26.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.008328095s
    May 17 05:57:28.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009154225s
    May 17 05:57:30.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.008347793s
    May 17 05:57:32.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008359557s
    May 17 05:57:34.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.007674768s
    May 17 05:57:36.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.0084819s
    May 17 05:57:38.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008571007s
    May 17 05:57:40.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007655414s
    May 17 05:57:42.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008622182s
    May 17 05:57:44.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.008311468s
    May 17 05:57:46.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.008977271s
    May 17 05:57:48.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009988934s
    May 17 05:57:50.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007634491s
    May 17 05:57:52.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008506515s
    May 17 05:57:54.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009255587s
    May 17 05:57:56.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008794273s
    May 17 05:57:58.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008877743s
    May 17 05:58:00.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00864326s
    May 17 05:58:02.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008908478s
    May 17 05:58:04.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009719873s
    May 17 05:58:06.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007920689s
    May 17 05:58:08.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009060637s
    May 17 05:58:10.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.008486971s
    May 17 05:58:12.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.0079313s
    May 17 05:58:14.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.00781706s
    May 17 05:58:16.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008238675s
    May 17 05:58:18.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009030047s
    May 17 05:58:20.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.008077788s
    May 17 05:58:22.902: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010096957s
    May 17 05:58:22.905: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.013444815s
    STEP: removing the label kubernetes.io/e2e-5d5ff0da-2f0d-4b4f-8cd4-8deac6f6696f off the node aks-agentpool-72615086-vmss00000c 05/17/23 05:58:22.905
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-5d5ff0da-2f0d-4b4f-8cd4-8deac6f6696f 05/17/23 05:58:22.936
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May 17 05:58:22.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6547" for this suite. 05/17/23 05:58:22.947
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:58:22.954
May 17 05:58:22.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename svcaccounts 05/17/23 05:58:22.955
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:58:22.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:58:22.977
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
May 17 05:58:23.002: INFO: created pod pod-service-account-defaultsa
May 17 05:58:23.002: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 17 05:58:23.008: INFO: created pod pod-service-account-mountsa
May 17 05:58:23.008: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 17 05:58:23.014: INFO: created pod pod-service-account-nomountsa
May 17 05:58:23.014: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 17 05:58:23.020: INFO: created pod pod-service-account-defaultsa-mountspec
May 17 05:58:23.020: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 17 05:58:23.026: INFO: created pod pod-service-account-mountsa-mountspec
May 17 05:58:23.026: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 17 05:58:23.032: INFO: created pod pod-service-account-nomountsa-mountspec
May 17 05:58:23.032: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 17 05:58:23.039: INFO: created pod pod-service-account-defaultsa-nomountspec
May 17 05:58:23.039: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 17 05:58:23.045: INFO: created pod pod-service-account-mountsa-nomountspec
May 17 05:58:23.045: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 17 05:58:23.051: INFO: created pod pod-service-account-nomountsa-nomountspec
May 17 05:58:23.051: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May 17 05:58:23.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9208" for this suite. 05/17/23 05:58:23.057
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":81,"skipped":1426,"failed":0}
------------------------------
â€¢ [0.110 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:58:22.954
    May 17 05:58:22.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 05:58:22.955
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:58:22.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:58:22.977
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    May 17 05:58:23.002: INFO: created pod pod-service-account-defaultsa
    May 17 05:58:23.002: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    May 17 05:58:23.008: INFO: created pod pod-service-account-mountsa
    May 17 05:58:23.008: INFO: pod pod-service-account-mountsa service account token volume mount: true
    May 17 05:58:23.014: INFO: created pod pod-service-account-nomountsa
    May 17 05:58:23.014: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    May 17 05:58:23.020: INFO: created pod pod-service-account-defaultsa-mountspec
    May 17 05:58:23.020: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    May 17 05:58:23.026: INFO: created pod pod-service-account-mountsa-mountspec
    May 17 05:58:23.026: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    May 17 05:58:23.032: INFO: created pod pod-service-account-nomountsa-mountspec
    May 17 05:58:23.032: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    May 17 05:58:23.039: INFO: created pod pod-service-account-defaultsa-nomountspec
    May 17 05:58:23.039: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    May 17 05:58:23.045: INFO: created pod pod-service-account-mountsa-nomountspec
    May 17 05:58:23.045: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    May 17 05:58:23.051: INFO: created pod pod-service-account-nomountsa-nomountspec
    May 17 05:58:23.051: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May 17 05:58:23.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9208" for this suite. 05/17/23 05:58:23.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:58:23.065
May 17 05:58:23.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename crd-webhook 05/17/23 05:58:23.065
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:58:23.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:58:23.096
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/17/23 05:58:23.1
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/17/23 05:58:23.398
STEP: Deploying the custom resource conversion webhook pod 05/17/23 05:58:23.404
STEP: Wait for the deployment to be ready 05/17/23 05:58:23.414
May 17 05:58:23.422: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 05:58:25.433
STEP: Verifying the service has paired with the endpoint 05/17/23 05:58:25.442
May 17 05:58:26.443: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
May 17 05:58:26.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Creating a v1 custom resource 05/17/23 05:58:29.199
STEP: Create a v2 custom resource 05/17/23 05:58:29.218
STEP: List CRs in v1 05/17/23 05:58:29.372
STEP: List CRs in v2 05/17/23 05:58:29.38
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 05:58:29.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-844" for this suite. 05/17/23 05:58:29.922
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":82,"skipped":1431,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.897 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:58:23.065
    May 17 05:58:23.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename crd-webhook 05/17/23 05:58:23.065
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:58:23.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:58:23.096
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/17/23 05:58:23.1
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/17/23 05:58:23.398
    STEP: Deploying the custom resource conversion webhook pod 05/17/23 05:58:23.404
    STEP: Wait for the deployment to be ready 05/17/23 05:58:23.414
    May 17 05:58:23.422: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 05:58:25.433
    STEP: Verifying the service has paired with the endpoint 05/17/23 05:58:25.442
    May 17 05:58:26.443: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    May 17 05:58:26.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Creating a v1 custom resource 05/17/23 05:58:29.199
    STEP: Create a v2 custom resource 05/17/23 05:58:29.218
    STEP: List CRs in v1 05/17/23 05:58:29.372
    STEP: List CRs in v2 05/17/23 05:58:29.38
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 05:58:29.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-844" for this suite. 05/17/23 05:58:29.922
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:58:29.962
May 17 05:58:29.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename gc 05/17/23 05:58:29.963
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:58:29.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:58:29.989
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 05/17/23 05:58:30.015
STEP: delete the rc 05/17/23 05:58:35.029
STEP: wait for the rc to be deleted 05/17/23 05:58:35.035
STEP: Gathering metrics 05/17/23 05:58:36.048
W0517 05:58:36.063580      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 17 05:58:36.063: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May 17 05:58:36.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5714" for this suite. 05/17/23 05:58:36.072
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":83,"skipped":1460,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.116 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:58:29.962
    May 17 05:58:29.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename gc 05/17/23 05:58:29.963
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:58:29.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:58:29.989
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 05/17/23 05:58:30.015
    STEP: delete the rc 05/17/23 05:58:35.029
    STEP: wait for the rc to be deleted 05/17/23 05:58:35.035
    STEP: Gathering metrics 05/17/23 05:58:36.048
    W0517 05:58:36.063580      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May 17 05:58:36.063: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May 17 05:58:36.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5714" for this suite. 05/17/23 05:58:36.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:58:36.079
May 17 05:58:36.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename var-expansion 05/17/23 05:58:36.08
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:58:36.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:58:36.097
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 05/17/23 05:58:36.1
May 17 05:58:36.110: INFO: Waiting up to 5m0s for pod "var-expansion-9a0563b4-a498-4291-a997-c0a049a50282" in namespace "var-expansion-5075" to be "Succeeded or Failed"
May 17 05:58:36.114: INFO: Pod "var-expansion-9a0563b4-a498-4291-a997-c0a049a50282": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980941ms
May 17 05:58:38.119: INFO: Pod "var-expansion-9a0563b4-a498-4291-a997-c0a049a50282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009198348s
May 17 05:58:40.119: INFO: Pod "var-expansion-9a0563b4-a498-4291-a997-c0a049a50282": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008713047s
May 17 05:58:42.119: INFO: Pod "var-expansion-9a0563b4-a498-4291-a997-c0a049a50282": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009282581s
STEP: Saw pod success 05/17/23 05:58:42.119
May 17 05:58:42.119: INFO: Pod "var-expansion-9a0563b4-a498-4291-a997-c0a049a50282" satisfied condition "Succeeded or Failed"
May 17 05:58:42.123: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod var-expansion-9a0563b4-a498-4291-a997-c0a049a50282 container dapi-container: <nil>
STEP: delete the pod 05/17/23 05:58:42.131
May 17 05:58:42.141: INFO: Waiting for pod var-expansion-9a0563b4-a498-4291-a997-c0a049a50282 to disappear
May 17 05:58:42.144: INFO: Pod var-expansion-9a0563b4-a498-4291-a997-c0a049a50282 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May 17 05:58:42.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5075" for this suite. 05/17/23 05:58:42.151
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":84,"skipped":1473,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.083 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:58:36.079
    May 17 05:58:36.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename var-expansion 05/17/23 05:58:36.08
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:58:36.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:58:36.097
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 05/17/23 05:58:36.1
    May 17 05:58:36.110: INFO: Waiting up to 5m0s for pod "var-expansion-9a0563b4-a498-4291-a997-c0a049a50282" in namespace "var-expansion-5075" to be "Succeeded or Failed"
    May 17 05:58:36.114: INFO: Pod "var-expansion-9a0563b4-a498-4291-a997-c0a049a50282": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980941ms
    May 17 05:58:38.119: INFO: Pod "var-expansion-9a0563b4-a498-4291-a997-c0a049a50282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009198348s
    May 17 05:58:40.119: INFO: Pod "var-expansion-9a0563b4-a498-4291-a997-c0a049a50282": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008713047s
    May 17 05:58:42.119: INFO: Pod "var-expansion-9a0563b4-a498-4291-a997-c0a049a50282": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009282581s
    STEP: Saw pod success 05/17/23 05:58:42.119
    May 17 05:58:42.119: INFO: Pod "var-expansion-9a0563b4-a498-4291-a997-c0a049a50282" satisfied condition "Succeeded or Failed"
    May 17 05:58:42.123: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod var-expansion-9a0563b4-a498-4291-a997-c0a049a50282 container dapi-container: <nil>
    STEP: delete the pod 05/17/23 05:58:42.131
    May 17 05:58:42.141: INFO: Waiting for pod var-expansion-9a0563b4-a498-4291-a997-c0a049a50282 to disappear
    May 17 05:58:42.144: INFO: Pod var-expansion-9a0563b4-a498-4291-a997-c0a049a50282 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May 17 05:58:42.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5075" for this suite. 05/17/23 05:58:42.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:58:42.163
May 17 05:58:42.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename subpath 05/17/23 05:58:42.164
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:58:42.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:58:42.182
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/17/23 05:58:42.186
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-s48v 05/17/23 05:58:42.196
STEP: Creating a pod to test atomic-volume-subpath 05/17/23 05:58:42.196
May 17 05:58:42.205: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-s48v" in namespace "subpath-3152" to be "Succeeded or Failed"
May 17 05:58:42.209: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065159ms
May 17 05:58:44.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 2.008904827s
May 17 05:58:46.215: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 4.00953415s
May 17 05:58:48.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 6.009014444s
May 17 05:58:50.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 8.008252041s
May 17 05:58:52.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 10.008780791s
May 17 05:58:54.215: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 12.009669403s
May 17 05:58:56.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 14.008760866s
May 17 05:58:58.215: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 16.009405988s
May 17 05:59:00.215: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 18.009831884s
May 17 05:59:02.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 20.009012458s
May 17 05:59:04.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=false. Elapsed: 22.008911434s
May 17 05:59:06.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008250718s
STEP: Saw pod success 05/17/23 05:59:06.214
May 17 05:59:06.214: INFO: Pod "pod-subpath-test-downwardapi-s48v" satisfied condition "Succeeded or Failed"
May 17 05:59:06.217: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod pod-subpath-test-downwardapi-s48v container test-container-subpath-downwardapi-s48v: <nil>
STEP: delete the pod 05/17/23 05:59:06.226
May 17 05:59:06.239: INFO: Waiting for pod pod-subpath-test-downwardapi-s48v to disappear
May 17 05:59:06.242: INFO: Pod pod-subpath-test-downwardapi-s48v no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-s48v 05/17/23 05:59:06.242
May 17 05:59:06.242: INFO: Deleting pod "pod-subpath-test-downwardapi-s48v" in namespace "subpath-3152"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May 17 05:59:06.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3152" for this suite. 05/17/23 05:59:06.254
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":85,"skipped":1488,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.097 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:58:42.163
    May 17 05:58:42.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename subpath 05/17/23 05:58:42.164
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:58:42.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:58:42.182
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/17/23 05:58:42.186
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-s48v 05/17/23 05:58:42.196
    STEP: Creating a pod to test atomic-volume-subpath 05/17/23 05:58:42.196
    May 17 05:58:42.205: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-s48v" in namespace "subpath-3152" to be "Succeeded or Failed"
    May 17 05:58:42.209: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065159ms
    May 17 05:58:44.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 2.008904827s
    May 17 05:58:46.215: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 4.00953415s
    May 17 05:58:48.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 6.009014444s
    May 17 05:58:50.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 8.008252041s
    May 17 05:58:52.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 10.008780791s
    May 17 05:58:54.215: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 12.009669403s
    May 17 05:58:56.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 14.008760866s
    May 17 05:58:58.215: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 16.009405988s
    May 17 05:59:00.215: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 18.009831884s
    May 17 05:59:02.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=true. Elapsed: 20.009012458s
    May 17 05:59:04.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Running", Reason="", readiness=false. Elapsed: 22.008911434s
    May 17 05:59:06.214: INFO: Pod "pod-subpath-test-downwardapi-s48v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008250718s
    STEP: Saw pod success 05/17/23 05:59:06.214
    May 17 05:59:06.214: INFO: Pod "pod-subpath-test-downwardapi-s48v" satisfied condition "Succeeded or Failed"
    May 17 05:59:06.217: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod pod-subpath-test-downwardapi-s48v container test-container-subpath-downwardapi-s48v: <nil>
    STEP: delete the pod 05/17/23 05:59:06.226
    May 17 05:59:06.239: INFO: Waiting for pod pod-subpath-test-downwardapi-s48v to disappear
    May 17 05:59:06.242: INFO: Pod pod-subpath-test-downwardapi-s48v no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-s48v 05/17/23 05:59:06.242
    May 17 05:59:06.242: INFO: Deleting pod "pod-subpath-test-downwardapi-s48v" in namespace "subpath-3152"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May 17 05:59:06.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3152" for this suite. 05/17/23 05:59:06.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:59:06.264
May 17 05:59:06.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 05:59:06.264
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:59:06.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:59:06.296
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/17/23 05:59:06.299
May 17 05:59:06.310: INFO: Waiting up to 5m0s for pod "pod-e53b64cd-0109-4e48-b54d-3edc786ae179" in namespace "emptydir-2145" to be "Succeeded or Failed"
May 17 05:59:06.313: INFO: Pod "pod-e53b64cd-0109-4e48-b54d-3edc786ae179": Phase="Pending", Reason="", readiness=false. Elapsed: 3.356322ms
May 17 05:59:08.318: INFO: Pod "pod-e53b64cd-0109-4e48-b54d-3edc786ae179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007763733s
May 17 05:59:10.318: INFO: Pod "pod-e53b64cd-0109-4e48-b54d-3edc786ae179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007792151s
STEP: Saw pod success 05/17/23 05:59:10.318
May 17 05:59:10.318: INFO: Pod "pod-e53b64cd-0109-4e48-b54d-3edc786ae179" satisfied condition "Succeeded or Failed"
May 17 05:59:10.321: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod pod-e53b64cd-0109-4e48-b54d-3edc786ae179 container test-container: <nil>
STEP: delete the pod 05/17/23 05:59:10.329
May 17 05:59:10.342: INFO: Waiting for pod pod-e53b64cd-0109-4e48-b54d-3edc786ae179 to disappear
May 17 05:59:10.345: INFO: Pod pod-e53b64cd-0109-4e48-b54d-3edc786ae179 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 05:59:10.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2145" for this suite. 05/17/23 05:59:10.352
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":86,"skipped":1616,"failed":0}
------------------------------
â€¢ [4.095 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:59:06.264
    May 17 05:59:06.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 05:59:06.264
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:59:06.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:59:06.296
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/17/23 05:59:06.299
    May 17 05:59:06.310: INFO: Waiting up to 5m0s for pod "pod-e53b64cd-0109-4e48-b54d-3edc786ae179" in namespace "emptydir-2145" to be "Succeeded or Failed"
    May 17 05:59:06.313: INFO: Pod "pod-e53b64cd-0109-4e48-b54d-3edc786ae179": Phase="Pending", Reason="", readiness=false. Elapsed: 3.356322ms
    May 17 05:59:08.318: INFO: Pod "pod-e53b64cd-0109-4e48-b54d-3edc786ae179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007763733s
    May 17 05:59:10.318: INFO: Pod "pod-e53b64cd-0109-4e48-b54d-3edc786ae179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007792151s
    STEP: Saw pod success 05/17/23 05:59:10.318
    May 17 05:59:10.318: INFO: Pod "pod-e53b64cd-0109-4e48-b54d-3edc786ae179" satisfied condition "Succeeded or Failed"
    May 17 05:59:10.321: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod pod-e53b64cd-0109-4e48-b54d-3edc786ae179 container test-container: <nil>
    STEP: delete the pod 05/17/23 05:59:10.329
    May 17 05:59:10.342: INFO: Waiting for pod pod-e53b64cd-0109-4e48-b54d-3edc786ae179 to disappear
    May 17 05:59:10.345: INFO: Pod pod-e53b64cd-0109-4e48-b54d-3edc786ae179 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 05:59:10.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2145" for this suite. 05/17/23 05:59:10.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:59:10.359
May 17 05:59:10.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename dns 05/17/23 05:59:10.36
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:59:10.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:59:10.381
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 05/17/23 05:59:10.384
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6839.svc.cluster.local;sleep 1; done
 05/17/23 05:59:10.389
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6839.svc.cluster.local;sleep 1; done
 05/17/23 05:59:10.389
STEP: creating a pod to probe DNS 05/17/23 05:59:10.389
STEP: submitting the pod to kubernetes 05/17/23 05:59:10.389
May 17 05:59:10.400: INFO: Waiting up to 15m0s for pod "dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339" in namespace "dns-6839" to be "running"
May 17 05:59:10.404: INFO: Pod "dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339": Phase="Pending", Reason="", readiness=false. Elapsed: 3.541069ms
May 17 05:59:12.409: INFO: Pod "dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008506678s
May 17 05:59:14.409: INFO: Pod "dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008687981s
May 17 05:59:16.412: INFO: Pod "dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339": Phase="Running", Reason="", readiness=true. Elapsed: 6.011283569s
May 17 05:59:16.412: INFO: Pod "dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339" satisfied condition "running"
STEP: retrieving the pod 05/17/23 05:59:16.412
STEP: looking for the results for each expected name from probers 05/17/23 05:59:16.416
May 17 05:59:16.427: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
May 17 05:59:16.431: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
May 17 05:59:16.437: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
May 17 05:59:16.442: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
May 17 05:59:16.447: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
May 17 05:59:16.452: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
May 17 05:59:16.456: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
May 17 05:59:16.461: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
May 17 05:59:16.461: INFO: Lookups using dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6839.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local jessie_udp@dns-test-service-2.dns-6839.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6839.svc.cluster.local]

May 17 05:59:21.502: INFO: DNS probes using dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339 succeeded

STEP: deleting the pod 05/17/23 05:59:21.502
STEP: deleting the test headless service 05/17/23 05:59:21.514
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May 17 05:59:21.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6839" for this suite. 05/17/23 05:59:21.535
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":87,"skipped":1630,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.186 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:59:10.359
    May 17 05:59:10.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename dns 05/17/23 05:59:10.36
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:59:10.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:59:10.381
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 05/17/23 05:59:10.384
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6839.svc.cluster.local;sleep 1; done
     05/17/23 05:59:10.389
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6839.svc.cluster.local;sleep 1; done
     05/17/23 05:59:10.389
    STEP: creating a pod to probe DNS 05/17/23 05:59:10.389
    STEP: submitting the pod to kubernetes 05/17/23 05:59:10.389
    May 17 05:59:10.400: INFO: Waiting up to 15m0s for pod "dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339" in namespace "dns-6839" to be "running"
    May 17 05:59:10.404: INFO: Pod "dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339": Phase="Pending", Reason="", readiness=false. Elapsed: 3.541069ms
    May 17 05:59:12.409: INFO: Pod "dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008506678s
    May 17 05:59:14.409: INFO: Pod "dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008687981s
    May 17 05:59:16.412: INFO: Pod "dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339": Phase="Running", Reason="", readiness=true. Elapsed: 6.011283569s
    May 17 05:59:16.412: INFO: Pod "dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 05:59:16.412
    STEP: looking for the results for each expected name from probers 05/17/23 05:59:16.416
    May 17 05:59:16.427: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
    May 17 05:59:16.431: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
    May 17 05:59:16.437: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
    May 17 05:59:16.442: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
    May 17 05:59:16.447: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
    May 17 05:59:16.452: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
    May 17 05:59:16.456: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
    May 17 05:59:16.461: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6839.svc.cluster.local from pod dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339: the server could not find the requested resource (get pods dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339)
    May 17 05:59:16.461: INFO: Lookups using dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6839.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6839.svc.cluster.local jessie_udp@dns-test-service-2.dns-6839.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6839.svc.cluster.local]

    May 17 05:59:21.502: INFO: DNS probes using dns-6839/dns-test-c6b96077-6a45-4af7-8ed1-9be7c7b09339 succeeded

    STEP: deleting the pod 05/17/23 05:59:21.502
    STEP: deleting the test headless service 05/17/23 05:59:21.514
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May 17 05:59:21.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6839" for this suite. 05/17/23 05:59:21.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:59:21.546
May 17 05:59:21.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 05:59:21.546
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:59:21.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:59:21.573
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 05:59:21.597
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 05:59:21.972
STEP: Deploying the webhook pod 05/17/23 05:59:21.981
STEP: Wait for the deployment to be ready 05/17/23 05:59:21.993
May 17 05:59:22.000: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 05:59:24.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 5, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 5, 59, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 5, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 5, 59, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/17/23 05:59:26.017
STEP: Verifying the service has paired with the endpoint 05/17/23 05:59:26.026
May 17 05:59:27.027: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/17/23 05:59:27.031
STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 05:59:27.031
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/17/23 05:59:27.048
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/17/23 05:59:28.057
STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 05:59:28.057
STEP: Having no error when timeout is longer than webhook latency 05/17/23 05:59:29.096
STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 05:59:29.096
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/17/23 05:59:34.14
STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 05:59:34.14
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 05:59:39.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3353" for this suite. 05/17/23 05:59:39.184
STEP: Destroying namespace "webhook-3353-markers" for this suite. 05/17/23 05:59:39.191
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":88,"skipped":1651,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.687 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:59:21.546
    May 17 05:59:21.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 05:59:21.546
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:59:21.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:59:21.573
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 05:59:21.597
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 05:59:21.972
    STEP: Deploying the webhook pod 05/17/23 05:59:21.981
    STEP: Wait for the deployment to be ready 05/17/23 05:59:21.993
    May 17 05:59:22.000: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May 17 05:59:24.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 5, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 5, 59, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 5, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 5, 59, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/17/23 05:59:26.017
    STEP: Verifying the service has paired with the endpoint 05/17/23 05:59:26.026
    May 17 05:59:27.027: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/17/23 05:59:27.031
    STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 05:59:27.031
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/17/23 05:59:27.048
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/17/23 05:59:28.057
    STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 05:59:28.057
    STEP: Having no error when timeout is longer than webhook latency 05/17/23 05:59:29.096
    STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 05:59:29.096
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/17/23 05:59:34.14
    STEP: Registering slow webhook via the AdmissionRegistration API 05/17/23 05:59:34.14
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 05:59:39.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3353" for this suite. 05/17/23 05:59:39.184
    STEP: Destroying namespace "webhook-3353-markers" for this suite. 05/17/23 05:59:39.191
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 05:59:39.235
May 17 05:59:39.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename statefulset 05/17/23 05:59:39.236
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:59:39.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:59:39.254
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4478 05/17/23 05:59:39.258
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 05/17/23 05:59:39.262
May 17 05:59:39.271: INFO: Found 0 stateful pods, waiting for 3
May 17 05:59:49.277: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 05:59:49.277: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 05:59:49.277: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/17/23 05:59:49.289
May 17 05:59:49.314: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/17/23 05:59:49.314
STEP: Not applying an update when the partition is greater than the number of replicas 05/17/23 05:59:59.337
STEP: Performing a canary update 05/17/23 05:59:59.337
May 17 05:59:59.359: INFO: Updating stateful set ss2
May 17 05:59:59.367: INFO: Waiting for Pod statefulset-4478/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 05/17/23 06:00:09.379
May 17 06:00:09.407: INFO: Found 1 stateful pods, waiting for 3
May 17 06:00:19.416: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 06:00:19.416: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 06:00:19.416: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 05/17/23 06:00:19.424
May 17 06:00:19.444: INFO: Updating stateful set ss2
May 17 06:00:19.452: INFO: Waiting for Pod statefulset-4478/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
May 17 06:00:29.481: INFO: Updating stateful set ss2
May 17 06:00:29.488: INFO: Waiting for StatefulSet statefulset-4478/ss2 to complete update
May 17 06:00:29.488: INFO: Waiting for Pod statefulset-4478/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May 17 06:00:39.498: INFO: Deleting all statefulset in ns statefulset-4478
May 17 06:00:39.501: INFO: Scaling statefulset ss2 to 0
May 17 06:00:49.538: INFO: Waiting for statefulset status.replicas updated to 0
May 17 06:00:49.541: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May 17 06:00:49.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4478" for this suite. 05/17/23 06:00:49.562
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":89,"skipped":1687,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.333 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 05:59:39.235
    May 17 05:59:39.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename statefulset 05/17/23 05:59:39.236
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 05:59:39.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 05:59:39.254
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4478 05/17/23 05:59:39.258
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 05/17/23 05:59:39.262
    May 17 05:59:39.271: INFO: Found 0 stateful pods, waiting for 3
    May 17 05:59:49.277: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May 17 05:59:49.277: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May 17 05:59:49.277: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/17/23 05:59:49.289
    May 17 05:59:49.314: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/17/23 05:59:49.314
    STEP: Not applying an update when the partition is greater than the number of replicas 05/17/23 05:59:59.337
    STEP: Performing a canary update 05/17/23 05:59:59.337
    May 17 05:59:59.359: INFO: Updating stateful set ss2
    May 17 05:59:59.367: INFO: Waiting for Pod statefulset-4478/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 05/17/23 06:00:09.379
    May 17 06:00:09.407: INFO: Found 1 stateful pods, waiting for 3
    May 17 06:00:19.416: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May 17 06:00:19.416: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May 17 06:00:19.416: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 05/17/23 06:00:19.424
    May 17 06:00:19.444: INFO: Updating stateful set ss2
    May 17 06:00:19.452: INFO: Waiting for Pod statefulset-4478/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    May 17 06:00:29.481: INFO: Updating stateful set ss2
    May 17 06:00:29.488: INFO: Waiting for StatefulSet statefulset-4478/ss2 to complete update
    May 17 06:00:29.488: INFO: Waiting for Pod statefulset-4478/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May 17 06:00:39.498: INFO: Deleting all statefulset in ns statefulset-4478
    May 17 06:00:39.501: INFO: Scaling statefulset ss2 to 0
    May 17 06:00:49.538: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 06:00:49.541: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May 17 06:00:49.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4478" for this suite. 05/17/23 06:00:49.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:00:49.569
May 17 06:00:49.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pods 05/17/23 06:00:49.569
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:00:49.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:00:49.592
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 05/17/23 06:00:49.595
May 17 06:00:49.610: INFO: Waiting up to 5m0s for pod "pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab" in namespace "pods-7803" to be "running and ready"
May 17 06:00:49.613: INFO: Pod "pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.52043ms
May 17 06:00:49.613: INFO: The phase of Pod pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab is Pending, waiting for it to be Running (with Ready = true)
May 17 06:00:51.617: INFO: Pod "pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.007341417s
May 17 06:00:51.617: INFO: The phase of Pod pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab is Running (Ready = true)
May 17 06:00:51.617: INFO: Pod "pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab" satisfied condition "running and ready"
May 17 06:00:51.624: INFO: Pod pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab has hostIP: 10.224.0.7
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May 17 06:00:51.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7803" for this suite. 05/17/23 06:00:51.63
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":90,"skipped":1704,"failed":0}
------------------------------
â€¢ [2.068 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:00:49.569
    May 17 06:00:49.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pods 05/17/23 06:00:49.569
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:00:49.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:00:49.592
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 05/17/23 06:00:49.595
    May 17 06:00:49.610: INFO: Waiting up to 5m0s for pod "pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab" in namespace "pods-7803" to be "running and ready"
    May 17 06:00:49.613: INFO: Pod "pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.52043ms
    May 17 06:00:49.613: INFO: The phase of Pod pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:00:51.617: INFO: Pod "pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.007341417s
    May 17 06:00:51.617: INFO: The phase of Pod pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab is Running (Ready = true)
    May 17 06:00:51.617: INFO: Pod "pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab" satisfied condition "running and ready"
    May 17 06:00:51.624: INFO: Pod pod-hostip-854802f4-b924-4eb6-bf5e-e25e32d4c7ab has hostIP: 10.224.0.7
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May 17 06:00:51.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7803" for this suite. 05/17/23 06:00:51.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:00:51.638
May 17 06:00:51.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename watch 05/17/23 06:00:51.638
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:00:51.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:00:51.656
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 05/17/23 06:00:51.66
STEP: creating a new configmap 05/17/23 06:00:51.661
STEP: modifying the configmap once 05/17/23 06:00:51.666
STEP: changing the label value of the configmap 05/17/23 06:00:51.674
STEP: Expecting to observe a delete notification for the watched object 05/17/23 06:00:51.681
May 17 06:00:51.681: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7164  daef5344-6cfa-4e1f-9ef2-3a2b68faad6f 1449161 0 2023-05-17 06:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 06:00:51.681: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7164  daef5344-6cfa-4e1f-9ef2-3a2b68faad6f 1449162 0 2023-05-17 06:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 06:00:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 06:00:51.682: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7164  daef5344-6cfa-4e1f-9ef2-3a2b68faad6f 1449163 0 2023-05-17 06:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 06:00:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 05/17/23 06:00:51.682
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/17/23 06:00:51.695
STEP: changing the label value of the configmap back 05/17/23 06:01:01.695
STEP: modifying the configmap a third time 05/17/23 06:01:01.704
STEP: deleting the configmap 05/17/23 06:01:01.712
STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/17/23 06:01:01.718
May 17 06:01:01.718: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7164  daef5344-6cfa-4e1f-9ef2-3a2b68faad6f 1449286 0 2023-05-17 06:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 06:01:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 06:01:01.718: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7164  daef5344-6cfa-4e1f-9ef2-3a2b68faad6f 1449287 0 2023-05-17 06:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 06:01:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 06:01:01.718: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7164  daef5344-6cfa-4e1f-9ef2-3a2b68faad6f 1449288 0 2023-05-17 06:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 06:01:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May 17 06:01:01.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7164" for this suite. 05/17/23 06:01:01.726
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":91,"skipped":1711,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.094 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:00:51.638
    May 17 06:00:51.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename watch 05/17/23 06:00:51.638
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:00:51.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:00:51.656
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 05/17/23 06:00:51.66
    STEP: creating a new configmap 05/17/23 06:00:51.661
    STEP: modifying the configmap once 05/17/23 06:00:51.666
    STEP: changing the label value of the configmap 05/17/23 06:00:51.674
    STEP: Expecting to observe a delete notification for the watched object 05/17/23 06:00:51.681
    May 17 06:00:51.681: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7164  daef5344-6cfa-4e1f-9ef2-3a2b68faad6f 1449161 0 2023-05-17 06:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 06:00:51.681: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7164  daef5344-6cfa-4e1f-9ef2-3a2b68faad6f 1449162 0 2023-05-17 06:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 06:00:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 06:00:51.682: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7164  daef5344-6cfa-4e1f-9ef2-3a2b68faad6f 1449163 0 2023-05-17 06:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 06:00:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 05/17/23 06:00:51.682
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/17/23 06:00:51.695
    STEP: changing the label value of the configmap back 05/17/23 06:01:01.695
    STEP: modifying the configmap a third time 05/17/23 06:01:01.704
    STEP: deleting the configmap 05/17/23 06:01:01.712
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/17/23 06:01:01.718
    May 17 06:01:01.718: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7164  daef5344-6cfa-4e1f-9ef2-3a2b68faad6f 1449286 0 2023-05-17 06:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 06:01:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 06:01:01.718: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7164  daef5344-6cfa-4e1f-9ef2-3a2b68faad6f 1449287 0 2023-05-17 06:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 06:01:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 06:01:01.718: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7164  daef5344-6cfa-4e1f-9ef2-3a2b68faad6f 1449288 0 2023-05-17 06:00:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-17 06:01:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May 17 06:01:01.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7164" for this suite. 05/17/23 06:01:01.726
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:01:01.732
May 17 06:01:01.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:01:01.733
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:01.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:01.752
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-2707 05/17/23 06:01:01.755
STEP: creating replication controller nodeport-test in namespace services-2707 05/17/23 06:01:01.769
I0517 06:01:01.775404      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2707, replica count: 2
I0517 06:01:04.827169      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 06:01:04.827: INFO: Creating new exec pod
May 17 06:01:04.837: INFO: Waiting up to 5m0s for pod "execpodcqhd9" in namespace "services-2707" to be "running"
May 17 06:01:04.842: INFO: Pod "execpodcqhd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.986674ms
May 17 06:01:06.846: INFO: Pod "execpodcqhd9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009539622s
May 17 06:01:06.846: INFO: Pod "execpodcqhd9" satisfied condition "running"
May 17 06:01:07.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2707 exec execpodcqhd9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 17 06:01:08.024: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 17 06:01:08.024: INFO: stdout: "nodeport-test-vv7wf"
May 17 06:01:08.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2707 exec execpodcqhd9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.76.45 80'
May 17 06:01:08.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.76.45 80\nConnection to 10.0.76.45 80 port [tcp/http] succeeded!\n"
May 17 06:01:08.222: INFO: stdout: "nodeport-test-vv7wf"
May 17 06:01:08.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2707 exec execpodcqhd9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.6 31474'
May 17 06:01:08.391: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.6 31474\nConnection to 10.224.0.6 31474 port [tcp/*] succeeded!\n"
May 17 06:01:08.391: INFO: stdout: ""
May 17 06:01:09.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2707 exec execpodcqhd9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.6 31474'
May 17 06:01:09.561: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.6 31474\nConnection to 10.224.0.6 31474 port [tcp/*] succeeded!\n"
May 17 06:01:09.561: INFO: stdout: "nodeport-test-cs6wx"
May 17 06:01:09.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2707 exec execpodcqhd9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.7 31474'
May 17 06:01:09.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.7 31474\nConnection to 10.224.0.7 31474 port [tcp/*] succeeded!\n"
May 17 06:01:09.755: INFO: stdout: ""
May 17 06:01:10.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2707 exec execpodcqhd9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.7 31474'
May 17 06:01:10.947: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.7 31474\nConnection to 10.224.0.7 31474 port [tcp/*] succeeded!\n"
May 17 06:01:10.947: INFO: stdout: "nodeport-test-cs6wx"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:01:10.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2707" for this suite. 05/17/23 06:01:10.955
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":92,"skipped":1711,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.230 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:01:01.732
    May 17 06:01:01.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:01:01.733
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:01.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:01.752
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-2707 05/17/23 06:01:01.755
    STEP: creating replication controller nodeport-test in namespace services-2707 05/17/23 06:01:01.769
    I0517 06:01:01.775404      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2707, replica count: 2
    I0517 06:01:04.827169      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 06:01:04.827: INFO: Creating new exec pod
    May 17 06:01:04.837: INFO: Waiting up to 5m0s for pod "execpodcqhd9" in namespace "services-2707" to be "running"
    May 17 06:01:04.842: INFO: Pod "execpodcqhd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.986674ms
    May 17 06:01:06.846: INFO: Pod "execpodcqhd9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009539622s
    May 17 06:01:06.846: INFO: Pod "execpodcqhd9" satisfied condition "running"
    May 17 06:01:07.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2707 exec execpodcqhd9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    May 17 06:01:08.024: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    May 17 06:01:08.024: INFO: stdout: "nodeport-test-vv7wf"
    May 17 06:01:08.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2707 exec execpodcqhd9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.76.45 80'
    May 17 06:01:08.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.76.45 80\nConnection to 10.0.76.45 80 port [tcp/http] succeeded!\n"
    May 17 06:01:08.222: INFO: stdout: "nodeport-test-vv7wf"
    May 17 06:01:08.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2707 exec execpodcqhd9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.6 31474'
    May 17 06:01:08.391: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.6 31474\nConnection to 10.224.0.6 31474 port [tcp/*] succeeded!\n"
    May 17 06:01:08.391: INFO: stdout: ""
    May 17 06:01:09.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2707 exec execpodcqhd9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.6 31474'
    May 17 06:01:09.561: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.6 31474\nConnection to 10.224.0.6 31474 port [tcp/*] succeeded!\n"
    May 17 06:01:09.561: INFO: stdout: "nodeport-test-cs6wx"
    May 17 06:01:09.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2707 exec execpodcqhd9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.7 31474'
    May 17 06:01:09.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.7 31474\nConnection to 10.224.0.7 31474 port [tcp/*] succeeded!\n"
    May 17 06:01:09.755: INFO: stdout: ""
    May 17 06:01:10.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2707 exec execpodcqhd9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.7 31474'
    May 17 06:01:10.947: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.7 31474\nConnection to 10.224.0.7 31474 port [tcp/*] succeeded!\n"
    May 17 06:01:10.947: INFO: stdout: "nodeport-test-cs6wx"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:01:10.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2707" for this suite. 05/17/23 06:01:10.955
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:01:10.962
May 17 06:01:10.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:01:10.963
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:10.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:10.981
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2714 05/17/23 06:01:10.984
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/17/23 06:01:10.993
STEP: creating service externalsvc in namespace services-2714 05/17/23 06:01:10.993
STEP: creating replication controller externalsvc in namespace services-2714 05/17/23 06:01:11.001
I0517 06:01:11.007558      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2714, replica count: 2
I0517 06:01:14.058658      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 05/17/23 06:01:14.063
May 17 06:01:14.080: INFO: Creating new exec pod
May 17 06:01:14.091: INFO: Waiting up to 5m0s for pod "execpodmlgjn" in namespace "services-2714" to be "running"
May 17 06:01:14.094: INFO: Pod "execpodmlgjn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.306451ms
May 17 06:01:16.099: INFO: Pod "execpodmlgjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.00791446s
May 17 06:01:16.099: INFO: Pod "execpodmlgjn" satisfied condition "running"
May 17 06:01:16.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2714 exec execpodmlgjn -- /bin/sh -x -c nslookup clusterip-service.services-2714.svc.cluster.local'
May 17 06:01:16.307: INFO: stderr: "+ nslookup clusterip-service.services-2714.svc.cluster.local\n"
May 17 06:01:16.307: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-2714.svc.cluster.local\tcanonical name = externalsvc.services-2714.svc.cluster.local.\nName:\texternalsvc.services-2714.svc.cluster.local\nAddress: 10.0.240.192\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2714, will wait for the garbage collector to delete the pods 05/17/23 06:01:16.307
May 17 06:01:16.368: INFO: Deleting ReplicationController externalsvc took: 6.396539ms
May 17 06:01:16.469: INFO: Terminating ReplicationController externalsvc pods took: 100.566331ms
May 17 06:01:20.088: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:01:20.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2714" for this suite. 05/17/23 06:01:20.116
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":93,"skipped":1721,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.160 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:01:10.962
    May 17 06:01:10.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:01:10.963
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:10.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:10.981
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2714 05/17/23 06:01:10.984
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/17/23 06:01:10.993
    STEP: creating service externalsvc in namespace services-2714 05/17/23 06:01:10.993
    STEP: creating replication controller externalsvc in namespace services-2714 05/17/23 06:01:11.001
    I0517 06:01:11.007558      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2714, replica count: 2
    I0517 06:01:14.058658      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 05/17/23 06:01:14.063
    May 17 06:01:14.080: INFO: Creating new exec pod
    May 17 06:01:14.091: INFO: Waiting up to 5m0s for pod "execpodmlgjn" in namespace "services-2714" to be "running"
    May 17 06:01:14.094: INFO: Pod "execpodmlgjn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.306451ms
    May 17 06:01:16.099: INFO: Pod "execpodmlgjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.00791446s
    May 17 06:01:16.099: INFO: Pod "execpodmlgjn" satisfied condition "running"
    May 17 06:01:16.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2714 exec execpodmlgjn -- /bin/sh -x -c nslookup clusterip-service.services-2714.svc.cluster.local'
    May 17 06:01:16.307: INFO: stderr: "+ nslookup clusterip-service.services-2714.svc.cluster.local\n"
    May 17 06:01:16.307: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-2714.svc.cluster.local\tcanonical name = externalsvc.services-2714.svc.cluster.local.\nName:\texternalsvc.services-2714.svc.cluster.local\nAddress: 10.0.240.192\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2714, will wait for the garbage collector to delete the pods 05/17/23 06:01:16.307
    May 17 06:01:16.368: INFO: Deleting ReplicationController externalsvc took: 6.396539ms
    May 17 06:01:16.469: INFO: Terminating ReplicationController externalsvc pods took: 100.566331ms
    May 17 06:01:20.088: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:01:20.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2714" for this suite. 05/17/23 06:01:20.116
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:01:20.123
May 17 06:01:20.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:01:20.124
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:20.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:20.142
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 05/17/23 06:01:20.145
May 17 06:01:20.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
May 17 06:01:20.222: INFO: stderr: ""
May 17 06:01:20.222: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 05/17/23 06:01:20.222
May 17 06:01:20.222: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 17 06:01:20.222: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3415" to be "running and ready, or succeeded"
May 17 06:01:20.226: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.870014ms
May 17 06:01:20.226: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'aks-agentpool-72615086-vmss00000e' to be 'Running' but was 'Pending'
May 17 06:01:22.233: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.010202149s
May 17 06:01:22.233: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 17 06:01:22.233: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 05/17/23 06:01:22.233
May 17 06:01:22.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 logs logs-generator logs-generator'
May 17 06:01:22.311: INFO: stderr: ""
May 17 06:01:22.311: INFO: stdout: "I0517 06:01:20.824063       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/h7l 372\nI0517 06:01:21.024214       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/77kw 247\nI0517 06:01:21.224735       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/lnh 442\nI0517 06:01:21.425127       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/sbtn 530\nI0517 06:01:21.624428       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/2qn 526\nI0517 06:01:21.824802       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/f7d7 553\nI0517 06:01:22.025158       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/k2w2 293\nI0517 06:01:22.224473       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/5c8n 269\n"
STEP: limiting log lines 05/17/23 06:01:22.311
May 17 06:01:22.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 logs logs-generator logs-generator --tail=1'
May 17 06:01:22.377: INFO: stderr: ""
May 17 06:01:22.377: INFO: stdout: "I0517 06:01:22.224473       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/5c8n 269\n"
May 17 06:01:22.378: INFO: got output "I0517 06:01:22.224473       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/5c8n 269\n"
STEP: limiting log bytes 05/17/23 06:01:22.378
May 17 06:01:22.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 logs logs-generator logs-generator --limit-bytes=1'
May 17 06:01:22.449: INFO: stderr: ""
May 17 06:01:22.449: INFO: stdout: "I"
May 17 06:01:22.449: INFO: got output "I"
STEP: exposing timestamps 05/17/23 06:01:22.449
May 17 06:01:22.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 logs logs-generator logs-generator --tail=1 --timestamps'
May 17 06:01:22.522: INFO: stderr: ""
May 17 06:01:22.522: INFO: stdout: "2023-05-17T06:01:22.424841968Z I0517 06:01:22.424768       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/p7n 220\n"
May 17 06:01:22.522: INFO: got output "2023-05-17T06:01:22.424841968Z I0517 06:01:22.424768       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/p7n 220\n"
STEP: restricting to a time range 05/17/23 06:01:22.522
May 17 06:01:25.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 logs logs-generator logs-generator --since=1s'
May 17 06:01:25.100: INFO: stderr: ""
May 17 06:01:25.100: INFO: stdout: "I0517 06:01:24.224486       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/z6t 463\nI0517 06:01:24.424842       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/ttm 401\nI0517 06:01:24.624100       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/mwhz 545\nI0517 06:01:24.824422       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/74rl 579\nI0517 06:01:25.024775       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/vl4 322\n"
May 17 06:01:25.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 logs logs-generator logs-generator --since=24h'
May 17 06:01:25.169: INFO: stderr: ""
May 17 06:01:25.169: INFO: stdout: "I0517 06:01:20.824063       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/h7l 372\nI0517 06:01:21.024214       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/77kw 247\nI0517 06:01:21.224735       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/lnh 442\nI0517 06:01:21.425127       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/sbtn 530\nI0517 06:01:21.624428       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/2qn 526\nI0517 06:01:21.824802       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/f7d7 553\nI0517 06:01:22.025158       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/k2w2 293\nI0517 06:01:22.224473       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/5c8n 269\nI0517 06:01:22.424768       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/p7n 220\nI0517 06:01:22.625063       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/ktn 569\nI0517 06:01:22.824403       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/4lr 535\nI0517 06:01:23.024593       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/j24 367\nI0517 06:01:23.224923       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/8vz 543\nI0517 06:01:23.424208       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/drdc 367\nI0517 06:01:23.624559       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/rqs 255\nI0517 06:01:23.824920       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/9kcl 462\nI0517 06:01:24.024158       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/48f 494\nI0517 06:01:24.224486       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/z6t 463\nI0517 06:01:24.424842       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/ttm 401\nI0517 06:01:24.624100       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/mwhz 545\nI0517 06:01:24.824422       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/74rl 579\nI0517 06:01:25.024775       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/vl4 322\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
May 17 06:01:25.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 delete pod logs-generator'
May 17 06:01:25.851: INFO: stderr: ""
May 17 06:01:25.851: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:01:25.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3415" for this suite. 05/17/23 06:01:25.858
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":94,"skipped":1734,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.742 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:01:20.123
    May 17 06:01:20.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:01:20.124
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:20.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:20.142
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 05/17/23 06:01:20.145
    May 17 06:01:20.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    May 17 06:01:20.222: INFO: stderr: ""
    May 17 06:01:20.222: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 05/17/23 06:01:20.222
    May 17 06:01:20.222: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    May 17 06:01:20.222: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3415" to be "running and ready, or succeeded"
    May 17 06:01:20.226: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.870014ms
    May 17 06:01:20.226: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'aks-agentpool-72615086-vmss00000e' to be 'Running' but was 'Pending'
    May 17 06:01:22.233: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.010202149s
    May 17 06:01:22.233: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    May 17 06:01:22.233: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 05/17/23 06:01:22.233
    May 17 06:01:22.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 logs logs-generator logs-generator'
    May 17 06:01:22.311: INFO: stderr: ""
    May 17 06:01:22.311: INFO: stdout: "I0517 06:01:20.824063       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/h7l 372\nI0517 06:01:21.024214       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/77kw 247\nI0517 06:01:21.224735       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/lnh 442\nI0517 06:01:21.425127       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/sbtn 530\nI0517 06:01:21.624428       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/2qn 526\nI0517 06:01:21.824802       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/f7d7 553\nI0517 06:01:22.025158       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/k2w2 293\nI0517 06:01:22.224473       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/5c8n 269\n"
    STEP: limiting log lines 05/17/23 06:01:22.311
    May 17 06:01:22.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 logs logs-generator logs-generator --tail=1'
    May 17 06:01:22.377: INFO: stderr: ""
    May 17 06:01:22.377: INFO: stdout: "I0517 06:01:22.224473       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/5c8n 269\n"
    May 17 06:01:22.378: INFO: got output "I0517 06:01:22.224473       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/5c8n 269\n"
    STEP: limiting log bytes 05/17/23 06:01:22.378
    May 17 06:01:22.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 logs logs-generator logs-generator --limit-bytes=1'
    May 17 06:01:22.449: INFO: stderr: ""
    May 17 06:01:22.449: INFO: stdout: "I"
    May 17 06:01:22.449: INFO: got output "I"
    STEP: exposing timestamps 05/17/23 06:01:22.449
    May 17 06:01:22.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 logs logs-generator logs-generator --tail=1 --timestamps'
    May 17 06:01:22.522: INFO: stderr: ""
    May 17 06:01:22.522: INFO: stdout: "2023-05-17T06:01:22.424841968Z I0517 06:01:22.424768       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/p7n 220\n"
    May 17 06:01:22.522: INFO: got output "2023-05-17T06:01:22.424841968Z I0517 06:01:22.424768       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/p7n 220\n"
    STEP: restricting to a time range 05/17/23 06:01:22.522
    May 17 06:01:25.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 logs logs-generator logs-generator --since=1s'
    May 17 06:01:25.100: INFO: stderr: ""
    May 17 06:01:25.100: INFO: stdout: "I0517 06:01:24.224486       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/z6t 463\nI0517 06:01:24.424842       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/ttm 401\nI0517 06:01:24.624100       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/mwhz 545\nI0517 06:01:24.824422       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/74rl 579\nI0517 06:01:25.024775       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/vl4 322\n"
    May 17 06:01:25.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 logs logs-generator logs-generator --since=24h'
    May 17 06:01:25.169: INFO: stderr: ""
    May 17 06:01:25.169: INFO: stdout: "I0517 06:01:20.824063       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/h7l 372\nI0517 06:01:21.024214       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/77kw 247\nI0517 06:01:21.224735       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/lnh 442\nI0517 06:01:21.425127       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/sbtn 530\nI0517 06:01:21.624428       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/2qn 526\nI0517 06:01:21.824802       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/f7d7 553\nI0517 06:01:22.025158       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/k2w2 293\nI0517 06:01:22.224473       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/5c8n 269\nI0517 06:01:22.424768       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/p7n 220\nI0517 06:01:22.625063       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/ktn 569\nI0517 06:01:22.824403       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/4lr 535\nI0517 06:01:23.024593       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/j24 367\nI0517 06:01:23.224923       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/8vz 543\nI0517 06:01:23.424208       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/drdc 367\nI0517 06:01:23.624559       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/rqs 255\nI0517 06:01:23.824920       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/9kcl 462\nI0517 06:01:24.024158       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/48f 494\nI0517 06:01:24.224486       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/z6t 463\nI0517 06:01:24.424842       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/ttm 401\nI0517 06:01:24.624100       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/mwhz 545\nI0517 06:01:24.824422       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/74rl 579\nI0517 06:01:25.024775       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/vl4 322\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    May 17 06:01:25.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-3415 delete pod logs-generator'
    May 17 06:01:25.851: INFO: stderr: ""
    May 17 06:01:25.851: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:01:25.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3415" for this suite. 05/17/23 06:01:25.858
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:01:25.865
May 17 06:01:25.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename resourcequota 05/17/23 06:01:25.866
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:25.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:25.885
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 05/17/23 06:01:25.888
STEP: Creating a ResourceQuota 05/17/23 06:01:30.892
STEP: Ensuring resource quota status is calculated 05/17/23 06:01:30.898
STEP: Creating a ReplicationController 05/17/23 06:01:32.902
STEP: Ensuring resource quota status captures replication controller creation 05/17/23 06:01:32.914
STEP: Deleting a ReplicationController 05/17/23 06:01:34.919
STEP: Ensuring resource quota status released usage 05/17/23 06:01:34.926
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May 17 06:01:36.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7353" for this suite. 05/17/23 06:01:36.938
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":95,"skipped":1735,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.079 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:01:25.865
    May 17 06:01:25.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename resourcequota 05/17/23 06:01:25.866
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:25.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:25.885
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 05/17/23 06:01:25.888
    STEP: Creating a ResourceQuota 05/17/23 06:01:30.892
    STEP: Ensuring resource quota status is calculated 05/17/23 06:01:30.898
    STEP: Creating a ReplicationController 05/17/23 06:01:32.902
    STEP: Ensuring resource quota status captures replication controller creation 05/17/23 06:01:32.914
    STEP: Deleting a ReplicationController 05/17/23 06:01:34.919
    STEP: Ensuring resource quota status released usage 05/17/23 06:01:34.926
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May 17 06:01:36.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7353" for this suite. 05/17/23 06:01:36.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:01:36.945
May 17 06:01:36.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename hostport 05/17/23 06:01:36.946
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:36.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:36.965
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/17/23 06:01:36.975
May 17 06:01:36.985: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-3273" to be "running and ready"
May 17 06:01:36.988: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.070399ms
May 17 06:01:36.988: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 17 06:01:38.992: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007491625s
May 17 06:01:38.992: INFO: The phase of Pod pod1 is Running (Ready = true)
May 17 06:01:38.992: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.224.0.6 on the node which pod1 resides and expect scheduled 05/17/23 06:01:38.992
May 17 06:01:39.000: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-3273" to be "running and ready"
May 17 06:01:39.003: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.25279ms
May 17 06:01:39.003: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 17 06:01:41.007: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007339407s
May 17 06:01:41.007: INFO: The phase of Pod pod2 is Running (Ready = true)
May 17 06:01:41.007: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.224.0.6 but use UDP protocol on the node which pod2 resides 05/17/23 06:01:41.007
May 17 06:01:41.015: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-3273" to be "running and ready"
May 17 06:01:41.018: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.361696ms
May 17 06:01:41.018: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May 17 06:01:43.023: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008129318s
May 17 06:01:43.023: INFO: The phase of Pod pod3 is Running (Ready = true)
May 17 06:01:43.023: INFO: Pod "pod3" satisfied condition "running and ready"
May 17 06:01:43.030: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-3273" to be "running and ready"
May 17 06:01:43.033: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.29939ms
May 17 06:01:43.033: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May 17 06:01:45.039: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008356236s
May 17 06:01:45.039: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
May 17 06:01:45.039: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/17/23 06:01:45.042
May 17 06:01:45.042: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.224.0.6 http://127.0.0.1:54323/hostname] Namespace:hostport-3273 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:01:45.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:01:45.043: INFO: ExecWithOptions: Clientset creation
May 17 06:01:45.043: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/hostport-3273/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.224.0.6+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.224.0.6, port: 54323 05/17/23 06:01:45.192
May 17 06:01:45.192: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.224.0.6:54323/hostname] Namespace:hostport-3273 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:01:45.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:01:45.192: INFO: ExecWithOptions: Clientset creation
May 17 06:01:45.192: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/hostport-3273/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.224.0.6%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.224.0.6, port: 54323 UDP 05/17/23 06:01:45.316
May 17 06:01:45.316: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.224.0.6 54323] Namespace:hostport-3273 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:01:45.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:01:45.316: INFO: ExecWithOptions: Clientset creation
May 17 06:01:45.316: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/hostport-3273/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.224.0.6+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
May 17 06:01:50.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-3273" for this suite. 05/17/23 06:01:50.426
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":96,"skipped":1764,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.487 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:01:36.945
    May 17 06:01:36.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename hostport 05/17/23 06:01:36.946
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:36.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:36.965
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/17/23 06:01:36.975
    May 17 06:01:36.985: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-3273" to be "running and ready"
    May 17 06:01:36.988: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.070399ms
    May 17 06:01:36.988: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:01:38.992: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007491625s
    May 17 06:01:38.992: INFO: The phase of Pod pod1 is Running (Ready = true)
    May 17 06:01:38.992: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.224.0.6 on the node which pod1 resides and expect scheduled 05/17/23 06:01:38.992
    May 17 06:01:39.000: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-3273" to be "running and ready"
    May 17 06:01:39.003: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.25279ms
    May 17 06:01:39.003: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:01:41.007: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007339407s
    May 17 06:01:41.007: INFO: The phase of Pod pod2 is Running (Ready = true)
    May 17 06:01:41.007: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.224.0.6 but use UDP protocol on the node which pod2 resides 05/17/23 06:01:41.007
    May 17 06:01:41.015: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-3273" to be "running and ready"
    May 17 06:01:41.018: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.361696ms
    May 17 06:01:41.018: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:01:43.023: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008129318s
    May 17 06:01:43.023: INFO: The phase of Pod pod3 is Running (Ready = true)
    May 17 06:01:43.023: INFO: Pod "pod3" satisfied condition "running and ready"
    May 17 06:01:43.030: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-3273" to be "running and ready"
    May 17 06:01:43.033: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.29939ms
    May 17 06:01:43.033: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:01:45.039: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008356236s
    May 17 06:01:45.039: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    May 17 06:01:45.039: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/17/23 06:01:45.042
    May 17 06:01:45.042: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.224.0.6 http://127.0.0.1:54323/hostname] Namespace:hostport-3273 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:01:45.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:01:45.043: INFO: ExecWithOptions: Clientset creation
    May 17 06:01:45.043: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/hostport-3273/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.224.0.6+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.224.0.6, port: 54323 05/17/23 06:01:45.192
    May 17 06:01:45.192: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.224.0.6:54323/hostname] Namespace:hostport-3273 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:01:45.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:01:45.192: INFO: ExecWithOptions: Clientset creation
    May 17 06:01:45.192: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/hostport-3273/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.224.0.6%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.224.0.6, port: 54323 UDP 05/17/23 06:01:45.316
    May 17 06:01:45.316: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.224.0.6 54323] Namespace:hostport-3273 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:01:45.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:01:45.316: INFO: ExecWithOptions: Clientset creation
    May 17 06:01:45.316: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/hostport-3273/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.224.0.6+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    May 17 06:01:50.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-3273" for this suite. 05/17/23 06:01:50.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:01:50.434
May 17 06:01:50.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 06:01:50.434
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:50.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:50.453
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 06:01:50.468
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:01:50.657
STEP: Deploying the webhook pod 05/17/23 06:01:50.665
STEP: Wait for the deployment to be ready 05/17/23 06:01:50.675
May 17 06:01:50.683: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 06:01:52.695
STEP: Verifying the service has paired with the endpoint 05/17/23 06:01:52.705
May 17 06:01:53.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/17/23 06:01:53.709
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/17/23 06:01:53.725
STEP: Creating a dummy validating-webhook-configuration object 05/17/23 06:01:53.739
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/17/23 06:01:53.747
STEP: Creating a dummy mutating-webhook-configuration object 05/17/23 06:01:53.753
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/17/23 06:01:53.762
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:01:53.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5579" for this suite. 05/17/23 06:01:53.787
STEP: Destroying namespace "webhook-5579-markers" for this suite. 05/17/23 06:01:53.794
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":97,"skipped":1782,"failed":0}
------------------------------
â€¢ [3.396 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:01:50.434
    May 17 06:01:50.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 06:01:50.434
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:50.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:50.453
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 06:01:50.468
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:01:50.657
    STEP: Deploying the webhook pod 05/17/23 06:01:50.665
    STEP: Wait for the deployment to be ready 05/17/23 06:01:50.675
    May 17 06:01:50.683: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 06:01:52.695
    STEP: Verifying the service has paired with the endpoint 05/17/23 06:01:52.705
    May 17 06:01:53.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/17/23 06:01:53.709
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/17/23 06:01:53.725
    STEP: Creating a dummy validating-webhook-configuration object 05/17/23 06:01:53.739
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/17/23 06:01:53.747
    STEP: Creating a dummy mutating-webhook-configuration object 05/17/23 06:01:53.753
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/17/23 06:01:53.762
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:01:53.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5579" for this suite. 05/17/23 06:01:53.787
    STEP: Destroying namespace "webhook-5579-markers" for this suite. 05/17/23 06:01:53.794
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:01:53.83
May 17 06:01:53.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename gc 05/17/23 06:01:53.831
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:53.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:53.853
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
May 17 06:01:53.891: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e147f3a0-4bc9-45be-a4ae-8803aeac7989", Controller:(*bool)(0xc003ad05fa), BlockOwnerDeletion:(*bool)(0xc003ad05fb)}}
May 17 06:01:53.902: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9f960e14-de71-47d5-8df1-d1b477e8a164", Controller:(*bool)(0xc003ad08b2), BlockOwnerDeletion:(*bool)(0xc003ad08b3)}}
May 17 06:01:53.909: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0360aaca-8202-40a3-8f58-0a780556a107", Controller:(*bool)(0xc0077746b2), BlockOwnerDeletion:(*bool)(0xc0077746b3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May 17 06:01:58.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4865" for this suite. 05/17/23 06:01:58.93
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":98,"skipped":1787,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.106 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:01:53.83
    May 17 06:01:53.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename gc 05/17/23 06:01:53.831
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:53.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:53.853
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    May 17 06:01:53.891: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e147f3a0-4bc9-45be-a4ae-8803aeac7989", Controller:(*bool)(0xc003ad05fa), BlockOwnerDeletion:(*bool)(0xc003ad05fb)}}
    May 17 06:01:53.902: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9f960e14-de71-47d5-8df1-d1b477e8a164", Controller:(*bool)(0xc003ad08b2), BlockOwnerDeletion:(*bool)(0xc003ad08b3)}}
    May 17 06:01:53.909: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0360aaca-8202-40a3-8f58-0a780556a107", Controller:(*bool)(0xc0077746b2), BlockOwnerDeletion:(*bool)(0xc0077746b3)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May 17 06:01:58.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4865" for this suite. 05/17/23 06:01:58.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:01:58.937
May 17 06:01:58.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-probe 05/17/23 06:01:58.938
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:58.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:58.959
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-c76c877f-463b-460a-83f0-8379a2105dcd in namespace container-probe-6301 05/17/23 06:01:58.962
May 17 06:01:58.973: INFO: Waiting up to 5m0s for pod "liveness-c76c877f-463b-460a-83f0-8379a2105dcd" in namespace "container-probe-6301" to be "not pending"
May 17 06:01:58.977: INFO: Pod "liveness-c76c877f-463b-460a-83f0-8379a2105dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.643761ms
May 17 06:02:00.982: INFO: Pod "liveness-c76c877f-463b-460a-83f0-8379a2105dcd": Phase="Running", Reason="", readiness=true. Elapsed: 2.008484099s
May 17 06:02:00.982: INFO: Pod "liveness-c76c877f-463b-460a-83f0-8379a2105dcd" satisfied condition "not pending"
May 17 06:02:00.982: INFO: Started pod liveness-c76c877f-463b-460a-83f0-8379a2105dcd in namespace container-probe-6301
STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 06:02:00.982
May 17 06:02:00.985: INFO: Initial restart count of pod liveness-c76c877f-463b-460a-83f0-8379a2105dcd is 0
May 17 06:02:21.041: INFO: Restart count of pod container-probe-6301/liveness-c76c877f-463b-460a-83f0-8379a2105dcd is now 1 (20.056077781s elapsed)
May 17 06:02:41.101: INFO: Restart count of pod container-probe-6301/liveness-c76c877f-463b-460a-83f0-8379a2105dcd is now 2 (40.115406133s elapsed)
May 17 06:03:01.153: INFO: Restart count of pod container-probe-6301/liveness-c76c877f-463b-460a-83f0-8379a2105dcd is now 3 (1m0.167999864s elapsed)
May 17 06:03:21.205: INFO: Restart count of pod container-probe-6301/liveness-c76c877f-463b-460a-83f0-8379a2105dcd is now 4 (1m20.219999175s elapsed)
May 17 06:04:29.390: INFO: Restart count of pod container-probe-6301/liveness-c76c877f-463b-460a-83f0-8379a2105dcd is now 5 (2m28.404769248s elapsed)
STEP: deleting the pod 05/17/23 06:04:29.39
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May 17 06:04:29.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6301" for this suite. 05/17/23 06:04:29.425
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":99,"skipped":1829,"failed":0}
------------------------------
â€¢ [SLOW TEST] [150.493 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:01:58.937
    May 17 06:01:58.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-probe 05/17/23 06:01:58.938
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:01:58.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:01:58.959
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-c76c877f-463b-460a-83f0-8379a2105dcd in namespace container-probe-6301 05/17/23 06:01:58.962
    May 17 06:01:58.973: INFO: Waiting up to 5m0s for pod "liveness-c76c877f-463b-460a-83f0-8379a2105dcd" in namespace "container-probe-6301" to be "not pending"
    May 17 06:01:58.977: INFO: Pod "liveness-c76c877f-463b-460a-83f0-8379a2105dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.643761ms
    May 17 06:02:00.982: INFO: Pod "liveness-c76c877f-463b-460a-83f0-8379a2105dcd": Phase="Running", Reason="", readiness=true. Elapsed: 2.008484099s
    May 17 06:02:00.982: INFO: Pod "liveness-c76c877f-463b-460a-83f0-8379a2105dcd" satisfied condition "not pending"
    May 17 06:02:00.982: INFO: Started pod liveness-c76c877f-463b-460a-83f0-8379a2105dcd in namespace container-probe-6301
    STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 06:02:00.982
    May 17 06:02:00.985: INFO: Initial restart count of pod liveness-c76c877f-463b-460a-83f0-8379a2105dcd is 0
    May 17 06:02:21.041: INFO: Restart count of pod container-probe-6301/liveness-c76c877f-463b-460a-83f0-8379a2105dcd is now 1 (20.056077781s elapsed)
    May 17 06:02:41.101: INFO: Restart count of pod container-probe-6301/liveness-c76c877f-463b-460a-83f0-8379a2105dcd is now 2 (40.115406133s elapsed)
    May 17 06:03:01.153: INFO: Restart count of pod container-probe-6301/liveness-c76c877f-463b-460a-83f0-8379a2105dcd is now 3 (1m0.167999864s elapsed)
    May 17 06:03:21.205: INFO: Restart count of pod container-probe-6301/liveness-c76c877f-463b-460a-83f0-8379a2105dcd is now 4 (1m20.219999175s elapsed)
    May 17 06:04:29.390: INFO: Restart count of pod container-probe-6301/liveness-c76c877f-463b-460a-83f0-8379a2105dcd is now 5 (2m28.404769248s elapsed)
    STEP: deleting the pod 05/17/23 06:04:29.39
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May 17 06:04:29.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6301" for this suite. 05/17/23 06:04:29.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:04:29.431
May 17 06:04:29.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename crd-webhook 05/17/23 06:04:29.432
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:04:29.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:04:29.456
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/17/23 06:04:29.459
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/17/23 06:04:29.87
STEP: Deploying the custom resource conversion webhook pod 05/17/23 06:04:29.878
STEP: Wait for the deployment to be ready 05/17/23 06:04:29.889
May 17 06:04:29.897: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/17/23 06:04:31.909
STEP: Verifying the service has paired with the endpoint 05/17/23 06:04:31.92
May 17 06:04:32.921: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
May 17 06:04:32.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Creating a v1 custom resource 05/17/23 06:04:35.674
STEP: v2 custom resource should be converted 05/17/23 06:04:35.679
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:04:36.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4956" for this suite. 05/17/23 06:04:36.205
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":100,"skipped":1834,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.819 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:04:29.431
    May 17 06:04:29.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename crd-webhook 05/17/23 06:04:29.432
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:04:29.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:04:29.456
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/17/23 06:04:29.459
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/17/23 06:04:29.87
    STEP: Deploying the custom resource conversion webhook pod 05/17/23 06:04:29.878
    STEP: Wait for the deployment to be ready 05/17/23 06:04:29.889
    May 17 06:04:29.897: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/17/23 06:04:31.909
    STEP: Verifying the service has paired with the endpoint 05/17/23 06:04:31.92
    May 17 06:04:32.921: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    May 17 06:04:32.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Creating a v1 custom resource 05/17/23 06:04:35.674
    STEP: v2 custom resource should be converted 05/17/23 06:04:35.679
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:04:36.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-4956" for this suite. 05/17/23 06:04:36.205
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:04:36.252
May 17 06:04:36.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 06:04:36.252
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:04:36.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:04:36.274
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 05/17/23 06:04:36.278
May 17 06:04:36.293: INFO: Waiting up to 5m0s for pod "pod-c59f10a3-8f6e-472e-98c1-68638a6568ca" in namespace "emptydir-441" to be "Succeeded or Failed"
May 17 06:04:36.297: INFO: Pod "pod-c59f10a3-8f6e-472e-98c1-68638a6568ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.844086ms
May 17 06:04:38.303: INFO: Pod "pod-c59f10a3-8f6e-472e-98c1-68638a6568ca": Phase="Running", Reason="", readiness=false. Elapsed: 2.009318727s
May 17 06:04:40.302: INFO: Pod "pod-c59f10a3-8f6e-472e-98c1-68638a6568ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008147002s
STEP: Saw pod success 05/17/23 06:04:40.302
May 17 06:04:40.302: INFO: Pod "pod-c59f10a3-8f6e-472e-98c1-68638a6568ca" satisfied condition "Succeeded or Failed"
May 17 06:04:40.306: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod pod-c59f10a3-8f6e-472e-98c1-68638a6568ca container test-container: <nil>
STEP: delete the pod 05/17/23 06:04:40.314
May 17 06:04:40.325: INFO: Waiting for pod pod-c59f10a3-8f6e-472e-98c1-68638a6568ca to disappear
May 17 06:04:40.328: INFO: Pod pod-c59f10a3-8f6e-472e-98c1-68638a6568ca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 06:04:40.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-441" for this suite. 05/17/23 06:04:40.335
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":101,"skipped":1851,"failed":0}
------------------------------
â€¢ [4.089 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:04:36.252
    May 17 06:04:36.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 06:04:36.252
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:04:36.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:04:36.274
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/17/23 06:04:36.278
    May 17 06:04:36.293: INFO: Waiting up to 5m0s for pod "pod-c59f10a3-8f6e-472e-98c1-68638a6568ca" in namespace "emptydir-441" to be "Succeeded or Failed"
    May 17 06:04:36.297: INFO: Pod "pod-c59f10a3-8f6e-472e-98c1-68638a6568ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.844086ms
    May 17 06:04:38.303: INFO: Pod "pod-c59f10a3-8f6e-472e-98c1-68638a6568ca": Phase="Running", Reason="", readiness=false. Elapsed: 2.009318727s
    May 17 06:04:40.302: INFO: Pod "pod-c59f10a3-8f6e-472e-98c1-68638a6568ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008147002s
    STEP: Saw pod success 05/17/23 06:04:40.302
    May 17 06:04:40.302: INFO: Pod "pod-c59f10a3-8f6e-472e-98c1-68638a6568ca" satisfied condition "Succeeded or Failed"
    May 17 06:04:40.306: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod pod-c59f10a3-8f6e-472e-98c1-68638a6568ca container test-container: <nil>
    STEP: delete the pod 05/17/23 06:04:40.314
    May 17 06:04:40.325: INFO: Waiting for pod pod-c59f10a3-8f6e-472e-98c1-68638a6568ca to disappear
    May 17 06:04:40.328: INFO: Pod pod-c59f10a3-8f6e-472e-98c1-68638a6568ca no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 06:04:40.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-441" for this suite. 05/17/23 06:04:40.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:04:40.342
May 17 06:04:40.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 06:04:40.343
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:04:40.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:04:40.362
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 05/17/23 06:04:40.365
STEP: listing secrets in all namespaces to ensure that there are more than zero 05/17/23 06:04:40.37
STEP: patching the secret 05/17/23 06:04:40.378
STEP: deleting the secret using a LabelSelector 05/17/23 06:04:40.386
STEP: listing secrets in all namespaces, searching for label name and value in patch 05/17/23 06:04:40.393
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May 17 06:04:40.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6110" for this suite. 05/17/23 06:04:40.408
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":102,"skipped":1875,"failed":0}
------------------------------
â€¢ [0.071 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:04:40.342
    May 17 06:04:40.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 06:04:40.343
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:04:40.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:04:40.362
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 05/17/23 06:04:40.365
    STEP: listing secrets in all namespaces to ensure that there are more than zero 05/17/23 06:04:40.37
    STEP: patching the secret 05/17/23 06:04:40.378
    STEP: deleting the secret using a LabelSelector 05/17/23 06:04:40.386
    STEP: listing secrets in all namespaces, searching for label name and value in patch 05/17/23 06:04:40.393
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May 17 06:04:40.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6110" for this suite. 05/17/23 06:04:40.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:04:40.414
May 17 06:04:40.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:04:40.415
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:04:40.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:04:40.433
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-7b81e34e-b915-4629-b85f-cecec955dc0b 05/17/23 06:04:40.436
STEP: Creating secret with name secret-projected-all-test-volume-461f4210-e532-4a10-9728-25927eff1314 05/17/23 06:04:40.44
STEP: Creating a pod to test Check all projections for projected volume plugin 05/17/23 06:04:40.444
May 17 06:04:40.454: INFO: Waiting up to 5m0s for pod "projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a" in namespace "projected-1091" to be "Succeeded or Failed"
May 17 06:04:40.457: INFO: Pod "projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.431874ms
May 17 06:04:42.463: INFO: Pod "projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009350513s
May 17 06:04:44.463: INFO: Pod "projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0086455s
STEP: Saw pod success 05/17/23 06:04:44.463
May 17 06:04:44.463: INFO: Pod "projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a" satisfied condition "Succeeded or Failed"
May 17 06:04:44.466: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a container projected-all-volume-test: <nil>
STEP: delete the pod 05/17/23 06:04:44.475
May 17 06:04:44.487: INFO: Waiting for pod projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a to disappear
May 17 06:04:44.490: INFO: Pod projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
May 17 06:04:44.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1091" for this suite. 05/17/23 06:04:44.497
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":103,"skipped":1897,"failed":0}
------------------------------
â€¢ [4.092 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:04:40.414
    May 17 06:04:40.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:04:40.415
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:04:40.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:04:40.433
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-7b81e34e-b915-4629-b85f-cecec955dc0b 05/17/23 06:04:40.436
    STEP: Creating secret with name secret-projected-all-test-volume-461f4210-e532-4a10-9728-25927eff1314 05/17/23 06:04:40.44
    STEP: Creating a pod to test Check all projections for projected volume plugin 05/17/23 06:04:40.444
    May 17 06:04:40.454: INFO: Waiting up to 5m0s for pod "projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a" in namespace "projected-1091" to be "Succeeded or Failed"
    May 17 06:04:40.457: INFO: Pod "projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.431874ms
    May 17 06:04:42.463: INFO: Pod "projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009350513s
    May 17 06:04:44.463: INFO: Pod "projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0086455s
    STEP: Saw pod success 05/17/23 06:04:44.463
    May 17 06:04:44.463: INFO: Pod "projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a" satisfied condition "Succeeded or Failed"
    May 17 06:04:44.466: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a container projected-all-volume-test: <nil>
    STEP: delete the pod 05/17/23 06:04:44.475
    May 17 06:04:44.487: INFO: Waiting for pod projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a to disappear
    May 17 06:04:44.490: INFO: Pod projected-volume-84bf69de-c006-437a-a307-aa4cc98be17a no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    May 17 06:04:44.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1091" for this suite. 05/17/23 06:04:44.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:04:44.511
May 17 06:04:44.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename init-container 05/17/23 06:04:44.512
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:04:44.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:04:44.534
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 05/17/23 06:04:44.538
May 17 06:04:44.538: INFO: PodSpec: initContainers in spec.initContainers
May 17 06:05:29.938: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1deaee31-be5a-4aa6-afc4-a090b674a955", GenerateName:"", Namespace:"init-container-5470", SelfLink:"", UID:"8fd8a67c-757c-44bf-905d-36c2052b2714", ResourceVersion:"1452522", Generation:0, CreationTimestamp:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"538836860"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003a490c8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 17, 6, 5, 29, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003a490f8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-zvnhd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003b8b2c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zvnhd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zvnhd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zvnhd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003ad1a08), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"aks-agentpool-72615086-vmss00000e", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0005c3500), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003ad1a80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003ad1aa0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003ad1abc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003ad1ac0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0081a0690), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.224.0.7", PodIP:"10.244.3.48", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.3.48"}}, StartTime:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005c35e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005c3650)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://f207ee4a0aef3383c9a768e88d05b58128763551d7d575017da585475fda7a6b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003b8b340), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003b8b320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc003ad1b3f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May 17 06:05:29.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5470" for this suite. 05/17/23 06:05:29.946
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":104,"skipped":2014,"failed":0}
------------------------------
â€¢ [SLOW TEST] [45.443 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:04:44.511
    May 17 06:04:44.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename init-container 05/17/23 06:04:44.512
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:04:44.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:04:44.534
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 05/17/23 06:04:44.538
    May 17 06:04:44.538: INFO: PodSpec: initContainers in spec.initContainers
    May 17 06:05:29.938: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1deaee31-be5a-4aa6-afc4-a090b674a955", GenerateName:"", Namespace:"init-container-5470", SelfLink:"", UID:"8fd8a67c-757c-44bf-905d-36c2052b2714", ResourceVersion:"1452522", Generation:0, CreationTimestamp:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"538836860"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003a490c8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 17, 6, 5, 29, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003a490f8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-zvnhd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003b8b2c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zvnhd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zvnhd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zvnhd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003ad1a08), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"aks-agentpool-72615086-vmss00000e", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0005c3500), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003ad1a80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003ad1aa0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003ad1abc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003ad1ac0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0081a0690), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.224.0.7", PodIP:"10.244.3.48", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.3.48"}}, StartTime:time.Date(2023, time.May, 17, 6, 4, 44, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005c35e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005c3650)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://f207ee4a0aef3383c9a768e88d05b58128763551d7d575017da585475fda7a6b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003b8b340), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003b8b320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc003ad1b3f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May 17 06:05:29.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5470" for this suite. 05/17/23 06:05:29.946
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:05:29.955
May 17 06:05:29.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:05:29.956
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:05:29.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:05:29.975
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-d304b80e-0d61-4d87-adac-ba6b33aedfce 05/17/23 06:05:29.985
STEP: Creating configMap with name cm-test-opt-upd-ae0f85eb-96c0-4946-89d8-5a69de9aae8c 05/17/23 06:05:30.001
STEP: Creating the pod 05/17/23 06:05:30.006
May 17 06:05:30.018: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2378a099-ee57-4d88-8cdc-a691ca30c7a5" in namespace "projected-6118" to be "running and ready"
May 17 06:05:30.022: INFO: Pod "pod-projected-configmaps-2378a099-ee57-4d88-8cdc-a691ca30c7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261233ms
May 17 06:05:30.022: INFO: The phase of Pod pod-projected-configmaps-2378a099-ee57-4d88-8cdc-a691ca30c7a5 is Pending, waiting for it to be Running (with Ready = true)
May 17 06:05:32.031: INFO: Pod "pod-projected-configmaps-2378a099-ee57-4d88-8cdc-a691ca30c7a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013518267s
May 17 06:05:32.031: INFO: The phase of Pod pod-projected-configmaps-2378a099-ee57-4d88-8cdc-a691ca30c7a5 is Running (Ready = true)
May 17 06:05:32.031: INFO: Pod "pod-projected-configmaps-2378a099-ee57-4d88-8cdc-a691ca30c7a5" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-d304b80e-0d61-4d87-adac-ba6b33aedfce 05/17/23 06:05:32.06
STEP: Updating configmap cm-test-opt-upd-ae0f85eb-96c0-4946-89d8-5a69de9aae8c 05/17/23 06:05:32.066
STEP: Creating configMap with name cm-test-opt-create-c3aa4151-b725-4b7e-8c4c-4d0c963a2374 05/17/23 06:05:32.074
STEP: waiting to observe update in volume 05/17/23 06:05:32.08
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May 17 06:05:34.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6118" for this suite. 05/17/23 06:05:34.126
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":105,"skipped":2014,"failed":0}
------------------------------
â€¢ [4.177 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:05:29.955
    May 17 06:05:29.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:05:29.956
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:05:29.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:05:29.975
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-d304b80e-0d61-4d87-adac-ba6b33aedfce 05/17/23 06:05:29.985
    STEP: Creating configMap with name cm-test-opt-upd-ae0f85eb-96c0-4946-89d8-5a69de9aae8c 05/17/23 06:05:30.001
    STEP: Creating the pod 05/17/23 06:05:30.006
    May 17 06:05:30.018: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2378a099-ee57-4d88-8cdc-a691ca30c7a5" in namespace "projected-6118" to be "running and ready"
    May 17 06:05:30.022: INFO: Pod "pod-projected-configmaps-2378a099-ee57-4d88-8cdc-a691ca30c7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261233ms
    May 17 06:05:30.022: INFO: The phase of Pod pod-projected-configmaps-2378a099-ee57-4d88-8cdc-a691ca30c7a5 is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:05:32.031: INFO: Pod "pod-projected-configmaps-2378a099-ee57-4d88-8cdc-a691ca30c7a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013518267s
    May 17 06:05:32.031: INFO: The phase of Pod pod-projected-configmaps-2378a099-ee57-4d88-8cdc-a691ca30c7a5 is Running (Ready = true)
    May 17 06:05:32.031: INFO: Pod "pod-projected-configmaps-2378a099-ee57-4d88-8cdc-a691ca30c7a5" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-d304b80e-0d61-4d87-adac-ba6b33aedfce 05/17/23 06:05:32.06
    STEP: Updating configmap cm-test-opt-upd-ae0f85eb-96c0-4946-89d8-5a69de9aae8c 05/17/23 06:05:32.066
    STEP: Creating configMap with name cm-test-opt-create-c3aa4151-b725-4b7e-8c4c-4d0c963a2374 05/17/23 06:05:32.074
    STEP: waiting to observe update in volume 05/17/23 06:05:32.08
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May 17 06:05:34.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6118" for this suite. 05/17/23 06:05:34.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:05:34.134
May 17 06:05:34.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:05:34.135
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:05:34.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:05:34.153
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2413 05/17/23 06:05:34.156
STEP: changing the ExternalName service to type=ClusterIP 05/17/23 06:05:34.165
STEP: creating replication controller externalname-service in namespace services-2413 05/17/23 06:05:34.179
I0517 06:05:34.185203      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2413, replica count: 2
I0517 06:05:37.237126      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 06:05:37.237: INFO: Creating new exec pod
May 17 06:05:37.247: INFO: Waiting up to 5m0s for pod "execpodj7tfh" in namespace "services-2413" to be "running"
May 17 06:05:37.251: INFO: Pod "execpodj7tfh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.241966ms
May 17 06:05:39.256: INFO: Pod "execpodj7tfh": Phase="Running", Reason="", readiness=true. Elapsed: 2.008559464s
May 17 06:05:39.256: INFO: Pod "execpodj7tfh" satisfied condition "running"
May 17 06:05:40.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2413 exec execpodj7tfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 17 06:05:40.438: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 17 06:05:40.438: INFO: stdout: "externalname-service-7z2ss"
May 17 06:05:40.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2413 exec execpodj7tfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.7.142 80'
May 17 06:05:40.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.7.142 80\nConnection to 10.0.7.142 80 port [tcp/http] succeeded!\n"
May 17 06:05:40.619: INFO: stdout: ""
May 17 06:05:41.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2413 exec execpodj7tfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.7.142 80'
May 17 06:05:41.819: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.7.142 80\nConnection to 10.0.7.142 80 port [tcp/http] succeeded!\n"
May 17 06:05:41.819: INFO: stdout: ""
May 17 06:05:42.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2413 exec execpodj7tfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.7.142 80'
May 17 06:05:42.782: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.7.142 80\nConnection to 10.0.7.142 80 port [tcp/http] succeeded!\n"
May 17 06:05:42.782: INFO: stdout: ""
May 17 06:05:43.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2413 exec execpodj7tfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.7.142 80'
May 17 06:05:43.821: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.7.142 80\nConnection to 10.0.7.142 80 port [tcp/http] succeeded!\n"
May 17 06:05:43.821: INFO: stdout: "externalname-service-7z2ss"
May 17 06:05:43.821: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:05:43.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2413" for this suite. 05/17/23 06:05:43.842
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":106,"skipped":2071,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.714 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:05:34.134
    May 17 06:05:34.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:05:34.135
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:05:34.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:05:34.153
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2413 05/17/23 06:05:34.156
    STEP: changing the ExternalName service to type=ClusterIP 05/17/23 06:05:34.165
    STEP: creating replication controller externalname-service in namespace services-2413 05/17/23 06:05:34.179
    I0517 06:05:34.185203      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2413, replica count: 2
    I0517 06:05:37.237126      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 06:05:37.237: INFO: Creating new exec pod
    May 17 06:05:37.247: INFO: Waiting up to 5m0s for pod "execpodj7tfh" in namespace "services-2413" to be "running"
    May 17 06:05:37.251: INFO: Pod "execpodj7tfh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.241966ms
    May 17 06:05:39.256: INFO: Pod "execpodj7tfh": Phase="Running", Reason="", readiness=true. Elapsed: 2.008559464s
    May 17 06:05:39.256: INFO: Pod "execpodj7tfh" satisfied condition "running"
    May 17 06:05:40.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2413 exec execpodj7tfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    May 17 06:05:40.438: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May 17 06:05:40.438: INFO: stdout: "externalname-service-7z2ss"
    May 17 06:05:40.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2413 exec execpodj7tfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.7.142 80'
    May 17 06:05:40.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.7.142 80\nConnection to 10.0.7.142 80 port [tcp/http] succeeded!\n"
    May 17 06:05:40.619: INFO: stdout: ""
    May 17 06:05:41.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2413 exec execpodj7tfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.7.142 80'
    May 17 06:05:41.819: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.7.142 80\nConnection to 10.0.7.142 80 port [tcp/http] succeeded!\n"
    May 17 06:05:41.819: INFO: stdout: ""
    May 17 06:05:42.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2413 exec execpodj7tfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.7.142 80'
    May 17 06:05:42.782: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.7.142 80\nConnection to 10.0.7.142 80 port [tcp/http] succeeded!\n"
    May 17 06:05:42.782: INFO: stdout: ""
    May 17 06:05:43.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-2413 exec execpodj7tfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.7.142 80'
    May 17 06:05:43.821: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.7.142 80\nConnection to 10.0.7.142 80 port [tcp/http] succeeded!\n"
    May 17 06:05:43.821: INFO: stdout: "externalname-service-7z2ss"
    May 17 06:05:43.821: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:05:43.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2413" for this suite. 05/17/23 06:05:43.842
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:05:43.848
May 17 06:05:43.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename endpointslicemirroring 05/17/23 06:05:43.849
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:05:43.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:05:43.867
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 05/17/23 06:05:43.881
May 17 06:05:43.889: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 05/17/23 06:05:45.893
May 17 06:05:45.901: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 05/17/23 06:05:47.906
May 17 06:05:47.916: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
May 17 06:05:49.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9968" for this suite. 05/17/23 06:05:49.927
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":107,"skipped":2072,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.085 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:05:43.848
    May 17 06:05:43.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename endpointslicemirroring 05/17/23 06:05:43.849
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:05:43.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:05:43.867
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 05/17/23 06:05:43.881
    May 17 06:05:43.889: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 05/17/23 06:05:45.893
    May 17 06:05:45.901: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 05/17/23 06:05:47.906
    May 17 06:05:47.916: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    May 17 06:05:49.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-9968" for this suite. 05/17/23 06:05:49.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:05:49.934
May 17 06:05:49.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:05:49.934
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:05:49.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:05:49.953
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-06298275-c522-4f46-b792-52b60352a877 05/17/23 06:05:49.957
STEP: Creating a pod to test consume configMaps 05/17/23 06:05:49.961
May 17 06:05:49.971: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4" in namespace "projected-6091" to be "Succeeded or Failed"
May 17 06:05:49.974: INFO: Pod "pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.710052ms
May 17 06:05:51.980: INFO: Pod "pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009131622s
May 17 06:05:53.980: INFO: Pod "pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009739178s
STEP: Saw pod success 05/17/23 06:05:53.98
May 17 06:05:53.980: INFO: Pod "pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4" satisfied condition "Succeeded or Failed"
May 17 06:05:53.984: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4 container projected-configmap-volume-test: <nil>
STEP: delete the pod 05/17/23 06:05:53.992
May 17 06:05:54.004: INFO: Waiting for pod pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4 to disappear
May 17 06:05:54.007: INFO: Pod pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May 17 06:05:54.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6091" for this suite. 05/17/23 06:05:54.014
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":108,"skipped":2078,"failed":0}
------------------------------
â€¢ [4.085 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:05:49.934
    May 17 06:05:49.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:05:49.934
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:05:49.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:05:49.953
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-06298275-c522-4f46-b792-52b60352a877 05/17/23 06:05:49.957
    STEP: Creating a pod to test consume configMaps 05/17/23 06:05:49.961
    May 17 06:05:49.971: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4" in namespace "projected-6091" to be "Succeeded or Failed"
    May 17 06:05:49.974: INFO: Pod "pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.710052ms
    May 17 06:05:51.980: INFO: Pod "pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009131622s
    May 17 06:05:53.980: INFO: Pod "pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009739178s
    STEP: Saw pod success 05/17/23 06:05:53.98
    May 17 06:05:53.980: INFO: Pod "pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4" satisfied condition "Succeeded or Failed"
    May 17 06:05:53.984: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 05/17/23 06:05:53.992
    May 17 06:05:54.004: INFO: Waiting for pod pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4 to disappear
    May 17 06:05:54.007: INFO: Pod pod-projected-configmaps-59212c71-3b16-44c9-9e55-3e39404464f4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May 17 06:05:54.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6091" for this suite. 05/17/23 06:05:54.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:05:54.02
May 17 06:05:54.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename var-expansion 05/17/23 06:05:54.021
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:05:54.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:05:54.042
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 05/17/23 06:05:54.046
May 17 06:05:54.055: INFO: Waiting up to 2m0s for pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364" in namespace "var-expansion-6179" to be "running"
May 17 06:05:54.059: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 3.705747ms
May 17 06:05:56.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008267498s
May 17 06:05:58.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008349107s
May 17 06:06:00.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008544555s
May 17 06:06:02.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009348884s
May 17 06:06:04.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008932959s
May 17 06:06:06.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008652507s
May 17 06:06:08.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009620587s
May 17 06:06:10.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009910994s
May 17 06:06:12.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008890019s
May 17 06:06:14.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 20.011695778s
May 17 06:06:16.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01008027s
May 17 06:06:18.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010805355s
May 17 06:06:20.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009921871s
May 17 06:06:22.073: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 28.017758969s
May 17 06:06:24.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 30.01034584s
May 17 06:06:26.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 32.01172124s
May 17 06:06:28.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011977733s
May 17 06:06:30.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009557601s
May 17 06:06:32.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008772693s
May 17 06:06:34.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009508995s
May 17 06:06:36.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010004628s
May 17 06:06:38.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008254074s
May 17 06:06:40.063: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008061841s
May 17 06:06:42.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009241469s
May 17 06:06:44.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009212889s
May 17 06:06:46.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008541924s
May 17 06:06:48.073: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 54.017306358s
May 17 06:06:50.068: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 56.013064133s
May 17 06:06:52.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008215719s
May 17 06:06:54.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009147347s
May 17 06:06:56.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011184299s
May 17 06:06:58.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009368671s
May 17 06:07:00.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.00979272s
May 17 06:07:02.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.00848035s
May 17 06:07:04.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010165555s
May 17 06:07:06.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009365039s
May 17 06:07:08.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009553447s
May 17 06:07:10.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011763439s
May 17 06:07:12.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009529414s
May 17 06:07:14.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.01038879s
May 17 06:07:16.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011638082s
May 17 06:07:18.069: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.013303571s
May 17 06:07:20.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009830201s
May 17 06:07:22.069: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.013549497s
May 17 06:07:24.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010872894s
May 17 06:07:26.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008998897s
May 17 06:07:28.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009175354s
May 17 06:07:30.063: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008003839s
May 17 06:07:32.068: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.012838102s
May 17 06:07:34.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009748039s
May 17 06:07:36.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010831278s
May 17 06:07:38.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008513283s
May 17 06:07:40.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008696829s
May 17 06:07:42.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010609577s
May 17 06:07:44.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011902061s
May 17 06:07:46.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008782797s
May 17 06:07:48.069: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.014112124s
May 17 06:07:50.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009239519s
May 17 06:07:52.063: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008067638s
May 17 06:07:54.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009253366s
May 17 06:07:54.069: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013269054s
STEP: updating the pod 05/17/23 06:07:54.069
May 17 06:07:54.581: INFO: Successfully updated pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364"
STEP: waiting for pod running 05/17/23 06:07:54.581
May 17 06:07:54.581: INFO: Waiting up to 2m0s for pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364" in namespace "var-expansion-6179" to be "running"
May 17 06:07:54.585: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 3.591315ms
May 17 06:07:56.590: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Running", Reason="", readiness=true. Elapsed: 2.008063247s
May 17 06:07:56.590: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364" satisfied condition "running"
STEP: deleting the pod gracefully 05/17/23 06:07:56.59
May 17 06:07:56.590: INFO: Deleting pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364" in namespace "var-expansion-6179"
May 17 06:07:56.596: INFO: Wait up to 5m0s for pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May 17 06:08:28.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6179" for this suite. 05/17/23 06:08:28.613
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":109,"skipped":2106,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.599 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:05:54.02
    May 17 06:05:54.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename var-expansion 05/17/23 06:05:54.021
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:05:54.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:05:54.042
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 05/17/23 06:05:54.046
    May 17 06:05:54.055: INFO: Waiting up to 2m0s for pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364" in namespace "var-expansion-6179" to be "running"
    May 17 06:05:54.059: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 3.705747ms
    May 17 06:05:56.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008267498s
    May 17 06:05:58.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008349107s
    May 17 06:06:00.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008544555s
    May 17 06:06:02.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009348884s
    May 17 06:06:04.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008932959s
    May 17 06:06:06.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008652507s
    May 17 06:06:08.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009620587s
    May 17 06:06:10.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009910994s
    May 17 06:06:12.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008890019s
    May 17 06:06:14.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 20.011695778s
    May 17 06:06:16.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01008027s
    May 17 06:06:18.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010805355s
    May 17 06:06:20.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009921871s
    May 17 06:06:22.073: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 28.017758969s
    May 17 06:06:24.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 30.01034584s
    May 17 06:06:26.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 32.01172124s
    May 17 06:06:28.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011977733s
    May 17 06:06:30.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009557601s
    May 17 06:06:32.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008772693s
    May 17 06:06:34.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009508995s
    May 17 06:06:36.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010004628s
    May 17 06:06:38.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008254074s
    May 17 06:06:40.063: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008061841s
    May 17 06:06:42.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009241469s
    May 17 06:06:44.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009212889s
    May 17 06:06:46.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008541924s
    May 17 06:06:48.073: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 54.017306358s
    May 17 06:06:50.068: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 56.013064133s
    May 17 06:06:52.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008215719s
    May 17 06:06:54.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009147347s
    May 17 06:06:56.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011184299s
    May 17 06:06:58.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009368671s
    May 17 06:07:00.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.00979272s
    May 17 06:07:02.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.00848035s
    May 17 06:07:04.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010165555s
    May 17 06:07:06.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009365039s
    May 17 06:07:08.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009553447s
    May 17 06:07:10.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011763439s
    May 17 06:07:12.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009529414s
    May 17 06:07:14.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.01038879s
    May 17 06:07:16.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011638082s
    May 17 06:07:18.069: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.013303571s
    May 17 06:07:20.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009830201s
    May 17 06:07:22.069: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.013549497s
    May 17 06:07:24.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010872894s
    May 17 06:07:26.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008998897s
    May 17 06:07:28.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009175354s
    May 17 06:07:30.063: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008003839s
    May 17 06:07:32.068: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.012838102s
    May 17 06:07:34.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009748039s
    May 17 06:07:36.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010831278s
    May 17 06:07:38.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008513283s
    May 17 06:07:40.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008696829s
    May 17 06:07:42.066: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010609577s
    May 17 06:07:44.067: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011902061s
    May 17 06:07:46.064: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008782797s
    May 17 06:07:48.069: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.014112124s
    May 17 06:07:50.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009239519s
    May 17 06:07:52.063: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008067638s
    May 17 06:07:54.065: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009253366s
    May 17 06:07:54.069: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013269054s
    STEP: updating the pod 05/17/23 06:07:54.069
    May 17 06:07:54.581: INFO: Successfully updated pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364"
    STEP: waiting for pod running 05/17/23 06:07:54.581
    May 17 06:07:54.581: INFO: Waiting up to 2m0s for pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364" in namespace "var-expansion-6179" to be "running"
    May 17 06:07:54.585: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Pending", Reason="", readiness=false. Elapsed: 3.591315ms
    May 17 06:07:56.590: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364": Phase="Running", Reason="", readiness=true. Elapsed: 2.008063247s
    May 17 06:07:56.590: INFO: Pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364" satisfied condition "running"
    STEP: deleting the pod gracefully 05/17/23 06:07:56.59
    May 17 06:07:56.590: INFO: Deleting pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364" in namespace "var-expansion-6179"
    May 17 06:07:56.596: INFO: Wait up to 5m0s for pod "var-expansion-ee162065-eb4b-4778-971e-fc6924495364" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May 17 06:08:28.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6179" for this suite. 05/17/23 06:08:28.613
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:08:28.62
May 17 06:08:28.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename security-context 05/17/23 06:08:28.62
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:08:28.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:08:28.64
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/17/23 06:08:28.644
May 17 06:08:28.660: INFO: Waiting up to 5m0s for pod "security-context-18cb6878-1425-478d-b33a-7af1831abd06" in namespace "security-context-8134" to be "Succeeded or Failed"
May 17 06:08:28.664: INFO: Pod "security-context-18cb6878-1425-478d-b33a-7af1831abd06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.000455ms
May 17 06:08:30.668: INFO: Pod "security-context-18cb6878-1425-478d-b33a-7af1831abd06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008670122s
May 17 06:08:32.670: INFO: Pod "security-context-18cb6878-1425-478d-b33a-7af1831abd06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009882682s
STEP: Saw pod success 05/17/23 06:08:32.67
May 17 06:08:32.670: INFO: Pod "security-context-18cb6878-1425-478d-b33a-7af1831abd06" satisfied condition "Succeeded or Failed"
May 17 06:08:32.673: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod security-context-18cb6878-1425-478d-b33a-7af1831abd06 container test-container: <nil>
STEP: delete the pod 05/17/23 06:08:32.681
May 17 06:08:32.690: INFO: Waiting for pod security-context-18cb6878-1425-478d-b33a-7af1831abd06 to disappear
May 17 06:08:32.694: INFO: Pod security-context-18cb6878-1425-478d-b33a-7af1831abd06 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May 17 06:08:32.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8134" for this suite. 05/17/23 06:08:32.701
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":110,"skipped":2108,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:08:28.62
    May 17 06:08:28.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename security-context 05/17/23 06:08:28.62
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:08:28.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:08:28.64
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/17/23 06:08:28.644
    May 17 06:08:28.660: INFO: Waiting up to 5m0s for pod "security-context-18cb6878-1425-478d-b33a-7af1831abd06" in namespace "security-context-8134" to be "Succeeded or Failed"
    May 17 06:08:28.664: INFO: Pod "security-context-18cb6878-1425-478d-b33a-7af1831abd06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.000455ms
    May 17 06:08:30.668: INFO: Pod "security-context-18cb6878-1425-478d-b33a-7af1831abd06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008670122s
    May 17 06:08:32.670: INFO: Pod "security-context-18cb6878-1425-478d-b33a-7af1831abd06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009882682s
    STEP: Saw pod success 05/17/23 06:08:32.67
    May 17 06:08:32.670: INFO: Pod "security-context-18cb6878-1425-478d-b33a-7af1831abd06" satisfied condition "Succeeded or Failed"
    May 17 06:08:32.673: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000e pod security-context-18cb6878-1425-478d-b33a-7af1831abd06 container test-container: <nil>
    STEP: delete the pod 05/17/23 06:08:32.681
    May 17 06:08:32.690: INFO: Waiting for pod security-context-18cb6878-1425-478d-b33a-7af1831abd06 to disappear
    May 17 06:08:32.694: INFO: Pod security-context-18cb6878-1425-478d-b33a-7af1831abd06 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May 17 06:08:32.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-8134" for this suite. 05/17/23 06:08:32.701
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:08:32.707
May 17 06:08:32.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename resourcequota 05/17/23 06:08:32.708
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:08:32.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:08:32.726
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 05/17/23 06:08:49.734
STEP: Creating a ResourceQuota 05/17/23 06:08:54.738
STEP: Ensuring resource quota status is calculated 05/17/23 06:08:54.746
STEP: Creating a ConfigMap 05/17/23 06:08:56.75
STEP: Ensuring resource quota status captures configMap creation 05/17/23 06:08:56.76
STEP: Deleting a ConfigMap 05/17/23 06:08:58.766
STEP: Ensuring resource quota status released usage 05/17/23 06:08:58.772
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May 17 06:09:00.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9035" for this suite. 05/17/23 06:09:00.784
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":111,"skipped":2112,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.084 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:08:32.707
    May 17 06:08:32.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename resourcequota 05/17/23 06:08:32.708
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:08:32.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:08:32.726
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 05/17/23 06:08:49.734
    STEP: Creating a ResourceQuota 05/17/23 06:08:54.738
    STEP: Ensuring resource quota status is calculated 05/17/23 06:08:54.746
    STEP: Creating a ConfigMap 05/17/23 06:08:56.75
    STEP: Ensuring resource quota status captures configMap creation 05/17/23 06:08:56.76
    STEP: Deleting a ConfigMap 05/17/23 06:08:58.766
    STEP: Ensuring resource quota status released usage 05/17/23 06:08:58.772
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May 17 06:09:00.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9035" for this suite. 05/17/23 06:09:00.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:09:00.791
May 17 06:09:00.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:09:00.792
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:00.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:00.81
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-0ddaa097-cd54-4d87-8c16-7f8cd5bce5bb 05/17/23 06:09:00.814
STEP: Creating a pod to test consume configMaps 05/17/23 06:09:00.818
May 17 06:09:00.828: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533" in namespace "projected-4324" to be "Succeeded or Failed"
May 17 06:09:00.831: INFO: Pod "pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533": Phase="Pending", Reason="", readiness=false. Elapsed: 3.352874ms
May 17 06:09:02.836: INFO: Pod "pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533": Phase="Running", Reason="", readiness=false. Elapsed: 2.008536123s
May 17 06:09:04.836: INFO: Pod "pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007854213s
STEP: Saw pod success 05/17/23 06:09:04.836
May 17 06:09:04.836: INFO: Pod "pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533" satisfied condition "Succeeded or Failed"
May 17 06:09:04.839: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 06:09:04.849
May 17 06:09:04.859: INFO: Waiting for pod pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533 to disappear
May 17 06:09:04.862: INFO: Pod pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May 17 06:09:04.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4324" for this suite. 05/17/23 06:09:04.87
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":112,"skipped":2129,"failed":0}
------------------------------
â€¢ [4.085 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:09:00.791
    May 17 06:09:00.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:09:00.792
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:00.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:00.81
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-0ddaa097-cd54-4d87-8c16-7f8cd5bce5bb 05/17/23 06:09:00.814
    STEP: Creating a pod to test consume configMaps 05/17/23 06:09:00.818
    May 17 06:09:00.828: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533" in namespace "projected-4324" to be "Succeeded or Failed"
    May 17 06:09:00.831: INFO: Pod "pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533": Phase="Pending", Reason="", readiness=false. Elapsed: 3.352874ms
    May 17 06:09:02.836: INFO: Pod "pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533": Phase="Running", Reason="", readiness=false. Elapsed: 2.008536123s
    May 17 06:09:04.836: INFO: Pod "pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007854213s
    STEP: Saw pod success 05/17/23 06:09:04.836
    May 17 06:09:04.836: INFO: Pod "pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533" satisfied condition "Succeeded or Failed"
    May 17 06:09:04.839: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 06:09:04.849
    May 17 06:09:04.859: INFO: Waiting for pod pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533 to disappear
    May 17 06:09:04.862: INFO: Pod pod-projected-configmaps-d5f5bf7d-d9f3-4410-bd0e-ce2c916f0533 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May 17 06:09:04.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4324" for this suite. 05/17/23 06:09:04.87
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:09:04.876
May 17 06:09:04.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename deployment 05/17/23 06:09:04.877
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:04.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:04.895
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
May 17 06:09:04.907: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 17 06:09:09.915: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/17/23 06:09:09.915
May 17 06:09:09.915: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/17/23 06:09:09.926
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 06:09:09.938: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3268  2ce8935b-175c-4279-a0f6-1d954a0969e8 1455033 1 2023-05-17 06:09:09 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-17 06:09:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0079b07d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 17 06:09:09.941: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
May 17 06:09:09.941: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 17 06:09:09.942: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3268  6b9c89e9-2b15-426e-9329-c87ad56191b0 1455034 1 2023-05-17 06:09:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 2ce8935b-175c-4279-a0f6-1d954a0969e8 0xc0079b0b37 0xc0079b0b38}] [] [{e2e.test Update apps/v1 2023-05-17 06:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:09:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-17 06:09:09 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"2ce8935b-175c-4279-a0f6-1d954a0969e8\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0079b0bf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 06:09:09.946: INFO: Pod "test-cleanup-controller-7v4l9" is available:
&Pod{ObjectMeta:{test-cleanup-controller-7v4l9 test-cleanup-controller- deployment-3268  67b8b489-d34a-4a87-91ac-eed832c1690b 1454979 0 2023-05-17 06:09:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 6b9c89e9-2b15-426e-9329-c87ad56191b0 0xc007960b27 0xc007960b28}] [] [{kube-controller-manager Update v1 2023-05-17 06:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b9c89e9-2b15-426e-9329-c87ad56191b0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:09:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68n6r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68n6r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:09:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:09:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.116,StartTime:2023-05-17 06:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:09:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://aec8d498c7ee292b27b1bb7859732e7260737f0a293998d656e25c08b885919e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May 17 06:09:09.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3268" for this suite. 05/17/23 06:09:09.953
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":113,"skipped":2129,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.084 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:09:04.876
    May 17 06:09:04.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename deployment 05/17/23 06:09:04.877
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:04.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:04.895
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    May 17 06:09:04.907: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    May 17 06:09:09.915: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/17/23 06:09:09.915
    May 17 06:09:09.915: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/17/23 06:09:09.926
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 06:09:09.938: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3268  2ce8935b-175c-4279-a0f6-1d954a0969e8 1455033 1 2023-05-17 06:09:09 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-17 06:09:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0079b07d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    May 17 06:09:09.941: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    May 17 06:09:09.941: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    May 17 06:09:09.942: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3268  6b9c89e9-2b15-426e-9329-c87ad56191b0 1455034 1 2023-05-17 06:09:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 2ce8935b-175c-4279-a0f6-1d954a0969e8 0xc0079b0b37 0xc0079b0b38}] [] [{e2e.test Update apps/v1 2023-05-17 06:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:09:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-17 06:09:09 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"2ce8935b-175c-4279-a0f6-1d954a0969e8\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0079b0bf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 17 06:09:09.946: INFO: Pod "test-cleanup-controller-7v4l9" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-7v4l9 test-cleanup-controller- deployment-3268  67b8b489-d34a-4a87-91ac-eed832c1690b 1454979 0 2023-05-17 06:09:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 6b9c89e9-2b15-426e-9329-c87ad56191b0 0xc007960b27 0xc007960b28}] [] [{kube-controller-manager Update v1 2023-05-17 06:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b9c89e9-2b15-426e-9329-c87ad56191b0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:09:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68n6r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68n6r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:09:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:09:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.116,StartTime:2023-05-17 06:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:09:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://aec8d498c7ee292b27b1bb7859732e7260737f0a293998d656e25c08b885919e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May 17 06:09:09.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3268" for this suite. 05/17/23 06:09:09.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:09:09.961
May 17 06:09:09.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:09:09.961
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:09.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:09.979
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 05/17/23 06:09:09.982
May 17 06:09:09.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 create -f -'
May 17 06:09:10.208: INFO: stderr: ""
May 17 06:09:10.208: INFO: stdout: "pod/pause created\n"
May 17 06:09:10.208: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 17 06:09:10.208: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7023" to be "running and ready"
May 17 06:09:10.212: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.881457ms
May 17 06:09:10.212: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'aks-agentpool-72615086-vmss00000c' to be 'Running' but was 'Pending'
May 17 06:09:12.217: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009143245s
May 17 06:09:12.217: INFO: Pod "pause" satisfied condition "running and ready"
May 17 06:09:12.217: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 05/17/23 06:09:12.217
May 17 06:09:12.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 label pods pause testing-label=testing-label-value'
May 17 06:09:12.290: INFO: stderr: ""
May 17 06:09:12.290: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 05/17/23 06:09:12.29
May 17 06:09:12.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 get pod pause -L testing-label'
May 17 06:09:12.351: INFO: stderr: ""
May 17 06:09:12.351: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 05/17/23 06:09:12.351
May 17 06:09:12.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 label pods pause testing-label-'
May 17 06:09:12.419: INFO: stderr: ""
May 17 06:09:12.419: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 05/17/23 06:09:12.419
May 17 06:09:12.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 get pod pause -L testing-label'
May 17 06:09:12.480: INFO: stderr: ""
May 17 06:09:12.480: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 05/17/23 06:09:12.48
May 17 06:09:12.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 delete --grace-period=0 --force -f -'
May 17 06:09:12.578: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 06:09:12.578: INFO: stdout: "pod \"pause\" force deleted\n"
May 17 06:09:12.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 get rc,svc -l name=pause --no-headers'
May 17 06:09:12.646: INFO: stderr: "No resources found in kubectl-7023 namespace.\n"
May 17 06:09:12.646: INFO: stdout: ""
May 17 06:09:12.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 17 06:09:12.706: INFO: stderr: ""
May 17 06:09:12.706: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:09:12.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7023" for this suite. 05/17/23 06:09:12.714
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":114,"skipped":2137,"failed":0}
------------------------------
â€¢ [2.759 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:09:09.961
    May 17 06:09:09.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:09:09.961
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:09.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:09.979
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 05/17/23 06:09:09.982
    May 17 06:09:09.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 create -f -'
    May 17 06:09:10.208: INFO: stderr: ""
    May 17 06:09:10.208: INFO: stdout: "pod/pause created\n"
    May 17 06:09:10.208: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    May 17 06:09:10.208: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7023" to be "running and ready"
    May 17 06:09:10.212: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.881457ms
    May 17 06:09:10.212: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'aks-agentpool-72615086-vmss00000c' to be 'Running' but was 'Pending'
    May 17 06:09:12.217: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009143245s
    May 17 06:09:12.217: INFO: Pod "pause" satisfied condition "running and ready"
    May 17 06:09:12.217: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 05/17/23 06:09:12.217
    May 17 06:09:12.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 label pods pause testing-label=testing-label-value'
    May 17 06:09:12.290: INFO: stderr: ""
    May 17 06:09:12.290: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 05/17/23 06:09:12.29
    May 17 06:09:12.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 get pod pause -L testing-label'
    May 17 06:09:12.351: INFO: stderr: ""
    May 17 06:09:12.351: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 05/17/23 06:09:12.351
    May 17 06:09:12.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 label pods pause testing-label-'
    May 17 06:09:12.419: INFO: stderr: ""
    May 17 06:09:12.419: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 05/17/23 06:09:12.419
    May 17 06:09:12.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 get pod pause -L testing-label'
    May 17 06:09:12.480: INFO: stderr: ""
    May 17 06:09:12.480: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 05/17/23 06:09:12.48
    May 17 06:09:12.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 delete --grace-period=0 --force -f -'
    May 17 06:09:12.578: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 06:09:12.578: INFO: stdout: "pod \"pause\" force deleted\n"
    May 17 06:09:12.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 get rc,svc -l name=pause --no-headers'
    May 17 06:09:12.646: INFO: stderr: "No resources found in kubectl-7023 namespace.\n"
    May 17 06:09:12.646: INFO: stdout: ""
    May 17 06:09:12.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7023 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May 17 06:09:12.706: INFO: stderr: ""
    May 17 06:09:12.706: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:09:12.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7023" for this suite. 05/17/23 06:09:12.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:09:12.721
May 17 06:09:12.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:09:12.722
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:12.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:12.739
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 05/17/23 06:09:12.746
STEP: waiting for available Endpoint 05/17/23 06:09:12.75
STEP: listing all Endpoints 05/17/23 06:09:12.752
STEP: updating the Endpoint 05/17/23 06:09:12.756
STEP: fetching the Endpoint 05/17/23 06:09:12.762
STEP: patching the Endpoint 05/17/23 06:09:12.765
STEP: fetching the Endpoint 05/17/23 06:09:12.773
STEP: deleting the Endpoint by Collection 05/17/23 06:09:12.776
STEP: waiting for Endpoint deletion 05/17/23 06:09:12.784
STEP: fetching the Endpoint 05/17/23 06:09:12.786
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:09:12.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5069" for this suite. 05/17/23 06:09:12.794
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":115,"skipped":2154,"failed":0}
------------------------------
â€¢ [0.079 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:09:12.721
    May 17 06:09:12.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:09:12.722
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:12.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:12.739
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 05/17/23 06:09:12.746
    STEP: waiting for available Endpoint 05/17/23 06:09:12.75
    STEP: listing all Endpoints 05/17/23 06:09:12.752
    STEP: updating the Endpoint 05/17/23 06:09:12.756
    STEP: fetching the Endpoint 05/17/23 06:09:12.762
    STEP: patching the Endpoint 05/17/23 06:09:12.765
    STEP: fetching the Endpoint 05/17/23 06:09:12.773
    STEP: deleting the Endpoint by Collection 05/17/23 06:09:12.776
    STEP: waiting for Endpoint deletion 05/17/23 06:09:12.784
    STEP: fetching the Endpoint 05/17/23 06:09:12.786
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:09:12.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5069" for this suite. 05/17/23 06:09:12.794
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:09:12.8
May 17 06:09:12.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 06:09:12.801
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:12.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:12.815
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-2026/configmap-test-5057a3f2-f263-404b-a64d-eda94c87e387 05/17/23 06:09:12.819
STEP: Creating a pod to test consume configMaps 05/17/23 06:09:12.823
May 17 06:09:12.834: INFO: Waiting up to 5m0s for pod "pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31" in namespace "configmap-2026" to be "Succeeded or Failed"
May 17 06:09:12.839: INFO: Pod "pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31": Phase="Pending", Reason="", readiness=false. Elapsed: 5.015311ms
May 17 06:09:14.844: INFO: Pod "pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31": Phase="Running", Reason="", readiness=false. Elapsed: 2.009991883s
May 17 06:09:16.845: INFO: Pod "pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010333464s
STEP: Saw pod success 05/17/23 06:09:16.845
May 17 06:09:16.845: INFO: Pod "pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31" satisfied condition "Succeeded or Failed"
May 17 06:09:16.848: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31 container env-test: <nil>
STEP: delete the pod 05/17/23 06:09:16.858
May 17 06:09:16.870: INFO: Waiting for pod pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31 to disappear
May 17 06:09:16.874: INFO: Pod pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May 17 06:09:16.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2026" for this suite. 05/17/23 06:09:16.881
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":116,"skipped":2160,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:09:12.8
    May 17 06:09:12.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 06:09:12.801
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:12.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:12.815
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-2026/configmap-test-5057a3f2-f263-404b-a64d-eda94c87e387 05/17/23 06:09:12.819
    STEP: Creating a pod to test consume configMaps 05/17/23 06:09:12.823
    May 17 06:09:12.834: INFO: Waiting up to 5m0s for pod "pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31" in namespace "configmap-2026" to be "Succeeded or Failed"
    May 17 06:09:12.839: INFO: Pod "pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31": Phase="Pending", Reason="", readiness=false. Elapsed: 5.015311ms
    May 17 06:09:14.844: INFO: Pod "pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31": Phase="Running", Reason="", readiness=false. Elapsed: 2.009991883s
    May 17 06:09:16.845: INFO: Pod "pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010333464s
    STEP: Saw pod success 05/17/23 06:09:16.845
    May 17 06:09:16.845: INFO: Pod "pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31" satisfied condition "Succeeded or Failed"
    May 17 06:09:16.848: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31 container env-test: <nil>
    STEP: delete the pod 05/17/23 06:09:16.858
    May 17 06:09:16.870: INFO: Waiting for pod pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31 to disappear
    May 17 06:09:16.874: INFO: Pod pod-configmaps-40c7a0e1-9fd6-4b62-9798-29291db3db31 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 06:09:16.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2026" for this suite. 05/17/23 06:09:16.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:09:16.888
May 17 06:09:16.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename resourcequota 05/17/23 06:09:16.889
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:16.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:16.903
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 05/17/23 06:09:16.907
STEP: Ensuring ResourceQuota status is calculated 05/17/23 06:09:16.912
STEP: Creating a ResourceQuota with not best effort scope 05/17/23 06:09:18.916
STEP: Ensuring ResourceQuota status is calculated 05/17/23 06:09:18.921
STEP: Creating a best-effort pod 05/17/23 06:09:20.926
STEP: Ensuring resource quota with best effort scope captures the pod usage 05/17/23 06:09:20.94
STEP: Ensuring resource quota with not best effort ignored the pod usage 05/17/23 06:09:22.944
STEP: Deleting the pod 05/17/23 06:09:24.949
STEP: Ensuring resource quota status released the pod usage 05/17/23 06:09:24.96
STEP: Creating a not best-effort pod 05/17/23 06:09:26.964
STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/17/23 06:09:26.976
STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/17/23 06:09:28.983
STEP: Deleting the pod 05/17/23 06:09:30.987
STEP: Ensuring resource quota status released the pod usage 05/17/23 06:09:30.998
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May 17 06:09:33.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-457" for this suite. 05/17/23 06:09:33.01
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":117,"skipped":2178,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.129 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:09:16.888
    May 17 06:09:16.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename resourcequota 05/17/23 06:09:16.889
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:16.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:16.903
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 05/17/23 06:09:16.907
    STEP: Ensuring ResourceQuota status is calculated 05/17/23 06:09:16.912
    STEP: Creating a ResourceQuota with not best effort scope 05/17/23 06:09:18.916
    STEP: Ensuring ResourceQuota status is calculated 05/17/23 06:09:18.921
    STEP: Creating a best-effort pod 05/17/23 06:09:20.926
    STEP: Ensuring resource quota with best effort scope captures the pod usage 05/17/23 06:09:20.94
    STEP: Ensuring resource quota with not best effort ignored the pod usage 05/17/23 06:09:22.944
    STEP: Deleting the pod 05/17/23 06:09:24.949
    STEP: Ensuring resource quota status released the pod usage 05/17/23 06:09:24.96
    STEP: Creating a not best-effort pod 05/17/23 06:09:26.964
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/17/23 06:09:26.976
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/17/23 06:09:28.983
    STEP: Deleting the pod 05/17/23 06:09:30.987
    STEP: Ensuring resource quota status released the pod usage 05/17/23 06:09:30.998
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May 17 06:09:33.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-457" for this suite. 05/17/23 06:09:33.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:09:33.018
May 17 06:09:33.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename tables 05/17/23 06:09:33.019
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:33.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:33.034
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
May 17 06:09:33.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2262" for this suite. 05/17/23 06:09:33.046
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":118,"skipped":2211,"failed":0}
------------------------------
â€¢ [0.034 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:09:33.018
    May 17 06:09:33.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename tables 05/17/23 06:09:33.019
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:33.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:33.034
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    May 17 06:09:33.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-2262" for this suite. 05/17/23 06:09:33.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:09:33.055
May 17 06:09:33.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 06:09:33.056
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:33.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:33.077
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 05/17/23 06:09:33.081
May 17 06:09:33.090: INFO: Waiting up to 5m0s for pod "pod-25b8261f-af42-424b-b463-49b4d5e911db" in namespace "emptydir-90" to be "Succeeded or Failed"
May 17 06:09:33.093: INFO: Pod "pod-25b8261f-af42-424b-b463-49b4d5e911db": Phase="Pending", Reason="", readiness=false. Elapsed: 3.427084ms
May 17 06:09:35.097: INFO: Pod "pod-25b8261f-af42-424b-b463-49b4d5e911db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007450752s
May 17 06:09:37.099: INFO: Pod "pod-25b8261f-af42-424b-b463-49b4d5e911db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009266456s
STEP: Saw pod success 05/17/23 06:09:37.099
May 17 06:09:37.099: INFO: Pod "pod-25b8261f-af42-424b-b463-49b4d5e911db" satisfied condition "Succeeded or Failed"
May 17 06:09:37.103: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-25b8261f-af42-424b-b463-49b4d5e911db container test-container: <nil>
STEP: delete the pod 05/17/23 06:09:37.112
May 17 06:09:37.124: INFO: Waiting for pod pod-25b8261f-af42-424b-b463-49b4d5e911db to disappear
May 17 06:09:37.127: INFO: Pod pod-25b8261f-af42-424b-b463-49b4d5e911db no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 06:09:37.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-90" for this suite. 05/17/23 06:09:37.135
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":119,"skipped":2306,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:09:33.055
    May 17 06:09:33.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 06:09:33.056
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:33.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:33.077
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/17/23 06:09:33.081
    May 17 06:09:33.090: INFO: Waiting up to 5m0s for pod "pod-25b8261f-af42-424b-b463-49b4d5e911db" in namespace "emptydir-90" to be "Succeeded or Failed"
    May 17 06:09:33.093: INFO: Pod "pod-25b8261f-af42-424b-b463-49b4d5e911db": Phase="Pending", Reason="", readiness=false. Elapsed: 3.427084ms
    May 17 06:09:35.097: INFO: Pod "pod-25b8261f-af42-424b-b463-49b4d5e911db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007450752s
    May 17 06:09:37.099: INFO: Pod "pod-25b8261f-af42-424b-b463-49b4d5e911db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009266456s
    STEP: Saw pod success 05/17/23 06:09:37.099
    May 17 06:09:37.099: INFO: Pod "pod-25b8261f-af42-424b-b463-49b4d5e911db" satisfied condition "Succeeded or Failed"
    May 17 06:09:37.103: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-25b8261f-af42-424b-b463-49b4d5e911db container test-container: <nil>
    STEP: delete the pod 05/17/23 06:09:37.112
    May 17 06:09:37.124: INFO: Waiting for pod pod-25b8261f-af42-424b-b463-49b4d5e911db to disappear
    May 17 06:09:37.127: INFO: Pod pod-25b8261f-af42-424b-b463-49b4d5e911db no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 06:09:37.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-90" for this suite. 05/17/23 06:09:37.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:09:37.146
May 17 06:09:37.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename replicaset 05/17/23 06:09:37.146
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:37.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:37.161
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 05/17/23 06:09:37.164
STEP: Verify that the required pods have come up 05/17/23 06:09:37.169
May 17 06:09:37.173: INFO: Pod name sample-pod: Found 0 pods out of 3
May 17 06:09:42.178: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 05/17/23 06:09:42.178
May 17 06:09:42.181: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 05/17/23 06:09:42.181
STEP: DeleteCollection of the ReplicaSets 05/17/23 06:09:42.189
STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/17/23 06:09:42.202
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May 17 06:09:42.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1400" for this suite. 05/17/23 06:09:42.219
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":120,"skipped":2341,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.079 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:09:37.146
    May 17 06:09:37.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename replicaset 05/17/23 06:09:37.146
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:37.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:37.161
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 05/17/23 06:09:37.164
    STEP: Verify that the required pods have come up 05/17/23 06:09:37.169
    May 17 06:09:37.173: INFO: Pod name sample-pod: Found 0 pods out of 3
    May 17 06:09:42.178: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 05/17/23 06:09:42.178
    May 17 06:09:42.181: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 05/17/23 06:09:42.181
    STEP: DeleteCollection of the ReplicaSets 05/17/23 06:09:42.189
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/17/23 06:09:42.202
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May 17 06:09:42.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1400" for this suite. 05/17/23 06:09:42.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:09:42.226
May 17 06:09:42.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 06:09:42.227
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:42.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:42.242
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/17/23 06:09:42.272
May 17 06:09:42.289: INFO: Waiting up to 5m0s for pod "pod-ecb628dc-284c-4e69-9627-b4d3a27ba560" in namespace "emptydir-6009" to be "Succeeded or Failed"
May 17 06:09:42.293: INFO: Pod "pod-ecb628dc-284c-4e69-9627-b4d3a27ba560": Phase="Pending", Reason="", readiness=false. Elapsed: 4.390416ms
May 17 06:09:44.298: INFO: Pod "pod-ecb628dc-284c-4e69-9627-b4d3a27ba560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00920207s
May 17 06:09:46.299: INFO: Pod "pod-ecb628dc-284c-4e69-9627-b4d3a27ba560": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010121303s
STEP: Saw pod success 05/17/23 06:09:46.299
May 17 06:09:46.299: INFO: Pod "pod-ecb628dc-284c-4e69-9627-b4d3a27ba560" satisfied condition "Succeeded or Failed"
May 17 06:09:46.303: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-ecb628dc-284c-4e69-9627-b4d3a27ba560 container test-container: <nil>
STEP: delete the pod 05/17/23 06:09:46.312
May 17 06:09:46.328: INFO: Waiting for pod pod-ecb628dc-284c-4e69-9627-b4d3a27ba560 to disappear
May 17 06:09:46.332: INFO: Pod pod-ecb628dc-284c-4e69-9627-b4d3a27ba560 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 06:09:46.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6009" for this suite. 05/17/23 06:09:46.34
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":121,"skipped":2369,"failed":0}
------------------------------
â€¢ [4.122 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:09:42.226
    May 17 06:09:42.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 06:09:42.227
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:42.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:42.242
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/17/23 06:09:42.272
    May 17 06:09:42.289: INFO: Waiting up to 5m0s for pod "pod-ecb628dc-284c-4e69-9627-b4d3a27ba560" in namespace "emptydir-6009" to be "Succeeded or Failed"
    May 17 06:09:42.293: INFO: Pod "pod-ecb628dc-284c-4e69-9627-b4d3a27ba560": Phase="Pending", Reason="", readiness=false. Elapsed: 4.390416ms
    May 17 06:09:44.298: INFO: Pod "pod-ecb628dc-284c-4e69-9627-b4d3a27ba560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00920207s
    May 17 06:09:46.299: INFO: Pod "pod-ecb628dc-284c-4e69-9627-b4d3a27ba560": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010121303s
    STEP: Saw pod success 05/17/23 06:09:46.299
    May 17 06:09:46.299: INFO: Pod "pod-ecb628dc-284c-4e69-9627-b4d3a27ba560" satisfied condition "Succeeded or Failed"
    May 17 06:09:46.303: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-ecb628dc-284c-4e69-9627-b4d3a27ba560 container test-container: <nil>
    STEP: delete the pod 05/17/23 06:09:46.312
    May 17 06:09:46.328: INFO: Waiting for pod pod-ecb628dc-284c-4e69-9627-b4d3a27ba560 to disappear
    May 17 06:09:46.332: INFO: Pod pod-ecb628dc-284c-4e69-9627-b4d3a27ba560 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 06:09:46.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6009" for this suite. 05/17/23 06:09:46.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:09:46.349
May 17 06:09:46.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename taint-multiple-pods 05/17/23 06:09:46.35
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:46.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:46.371
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
May 17 06:09:46.380: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 06:10:46.441: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
May 17 06:10:46.445: INFO: Starting informer...
STEP: Starting pods... 05/17/23 06:10:46.445
May 17 06:10:46.669: INFO: Pod1 is running on aks-agentpool-72615086-vmss00000c. Tainting Node
May 17 06:10:46.881: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-1382" to be "running"
May 17 06:10:46.886: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.80539ms
May 17 06:10:48.891: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009740328s
May 17 06:10:48.891: INFO: Pod "taint-eviction-b1" satisfied condition "running"
May 17 06:10:48.891: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-1382" to be "running"
May 17 06:10:48.895: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.710435ms
May 17 06:10:48.895: INFO: Pod "taint-eviction-b2" satisfied condition "running"
May 17 06:10:48.895: INFO: Pod2 is running on aks-agentpool-72615086-vmss00000c. Tainting Node
STEP: Trying to apply a taint on the Node 05/17/23 06:10:48.895
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 06:10:48.937
STEP: Waiting for Pod1 and Pod2 to be deleted 05/17/23 06:10:48.941
May 17 06:10:54.675: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 17 06:11:14.704: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 06:11:14.739
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
May 17 06:11:14.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1382" for this suite. 05/17/23 06:11:14.779
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":122,"skipped":2389,"failed":0}
------------------------------
â€¢ [SLOW TEST] [88.439 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:09:46.349
    May 17 06:09:46.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename taint-multiple-pods 05/17/23 06:09:46.35
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:09:46.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:09:46.371
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    May 17 06:09:46.380: INFO: Waiting up to 1m0s for all nodes to be ready
    May 17 06:10:46.441: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    May 17 06:10:46.445: INFO: Starting informer...
    STEP: Starting pods... 05/17/23 06:10:46.445
    May 17 06:10:46.669: INFO: Pod1 is running on aks-agentpool-72615086-vmss00000c. Tainting Node
    May 17 06:10:46.881: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-1382" to be "running"
    May 17 06:10:46.886: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.80539ms
    May 17 06:10:48.891: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009740328s
    May 17 06:10:48.891: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    May 17 06:10:48.891: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-1382" to be "running"
    May 17 06:10:48.895: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.710435ms
    May 17 06:10:48.895: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    May 17 06:10:48.895: INFO: Pod2 is running on aks-agentpool-72615086-vmss00000c. Tainting Node
    STEP: Trying to apply a taint on the Node 05/17/23 06:10:48.895
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 06:10:48.937
    STEP: Waiting for Pod1 and Pod2 to be deleted 05/17/23 06:10:48.941
    May 17 06:10:54.675: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    May 17 06:11:14.704: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/17/23 06:11:14.739
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    May 17 06:11:14.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-1382" for this suite. 05/17/23 06:11:14.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:11:14.789
May 17 06:11:14.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename var-expansion 05/17/23 06:11:14.79
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:11:14.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:11:14.813
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
May 17 06:11:14.838: INFO: Waiting up to 2m0s for pod "var-expansion-eb6263a6-182d-4e94-a5c6-aa45a089c1c3" in namespace "var-expansion-9415" to be "container 0 failed with reason CreateContainerConfigError"
May 17 06:11:14.844: INFO: Pod "var-expansion-eb6263a6-182d-4e94-a5c6-aa45a089c1c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.952883ms
May 17 06:11:16.851: INFO: Pod "var-expansion-eb6263a6-182d-4e94-a5c6-aa45a089c1c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012529216s
May 17 06:11:16.851: INFO: Pod "var-expansion-eb6263a6-182d-4e94-a5c6-aa45a089c1c3" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May 17 06:11:16.851: INFO: Deleting pod "var-expansion-eb6263a6-182d-4e94-a5c6-aa45a089c1c3" in namespace "var-expansion-9415"
May 17 06:11:16.862: INFO: Wait up to 5m0s for pod "var-expansion-eb6263a6-182d-4e94-a5c6-aa45a089c1c3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May 17 06:11:18.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9415" for this suite. 05/17/23 06:11:18.886
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":123,"skipped":2420,"failed":0}
------------------------------
â€¢ [4.106 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:11:14.789
    May 17 06:11:14.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename var-expansion 05/17/23 06:11:14.79
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:11:14.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:11:14.813
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    May 17 06:11:14.838: INFO: Waiting up to 2m0s for pod "var-expansion-eb6263a6-182d-4e94-a5c6-aa45a089c1c3" in namespace "var-expansion-9415" to be "container 0 failed with reason CreateContainerConfigError"
    May 17 06:11:14.844: INFO: Pod "var-expansion-eb6263a6-182d-4e94-a5c6-aa45a089c1c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.952883ms
    May 17 06:11:16.851: INFO: Pod "var-expansion-eb6263a6-182d-4e94-a5c6-aa45a089c1c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012529216s
    May 17 06:11:16.851: INFO: Pod "var-expansion-eb6263a6-182d-4e94-a5c6-aa45a089c1c3" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May 17 06:11:16.851: INFO: Deleting pod "var-expansion-eb6263a6-182d-4e94-a5c6-aa45a089c1c3" in namespace "var-expansion-9415"
    May 17 06:11:16.862: INFO: Wait up to 5m0s for pod "var-expansion-eb6263a6-182d-4e94-a5c6-aa45a089c1c3" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May 17 06:11:18.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9415" for this suite. 05/17/23 06:11:18.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:11:18.896
May 17 06:11:18.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:11:18.897
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:11:18.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:11:18.919
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 05/17/23 06:11:18.924
May 17 06:11:18.924: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-1797 proxy --unix-socket=/tmp/kubectl-proxy-unix3532452378/test'
STEP: retrieving proxy /api/ output 05/17/23 06:11:18.953
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:11:18.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1797" for this suite. 05/17/23 06:11:18.964
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":124,"skipped":2442,"failed":0}
------------------------------
â€¢ [0.080 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:11:18.896
    May 17 06:11:18.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:11:18.897
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:11:18.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:11:18.919
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 05/17/23 06:11:18.924
    May 17 06:11:18.924: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-1797 proxy --unix-socket=/tmp/kubectl-proxy-unix3532452378/test'
    STEP: retrieving proxy /api/ output 05/17/23 06:11:18.953
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:11:18.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1797" for this suite. 05/17/23 06:11:18.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:11:18.977
May 17 06:11:18.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 06:11:18.978
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:11:18.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:11:19.001
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
May 17 06:11:19.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/17/23 06:11:28.766
May 17 06:11:28.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-8657 --namespace=crd-publish-openapi-8657 create -f -'
May 17 06:11:30.119: INFO: stderr: ""
May 17 06:11:30.119: INFO: stdout: "e2e-test-crd-publish-openapi-2626-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 17 06:11:30.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-8657 --namespace=crd-publish-openapi-8657 delete e2e-test-crd-publish-openapi-2626-crds test-cr'
May 17 06:11:30.266: INFO: stderr: ""
May 17 06:11:30.266: INFO: stdout: "e2e-test-crd-publish-openapi-2626-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 17 06:11:30.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-8657 --namespace=crd-publish-openapi-8657 apply -f -'
May 17 06:11:31.667: INFO: stderr: ""
May 17 06:11:31.667: INFO: stdout: "e2e-test-crd-publish-openapi-2626-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 17 06:11:31.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-8657 --namespace=crd-publish-openapi-8657 delete e2e-test-crd-publish-openapi-2626-crds test-cr'
May 17 06:11:31.756: INFO: stderr: ""
May 17 06:11:31.756: INFO: stdout: "e2e-test-crd-publish-openapi-2626-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/17/23 06:11:31.756
May 17 06:11:31.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-8657 explain e2e-test-crd-publish-openapi-2626-crds'
May 17 06:11:32.974: INFO: stderr: ""
May 17 06:11:32.974: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2626-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:11:43.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8657" for this suite. 05/17/23 06:11:43.239
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":125,"skipped":2453,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.271 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:11:18.977
    May 17 06:11:18.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 06:11:18.978
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:11:18.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:11:19.001
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    May 17 06:11:19.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/17/23 06:11:28.766
    May 17 06:11:28.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-8657 --namespace=crd-publish-openapi-8657 create -f -'
    May 17 06:11:30.119: INFO: stderr: ""
    May 17 06:11:30.119: INFO: stdout: "e2e-test-crd-publish-openapi-2626-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May 17 06:11:30.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-8657 --namespace=crd-publish-openapi-8657 delete e2e-test-crd-publish-openapi-2626-crds test-cr'
    May 17 06:11:30.266: INFO: stderr: ""
    May 17 06:11:30.266: INFO: stdout: "e2e-test-crd-publish-openapi-2626-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    May 17 06:11:30.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-8657 --namespace=crd-publish-openapi-8657 apply -f -'
    May 17 06:11:31.667: INFO: stderr: ""
    May 17 06:11:31.667: INFO: stdout: "e2e-test-crd-publish-openapi-2626-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May 17 06:11:31.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-8657 --namespace=crd-publish-openapi-8657 delete e2e-test-crd-publish-openapi-2626-crds test-cr'
    May 17 06:11:31.756: INFO: stderr: ""
    May 17 06:11:31.756: INFO: stdout: "e2e-test-crd-publish-openapi-2626-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/17/23 06:11:31.756
    May 17 06:11:31.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-8657 explain e2e-test-crd-publish-openapi-2626-crds'
    May 17 06:11:32.974: INFO: stderr: ""
    May 17 06:11:32.974: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2626-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:11:43.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8657" for this suite. 05/17/23 06:11:43.239
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:11:43.248
May 17 06:11:43.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename statefulset 05/17/23 06:11:43.249
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:11:43.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:11:43.274
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3113 05/17/23 06:11:43.279
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 05/17/23 06:11:43.286
STEP: Creating pod with conflicting port in namespace statefulset-3113 05/17/23 06:11:43.3
STEP: Waiting until pod test-pod will start running in namespace statefulset-3113 05/17/23 06:11:43.323
May 17 06:11:43.323: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3113" to be "running"
May 17 06:11:43.328: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.336426ms
May 17 06:11:45.335: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012031089s
May 17 06:11:45.335: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3113 05/17/23 06:11:45.335
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3113 05/17/23 06:11:45.342
May 17 06:11:45.360: INFO: Observed stateful pod in namespace: statefulset-3113, name: ss-0, uid: 46316898-d327-4517-baa1-5b56c615be49, status phase: Pending. Waiting for statefulset controller to delete.
May 17 06:11:45.373: INFO: Observed stateful pod in namespace: statefulset-3113, name: ss-0, uid: 46316898-d327-4517-baa1-5b56c615be49, status phase: Failed. Waiting for statefulset controller to delete.
May 17 06:11:45.381: INFO: Observed stateful pod in namespace: statefulset-3113, name: ss-0, uid: 46316898-d327-4517-baa1-5b56c615be49, status phase: Failed. Waiting for statefulset controller to delete.
May 17 06:11:45.387: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3113
STEP: Removing pod with conflicting port in namespace statefulset-3113 05/17/23 06:11:45.387
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3113 and will be in running state 05/17/23 06:11:45.402
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May 17 06:11:47.415: INFO: Deleting all statefulset in ns statefulset-3113
May 17 06:11:47.419: INFO: Scaling statefulset ss to 0
May 17 06:11:57.445: INFO: Waiting for statefulset status.replicas updated to 0
May 17 06:11:57.450: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May 17 06:11:57.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3113" for this suite. 05/17/23 06:11:57.481
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":126,"skipped":2455,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.242 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:11:43.248
    May 17 06:11:43.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename statefulset 05/17/23 06:11:43.249
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:11:43.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:11:43.274
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3113 05/17/23 06:11:43.279
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 05/17/23 06:11:43.286
    STEP: Creating pod with conflicting port in namespace statefulset-3113 05/17/23 06:11:43.3
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3113 05/17/23 06:11:43.323
    May 17 06:11:43.323: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3113" to be "running"
    May 17 06:11:43.328: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.336426ms
    May 17 06:11:45.335: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012031089s
    May 17 06:11:45.335: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3113 05/17/23 06:11:45.335
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3113 05/17/23 06:11:45.342
    May 17 06:11:45.360: INFO: Observed stateful pod in namespace: statefulset-3113, name: ss-0, uid: 46316898-d327-4517-baa1-5b56c615be49, status phase: Pending. Waiting for statefulset controller to delete.
    May 17 06:11:45.373: INFO: Observed stateful pod in namespace: statefulset-3113, name: ss-0, uid: 46316898-d327-4517-baa1-5b56c615be49, status phase: Failed. Waiting for statefulset controller to delete.
    May 17 06:11:45.381: INFO: Observed stateful pod in namespace: statefulset-3113, name: ss-0, uid: 46316898-d327-4517-baa1-5b56c615be49, status phase: Failed. Waiting for statefulset controller to delete.
    May 17 06:11:45.387: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3113
    STEP: Removing pod with conflicting port in namespace statefulset-3113 05/17/23 06:11:45.387
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3113 and will be in running state 05/17/23 06:11:45.402
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May 17 06:11:47.415: INFO: Deleting all statefulset in ns statefulset-3113
    May 17 06:11:47.419: INFO: Scaling statefulset ss to 0
    May 17 06:11:57.445: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 06:11:57.450: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May 17 06:11:57.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3113" for this suite. 05/17/23 06:11:57.481
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:11:57.49
May 17 06:11:57.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-runtime 05/17/23 06:11:57.491
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:11:57.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:11:57.516
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 05/17/23 06:11:57.521
STEP: wait for the container to reach Succeeded 05/17/23 06:11:57.534
STEP: get the container status 05/17/23 06:12:01.562
STEP: the container should be terminated 05/17/23 06:12:01.567
STEP: the termination message should be set 05/17/23 06:12:01.567
May 17 06:12:01.567: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/17/23 06:12:01.567
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May 17 06:12:01.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8363" for this suite. 05/17/23 06:12:01.601
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":127,"skipped":2456,"failed":0}
------------------------------
â€¢ [4.134 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:11:57.49
    May 17 06:11:57.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-runtime 05/17/23 06:11:57.491
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:11:57.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:11:57.516
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 05/17/23 06:11:57.521
    STEP: wait for the container to reach Succeeded 05/17/23 06:11:57.534
    STEP: get the container status 05/17/23 06:12:01.562
    STEP: the container should be terminated 05/17/23 06:12:01.567
    STEP: the termination message should be set 05/17/23 06:12:01.567
    May 17 06:12:01.567: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/17/23 06:12:01.567
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May 17 06:12:01.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8363" for this suite. 05/17/23 06:12:01.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:12:01.625
May 17 06:12:01.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pods 05/17/23 06:12:01.626
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:12:01.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:12:01.652
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 05/17/23 06:12:01.664
STEP: setting up watch 05/17/23 06:12:01.664
STEP: submitting the pod to kubernetes 05/17/23 06:12:01.774
STEP: verifying the pod is in kubernetes 05/17/23 06:12:01.787
STEP: verifying pod creation was observed 05/17/23 06:12:01.792
May 17 06:12:01.792: INFO: Waiting up to 5m0s for pod "pod-submit-remove-e7390457-4455-4a49-8c8d-0224009316e1" in namespace "pods-1045" to be "running"
May 17 06:12:01.797: INFO: Pod "pod-submit-remove-e7390457-4455-4a49-8c8d-0224009316e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.098402ms
May 17 06:12:03.804: INFO: Pod "pod-submit-remove-e7390457-4455-4a49-8c8d-0224009316e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011662704s
May 17 06:12:03.804: INFO: Pod "pod-submit-remove-e7390457-4455-4a49-8c8d-0224009316e1" satisfied condition "running"
STEP: deleting the pod gracefully 05/17/23 06:12:03.808
STEP: verifying pod deletion was observed 05/17/23 06:12:03.818
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May 17 06:12:05.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1045" for this suite. 05/17/23 06:12:05.815
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":128,"skipped":2471,"failed":0}
------------------------------
â€¢ [4.197 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:12:01.625
    May 17 06:12:01.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pods 05/17/23 06:12:01.626
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:12:01.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:12:01.652
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 05/17/23 06:12:01.664
    STEP: setting up watch 05/17/23 06:12:01.664
    STEP: submitting the pod to kubernetes 05/17/23 06:12:01.774
    STEP: verifying the pod is in kubernetes 05/17/23 06:12:01.787
    STEP: verifying pod creation was observed 05/17/23 06:12:01.792
    May 17 06:12:01.792: INFO: Waiting up to 5m0s for pod "pod-submit-remove-e7390457-4455-4a49-8c8d-0224009316e1" in namespace "pods-1045" to be "running"
    May 17 06:12:01.797: INFO: Pod "pod-submit-remove-e7390457-4455-4a49-8c8d-0224009316e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.098402ms
    May 17 06:12:03.804: INFO: Pod "pod-submit-remove-e7390457-4455-4a49-8c8d-0224009316e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011662704s
    May 17 06:12:03.804: INFO: Pod "pod-submit-remove-e7390457-4455-4a49-8c8d-0224009316e1" satisfied condition "running"
    STEP: deleting the pod gracefully 05/17/23 06:12:03.808
    STEP: verifying pod deletion was observed 05/17/23 06:12:03.818
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May 17 06:12:05.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1045" for this suite. 05/17/23 06:12:05.815
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:12:05.823
May 17 06:12:05.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 06:12:05.824
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:12:05.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:12:05.85
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-9069/configmap-test-d52da5c9-d9bd-4cc8-a85f-28b683e47364 05/17/23 06:12:05.854
STEP: Creating a pod to test consume configMaps 05/17/23 06:12:05.86
May 17 06:12:05.873: INFO: Waiting up to 5m0s for pod "pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5" in namespace "configmap-9069" to be "Succeeded or Failed"
May 17 06:12:05.878: INFO: Pod "pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.587962ms
May 17 06:12:07.885: INFO: Pod "pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5": Phase="Running", Reason="", readiness=false. Elapsed: 2.012518924s
May 17 06:12:09.884: INFO: Pod "pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011285296s
STEP: Saw pod success 05/17/23 06:12:09.884
May 17 06:12:09.884: INFO: Pod "pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5" satisfied condition "Succeeded or Failed"
May 17 06:12:09.889: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5 container env-test: <nil>
STEP: delete the pod 05/17/23 06:12:09.906
May 17 06:12:09.918: INFO: Waiting for pod pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5 to disappear
May 17 06:12:09.923: INFO: Pod pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May 17 06:12:09.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9069" for this suite. 05/17/23 06:12:09.933
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":129,"skipped":2473,"failed":0}
------------------------------
â€¢ [4.119 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:12:05.823
    May 17 06:12:05.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 06:12:05.824
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:12:05.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:12:05.85
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-9069/configmap-test-d52da5c9-d9bd-4cc8-a85f-28b683e47364 05/17/23 06:12:05.854
    STEP: Creating a pod to test consume configMaps 05/17/23 06:12:05.86
    May 17 06:12:05.873: INFO: Waiting up to 5m0s for pod "pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5" in namespace "configmap-9069" to be "Succeeded or Failed"
    May 17 06:12:05.878: INFO: Pod "pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.587962ms
    May 17 06:12:07.885: INFO: Pod "pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5": Phase="Running", Reason="", readiness=false. Elapsed: 2.012518924s
    May 17 06:12:09.884: INFO: Pod "pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011285296s
    STEP: Saw pod success 05/17/23 06:12:09.884
    May 17 06:12:09.884: INFO: Pod "pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5" satisfied condition "Succeeded or Failed"
    May 17 06:12:09.889: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5 container env-test: <nil>
    STEP: delete the pod 05/17/23 06:12:09.906
    May 17 06:12:09.918: INFO: Waiting for pod pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5 to disappear
    May 17 06:12:09.923: INFO: Pod pod-configmaps-4318d11c-8621-4753-93da-38ca6ae033b5 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 06:12:09.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9069" for this suite. 05/17/23 06:12:09.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:12:09.944
May 17 06:12:09.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename statefulset 05/17/23 06:12:09.944
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:12:09.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:12:09.967
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1658 05/17/23 06:12:09.972
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-1658 05/17/23 06:12:09.979
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1658 05/17/23 06:12:09.986
May 17 06:12:10.003: INFO: Found 0 stateful pods, waiting for 1
May 17 06:12:20.010: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/17/23 06:12:20.01
May 17 06:12:20.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 06:12:20.214: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 06:12:20.214: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 06:12:20.214: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 06:12:20.220: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 17 06:12:30.231: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 17 06:12:30.231: INFO: Waiting for statefulset status.replicas updated to 0
May 17 06:12:30.255: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
May 17 06:12:30.255: INFO: ss-0  aks-agentpool-72615086-vmss00000c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:10 +0000 UTC  }]
May 17 06:12:30.255: INFO: 
May 17 06:12:30.255: INFO: StatefulSet ss has not reached scale 3, at 1
May 17 06:12:31.262: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993787173s
May 17 06:12:32.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987557229s
May 17 06:12:33.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981431204s
May 17 06:12:34.280: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975631041s
May 17 06:12:35.286: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969768293s
May 17 06:12:36.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963302762s
May 17 06:12:37.298: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95742969s
May 17 06:12:38.304: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951345929s
May 17 06:12:39.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.522632ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1658 05/17/23 06:12:40.31
May 17 06:12:40.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 06:12:40.503: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 06:12:40.503: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 06:12:40.503: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 06:12:40.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 06:12:40.705: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 17 06:12:40.705: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 06:12:40.705: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 06:12:40.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 06:12:40.887: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 17 06:12:40.887: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 06:12:40.887: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 06:12:40.893: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 17 06:12:50.900: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 06:12:50.900: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 06:12:50.900: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 05/17/23 06:12:50.9
May 17 06:12:50.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 06:12:51.118: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 06:12:51.118: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 06:12:51.118: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 06:12:51.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 06:12:51.306: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 06:12:51.306: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 06:12:51.306: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 06:12:51.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 06:12:51.491: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 06:12:51.491: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 06:12:51.491: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 06:12:51.491: INFO: Waiting for statefulset status.replicas updated to 0
May 17 06:12:51.496: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 17 06:13:01.509: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 17 06:13:01.509: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 17 06:13:01.509: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 17 06:13:01.528: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
May 17 06:13:01.528: INFO: ss-0  aks-agentpool-72615086-vmss00000c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:10 +0000 UTC  }]
May 17 06:13:01.528: INFO: ss-1  aks-agentpool-72615086-vmss00000d  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:30 +0000 UTC  }]
May 17 06:13:01.528: INFO: ss-2  aks-agentpool-72615086-vmss00000c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:30 +0000 UTC  }]
May 17 06:13:01.528: INFO: 
May 17 06:13:01.528: INFO: StatefulSet ss has not reached scale 0, at 3
May 17 06:13:02.534: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
May 17 06:13:02.534: INFO: ss-0  aks-agentpool-72615086-vmss00000c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:10 +0000 UTC  }]
May 17 06:13:02.534: INFO: ss-2  aks-agentpool-72615086-vmss00000c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:30 +0000 UTC  }]
May 17 06:13:02.534: INFO: 
May 17 06:13:02.534: INFO: StatefulSet ss has not reached scale 0, at 2
May 17 06:13:03.539: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988070622s
May 17 06:13:04.545: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.982841527s
May 17 06:13:05.550: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.977179273s
May 17 06:13:06.557: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.971691117s
May 17 06:13:07.569: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.965421249s
May 17 06:13:08.576: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.95244283s
May 17 06:13:09.581: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.946486415s
May 17 06:13:10.587: INFO: Verifying statefulset ss doesn't scale past 0 for another 940.433654ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1658 05/17/23 06:13:11.587
May 17 06:13:11.593: INFO: Scaling statefulset ss to 0
May 17 06:13:11.609: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May 17 06:13:11.614: INFO: Deleting all statefulset in ns statefulset-1658
May 17 06:13:11.619: INFO: Scaling statefulset ss to 0
May 17 06:13:11.635: INFO: Waiting for statefulset status.replicas updated to 0
May 17 06:13:11.640: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May 17 06:13:11.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1658" for this suite. 05/17/23 06:13:11.669
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":130,"skipped":2517,"failed":0}
------------------------------
â€¢ [SLOW TEST] [61.733 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:12:09.944
    May 17 06:12:09.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename statefulset 05/17/23 06:12:09.944
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:12:09.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:12:09.967
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1658 05/17/23 06:12:09.972
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-1658 05/17/23 06:12:09.979
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1658 05/17/23 06:12:09.986
    May 17 06:12:10.003: INFO: Found 0 stateful pods, waiting for 1
    May 17 06:12:20.010: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/17/23 06:12:20.01
    May 17 06:12:20.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 06:12:20.214: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 06:12:20.214: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 06:12:20.214: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 06:12:20.220: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May 17 06:12:30.231: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May 17 06:12:30.231: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 06:12:30.255: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
    May 17 06:12:30.255: INFO: ss-0  aks-agentpool-72615086-vmss00000c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:10 +0000 UTC  }]
    May 17 06:12:30.255: INFO: 
    May 17 06:12:30.255: INFO: StatefulSet ss has not reached scale 3, at 1
    May 17 06:12:31.262: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993787173s
    May 17 06:12:32.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987557229s
    May 17 06:12:33.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981431204s
    May 17 06:12:34.280: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975631041s
    May 17 06:12:35.286: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969768293s
    May 17 06:12:36.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963302762s
    May 17 06:12:37.298: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95742969s
    May 17 06:12:38.304: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951345929s
    May 17 06:12:39.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.522632ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1658 05/17/23 06:12:40.31
    May 17 06:12:40.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 06:12:40.503: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 06:12:40.503: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 06:12:40.503: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 06:12:40.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 06:12:40.705: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May 17 06:12:40.705: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 06:12:40.705: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 06:12:40.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 06:12:40.887: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May 17 06:12:40.887: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 06:12:40.887: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 06:12:40.893: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    May 17 06:12:50.900: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May 17 06:12:50.900: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May 17 06:12:50.900: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 05/17/23 06:12:50.9
    May 17 06:12:50.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 06:12:51.118: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 06:12:51.118: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 06:12:51.118: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 06:12:51.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 06:12:51.306: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 06:12:51.306: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 06:12:51.306: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 06:12:51.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-1658 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 06:12:51.491: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 06:12:51.491: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 06:12:51.491: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 06:12:51.491: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 06:12:51.496: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    May 17 06:13:01.509: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May 17 06:13:01.509: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May 17 06:13:01.509: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May 17 06:13:01.528: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
    May 17 06:13:01.528: INFO: ss-0  aks-agentpool-72615086-vmss00000c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:10 +0000 UTC  }]
    May 17 06:13:01.528: INFO: ss-1  aks-agentpool-72615086-vmss00000d  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:30 +0000 UTC  }]
    May 17 06:13:01.528: INFO: ss-2  aks-agentpool-72615086-vmss00000c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:30 +0000 UTC  }]
    May 17 06:13:01.528: INFO: 
    May 17 06:13:01.528: INFO: StatefulSet ss has not reached scale 0, at 3
    May 17 06:13:02.534: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
    May 17 06:13:02.534: INFO: ss-0  aks-agentpool-72615086-vmss00000c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:10 +0000 UTC  }]
    May 17 06:13:02.534: INFO: ss-2  aks-agentpool-72615086-vmss00000c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-17 06:12:30 +0000 UTC  }]
    May 17 06:13:02.534: INFO: 
    May 17 06:13:02.534: INFO: StatefulSet ss has not reached scale 0, at 2
    May 17 06:13:03.539: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988070622s
    May 17 06:13:04.545: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.982841527s
    May 17 06:13:05.550: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.977179273s
    May 17 06:13:06.557: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.971691117s
    May 17 06:13:07.569: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.965421249s
    May 17 06:13:08.576: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.95244283s
    May 17 06:13:09.581: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.946486415s
    May 17 06:13:10.587: INFO: Verifying statefulset ss doesn't scale past 0 for another 940.433654ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1658 05/17/23 06:13:11.587
    May 17 06:13:11.593: INFO: Scaling statefulset ss to 0
    May 17 06:13:11.609: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May 17 06:13:11.614: INFO: Deleting all statefulset in ns statefulset-1658
    May 17 06:13:11.619: INFO: Scaling statefulset ss to 0
    May 17 06:13:11.635: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 06:13:11.640: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May 17 06:13:11.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1658" for this suite. 05/17/23 06:13:11.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:13:11.678
May 17 06:13:11.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 06:13:11.678
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:13:11.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:13:11.7
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-80d9e9af-8318-4d92-985b-899f012f0abb 05/17/23 06:13:11.704
STEP: Creating a pod to test consume secrets 05/17/23 06:13:11.71
May 17 06:13:11.722: INFO: Waiting up to 5m0s for pod "pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86" in namespace "secrets-35" to be "Succeeded or Failed"
May 17 06:13:11.728: INFO: Pod "pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86": Phase="Pending", Reason="", readiness=false. Elapsed: 5.856852ms
May 17 06:13:13.734: INFO: Pod "pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011569166s
May 17 06:13:15.735: INFO: Pod "pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012482474s
STEP: Saw pod success 05/17/23 06:13:15.735
May 17 06:13:15.735: INFO: Pod "pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86" satisfied condition "Succeeded or Failed"
May 17 06:13:15.740: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86 container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 06:13:15.753
May 17 06:13:15.770: INFO: Waiting for pod pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86 to disappear
May 17 06:13:15.775: INFO: Pod pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May 17 06:13:15.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-35" for this suite. 05/17/23 06:13:15.787
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":131,"skipped":2561,"failed":0}
------------------------------
â€¢ [4.118 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:13:11.678
    May 17 06:13:11.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 06:13:11.678
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:13:11.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:13:11.7
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-80d9e9af-8318-4d92-985b-899f012f0abb 05/17/23 06:13:11.704
    STEP: Creating a pod to test consume secrets 05/17/23 06:13:11.71
    May 17 06:13:11.722: INFO: Waiting up to 5m0s for pod "pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86" in namespace "secrets-35" to be "Succeeded or Failed"
    May 17 06:13:11.728: INFO: Pod "pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86": Phase="Pending", Reason="", readiness=false. Elapsed: 5.856852ms
    May 17 06:13:13.734: INFO: Pod "pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011569166s
    May 17 06:13:15.735: INFO: Pod "pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012482474s
    STEP: Saw pod success 05/17/23 06:13:15.735
    May 17 06:13:15.735: INFO: Pod "pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86" satisfied condition "Succeeded or Failed"
    May 17 06:13:15.740: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86 container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 06:13:15.753
    May 17 06:13:15.770: INFO: Waiting for pod pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86 to disappear
    May 17 06:13:15.775: INFO: Pod pod-secrets-2fd4ac9b-9ed3-496c-b2a1-5699ddf57b86 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May 17 06:13:15.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-35" for this suite. 05/17/23 06:13:15.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:13:15.796
May 17 06:13:15.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 06:13:15.797
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:13:15.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:13:15.82
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 06:13:15.841
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:13:16.033
STEP: Deploying the webhook pod 05/17/23 06:13:16.044
STEP: Wait for the deployment to be ready 05/17/23 06:13:16.06
May 17 06:13:16.071: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 06:13:18.094
STEP: Verifying the service has paired with the endpoint 05/17/23 06:13:18.11
May 17 06:13:19.111: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/17/23 06:13:19.117
STEP: create a pod that should be updated by the webhook 05/17/23 06:13:19.139
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:13:19.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3000" for this suite. 05/17/23 06:13:19.195
STEP: Destroying namespace "webhook-3000-markers" for this suite. 05/17/23 06:13:19.21
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":132,"skipped":2572,"failed":0}
------------------------------
â€¢ [3.470 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:13:15.796
    May 17 06:13:15.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 06:13:15.797
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:13:15.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:13:15.82
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 06:13:15.841
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:13:16.033
    STEP: Deploying the webhook pod 05/17/23 06:13:16.044
    STEP: Wait for the deployment to be ready 05/17/23 06:13:16.06
    May 17 06:13:16.071: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 06:13:18.094
    STEP: Verifying the service has paired with the endpoint 05/17/23 06:13:18.11
    May 17 06:13:19.111: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/17/23 06:13:19.117
    STEP: create a pod that should be updated by the webhook 05/17/23 06:13:19.139
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:13:19.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3000" for this suite. 05/17/23 06:13:19.195
    STEP: Destroying namespace "webhook-3000-markers" for this suite. 05/17/23 06:13:19.21
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:13:19.268
May 17 06:13:19.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename statefulset 05/17/23 06:13:19.268
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:13:19.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:13:19.289
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3781 05/17/23 06:13:19.295
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-3781 05/17/23 06:13:19.302
May 17 06:13:19.315: INFO: Found 0 stateful pods, waiting for 1
May 17 06:13:29.342: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 05/17/23 06:13:29.354
STEP: updating a scale subresource 05/17/23 06:13:29.36
STEP: verifying the statefulset Spec.Replicas was modified 05/17/23 06:13:29.369
STEP: Patch a scale subresource 05/17/23 06:13:29.375
STEP: verifying the statefulset Spec.Replicas was modified 05/17/23 06:13:29.383
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May 17 06:13:29.388: INFO: Deleting all statefulset in ns statefulset-3781
May 17 06:13:29.394: INFO: Scaling statefulset ss to 0
May 17 06:13:39.421: INFO: Waiting for statefulset status.replicas updated to 0
May 17 06:13:39.426: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May 17 06:13:39.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3781" for this suite. 05/17/23 06:13:39.457
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":133,"skipped":2588,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.198 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:13:19.268
    May 17 06:13:19.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename statefulset 05/17/23 06:13:19.268
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:13:19.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:13:19.289
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3781 05/17/23 06:13:19.295
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-3781 05/17/23 06:13:19.302
    May 17 06:13:19.315: INFO: Found 0 stateful pods, waiting for 1
    May 17 06:13:29.342: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 05/17/23 06:13:29.354
    STEP: updating a scale subresource 05/17/23 06:13:29.36
    STEP: verifying the statefulset Spec.Replicas was modified 05/17/23 06:13:29.369
    STEP: Patch a scale subresource 05/17/23 06:13:29.375
    STEP: verifying the statefulset Spec.Replicas was modified 05/17/23 06:13:29.383
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May 17 06:13:29.388: INFO: Deleting all statefulset in ns statefulset-3781
    May 17 06:13:29.394: INFO: Scaling statefulset ss to 0
    May 17 06:13:39.421: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 06:13:39.426: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May 17 06:13:39.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3781" for this suite. 05/17/23 06:13:39.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:13:39.467
May 17 06:13:39.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename resourcequota 05/17/23 06:13:39.468
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:13:39.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:13:39.493
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 05/17/23 06:13:39.498
STEP: Ensuring ResourceQuota status is calculated 05/17/23 06:13:39.505
STEP: Creating a ResourceQuota with not terminating scope 05/17/23 06:13:41.511
STEP: Ensuring ResourceQuota status is calculated 05/17/23 06:13:41.518
STEP: Creating a long running pod 05/17/23 06:13:43.524
STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/17/23 06:13:43.543
STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/17/23 06:13:45.549
STEP: Deleting the pod 05/17/23 06:13:47.555
STEP: Ensuring resource quota status released the pod usage 05/17/23 06:13:47.57
STEP: Creating a terminating pod 05/17/23 06:13:49.577
STEP: Ensuring resource quota with terminating scope captures the pod usage 05/17/23 06:13:49.591
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/17/23 06:13:51.596
STEP: Deleting the pod 05/17/23 06:13:53.602
STEP: Ensuring resource quota status released the pod usage 05/17/23 06:13:53.618
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May 17 06:13:55.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3703" for this suite. 05/17/23 06:13:55.637
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":134,"skipped":2634,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.180 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:13:39.467
    May 17 06:13:39.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename resourcequota 05/17/23 06:13:39.468
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:13:39.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:13:39.493
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 05/17/23 06:13:39.498
    STEP: Ensuring ResourceQuota status is calculated 05/17/23 06:13:39.505
    STEP: Creating a ResourceQuota with not terminating scope 05/17/23 06:13:41.511
    STEP: Ensuring ResourceQuota status is calculated 05/17/23 06:13:41.518
    STEP: Creating a long running pod 05/17/23 06:13:43.524
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/17/23 06:13:43.543
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/17/23 06:13:45.549
    STEP: Deleting the pod 05/17/23 06:13:47.555
    STEP: Ensuring resource quota status released the pod usage 05/17/23 06:13:47.57
    STEP: Creating a terminating pod 05/17/23 06:13:49.577
    STEP: Ensuring resource quota with terminating scope captures the pod usage 05/17/23 06:13:49.591
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/17/23 06:13:51.596
    STEP: Deleting the pod 05/17/23 06:13:53.602
    STEP: Ensuring resource quota status released the pod usage 05/17/23 06:13:53.618
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May 17 06:13:55.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3703" for this suite. 05/17/23 06:13:55.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:13:55.648
May 17 06:13:55.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:13:55.649
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:13:55.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:13:55.683
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-25897d1a-7ec9-49c4-9e83-4f1a00998e5a 05/17/23 06:13:55.689
STEP: Creating a pod to test consume secrets 05/17/23 06:13:55.696
May 17 06:13:55.710: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12" in namespace "projected-8869" to be "Succeeded or Failed"
May 17 06:13:55.717: INFO: Pod "pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12": Phase="Pending", Reason="", readiness=false. Elapsed: 6.619245ms
May 17 06:13:57.723: INFO: Pod "pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013015582s
May 17 06:13:59.724: INFO: Pod "pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014025868s
STEP: Saw pod success 05/17/23 06:13:59.724
May 17 06:13:59.725: INFO: Pod "pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12" satisfied condition "Succeeded or Failed"
May 17 06:13:59.730: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/17/23 06:13:59.745
May 17 06:13:59.768: INFO: Waiting for pod pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12 to disappear
May 17 06:13:59.773: INFO: Pod pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May 17 06:13:59.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8869" for this suite. 05/17/23 06:13:59.784
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":135,"skipped":2660,"failed":0}
------------------------------
â€¢ [4.149 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:13:55.648
    May 17 06:13:55.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:13:55.649
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:13:55.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:13:55.683
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-25897d1a-7ec9-49c4-9e83-4f1a00998e5a 05/17/23 06:13:55.689
    STEP: Creating a pod to test consume secrets 05/17/23 06:13:55.696
    May 17 06:13:55.710: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12" in namespace "projected-8869" to be "Succeeded or Failed"
    May 17 06:13:55.717: INFO: Pod "pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12": Phase="Pending", Reason="", readiness=false. Elapsed: 6.619245ms
    May 17 06:13:57.723: INFO: Pod "pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013015582s
    May 17 06:13:59.724: INFO: Pod "pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014025868s
    STEP: Saw pod success 05/17/23 06:13:59.724
    May 17 06:13:59.725: INFO: Pod "pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12" satisfied condition "Succeeded or Failed"
    May 17 06:13:59.730: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 06:13:59.745
    May 17 06:13:59.768: INFO: Waiting for pod pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12 to disappear
    May 17 06:13:59.773: INFO: Pod pod-projected-secrets-72b6b69a-4c94-4389-9fa3-3d4de7cb0d12 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May 17 06:13:59.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8869" for this suite. 05/17/23 06:13:59.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:13:59.797
May 17 06:13:59.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-probe 05/17/23 06:13:59.798
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:13:59.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:13:59.822
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e in namespace container-probe-2226 05/17/23 06:13:59.826
May 17 06:13:59.839: INFO: Waiting up to 5m0s for pod "test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e" in namespace "container-probe-2226" to be "not pending"
May 17 06:13:59.844: INFO: Pod "test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.768775ms
May 17 06:14:01.850: INFO: Pod "test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e": Phase="Running", Reason="", readiness=true. Elapsed: 2.011264969s
May 17 06:14:01.850: INFO: Pod "test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e" satisfied condition "not pending"
May 17 06:14:01.850: INFO: Started pod test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e in namespace container-probe-2226
STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 06:14:01.85
May 17 06:14:01.855: INFO: Initial restart count of pod test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e is 0
STEP: deleting the pod 05/17/23 06:18:02.65
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May 17 06:18:02.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2226" for this suite. 05/17/23 06:18:02.679
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":136,"skipped":2677,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.890 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:13:59.797
    May 17 06:13:59.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-probe 05/17/23 06:13:59.798
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:13:59.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:13:59.822
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e in namespace container-probe-2226 05/17/23 06:13:59.826
    May 17 06:13:59.839: INFO: Waiting up to 5m0s for pod "test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e" in namespace "container-probe-2226" to be "not pending"
    May 17 06:13:59.844: INFO: Pod "test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.768775ms
    May 17 06:14:01.850: INFO: Pod "test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e": Phase="Running", Reason="", readiness=true. Elapsed: 2.011264969s
    May 17 06:14:01.850: INFO: Pod "test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e" satisfied condition "not pending"
    May 17 06:14:01.850: INFO: Started pod test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e in namespace container-probe-2226
    STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 06:14:01.85
    May 17 06:14:01.855: INFO: Initial restart count of pod test-webserver-3a144b9a-c60f-427d-be69-17bc2ff3b71e is 0
    STEP: deleting the pod 05/17/23 06:18:02.65
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May 17 06:18:02.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2226" for this suite. 05/17/23 06:18:02.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:18:02.689
May 17 06:18:02.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:18:02.69
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:02.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:02.712
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 05/17/23 06:18:02.717
May 17 06:18:02.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 create -f -'
May 17 06:18:03.110: INFO: stderr: ""
May 17 06:18:03.110: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 06:18:03.11
May 17 06:18:03.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 06:18:03.179: INFO: stderr: ""
May 17 06:18:03.179: INFO: stdout: "update-demo-nautilus-2f2cm update-demo-nautilus-hr9d6 "
May 17 06:18:03.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods update-demo-nautilus-2f2cm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:18:03.246: INFO: stderr: ""
May 17 06:18:03.246: INFO: stdout: ""
May 17 06:18:03.246: INFO: update-demo-nautilus-2f2cm is created but not running
May 17 06:18:08.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 06:18:08.311: INFO: stderr: ""
May 17 06:18:08.311: INFO: stdout: "update-demo-nautilus-2f2cm update-demo-nautilus-hr9d6 "
May 17 06:18:08.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods update-demo-nautilus-2f2cm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:18:08.369: INFO: stderr: ""
May 17 06:18:08.369: INFO: stdout: "true"
May 17 06:18:08.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods update-demo-nautilus-2f2cm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 06:18:08.424: INFO: stderr: ""
May 17 06:18:08.424: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May 17 06:18:08.424: INFO: validating pod update-demo-nautilus-2f2cm
May 17 06:18:08.435: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 06:18:08.435: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 06:18:08.435: INFO: update-demo-nautilus-2f2cm is verified up and running
May 17 06:18:08.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods update-demo-nautilus-hr9d6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:18:08.502: INFO: stderr: ""
May 17 06:18:08.502: INFO: stdout: "true"
May 17 06:18:08.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods update-demo-nautilus-hr9d6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 06:18:08.578: INFO: stderr: ""
May 17 06:18:08.578: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May 17 06:18:08.578: INFO: validating pod update-demo-nautilus-hr9d6
May 17 06:18:08.591: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 06:18:08.591: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 06:18:08.591: INFO: update-demo-nautilus-hr9d6 is verified up and running
STEP: using delete to clean up resources 05/17/23 06:18:08.591
May 17 06:18:08.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 delete --grace-period=0 --force -f -'
May 17 06:18:08.653: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 06:18:08.653: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 17 06:18:08.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get rc,svc -l name=update-demo --no-headers'
May 17 06:18:08.733: INFO: stderr: "No resources found in kubectl-4319 namespace.\n"
May 17 06:18:08.733: INFO: stdout: ""
May 17 06:18:08.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 17 06:18:08.796: INFO: stderr: ""
May 17 06:18:08.796: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:18:08.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4319" for this suite. 05/17/23 06:18:08.808
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":137,"skipped":2723,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.129 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:18:02.689
    May 17 06:18:02.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:18:02.69
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:02.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:02.712
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 05/17/23 06:18:02.717
    May 17 06:18:02.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 create -f -'
    May 17 06:18:03.110: INFO: stderr: ""
    May 17 06:18:03.110: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 06:18:03.11
    May 17 06:18:03.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 06:18:03.179: INFO: stderr: ""
    May 17 06:18:03.179: INFO: stdout: "update-demo-nautilus-2f2cm update-demo-nautilus-hr9d6 "
    May 17 06:18:03.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods update-demo-nautilus-2f2cm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:18:03.246: INFO: stderr: ""
    May 17 06:18:03.246: INFO: stdout: ""
    May 17 06:18:03.246: INFO: update-demo-nautilus-2f2cm is created but not running
    May 17 06:18:08.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 06:18:08.311: INFO: stderr: ""
    May 17 06:18:08.311: INFO: stdout: "update-demo-nautilus-2f2cm update-demo-nautilus-hr9d6 "
    May 17 06:18:08.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods update-demo-nautilus-2f2cm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:18:08.369: INFO: stderr: ""
    May 17 06:18:08.369: INFO: stdout: "true"
    May 17 06:18:08.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods update-demo-nautilus-2f2cm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 06:18:08.424: INFO: stderr: ""
    May 17 06:18:08.424: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May 17 06:18:08.424: INFO: validating pod update-demo-nautilus-2f2cm
    May 17 06:18:08.435: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 06:18:08.435: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 06:18:08.435: INFO: update-demo-nautilus-2f2cm is verified up and running
    May 17 06:18:08.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods update-demo-nautilus-hr9d6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:18:08.502: INFO: stderr: ""
    May 17 06:18:08.502: INFO: stdout: "true"
    May 17 06:18:08.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods update-demo-nautilus-hr9d6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 06:18:08.578: INFO: stderr: ""
    May 17 06:18:08.578: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May 17 06:18:08.578: INFO: validating pod update-demo-nautilus-hr9d6
    May 17 06:18:08.591: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 06:18:08.591: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 06:18:08.591: INFO: update-demo-nautilus-hr9d6 is verified up and running
    STEP: using delete to clean up resources 05/17/23 06:18:08.591
    May 17 06:18:08.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 delete --grace-period=0 --force -f -'
    May 17 06:18:08.653: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 06:18:08.653: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May 17 06:18:08.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get rc,svc -l name=update-demo --no-headers'
    May 17 06:18:08.733: INFO: stderr: "No resources found in kubectl-4319 namespace.\n"
    May 17 06:18:08.733: INFO: stdout: ""
    May 17 06:18:08.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4319 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May 17 06:18:08.796: INFO: stderr: ""
    May 17 06:18:08.796: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:18:08.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4319" for this suite. 05/17/23 06:18:08.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:18:08.819
May 17 06:18:08.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename dns 05/17/23 06:18:08.82
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:08.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:08.843
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/17/23 06:18:08.847
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/17/23 06:18:08.847
STEP: creating a pod to probe DNS 05/17/23 06:18:08.848
STEP: submitting the pod to kubernetes 05/17/23 06:18:08.848
May 17 06:18:08.867: INFO: Waiting up to 15m0s for pod "dns-test-b5525e60-61a0-44ce-9a48-5ead13f125bc" in namespace "dns-6988" to be "running"
May 17 06:18:08.873: INFO: Pod "dns-test-b5525e60-61a0-44ce-9a48-5ead13f125bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.248199ms
May 17 06:18:10.882: INFO: Pod "dns-test-b5525e60-61a0-44ce-9a48-5ead13f125bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.015432137s
May 17 06:18:10.882: INFO: Pod "dns-test-b5525e60-61a0-44ce-9a48-5ead13f125bc" satisfied condition "running"
STEP: retrieving the pod 05/17/23 06:18:10.882
STEP: looking for the results for each expected name from probers 05/17/23 06:18:10.888
May 17 06:18:10.924: INFO: DNS probes using dns-6988/dns-test-b5525e60-61a0-44ce-9a48-5ead13f125bc succeeded

STEP: deleting the pod 05/17/23 06:18:10.924
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May 17 06:18:10.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6988" for this suite. 05/17/23 06:18:10.953
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":138,"skipped":2728,"failed":0}
------------------------------
â€¢ [2.143 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:18:08.819
    May 17 06:18:08.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename dns 05/17/23 06:18:08.82
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:08.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:08.843
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/17/23 06:18:08.847
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/17/23 06:18:08.847
    STEP: creating a pod to probe DNS 05/17/23 06:18:08.848
    STEP: submitting the pod to kubernetes 05/17/23 06:18:08.848
    May 17 06:18:08.867: INFO: Waiting up to 15m0s for pod "dns-test-b5525e60-61a0-44ce-9a48-5ead13f125bc" in namespace "dns-6988" to be "running"
    May 17 06:18:08.873: INFO: Pod "dns-test-b5525e60-61a0-44ce-9a48-5ead13f125bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.248199ms
    May 17 06:18:10.882: INFO: Pod "dns-test-b5525e60-61a0-44ce-9a48-5ead13f125bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.015432137s
    May 17 06:18:10.882: INFO: Pod "dns-test-b5525e60-61a0-44ce-9a48-5ead13f125bc" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 06:18:10.882
    STEP: looking for the results for each expected name from probers 05/17/23 06:18:10.888
    May 17 06:18:10.924: INFO: DNS probes using dns-6988/dns-test-b5525e60-61a0-44ce-9a48-5ead13f125bc succeeded

    STEP: deleting the pod 05/17/23 06:18:10.924
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May 17 06:18:10.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6988" for this suite. 05/17/23 06:18:10.953
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:18:10.962
May 17 06:18:10.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename replication-controller 05/17/23 06:18:10.963
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:10.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:10.983
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1 05/17/23 06:18:10.988
May 17 06:18:11.000: INFO: Pod name my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1: Found 0 pods out of 1
May 17 06:18:16.006: INFO: Pod name my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1: Found 1 pods out of 1
May 17 06:18:16.006: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1" are running
May 17 06:18:16.006: INFO: Waiting up to 5m0s for pod "my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1-lfbht" in namespace "replication-controller-4458" to be "running"
May 17 06:18:16.011: INFO: Pod "my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1-lfbht": Phase="Running", Reason="", readiness=true. Elapsed: 5.628444ms
May 17 06:18:16.011: INFO: Pod "my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1-lfbht" satisfied condition "running"
May 17 06:18:16.011: INFO: Pod "my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1-lfbht" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 06:18:11 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 06:18:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 06:18:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 06:18:11 +0000 UTC Reason: Message:}])
May 17 06:18:16.011: INFO: Trying to dial the pod
May 17 06:18:21.034: INFO: Controller my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1: Got expected result from replica 1 [my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1-lfbht]: "my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1-lfbht", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May 17 06:18:21.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4458" for this suite. 05/17/23 06:18:21.047
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":139,"skipped":2731,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.093 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:18:10.962
    May 17 06:18:10.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename replication-controller 05/17/23 06:18:10.963
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:10.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:10.983
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1 05/17/23 06:18:10.988
    May 17 06:18:11.000: INFO: Pod name my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1: Found 0 pods out of 1
    May 17 06:18:16.006: INFO: Pod name my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1: Found 1 pods out of 1
    May 17 06:18:16.006: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1" are running
    May 17 06:18:16.006: INFO: Waiting up to 5m0s for pod "my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1-lfbht" in namespace "replication-controller-4458" to be "running"
    May 17 06:18:16.011: INFO: Pod "my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1-lfbht": Phase="Running", Reason="", readiness=true. Elapsed: 5.628444ms
    May 17 06:18:16.011: INFO: Pod "my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1-lfbht" satisfied condition "running"
    May 17 06:18:16.011: INFO: Pod "my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1-lfbht" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 06:18:11 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 06:18:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 06:18:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-17 06:18:11 +0000 UTC Reason: Message:}])
    May 17 06:18:16.011: INFO: Trying to dial the pod
    May 17 06:18:21.034: INFO: Controller my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1: Got expected result from replica 1 [my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1-lfbht]: "my-hostname-basic-101eab12-7d25-4a8f-b86e-b8d305a9e7b1-lfbht", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May 17 06:18:21.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4458" for this suite. 05/17/23 06:18:21.047
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:18:21.056
May 17 06:18:21.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename runtimeclass 05/17/23 06:18:21.057
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:21.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:21.094
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
May 17 06:18:21.118: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6753 to be scheduled
May 17 06:18:21.123: INFO: 1 pods are not scheduled: [runtimeclass-6753/test-runtimeclass-runtimeclass-6753-preconfigured-handler-pg5dg(da204ce9-53c9-40b0-a5cd-24d5215b87d9)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May 17 06:18:23.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6753" for this suite. 05/17/23 06:18:23.15
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":140,"skipped":2732,"failed":0}
------------------------------
â€¢ [2.104 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:18:21.056
    May 17 06:18:21.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename runtimeclass 05/17/23 06:18:21.057
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:21.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:21.094
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    May 17 06:18:21.118: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6753 to be scheduled
    May 17 06:18:21.123: INFO: 1 pods are not scheduled: [runtimeclass-6753/test-runtimeclass-runtimeclass-6753-preconfigured-handler-pg5dg(da204ce9-53c9-40b0-a5cd-24d5215b87d9)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May 17 06:18:23.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6753" for this suite. 05/17/23 06:18:23.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:18:23.16
May 17 06:18:23.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename replication-controller 05/17/23 06:18:23.161
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:23.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:23.183
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
May 17 06:18:23.188: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/17/23 06:18:24.204
STEP: Checking rc "condition-test" has the desired failure condition set 05/17/23 06:18:24.211
STEP: Scaling down rc "condition-test" to satisfy pod quota 05/17/23 06:18:25.222
May 17 06:18:25.235: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 05/17/23 06:18:25.235
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May 17 06:18:26.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7296" for this suite. 05/17/23 06:18:26.257
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":141,"skipped":2738,"failed":0}
------------------------------
â€¢ [3.106 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:18:23.16
    May 17 06:18:23.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename replication-controller 05/17/23 06:18:23.161
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:23.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:23.183
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    May 17 06:18:23.188: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/17/23 06:18:24.204
    STEP: Checking rc "condition-test" has the desired failure condition set 05/17/23 06:18:24.211
    STEP: Scaling down rc "condition-test" to satisfy pod quota 05/17/23 06:18:25.222
    May 17 06:18:25.235: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 05/17/23 06:18:25.235
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May 17 06:18:26.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7296" for this suite. 05/17/23 06:18:26.257
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:18:26.267
May 17 06:18:26.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 06:18:26.268
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:26.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:26.29
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-215c48e6-b1c2-4dcc-9d89-6fcb279b4b6d 05/17/23 06:18:26.295
STEP: Creating a pod to test consume secrets 05/17/23 06:18:26.302
May 17 06:18:26.314: INFO: Waiting up to 5m0s for pod "pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066" in namespace "secrets-7582" to be "Succeeded or Failed"
May 17 06:18:26.346: INFO: Pod "pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066": Phase="Pending", Reason="", readiness=false. Elapsed: 32.361339ms
May 17 06:18:28.352: INFO: Pod "pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038523209s
May 17 06:18:30.351: INFO: Pod "pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03749975s
STEP: Saw pod success 05/17/23 06:18:30.351
May 17 06:18:30.351: INFO: Pod "pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066" satisfied condition "Succeeded or Failed"
May 17 06:18:30.355: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066 container secret-env-test: <nil>
STEP: delete the pod 05/17/23 06:18:30.367
May 17 06:18:30.380: INFO: Waiting for pod pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066 to disappear
May 17 06:18:30.384: INFO: Pod pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May 17 06:18:30.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7582" for this suite. 05/17/23 06:18:30.391
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":142,"skipped":2742,"failed":0}
------------------------------
â€¢ [4.132 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:18:26.267
    May 17 06:18:26.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 06:18:26.268
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:26.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:26.29
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-215c48e6-b1c2-4dcc-9d89-6fcb279b4b6d 05/17/23 06:18:26.295
    STEP: Creating a pod to test consume secrets 05/17/23 06:18:26.302
    May 17 06:18:26.314: INFO: Waiting up to 5m0s for pod "pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066" in namespace "secrets-7582" to be "Succeeded or Failed"
    May 17 06:18:26.346: INFO: Pod "pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066": Phase="Pending", Reason="", readiness=false. Elapsed: 32.361339ms
    May 17 06:18:28.352: INFO: Pod "pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038523209s
    May 17 06:18:30.351: INFO: Pod "pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03749975s
    STEP: Saw pod success 05/17/23 06:18:30.351
    May 17 06:18:30.351: INFO: Pod "pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066" satisfied condition "Succeeded or Failed"
    May 17 06:18:30.355: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066 container secret-env-test: <nil>
    STEP: delete the pod 05/17/23 06:18:30.367
    May 17 06:18:30.380: INFO: Waiting for pod pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066 to disappear
    May 17 06:18:30.384: INFO: Pod pod-secrets-e8db0c19-6c76-471a-96a3-860f70095066 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May 17 06:18:30.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7582" for this suite. 05/17/23 06:18:30.391
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:18:30.399
May 17 06:18:30.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:18:30.4
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:30.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:30.419
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 05/17/23 06:18:30.423
May 17 06:18:30.423: INFO: Creating e2e-svc-a-z8jc5
May 17 06:18:30.435: INFO: Creating e2e-svc-b-rhntn
May 17 06:18:30.445: INFO: Creating e2e-svc-c-g6qdv
STEP: deleting service collection 05/17/23 06:18:30.461
May 17 06:18:30.486: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:18:30.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5782" for this suite. 05/17/23 06:18:30.493
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":143,"skipped":2743,"failed":0}
------------------------------
â€¢ [0.103 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:18:30.399
    May 17 06:18:30.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:18:30.4
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:30.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:30.419
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 05/17/23 06:18:30.423
    May 17 06:18:30.423: INFO: Creating e2e-svc-a-z8jc5
    May 17 06:18:30.435: INFO: Creating e2e-svc-b-rhntn
    May 17 06:18:30.445: INFO: Creating e2e-svc-c-g6qdv
    STEP: deleting service collection 05/17/23 06:18:30.461
    May 17 06:18:30.486: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:18:30.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5782" for this suite. 05/17/23 06:18:30.493
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:18:30.502
May 17 06:18:30.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir-wrapper 05/17/23 06:18:30.503
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:30.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:30.529
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 05/17/23 06:18:30.533
STEP: Creating RC which spawns configmap-volume pods 05/17/23 06:18:30.778
May 17 06:18:30.861: INFO: Pod name wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8: Found 3 pods out of 5
May 17 06:18:35.872: INFO: Pod name wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/17/23 06:18:35.873
May 17 06:18:35.873: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:18:35.877: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.698814ms
May 17 06:18:37.884: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011386788s
May 17 06:18:39.885: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012143946s
May 17 06:18:41.885: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012176597s
May 17 06:18:43.882: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009795727s
May 17 06:18:45.884: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j": Phase="Running", Reason="", readiness=true. Elapsed: 10.011197866s
May 17 06:18:45.884: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j" satisfied condition "running"
May 17 06:18:45.884: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-m2ss2" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:18:45.888: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-m2ss2": Phase="Running", Reason="", readiness=true. Elapsed: 4.220497ms
May 17 06:18:45.888: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-m2ss2" satisfied condition "running"
May 17 06:18:45.888: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-s85nx" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:18:45.893: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-s85nx": Phase="Running", Reason="", readiness=true. Elapsed: 4.648601ms
May 17 06:18:45.893: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-s85nx" satisfied condition "running"
May 17 06:18:45.893: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-tn65f" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:18:45.897: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-tn65f": Phase="Running", Reason="", readiness=true. Elapsed: 4.349709ms
May 17 06:18:45.897: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-tn65f" satisfied condition "running"
May 17 06:18:45.897: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-w62ph" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:18:45.901: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-w62ph": Phase="Running", Reason="", readiness=true. Elapsed: 4.073757ms
May 17 06:18:45.901: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-w62ph" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8 in namespace emptydir-wrapper-957, will wait for the garbage collector to delete the pods 05/17/23 06:18:45.901
May 17 06:18:45.966: INFO: Deleting ReplicationController wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8 took: 7.795833ms
May 17 06:18:46.067: INFO: Terminating ReplicationController wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8 pods took: 100.869691ms
STEP: Creating RC which spawns configmap-volume pods 05/17/23 06:18:49.676
May 17 06:18:49.692: INFO: Pod name wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b: Found 0 pods out of 5
May 17 06:18:54.703: INFO: Pod name wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/17/23 06:18:54.703
May 17 06:18:54.703: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:18:54.707: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191826ms
May 17 06:18:56.714: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011007145s
May 17 06:18:58.714: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010937573s
May 17 06:19:00.713: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010017712s
May 17 06:19:02.715: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5": Phase="Running", Reason="", readiness=true. Elapsed: 8.011465212s
May 17 06:19:02.715: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5" satisfied condition "running"
May 17 06:19:02.715: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-j5r76" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:19:02.719: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-j5r76": Phase="Running", Reason="", readiness=true. Elapsed: 4.279149ms
May 17 06:19:02.719: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-j5r76" satisfied condition "running"
May 17 06:19:02.719: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-m9hzg" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:19:02.723: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-m9hzg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.886094ms
May 17 06:19:04.730: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-m9hzg": Phase="Running", Reason="", readiness=true. Elapsed: 2.010538401s
May 17 06:19:04.730: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-m9hzg" satisfied condition "running"
May 17 06:19:04.730: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-qzb9r" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:19:04.734: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-qzb9r": Phase="Running", Reason="", readiness=true. Elapsed: 4.401828ms
May 17 06:19:04.734: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-qzb9r" satisfied condition "running"
May 17 06:19:04.734: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-r9wf4" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:19:04.739: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-r9wf4": Phase="Running", Reason="", readiness=true. Elapsed: 4.519138ms
May 17 06:19:04.739: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-r9wf4" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b in namespace emptydir-wrapper-957, will wait for the garbage collector to delete the pods 05/17/23 06:19:04.739
May 17 06:19:04.803: INFO: Deleting ReplicationController wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b took: 8.114329ms
May 17 06:19:04.904: INFO: Terminating ReplicationController wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b pods took: 100.755416ms
STEP: Creating RC which spawns configmap-volume pods 05/17/23 06:19:08.413
May 17 06:19:08.433: INFO: Pod name wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d: Found 0 pods out of 5
May 17 06:19:13.445: INFO: Pod name wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/17/23 06:19:13.445
May 17 06:19:13.445: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:19:13.450: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.708433ms
May 17 06:19:15.456: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010703894s
May 17 06:19:17.457: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011103242s
May 17 06:19:19.460: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014914359s
May 17 06:19:21.457: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011181285s
May 17 06:19:23.457: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq": Phase="Running", Reason="", readiness=true. Elapsed: 10.011859403s
May 17 06:19:23.457: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq" satisfied condition "running"
May 17 06:19:23.457: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-b4cv4" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:19:23.462: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-b4cv4": Phase="Running", Reason="", readiness=true. Elapsed: 5.10942ms
May 17 06:19:23.462: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-b4cv4" satisfied condition "running"
May 17 06:19:23.462: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-dgzkc" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:19:23.468: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-dgzkc": Phase="Running", Reason="", readiness=true. Elapsed: 5.244003ms
May 17 06:19:23.468: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-dgzkc" satisfied condition "running"
May 17 06:19:23.468: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-l6zhb" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:19:23.483: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-l6zhb": Phase="Running", Reason="", readiness=true. Elapsed: 15.762907ms
May 17 06:19:23.483: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-l6zhb" satisfied condition "running"
May 17 06:19:23.483: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-wgk49" in namespace "emptydir-wrapper-957" to be "running"
May 17 06:19:23.489: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-wgk49": Phase="Running", Reason="", readiness=true. Elapsed: 5.111706ms
May 17 06:19:23.489: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-wgk49" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d in namespace emptydir-wrapper-957, will wait for the garbage collector to delete the pods 05/17/23 06:19:23.489
May 17 06:19:23.557: INFO: Deleting ReplicationController wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d took: 9.064384ms
May 17 06:19:23.658: INFO: Terminating ReplicationController wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d pods took: 100.501571ms
STEP: Cleaning up the configMaps 05/17/23 06:19:26.858
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
May 17 06:19:27.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-957" for this suite. 05/17/23 06:19:27.223
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":144,"skipped":2753,"failed":0}
------------------------------
â€¢ [SLOW TEST] [56.728 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:18:30.502
    May 17 06:18:30.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir-wrapper 05/17/23 06:18:30.503
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:18:30.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:18:30.529
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 05/17/23 06:18:30.533
    STEP: Creating RC which spawns configmap-volume pods 05/17/23 06:18:30.778
    May 17 06:18:30.861: INFO: Pod name wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8: Found 3 pods out of 5
    May 17 06:18:35.872: INFO: Pod name wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/17/23 06:18:35.873
    May 17 06:18:35.873: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:18:35.877: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.698814ms
    May 17 06:18:37.884: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011386788s
    May 17 06:18:39.885: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012143946s
    May 17 06:18:41.885: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012176597s
    May 17 06:18:43.882: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009795727s
    May 17 06:18:45.884: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j": Phase="Running", Reason="", readiness=true. Elapsed: 10.011197866s
    May 17 06:18:45.884: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-kb82j" satisfied condition "running"
    May 17 06:18:45.884: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-m2ss2" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:18:45.888: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-m2ss2": Phase="Running", Reason="", readiness=true. Elapsed: 4.220497ms
    May 17 06:18:45.888: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-m2ss2" satisfied condition "running"
    May 17 06:18:45.888: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-s85nx" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:18:45.893: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-s85nx": Phase="Running", Reason="", readiness=true. Elapsed: 4.648601ms
    May 17 06:18:45.893: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-s85nx" satisfied condition "running"
    May 17 06:18:45.893: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-tn65f" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:18:45.897: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-tn65f": Phase="Running", Reason="", readiness=true. Elapsed: 4.349709ms
    May 17 06:18:45.897: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-tn65f" satisfied condition "running"
    May 17 06:18:45.897: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-w62ph" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:18:45.901: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-w62ph": Phase="Running", Reason="", readiness=true. Elapsed: 4.073757ms
    May 17 06:18:45.901: INFO: Pod "wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8-w62ph" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8 in namespace emptydir-wrapper-957, will wait for the garbage collector to delete the pods 05/17/23 06:18:45.901
    May 17 06:18:45.966: INFO: Deleting ReplicationController wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8 took: 7.795833ms
    May 17 06:18:46.067: INFO: Terminating ReplicationController wrapped-volume-race-64249c6f-46ad-41a8-9572-ae34113e91a8 pods took: 100.869691ms
    STEP: Creating RC which spawns configmap-volume pods 05/17/23 06:18:49.676
    May 17 06:18:49.692: INFO: Pod name wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b: Found 0 pods out of 5
    May 17 06:18:54.703: INFO: Pod name wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/17/23 06:18:54.703
    May 17 06:18:54.703: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:18:54.707: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191826ms
    May 17 06:18:56.714: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011007145s
    May 17 06:18:58.714: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010937573s
    May 17 06:19:00.713: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010017712s
    May 17 06:19:02.715: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5": Phase="Running", Reason="", readiness=true. Elapsed: 8.011465212s
    May 17 06:19:02.715: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-8czw5" satisfied condition "running"
    May 17 06:19:02.715: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-j5r76" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:19:02.719: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-j5r76": Phase="Running", Reason="", readiness=true. Elapsed: 4.279149ms
    May 17 06:19:02.719: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-j5r76" satisfied condition "running"
    May 17 06:19:02.719: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-m9hzg" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:19:02.723: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-m9hzg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.886094ms
    May 17 06:19:04.730: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-m9hzg": Phase="Running", Reason="", readiness=true. Elapsed: 2.010538401s
    May 17 06:19:04.730: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-m9hzg" satisfied condition "running"
    May 17 06:19:04.730: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-qzb9r" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:19:04.734: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-qzb9r": Phase="Running", Reason="", readiness=true. Elapsed: 4.401828ms
    May 17 06:19:04.734: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-qzb9r" satisfied condition "running"
    May 17 06:19:04.734: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-r9wf4" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:19:04.739: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-r9wf4": Phase="Running", Reason="", readiness=true. Elapsed: 4.519138ms
    May 17 06:19:04.739: INFO: Pod "wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b-r9wf4" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b in namespace emptydir-wrapper-957, will wait for the garbage collector to delete the pods 05/17/23 06:19:04.739
    May 17 06:19:04.803: INFO: Deleting ReplicationController wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b took: 8.114329ms
    May 17 06:19:04.904: INFO: Terminating ReplicationController wrapped-volume-race-c3e470af-d42b-49fc-a4e7-4e46b0b7f09b pods took: 100.755416ms
    STEP: Creating RC which spawns configmap-volume pods 05/17/23 06:19:08.413
    May 17 06:19:08.433: INFO: Pod name wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d: Found 0 pods out of 5
    May 17 06:19:13.445: INFO: Pod name wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/17/23 06:19:13.445
    May 17 06:19:13.445: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:19:13.450: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.708433ms
    May 17 06:19:15.456: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010703894s
    May 17 06:19:17.457: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011103242s
    May 17 06:19:19.460: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014914359s
    May 17 06:19:21.457: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011181285s
    May 17 06:19:23.457: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq": Phase="Running", Reason="", readiness=true. Elapsed: 10.011859403s
    May 17 06:19:23.457: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-7hwhq" satisfied condition "running"
    May 17 06:19:23.457: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-b4cv4" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:19:23.462: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-b4cv4": Phase="Running", Reason="", readiness=true. Elapsed: 5.10942ms
    May 17 06:19:23.462: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-b4cv4" satisfied condition "running"
    May 17 06:19:23.462: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-dgzkc" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:19:23.468: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-dgzkc": Phase="Running", Reason="", readiness=true. Elapsed: 5.244003ms
    May 17 06:19:23.468: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-dgzkc" satisfied condition "running"
    May 17 06:19:23.468: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-l6zhb" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:19:23.483: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-l6zhb": Phase="Running", Reason="", readiness=true. Elapsed: 15.762907ms
    May 17 06:19:23.483: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-l6zhb" satisfied condition "running"
    May 17 06:19:23.483: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-wgk49" in namespace "emptydir-wrapper-957" to be "running"
    May 17 06:19:23.489: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-wgk49": Phase="Running", Reason="", readiness=true. Elapsed: 5.111706ms
    May 17 06:19:23.489: INFO: Pod "wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d-wgk49" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d in namespace emptydir-wrapper-957, will wait for the garbage collector to delete the pods 05/17/23 06:19:23.489
    May 17 06:19:23.557: INFO: Deleting ReplicationController wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d took: 9.064384ms
    May 17 06:19:23.658: INFO: Terminating ReplicationController wrapped-volume-race-dce9bb6e-5231-470d-9d85-6159b1eb380d pods took: 100.501571ms
    STEP: Cleaning up the configMaps 05/17/23 06:19:26.858
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    May 17 06:19:27.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-957" for this suite. 05/17/23 06:19:27.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:19:27.231
May 17 06:19:27.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 06:19:27.231
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:19:27.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:19:27.253
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-9f87c07f-78a8-4392-b191-939d430dd4e0 05/17/23 06:19:27.262
STEP: Creating configMap with name cm-test-opt-upd-3760ebfe-2a7f-453c-821f-9680f176cf8a 05/17/23 06:19:27.267
STEP: Creating the pod 05/17/23 06:19:27.272
May 17 06:19:27.292: INFO: Waiting up to 5m0s for pod "pod-configmaps-953149e5-ac4d-4ae0-bd91-568253dd2199" in namespace "configmap-6450" to be "running and ready"
May 17 06:19:27.296: INFO: Pod "pod-configmaps-953149e5-ac4d-4ae0-bd91-568253dd2199": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080891ms
May 17 06:19:27.296: INFO: The phase of Pod pod-configmaps-953149e5-ac4d-4ae0-bd91-568253dd2199 is Pending, waiting for it to be Running (with Ready = true)
May 17 06:19:29.302: INFO: Pod "pod-configmaps-953149e5-ac4d-4ae0-bd91-568253dd2199": Phase="Running", Reason="", readiness=true. Elapsed: 2.009758925s
May 17 06:19:29.302: INFO: The phase of Pod pod-configmaps-953149e5-ac4d-4ae0-bd91-568253dd2199 is Running (Ready = true)
May 17 06:19:29.302: INFO: Pod "pod-configmaps-953149e5-ac4d-4ae0-bd91-568253dd2199" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-9f87c07f-78a8-4392-b191-939d430dd4e0 05/17/23 06:19:29.339
STEP: Updating configmap cm-test-opt-upd-3760ebfe-2a7f-453c-821f-9680f176cf8a 05/17/23 06:19:29.346
STEP: Creating configMap with name cm-test-opt-create-25556de9-3483-4a48-bfc6-2a12f697b9b0 05/17/23 06:19:29.351
STEP: waiting to observe update in volume 05/17/23 06:19:29.356
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May 17 06:19:31.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6450" for this suite. 05/17/23 06:19:31.425
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":145,"skipped":2762,"failed":0}
------------------------------
â€¢ [4.201 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:19:27.231
    May 17 06:19:27.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 06:19:27.231
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:19:27.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:19:27.253
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-9f87c07f-78a8-4392-b191-939d430dd4e0 05/17/23 06:19:27.262
    STEP: Creating configMap with name cm-test-opt-upd-3760ebfe-2a7f-453c-821f-9680f176cf8a 05/17/23 06:19:27.267
    STEP: Creating the pod 05/17/23 06:19:27.272
    May 17 06:19:27.292: INFO: Waiting up to 5m0s for pod "pod-configmaps-953149e5-ac4d-4ae0-bd91-568253dd2199" in namespace "configmap-6450" to be "running and ready"
    May 17 06:19:27.296: INFO: Pod "pod-configmaps-953149e5-ac4d-4ae0-bd91-568253dd2199": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080891ms
    May 17 06:19:27.296: INFO: The phase of Pod pod-configmaps-953149e5-ac4d-4ae0-bd91-568253dd2199 is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:19:29.302: INFO: Pod "pod-configmaps-953149e5-ac4d-4ae0-bd91-568253dd2199": Phase="Running", Reason="", readiness=true. Elapsed: 2.009758925s
    May 17 06:19:29.302: INFO: The phase of Pod pod-configmaps-953149e5-ac4d-4ae0-bd91-568253dd2199 is Running (Ready = true)
    May 17 06:19:29.302: INFO: Pod "pod-configmaps-953149e5-ac4d-4ae0-bd91-568253dd2199" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-9f87c07f-78a8-4392-b191-939d430dd4e0 05/17/23 06:19:29.339
    STEP: Updating configmap cm-test-opt-upd-3760ebfe-2a7f-453c-821f-9680f176cf8a 05/17/23 06:19:29.346
    STEP: Creating configMap with name cm-test-opt-create-25556de9-3483-4a48-bfc6-2a12f697b9b0 05/17/23 06:19:29.351
    STEP: waiting to observe update in volume 05/17/23 06:19:29.356
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 06:19:31.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6450" for this suite. 05/17/23 06:19:31.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:19:31.433
May 17 06:19:31.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:19:31.433
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:19:31.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:19:31.451
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6527 05/17/23 06:19:31.455
STEP: changing the ExternalName service to type=NodePort 05/17/23 06:19:31.459
STEP: creating replication controller externalname-service in namespace services-6527 05/17/23 06:19:31.477
I0517 06:19:31.484529      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6527, replica count: 2
I0517 06:19:34.536833      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 06:19:34.536: INFO: Creating new exec pod
May 17 06:19:34.556: INFO: Waiting up to 5m0s for pod "execpoddvfvw" in namespace "services-6527" to be "running"
May 17 06:19:34.560: INFO: Pod "execpoddvfvw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010427ms
May 17 06:19:36.564: INFO: Pod "execpoddvfvw": Phase="Running", Reason="", readiness=true. Elapsed: 2.008063997s
May 17 06:19:36.564: INFO: Pod "execpoddvfvw" satisfied condition "running"
May 17 06:19:37.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-6527 exec execpoddvfvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 17 06:19:37.746: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 17 06:19:37.746: INFO: stdout: "externalname-service-bgncp"
May 17 06:19:37.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-6527 exec execpoddvfvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.78.143 80'
May 17 06:19:37.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.78.143 80\nConnection to 10.0.78.143 80 port [tcp/http] succeeded!\n"
May 17 06:19:37.922: INFO: stdout: "externalname-service-bgncp"
May 17 06:19:37.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-6527 exec execpoddvfvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.4 30870'
May 17 06:19:38.090: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.4 30870\nConnection to 10.224.0.4 30870 port [tcp/*] succeeded!\n"
May 17 06:19:38.090: INFO: stdout: ""
May 17 06:19:39.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-6527 exec execpoddvfvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.4 30870'
May 17 06:19:39.287: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.4 30870\nConnection to 10.224.0.4 30870 port [tcp/*] succeeded!\n"
May 17 06:19:39.287: INFO: stdout: "externalname-service-vn9km"
May 17 06:19:39.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-6527 exec execpoddvfvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.6 30870'
May 17 06:19:39.471: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.6 30870\nConnection to 10.224.0.6 30870 port [tcp/*] succeeded!\n"
May 17 06:19:39.471: INFO: stdout: "externalname-service-bgncp"
May 17 06:19:39.471: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:19:39.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6527" for this suite. 05/17/23 06:19:39.502
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":146,"skipped":2782,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.075 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:19:31.433
    May 17 06:19:31.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:19:31.433
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:19:31.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:19:31.451
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6527 05/17/23 06:19:31.455
    STEP: changing the ExternalName service to type=NodePort 05/17/23 06:19:31.459
    STEP: creating replication controller externalname-service in namespace services-6527 05/17/23 06:19:31.477
    I0517 06:19:31.484529      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6527, replica count: 2
    I0517 06:19:34.536833      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 06:19:34.536: INFO: Creating new exec pod
    May 17 06:19:34.556: INFO: Waiting up to 5m0s for pod "execpoddvfvw" in namespace "services-6527" to be "running"
    May 17 06:19:34.560: INFO: Pod "execpoddvfvw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010427ms
    May 17 06:19:36.564: INFO: Pod "execpoddvfvw": Phase="Running", Reason="", readiness=true. Elapsed: 2.008063997s
    May 17 06:19:36.564: INFO: Pod "execpoddvfvw" satisfied condition "running"
    May 17 06:19:37.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-6527 exec execpoddvfvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    May 17 06:19:37.746: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May 17 06:19:37.746: INFO: stdout: "externalname-service-bgncp"
    May 17 06:19:37.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-6527 exec execpoddvfvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.78.143 80'
    May 17 06:19:37.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.78.143 80\nConnection to 10.0.78.143 80 port [tcp/http] succeeded!\n"
    May 17 06:19:37.922: INFO: stdout: "externalname-service-bgncp"
    May 17 06:19:37.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-6527 exec execpoddvfvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.4 30870'
    May 17 06:19:38.090: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.4 30870\nConnection to 10.224.0.4 30870 port [tcp/*] succeeded!\n"
    May 17 06:19:38.090: INFO: stdout: ""
    May 17 06:19:39.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-6527 exec execpoddvfvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.4 30870'
    May 17 06:19:39.287: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.4 30870\nConnection to 10.224.0.4 30870 port [tcp/*] succeeded!\n"
    May 17 06:19:39.287: INFO: stdout: "externalname-service-vn9km"
    May 17 06:19:39.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-6527 exec execpoddvfvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.6 30870'
    May 17 06:19:39.471: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.6 30870\nConnection to 10.224.0.6 30870 port [tcp/*] succeeded!\n"
    May 17 06:19:39.471: INFO: stdout: "externalname-service-bgncp"
    May 17 06:19:39.471: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:19:39.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6527" for this suite. 05/17/23 06:19:39.502
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:19:39.508
May 17 06:19:39.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename sched-preemption 05/17/23 06:19:39.509
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:19:39.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:19:39.544
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May 17 06:19:39.563: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 06:20:39.613: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:20:39.617
May 17 06:20:39.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename sched-preemption-path 05/17/23 06:20:39.617
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:20:39.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:20:39.637
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 05/17/23 06:20:39.64
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/17/23 06:20:39.64
May 17 06:20:39.650: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8493" to be "running"
May 17 06:20:39.653: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.485344ms
May 17 06:20:41.658: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.00852247s
May 17 06:20:41.658: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/17/23 06:20:41.662
May 17 06:20:41.673: INFO: found a healthy node: aks-agentpool-72615086-vmss00000c
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
May 17 06:20:57.755: INFO: pods created so far: [1 1 1]
May 17 06:20:57.755: INFO: length of pods created so far: 3
May 17 06:20:59.766: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
May 17 06:21:06.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8493" for this suite. 05/17/23 06:21:06.774
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May 17 06:21:06.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6742" for this suite. 05/17/23 06:21:06.827
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":147,"skipped":2783,"failed":0}
------------------------------
â€¢ [SLOW TEST] [87.408 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:19:39.508
    May 17 06:19:39.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename sched-preemption 05/17/23 06:19:39.509
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:19:39.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:19:39.544
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May 17 06:19:39.563: INFO: Waiting up to 1m0s for all nodes to be ready
    May 17 06:20:39.613: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:20:39.617
    May 17 06:20:39.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename sched-preemption-path 05/17/23 06:20:39.617
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:20:39.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:20:39.637
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 05/17/23 06:20:39.64
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/17/23 06:20:39.64
    May 17 06:20:39.650: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8493" to be "running"
    May 17 06:20:39.653: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.485344ms
    May 17 06:20:41.658: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.00852247s
    May 17 06:20:41.658: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/17/23 06:20:41.662
    May 17 06:20:41.673: INFO: found a healthy node: aks-agentpool-72615086-vmss00000c
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    May 17 06:20:57.755: INFO: pods created so far: [1 1 1]
    May 17 06:20:57.755: INFO: length of pods created so far: 3
    May 17 06:20:59.766: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    May 17 06:21:06.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-8493" for this suite. 05/17/23 06:21:06.774
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May 17 06:21:06.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6742" for this suite. 05/17/23 06:21:06.827
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:21:06.917
May 17 06:21:06.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename init-container 05/17/23 06:21:06.918
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:06.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:06.939
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 05/17/23 06:21:06.943
May 17 06:21:06.943: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May 17 06:21:10.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1818" for this suite. 05/17/23 06:21:10.69
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":148,"skipped":2790,"failed":0}
------------------------------
â€¢ [3.779 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:21:06.917
    May 17 06:21:06.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename init-container 05/17/23 06:21:06.918
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:06.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:06.939
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 05/17/23 06:21:06.943
    May 17 06:21:06.943: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May 17 06:21:10.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1818" for this suite. 05/17/23 06:21:10.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:21:10.699
May 17 06:21:10.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename containers 05/17/23 06:21:10.7
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:10.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:10.717
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 05/17/23 06:21:10.72
May 17 06:21:10.729: INFO: Waiting up to 5m0s for pod "client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c" in namespace "containers-31" to be "Succeeded or Failed"
May 17 06:21:10.733: INFO: Pod "client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.590642ms
May 17 06:21:12.738: INFO: Pod "client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530854s
May 17 06:21:14.738: INFO: Pod "client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008336073s
STEP: Saw pod success 05/17/23 06:21:14.738
May 17 06:21:14.738: INFO: Pod "client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c" satisfied condition "Succeeded or Failed"
May 17 06:21:14.742: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c container agnhost-container: <nil>
STEP: delete the pod 05/17/23 06:21:14.751
May 17 06:21:14.761: INFO: Waiting for pod client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c to disappear
May 17 06:21:14.766: INFO: Pod client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May 17 06:21:14.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-31" for this suite. 05/17/23 06:21:14.772
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":149,"skipped":2866,"failed":0}
------------------------------
â€¢ [4.080 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:21:10.699
    May 17 06:21:10.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename containers 05/17/23 06:21:10.7
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:10.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:10.717
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 05/17/23 06:21:10.72
    May 17 06:21:10.729: INFO: Waiting up to 5m0s for pod "client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c" in namespace "containers-31" to be "Succeeded or Failed"
    May 17 06:21:10.733: INFO: Pod "client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.590642ms
    May 17 06:21:12.738: INFO: Pod "client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530854s
    May 17 06:21:14.738: INFO: Pod "client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008336073s
    STEP: Saw pod success 05/17/23 06:21:14.738
    May 17 06:21:14.738: INFO: Pod "client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c" satisfied condition "Succeeded or Failed"
    May 17 06:21:14.742: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 06:21:14.751
    May 17 06:21:14.761: INFO: Waiting for pod client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c to disappear
    May 17 06:21:14.766: INFO: Pod client-containers-8dfffac2-ed9e-4ede-b20e-1a7fa5b4230c no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May 17 06:21:14.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-31" for this suite. 05/17/23 06:21:14.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:21:14.78
May 17 06:21:14.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 06:21:14.781
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:14.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:14.804
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 06:21:14.819
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:21:15.119
STEP: Deploying the webhook pod 05/17/23 06:21:15.127
STEP: Wait for the deployment to be ready 05/17/23 06:21:15.138
May 17 06:21:15.146: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 06:21:17.159
STEP: Verifying the service has paired with the endpoint 05/17/23 06:21:17.169
May 17 06:21:18.169: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
May 17 06:21:18.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6945-crds.webhook.example.com via the AdmissionRegistration API 05/17/23 06:21:18.682
STEP: Creating a custom resource that should be mutated by the webhook 05/17/23 06:21:18.701
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:21:21.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2321" for this suite. 05/17/23 06:21:21.322
STEP: Destroying namespace "webhook-2321-markers" for this suite. 05/17/23 06:21:21.329
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":150,"skipped":2884,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.585 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:21:14.78
    May 17 06:21:14.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 06:21:14.781
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:14.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:14.804
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 06:21:14.819
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:21:15.119
    STEP: Deploying the webhook pod 05/17/23 06:21:15.127
    STEP: Wait for the deployment to be ready 05/17/23 06:21:15.138
    May 17 06:21:15.146: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 06:21:17.159
    STEP: Verifying the service has paired with the endpoint 05/17/23 06:21:17.169
    May 17 06:21:18.169: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    May 17 06:21:18.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6945-crds.webhook.example.com via the AdmissionRegistration API 05/17/23 06:21:18.682
    STEP: Creating a custom resource that should be mutated by the webhook 05/17/23 06:21:18.701
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:21:21.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2321" for this suite. 05/17/23 06:21:21.322
    STEP: Destroying namespace "webhook-2321-markers" for this suite. 05/17/23 06:21:21.329
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:21:21.366
May 17 06:21:21.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:21:21.367
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:21.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:21.385
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 05/17/23 06:21:21.388
May 17 06:21:21.400: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def" in namespace "projected-1480" to be "Succeeded or Failed"
May 17 06:21:21.404: INFO: Pod "downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def": Phase="Pending", Reason="", readiness=false. Elapsed: 4.451549ms
May 17 06:21:23.409: INFO: Pod "downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009046307s
May 17 06:21:25.419: INFO: Pod "downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019479454s
STEP: Saw pod success 05/17/23 06:21:25.419
May 17 06:21:25.419: INFO: Pod "downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def" satisfied condition "Succeeded or Failed"
May 17 06:21:25.423: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def container client-container: <nil>
STEP: delete the pod 05/17/23 06:21:25.432
May 17 06:21:25.444: INFO: Waiting for pod downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def to disappear
May 17 06:21:25.448: INFO: Pod downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May 17 06:21:25.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1480" for this suite. 05/17/23 06:21:25.454
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":151,"skipped":2893,"failed":0}
------------------------------
â€¢ [4.095 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:21:21.366
    May 17 06:21:21.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:21:21.367
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:21.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:21.385
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 05/17/23 06:21:21.388
    May 17 06:21:21.400: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def" in namespace "projected-1480" to be "Succeeded or Failed"
    May 17 06:21:21.404: INFO: Pod "downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def": Phase="Pending", Reason="", readiness=false. Elapsed: 4.451549ms
    May 17 06:21:23.409: INFO: Pod "downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009046307s
    May 17 06:21:25.419: INFO: Pod "downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019479454s
    STEP: Saw pod success 05/17/23 06:21:25.419
    May 17 06:21:25.419: INFO: Pod "downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def" satisfied condition "Succeeded or Failed"
    May 17 06:21:25.423: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def container client-container: <nil>
    STEP: delete the pod 05/17/23 06:21:25.432
    May 17 06:21:25.444: INFO: Waiting for pod downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def to disappear
    May 17 06:21:25.448: INFO: Pod downwardapi-volume-2c331137-6b0b-41d3-a6f0-dbb2fe158def no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May 17 06:21:25.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1480" for this suite. 05/17/23 06:21:25.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:21:25.463
May 17 06:21:25.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename deployment 05/17/23 06:21:25.464
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:25.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:25.486
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
May 17 06:21:25.490: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 17 06:21:25.499: INFO: Pod name sample-pod: Found 0 pods out of 1
May 17 06:21:30.504: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/17/23 06:21:30.504
May 17 06:21:30.504: INFO: Creating deployment "test-rolling-update-deployment"
May 17 06:21:30.509: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 17 06:21:30.516: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 17 06:21:32.525: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 17 06:21:32.528: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 06:21:32.538: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7961  90fe2a2c-2613-4740-bf6c-b3f17319acef 1464306 1 2023-05-17 06:21:30 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-17 06:21:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:21:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0079d3138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 06:21:30 +0000 UTC,LastTransitionTime:2023-05-17 06:21:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-05-17 06:21:31 +0000 UTC,LastTransitionTime:2023-05-17 06:21:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 17 06:21:32.542: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-7961  837fdaee-5911-49bc-ba57-cab9e76fe54c 1464298 1 2023-05-17 06:21:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 90fe2a2c-2613-4740-bf6c-b3f17319acef 0xc006e7b077 0xc006e7b078}] [] [{kube-controller-manager Update apps/v1 2023-05-17 06:21:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90fe2a2c-2613-4740-bf6c-b3f17319acef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:21:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e7b128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 06:21:32.542: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 17 06:21:32.542: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7961  dd94516f-14b2-40ad-8ae5-23baee8531de 1464304 2 2023-05-17 06:21:25 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 90fe2a2c-2613-4740-bf6c-b3f17319acef 0xc006e7af47 0xc006e7af48}] [] [{e2e.test Update apps/v1 2023-05-17 06:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:21:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90fe2a2c-2613-4740-bf6c-b3f17319acef\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:21:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006e7b008 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 06:21:32.546: INFO: Pod "test-rolling-update-deployment-78f575d8ff-ndprq" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-ndprq test-rolling-update-deployment-78f575d8ff- deployment-7961  22480383-5824-414a-a343-8520d9b6977f 1464297 0 2023-05-17 06:21:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 837fdaee-5911-49bc-ba57-cab9e76fe54c 0xc003ccc587 0xc003ccc588}] [] [{kube-controller-manager Update v1 2023-05-17 06:21:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837fdaee-5911-49bc-ba57-cab9e76fe54c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:21:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pmhf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pmhf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:21:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:21:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:21:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:21:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.161,StartTime:2023-05-17 06:21:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:21:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://af0abb9ed82fbc8012120d04e3ebe80b5f34b16f48ed4a46c2632654abcd9a45,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May 17 06:21:32.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7961" for this suite. 05/17/23 06:21:32.552
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":152,"skipped":2954,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.095 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:21:25.463
    May 17 06:21:25.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename deployment 05/17/23 06:21:25.464
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:25.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:25.486
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    May 17 06:21:25.490: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    May 17 06:21:25.499: INFO: Pod name sample-pod: Found 0 pods out of 1
    May 17 06:21:30.504: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/17/23 06:21:30.504
    May 17 06:21:30.504: INFO: Creating deployment "test-rolling-update-deployment"
    May 17 06:21:30.509: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    May 17 06:21:30.516: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    May 17 06:21:32.525: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    May 17 06:21:32.528: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 06:21:32.538: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7961  90fe2a2c-2613-4740-bf6c-b3f17319acef 1464306 1 2023-05-17 06:21:30 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-17 06:21:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:21:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0079d3138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 06:21:30 +0000 UTC,LastTransitionTime:2023-05-17 06:21:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-05-17 06:21:31 +0000 UTC,LastTransitionTime:2023-05-17 06:21:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 17 06:21:32.542: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-7961  837fdaee-5911-49bc-ba57-cab9e76fe54c 1464298 1 2023-05-17 06:21:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 90fe2a2c-2613-4740-bf6c-b3f17319acef 0xc006e7b077 0xc006e7b078}] [] [{kube-controller-manager Update apps/v1 2023-05-17 06:21:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90fe2a2c-2613-4740-bf6c-b3f17319acef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:21:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e7b128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 17 06:21:32.542: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    May 17 06:21:32.542: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7961  dd94516f-14b2-40ad-8ae5-23baee8531de 1464304 2 2023-05-17 06:21:25 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 90fe2a2c-2613-4740-bf6c-b3f17319acef 0xc006e7af47 0xc006e7af48}] [] [{e2e.test Update apps/v1 2023-05-17 06:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:21:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90fe2a2c-2613-4740-bf6c-b3f17319acef\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:21:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006e7b008 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 17 06:21:32.546: INFO: Pod "test-rolling-update-deployment-78f575d8ff-ndprq" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-ndprq test-rolling-update-deployment-78f575d8ff- deployment-7961  22480383-5824-414a-a343-8520d9b6977f 1464297 0 2023-05-17 06:21:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 837fdaee-5911-49bc-ba57-cab9e76fe54c 0xc003ccc587 0xc003ccc588}] [] [{kube-controller-manager Update v1 2023-05-17 06:21:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"837fdaee-5911-49bc-ba57-cab9e76fe54c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:21:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pmhf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pmhf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:21:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:21:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:21:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:21:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.161,StartTime:2023-05-17 06:21:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:21:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://af0abb9ed82fbc8012120d04e3ebe80b5f34b16f48ed4a46c2632654abcd9a45,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May 17 06:21:32.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7961" for this suite. 05/17/23 06:21:32.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:21:32.558
May 17 06:21:32.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename dns 05/17/23 06:21:32.559
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:32.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:32.579
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 05/17/23 06:21:32.584
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 202.65.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.65.202_udp@PTR;check="$$(dig +tcp +noall +answer +search 202.65.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.65.202_tcp@PTR;sleep 1; done
 05/17/23 06:21:32.6
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 202.65.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.65.202_udp@PTR;check="$$(dig +tcp +noall +answer +search 202.65.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.65.202_tcp@PTR;sleep 1; done
 05/17/23 06:21:32.6
STEP: creating a pod to probe DNS 05/17/23 06:21:32.6
STEP: submitting the pod to kubernetes 05/17/23 06:21:32.6
May 17 06:21:32.610: INFO: Waiting up to 15m0s for pod "dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43" in namespace "dns-3039" to be "running"
May 17 06:21:32.614: INFO: Pod "dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20479ms
May 17 06:21:34.619: INFO: Pod "dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43": Phase="Running", Reason="", readiness=true. Elapsed: 2.008595518s
May 17 06:21:34.619: INFO: Pod "dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43" satisfied condition "running"
STEP: retrieving the pod 05/17/23 06:21:34.619
STEP: looking for the results for each expected name from probers 05/17/23 06:21:34.623
May 17 06:21:34.631: INFO: Unable to read wheezy_udp@dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
May 17 06:21:34.637: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
May 17 06:21:34.643: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
May 17 06:21:34.648: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
May 17 06:21:34.677: INFO: Unable to read jessie_udp@dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
May 17 06:21:34.683: INFO: Unable to read jessie_tcp@dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
May 17 06:21:34.689: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
May 17 06:21:34.694: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
May 17 06:21:34.717: INFO: Lookups using dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43 failed for: [wheezy_udp@dns-test-service.dns-3039.svc.cluster.local wheezy_tcp@dns-test-service.dns-3039.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local jessie_udp@dns-test-service.dns-3039.svc.cluster.local jessie_tcp@dns-test-service.dns-3039.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local]

May 17 06:21:39.807: INFO: DNS probes using dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43 succeeded

STEP: deleting the pod 05/17/23 06:21:39.807
STEP: deleting the test service 05/17/23 06:21:39.817
STEP: deleting the test headless service 05/17/23 06:21:39.835
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May 17 06:21:39.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3039" for this suite. 05/17/23 06:21:39.861
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":153,"skipped":2959,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.310 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:21:32.558
    May 17 06:21:32.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename dns 05/17/23 06:21:32.559
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:32.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:32.579
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 05/17/23 06:21:32.584
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 202.65.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.65.202_udp@PTR;check="$$(dig +tcp +noall +answer +search 202.65.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.65.202_tcp@PTR;sleep 1; done
     05/17/23 06:21:32.6
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3039.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3039.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3039.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3039.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 202.65.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.65.202_udp@PTR;check="$$(dig +tcp +noall +answer +search 202.65.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.65.202_tcp@PTR;sleep 1; done
     05/17/23 06:21:32.6
    STEP: creating a pod to probe DNS 05/17/23 06:21:32.6
    STEP: submitting the pod to kubernetes 05/17/23 06:21:32.6
    May 17 06:21:32.610: INFO: Waiting up to 15m0s for pod "dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43" in namespace "dns-3039" to be "running"
    May 17 06:21:32.614: INFO: Pod "dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20479ms
    May 17 06:21:34.619: INFO: Pod "dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43": Phase="Running", Reason="", readiness=true. Elapsed: 2.008595518s
    May 17 06:21:34.619: INFO: Pod "dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 06:21:34.619
    STEP: looking for the results for each expected name from probers 05/17/23 06:21:34.623
    May 17 06:21:34.631: INFO: Unable to read wheezy_udp@dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
    May 17 06:21:34.637: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
    May 17 06:21:34.643: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
    May 17 06:21:34.648: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
    May 17 06:21:34.677: INFO: Unable to read jessie_udp@dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
    May 17 06:21:34.683: INFO: Unable to read jessie_tcp@dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
    May 17 06:21:34.689: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
    May 17 06:21:34.694: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local from pod dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43: the server could not find the requested resource (get pods dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43)
    May 17 06:21:34.717: INFO: Lookups using dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43 failed for: [wheezy_udp@dns-test-service.dns-3039.svc.cluster.local wheezy_tcp@dns-test-service.dns-3039.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local jessie_udp@dns-test-service.dns-3039.svc.cluster.local jessie_tcp@dns-test-service.dns-3039.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3039.svc.cluster.local]

    May 17 06:21:39.807: INFO: DNS probes using dns-3039/dns-test-c5b831eb-fc63-4401-9f8e-439186ae7d43 succeeded

    STEP: deleting the pod 05/17/23 06:21:39.807
    STEP: deleting the test service 05/17/23 06:21:39.817
    STEP: deleting the test headless service 05/17/23 06:21:39.835
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May 17 06:21:39.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3039" for this suite. 05/17/23 06:21:39.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:21:39.868
May 17 06:21:39.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 06:21:39.869
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:39.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:39.89
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 06:21:39.905
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:21:40.185
STEP: Deploying the webhook pod 05/17/23 06:21:40.19
STEP: Wait for the deployment to be ready 05/17/23 06:21:40.204
May 17 06:21:40.211: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/17/23 06:21:42.223
STEP: Verifying the service has paired with the endpoint 05/17/23 06:21:42.233
May 17 06:21:43.234: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 05/17/23 06:21:43.239
STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/17/23 06:21:43.261
STEP: Creating a configMap that should not be mutated 05/17/23 06:21:43.267
STEP: Patching a mutating webhook configuration's rules to include the create operation 05/17/23 06:21:43.277
STEP: Creating a configMap that should be mutated 05/17/23 06:21:43.284
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:21:43.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6411" for this suite. 05/17/23 06:21:43.317
STEP: Destroying namespace "webhook-6411-markers" for this suite. 05/17/23 06:21:43.323
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":154,"skipped":2966,"failed":0}
------------------------------
â€¢ [3.492 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:21:39.868
    May 17 06:21:39.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 06:21:39.869
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:39.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:39.89
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 06:21:39.905
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:21:40.185
    STEP: Deploying the webhook pod 05/17/23 06:21:40.19
    STEP: Wait for the deployment to be ready 05/17/23 06:21:40.204
    May 17 06:21:40.211: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/17/23 06:21:42.223
    STEP: Verifying the service has paired with the endpoint 05/17/23 06:21:42.233
    May 17 06:21:43.234: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 05/17/23 06:21:43.239
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/17/23 06:21:43.261
    STEP: Creating a configMap that should not be mutated 05/17/23 06:21:43.267
    STEP: Patching a mutating webhook configuration's rules to include the create operation 05/17/23 06:21:43.277
    STEP: Creating a configMap that should be mutated 05/17/23 06:21:43.284
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:21:43.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6411" for this suite. 05/17/23 06:21:43.317
    STEP: Destroying namespace "webhook-6411-markers" for this suite. 05/17/23 06:21:43.323
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:21:43.361
May 17 06:21:43.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:21:43.362
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:43.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:43.384
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:21:43.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7611" for this suite. 05/17/23 06:21:43.399
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":155,"skipped":2966,"failed":0}
------------------------------
â€¢ [0.046 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:21:43.361
    May 17 06:21:43.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:21:43.362
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:43.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:43.384
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:21:43.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7611" for this suite. 05/17/23 06:21:43.399
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:21:43.408
May 17 06:21:43.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 06:21:43.408
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:43.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:43.433
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 05/17/23 06:21:43.438
May 17 06:21:43.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54" in namespace "downward-api-9600" to be "Succeeded or Failed"
May 17 06:21:43.451: INFO: Pod "downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54": Phase="Pending", Reason="", readiness=false. Elapsed: 3.317851ms
May 17 06:21:45.456: INFO: Pod "downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008153896s
May 17 06:21:47.456: INFO: Pod "downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008337487s
STEP: Saw pod success 05/17/23 06:21:47.456
May 17 06:21:47.456: INFO: Pod "downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54" satisfied condition "Succeeded or Failed"
May 17 06:21:47.461: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54 container client-container: <nil>
STEP: delete the pod 05/17/23 06:21:47.469
May 17 06:21:47.480: INFO: Waiting for pod downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54 to disappear
May 17 06:21:47.484: INFO: Pod downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May 17 06:21:47.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9600" for this suite. 05/17/23 06:21:47.49
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":156,"skipped":2979,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:21:43.408
    May 17 06:21:43.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 06:21:43.408
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:43.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:43.433
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 05/17/23 06:21:43.438
    May 17 06:21:43.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54" in namespace "downward-api-9600" to be "Succeeded or Failed"
    May 17 06:21:43.451: INFO: Pod "downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54": Phase="Pending", Reason="", readiness=false. Elapsed: 3.317851ms
    May 17 06:21:45.456: INFO: Pod "downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008153896s
    May 17 06:21:47.456: INFO: Pod "downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008337487s
    STEP: Saw pod success 05/17/23 06:21:47.456
    May 17 06:21:47.456: INFO: Pod "downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54" satisfied condition "Succeeded or Failed"
    May 17 06:21:47.461: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54 container client-container: <nil>
    STEP: delete the pod 05/17/23 06:21:47.469
    May 17 06:21:47.480: INFO: Waiting for pod downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54 to disappear
    May 17 06:21:47.484: INFO: Pod downwardapi-volume-ce889bbc-7021-43f1-b31e-9358e9b27d54 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May 17 06:21:47.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9600" for this suite. 05/17/23 06:21:47.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:21:47.498
May 17 06:21:47.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename ephemeral-containers-test 05/17/23 06:21:47.499
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:47.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:47.53
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 05/17/23 06:21:47.533
May 17 06:21:47.544: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1171" to be "running and ready"
May 17 06:21:47.548: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.832048ms
May 17 06:21:47.548: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
May 17 06:21:49.553: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009046453s
May 17 06:21:49.553: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
May 17 06:21:49.553: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 05/17/23 06:21:49.557
May 17 06:21:49.569: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1171" to be "container debugger running"
May 17 06:21:49.573: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.426754ms
May 17 06:21:51.577: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007430545s
May 17 06:21:53.578: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008303249s
May 17 06:21:53.578: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 05/17/23 06:21:53.578
May 17 06:21:53.578: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1171 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:21:53.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:21:53.578: INFO: ExecWithOptions: Clientset creation
May 17 06:21:53.578: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/ephemeral-containers-test-1171/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
May 17 06:21:53.686: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
May 17 06:21:53.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-1171" for this suite. 05/17/23 06:21:53.701
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":157,"skipped":2987,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.209 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:21:47.498
    May 17 06:21:47.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename ephemeral-containers-test 05/17/23 06:21:47.499
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:47.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:47.53
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 05/17/23 06:21:47.533
    May 17 06:21:47.544: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1171" to be "running and ready"
    May 17 06:21:47.548: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.832048ms
    May 17 06:21:47.548: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:21:49.553: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009046453s
    May 17 06:21:49.553: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    May 17 06:21:49.553: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 05/17/23 06:21:49.557
    May 17 06:21:49.569: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1171" to be "container debugger running"
    May 17 06:21:49.573: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.426754ms
    May 17 06:21:51.577: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007430545s
    May 17 06:21:53.578: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008303249s
    May 17 06:21:53.578: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 05/17/23 06:21:53.578
    May 17 06:21:53.578: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1171 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:21:53.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:21:53.578: INFO: ExecWithOptions: Clientset creation
    May 17 06:21:53.578: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/ephemeral-containers-test-1171/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    May 17 06:21:53.686: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    May 17 06:21:53.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-1171" for this suite. 05/17/23 06:21:53.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:21:53.708
May 17 06:21:53.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 06:21:53.709
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:53.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:53.73
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-3961b7aa-20a1-491f-bdd4-2f14e1885694 05/17/23 06:21:53.733
STEP: Creating a pod to test consume configMaps 05/17/23 06:21:53.737
May 17 06:21:53.747: INFO: Waiting up to 5m0s for pod "pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105" in namespace "configmap-7453" to be "Succeeded or Failed"
May 17 06:21:53.750: INFO: Pod "pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105": Phase="Pending", Reason="", readiness=false. Elapsed: 3.310199ms
May 17 06:21:55.755: INFO: Pod "pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008128008s
May 17 06:21:57.755: INFO: Pod "pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008515718s
STEP: Saw pod success 05/17/23 06:21:57.755
May 17 06:21:57.755: INFO: Pod "pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105" satisfied condition "Succeeded or Failed"
May 17 06:21:57.759: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 06:21:57.767
May 17 06:21:57.778: INFO: Waiting for pod pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105 to disappear
May 17 06:21:57.781: INFO: Pod pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May 17 06:21:57.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7453" for this suite. 05/17/23 06:21:57.787
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":158,"skipped":3025,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:21:53.708
    May 17 06:21:53.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 06:21:53.709
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:53.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:53.73
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-3961b7aa-20a1-491f-bdd4-2f14e1885694 05/17/23 06:21:53.733
    STEP: Creating a pod to test consume configMaps 05/17/23 06:21:53.737
    May 17 06:21:53.747: INFO: Waiting up to 5m0s for pod "pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105" in namespace "configmap-7453" to be "Succeeded or Failed"
    May 17 06:21:53.750: INFO: Pod "pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105": Phase="Pending", Reason="", readiness=false. Elapsed: 3.310199ms
    May 17 06:21:55.755: INFO: Pod "pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008128008s
    May 17 06:21:57.755: INFO: Pod "pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008515718s
    STEP: Saw pod success 05/17/23 06:21:57.755
    May 17 06:21:57.755: INFO: Pod "pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105" satisfied condition "Succeeded or Failed"
    May 17 06:21:57.759: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 06:21:57.767
    May 17 06:21:57.778: INFO: Waiting for pod pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105 to disappear
    May 17 06:21:57.781: INFO: Pod pod-configmaps-d24ab69d-996e-44bf-ae93-d0f747ffd105 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 06:21:57.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7453" for this suite. 05/17/23 06:21:57.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:21:57.797
May 17 06:21:57.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename job 05/17/23 06:21:57.798
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:57.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:57.822
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 05/17/23 06:21:57.825
STEP: Ensuring job reaches completions 05/17/23 06:21:57.831
STEP: Ensuring pods with index for job exist 05/17/23 06:22:07.835
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May 17 06:22:07.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6796" for this suite. 05/17/23 06:22:07.846
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":159,"skipped":3051,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.055 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:21:57.797
    May 17 06:21:57.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename job 05/17/23 06:21:57.798
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:21:57.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:21:57.822
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 05/17/23 06:21:57.825
    STEP: Ensuring job reaches completions 05/17/23 06:21:57.831
    STEP: Ensuring pods with index for job exist 05/17/23 06:22:07.835
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May 17 06:22:07.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6796" for this suite. 05/17/23 06:22:07.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:22:07.854
May 17 06:22:07.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 06:22:07.854
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:22:07.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:22:07.876
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 05/17/23 06:22:07.88
May 17 06:22:07.889: INFO: Waiting up to 5m0s for pod "downward-api-fd165693-660f-401a-a392-393eb637f980" in namespace "downward-api-1908" to be "Succeeded or Failed"
May 17 06:22:07.893: INFO: Pod "downward-api-fd165693-660f-401a-a392-393eb637f980": Phase="Pending", Reason="", readiness=false. Elapsed: 3.23756ms
May 17 06:22:09.898: INFO: Pod "downward-api-fd165693-660f-401a-a392-393eb637f980": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008578084s
May 17 06:22:11.897: INFO: Pod "downward-api-fd165693-660f-401a-a392-393eb637f980": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007755432s
STEP: Saw pod success 05/17/23 06:22:11.897
May 17 06:22:11.897: INFO: Pod "downward-api-fd165693-660f-401a-a392-393eb637f980" satisfied condition "Succeeded or Failed"
May 17 06:22:11.902: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downward-api-fd165693-660f-401a-a392-393eb637f980 container dapi-container: <nil>
STEP: delete the pod 05/17/23 06:22:11.911
May 17 06:22:11.923: INFO: Waiting for pod downward-api-fd165693-660f-401a-a392-393eb637f980 to disappear
May 17 06:22:11.926: INFO: Pod downward-api-fd165693-660f-401a-a392-393eb637f980 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May 17 06:22:11.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1908" for this suite. 05/17/23 06:22:11.932
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":160,"skipped":3079,"failed":0}
------------------------------
â€¢ [4.084 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:22:07.854
    May 17 06:22:07.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 06:22:07.854
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:22:07.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:22:07.876
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 05/17/23 06:22:07.88
    May 17 06:22:07.889: INFO: Waiting up to 5m0s for pod "downward-api-fd165693-660f-401a-a392-393eb637f980" in namespace "downward-api-1908" to be "Succeeded or Failed"
    May 17 06:22:07.893: INFO: Pod "downward-api-fd165693-660f-401a-a392-393eb637f980": Phase="Pending", Reason="", readiness=false. Elapsed: 3.23756ms
    May 17 06:22:09.898: INFO: Pod "downward-api-fd165693-660f-401a-a392-393eb637f980": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008578084s
    May 17 06:22:11.897: INFO: Pod "downward-api-fd165693-660f-401a-a392-393eb637f980": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007755432s
    STEP: Saw pod success 05/17/23 06:22:11.897
    May 17 06:22:11.897: INFO: Pod "downward-api-fd165693-660f-401a-a392-393eb637f980" satisfied condition "Succeeded or Failed"
    May 17 06:22:11.902: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downward-api-fd165693-660f-401a-a392-393eb637f980 container dapi-container: <nil>
    STEP: delete the pod 05/17/23 06:22:11.911
    May 17 06:22:11.923: INFO: Waiting for pod downward-api-fd165693-660f-401a-a392-393eb637f980 to disappear
    May 17 06:22:11.926: INFO: Pod downward-api-fd165693-660f-401a-a392-393eb637f980 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May 17 06:22:11.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1908" for this suite. 05/17/23 06:22:11.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:22:11.94
May 17 06:22:11.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pods 05/17/23 06:22:11.94
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:22:11.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:22:11.962
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
May 17 06:22:11.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: creating the pod 05/17/23 06:22:11.965
STEP: submitting the pod to kubernetes 05/17/23 06:22:11.965
May 17 06:22:11.976: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-b8bfb81c-747b-4277-97ef-ddebe51a63fc" in namespace "pods-7212" to be "running and ready"
May 17 06:22:11.980: INFO: Pod "pod-logs-websocket-b8bfb81c-747b-4277-97ef-ddebe51a63fc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.532823ms
May 17 06:22:11.980: INFO: The phase of Pod pod-logs-websocket-b8bfb81c-747b-4277-97ef-ddebe51a63fc is Pending, waiting for it to be Running (with Ready = true)
May 17 06:22:13.985: INFO: Pod "pod-logs-websocket-b8bfb81c-747b-4277-97ef-ddebe51a63fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009028616s
May 17 06:22:13.985: INFO: The phase of Pod pod-logs-websocket-b8bfb81c-747b-4277-97ef-ddebe51a63fc is Running (Ready = true)
May 17 06:22:13.985: INFO: Pod "pod-logs-websocket-b8bfb81c-747b-4277-97ef-ddebe51a63fc" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May 17 06:22:14.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7212" for this suite. 05/17/23 06:22:14.038
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":161,"skipped":3118,"failed":0}
------------------------------
â€¢ [2.105 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:22:11.94
    May 17 06:22:11.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pods 05/17/23 06:22:11.94
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:22:11.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:22:11.962
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    May 17 06:22:11.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: creating the pod 05/17/23 06:22:11.965
    STEP: submitting the pod to kubernetes 05/17/23 06:22:11.965
    May 17 06:22:11.976: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-b8bfb81c-747b-4277-97ef-ddebe51a63fc" in namespace "pods-7212" to be "running and ready"
    May 17 06:22:11.980: INFO: Pod "pod-logs-websocket-b8bfb81c-747b-4277-97ef-ddebe51a63fc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.532823ms
    May 17 06:22:11.980: INFO: The phase of Pod pod-logs-websocket-b8bfb81c-747b-4277-97ef-ddebe51a63fc is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:22:13.985: INFO: Pod "pod-logs-websocket-b8bfb81c-747b-4277-97ef-ddebe51a63fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009028616s
    May 17 06:22:13.985: INFO: The phase of Pod pod-logs-websocket-b8bfb81c-747b-4277-97ef-ddebe51a63fc is Running (Ready = true)
    May 17 06:22:13.985: INFO: Pod "pod-logs-websocket-b8bfb81c-747b-4277-97ef-ddebe51a63fc" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May 17 06:22:14.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7212" for this suite. 05/17/23 06:22:14.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:22:14.045
May 17 06:22:14.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename svcaccounts 05/17/23 06:22:14.046
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:22:14.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:22:14.072
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
May 17 06:22:14.090: INFO: created pod
May 17 06:22:14.090: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1434" to be "Succeeded or Failed"
May 17 06:22:14.093: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383723ms
May 17 06:22:16.098: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008269557s
May 17 06:22:18.098: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008581826s
STEP: Saw pod success 05/17/23 06:22:18.098
May 17 06:22:18.098: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May 17 06:22:48.100: INFO: polling logs
May 17 06:22:48.110: INFO: Pod logs: 
I0517 06:22:14.664202       1 log.go:195] OK: Got token
I0517 06:22:14.664223       1 log.go:195] validating with in-cluster discovery
I0517 06:22:14.664438       1 log.go:195] OK: got issuer https://bh-aks-kubevirt-cluster-dns-mqlws0ya.hcp.eastus.azmk8s.io
I0517 06:22:14.664457       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://bh-aks-kubevirt-cluster-dns-mqlws0ya.hcp.eastus.azmk8s.io", Subject:"system:serviceaccount:svcaccounts-1434:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684305134, NotBefore:1684304534, IssuedAt:1684304534, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1434", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"87bcb98e-84b7-4b86-9631-6e3304108ca3"}}}
I0517 06:22:14.709361       1 log.go:195] OK: Constructed OIDC provider for issuer https://bh-aks-kubevirt-cluster-dns-mqlws0ya.hcp.eastus.azmk8s.io
I0517 06:22:14.711790       1 log.go:195] OK: Validated signature on JWT
I0517 06:22:14.711889       1 log.go:195] OK: Got valid claims from token!
I0517 06:22:14.711940       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://bh-aks-kubevirt-cluster-dns-mqlws0ya.hcp.eastus.azmk8s.io", Subject:"system:serviceaccount:svcaccounts-1434:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684305134, NotBefore:1684304534, IssuedAt:1684304534, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1434", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"87bcb98e-84b7-4b86-9631-6e3304108ca3"}}}

May 17 06:22:48.110: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May 17 06:22:48.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1434" for this suite. 05/17/23 06:22:48.127
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":162,"skipped":3128,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.088 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:22:14.045
    May 17 06:22:14.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 06:22:14.046
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:22:14.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:22:14.072
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    May 17 06:22:14.090: INFO: created pod
    May 17 06:22:14.090: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1434" to be "Succeeded or Failed"
    May 17 06:22:14.093: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383723ms
    May 17 06:22:16.098: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008269557s
    May 17 06:22:18.098: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008581826s
    STEP: Saw pod success 05/17/23 06:22:18.098
    May 17 06:22:18.098: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    May 17 06:22:48.100: INFO: polling logs
    May 17 06:22:48.110: INFO: Pod logs: 
    I0517 06:22:14.664202       1 log.go:195] OK: Got token
    I0517 06:22:14.664223       1 log.go:195] validating with in-cluster discovery
    I0517 06:22:14.664438       1 log.go:195] OK: got issuer https://bh-aks-kubevirt-cluster-dns-mqlws0ya.hcp.eastus.azmk8s.io
    I0517 06:22:14.664457       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://bh-aks-kubevirt-cluster-dns-mqlws0ya.hcp.eastus.azmk8s.io", Subject:"system:serviceaccount:svcaccounts-1434:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684305134, NotBefore:1684304534, IssuedAt:1684304534, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1434", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"87bcb98e-84b7-4b86-9631-6e3304108ca3"}}}
    I0517 06:22:14.709361       1 log.go:195] OK: Constructed OIDC provider for issuer https://bh-aks-kubevirt-cluster-dns-mqlws0ya.hcp.eastus.azmk8s.io
    I0517 06:22:14.711790       1 log.go:195] OK: Validated signature on JWT
    I0517 06:22:14.711889       1 log.go:195] OK: Got valid claims from token!
    I0517 06:22:14.711940       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://bh-aks-kubevirt-cluster-dns-mqlws0ya.hcp.eastus.azmk8s.io", Subject:"system:serviceaccount:svcaccounts-1434:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684305134, NotBefore:1684304534, IssuedAt:1684304534, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1434", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"87bcb98e-84b7-4b86-9631-6e3304108ca3"}}}

    May 17 06:22:48.110: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May 17 06:22:48.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1434" for this suite. 05/17/23 06:22:48.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:22:48.135
May 17 06:22:48.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename security-context-test 05/17/23 06:22:48.135
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:22:48.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:22:48.159
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
May 17 06:22:48.172: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c2993337-7b51-4617-a766-b6bdc9ac4acb" in namespace "security-context-test-4718" to be "Succeeded or Failed"
May 17 06:22:48.176: INFO: Pod "busybox-user-65534-c2993337-7b51-4617-a766-b6bdc9ac4acb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.695757ms
May 17 06:22:50.180: INFO: Pod "busybox-user-65534-c2993337-7b51-4617-a766-b6bdc9ac4acb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007978586s
May 17 06:22:52.180: INFO: Pod "busybox-user-65534-c2993337-7b51-4617-a766-b6bdc9ac4acb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008398033s
May 17 06:22:52.180: INFO: Pod "busybox-user-65534-c2993337-7b51-4617-a766-b6bdc9ac4acb" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May 17 06:22:52.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4718" for this suite. 05/17/23 06:22:52.186
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":163,"skipped":3144,"failed":0}
------------------------------
â€¢ [4.057 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:22:48.135
    May 17 06:22:48.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename security-context-test 05/17/23 06:22:48.135
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:22:48.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:22:48.159
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    May 17 06:22:48.172: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c2993337-7b51-4617-a766-b6bdc9ac4acb" in namespace "security-context-test-4718" to be "Succeeded or Failed"
    May 17 06:22:48.176: INFO: Pod "busybox-user-65534-c2993337-7b51-4617-a766-b6bdc9ac4acb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.695757ms
    May 17 06:22:50.180: INFO: Pod "busybox-user-65534-c2993337-7b51-4617-a766-b6bdc9ac4acb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007978586s
    May 17 06:22:52.180: INFO: Pod "busybox-user-65534-c2993337-7b51-4617-a766-b6bdc9ac4acb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008398033s
    May 17 06:22:52.180: INFO: Pod "busybox-user-65534-c2993337-7b51-4617-a766-b6bdc9ac4acb" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May 17 06:22:52.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4718" for this suite. 05/17/23 06:22:52.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:22:52.192
May 17 06:22:52.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename csistoragecapacity 05/17/23 06:22:52.193
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:22:52.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:22:52.216
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 05/17/23 06:22:52.219
STEP: getting /apis/storage.k8s.io 05/17/23 06:22:52.222
STEP: getting /apis/storage.k8s.io/v1 05/17/23 06:22:52.224
STEP: creating 05/17/23 06:22:52.226
STEP: watching 05/17/23 06:22:52.241
May 17 06:22:52.241: INFO: starting watch
STEP: getting 05/17/23 06:22:52.248
STEP: listing in namespace 05/17/23 06:22:52.251
STEP: listing across namespaces 05/17/23 06:22:52.255
STEP: patching 05/17/23 06:22:52.259
STEP: updating 05/17/23 06:22:52.263
May 17 06:22:52.268: INFO: waiting for watch events with expected annotations in namespace
May 17 06:22:52.268: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 05/17/23 06:22:52.268
STEP: deleting a collection 05/17/23 06:22:52.281
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
May 17 06:22:52.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-6801" for this suite. 05/17/23 06:22:52.297
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":164,"skipped":3150,"failed":0}
------------------------------
â€¢ [0.111 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:22:52.192
    May 17 06:22:52.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename csistoragecapacity 05/17/23 06:22:52.193
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:22:52.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:22:52.216
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 05/17/23 06:22:52.219
    STEP: getting /apis/storage.k8s.io 05/17/23 06:22:52.222
    STEP: getting /apis/storage.k8s.io/v1 05/17/23 06:22:52.224
    STEP: creating 05/17/23 06:22:52.226
    STEP: watching 05/17/23 06:22:52.241
    May 17 06:22:52.241: INFO: starting watch
    STEP: getting 05/17/23 06:22:52.248
    STEP: listing in namespace 05/17/23 06:22:52.251
    STEP: listing across namespaces 05/17/23 06:22:52.255
    STEP: patching 05/17/23 06:22:52.259
    STEP: updating 05/17/23 06:22:52.263
    May 17 06:22:52.268: INFO: waiting for watch events with expected annotations in namespace
    May 17 06:22:52.268: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 05/17/23 06:22:52.268
    STEP: deleting a collection 05/17/23 06:22:52.281
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    May 17 06:22:52.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-6801" for this suite. 05/17/23 06:22:52.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:22:52.305
May 17 06:22:52.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename crd-watch 05/17/23 06:22:52.305
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:22:52.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:22:52.327
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
May 17 06:22:52.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Creating first CR  05/17/23 06:22:54.934
May 17 06:22:54.942: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T06:22:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T06:22:54Z]] name:name1 resourceVersion:1465456 uid:4a26c82e-30b3-4fb1-ad7d-89e3e0e850a6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 05/17/23 06:23:04.945
May 17 06:23:04.954: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T06:23:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T06:23:04Z]] name:name2 resourceVersion:1465574 uid:8b3826a3-6c95-44a9-adf6-521e6001b17a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 05/17/23 06:23:14.954
May 17 06:23:14.965: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T06:22:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T06:23:14Z]] name:name1 resourceVersion:1465681 uid:4a26c82e-30b3-4fb1-ad7d-89e3e0e850a6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 05/17/23 06:23:24.967
May 17 06:23:24.975: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T06:23:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T06:23:24Z]] name:name2 resourceVersion:1465788 uid:8b3826a3-6c95-44a9-adf6-521e6001b17a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 05/17/23 06:23:34.979
May 17 06:23:34.991: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T06:22:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T06:23:14Z]] name:name1 resourceVersion:1465893 uid:4a26c82e-30b3-4fb1-ad7d-89e3e0e850a6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 05/17/23 06:23:44.991
May 17 06:23:45.004: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T06:23:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T06:23:24Z]] name:name2 resourceVersion:1466000 uid:8b3826a3-6c95-44a9-adf6-521e6001b17a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:23:55.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7431" for this suite. 05/17/23 06:23:55.527
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":165,"skipped":3181,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.231 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:22:52.305
    May 17 06:22:52.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename crd-watch 05/17/23 06:22:52.305
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:22:52.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:22:52.327
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    May 17 06:22:52.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Creating first CR  05/17/23 06:22:54.934
    May 17 06:22:54.942: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T06:22:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T06:22:54Z]] name:name1 resourceVersion:1465456 uid:4a26c82e-30b3-4fb1-ad7d-89e3e0e850a6] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 05/17/23 06:23:04.945
    May 17 06:23:04.954: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T06:23:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T06:23:04Z]] name:name2 resourceVersion:1465574 uid:8b3826a3-6c95-44a9-adf6-521e6001b17a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 05/17/23 06:23:14.954
    May 17 06:23:14.965: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T06:22:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T06:23:14Z]] name:name1 resourceVersion:1465681 uid:4a26c82e-30b3-4fb1-ad7d-89e3e0e850a6] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 05/17/23 06:23:24.967
    May 17 06:23:24.975: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T06:23:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T06:23:24Z]] name:name2 resourceVersion:1465788 uid:8b3826a3-6c95-44a9-adf6-521e6001b17a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 05/17/23 06:23:34.979
    May 17 06:23:34.991: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T06:22:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T06:23:14Z]] name:name1 resourceVersion:1465893 uid:4a26c82e-30b3-4fb1-ad7d-89e3e0e850a6] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 05/17/23 06:23:44.991
    May 17 06:23:45.004: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-17T06:23:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-17T06:23:24Z]] name:name2 resourceVersion:1466000 uid:8b3826a3-6c95-44a9-adf6-521e6001b17a] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:23:55.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-7431" for this suite. 05/17/23 06:23:55.527
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:23:55.536
May 17 06:23:55.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename security-context-test 05/17/23 06:23:55.537
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:23:55.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:23:55.557
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
May 17 06:23:55.570: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-d397bb65-5664-4cc6-91a9-3840fa87d337" in namespace "security-context-test-5589" to be "Succeeded or Failed"
May 17 06:23:55.573: INFO: Pod "busybox-privileged-false-d397bb65-5664-4cc6-91a9-3840fa87d337": Phase="Pending", Reason="", readiness=false. Elapsed: 3.187687ms
May 17 06:23:57.577: INFO: Pod "busybox-privileged-false-d397bb65-5664-4cc6-91a9-3840fa87d337": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007568315s
May 17 06:23:59.577: INFO: Pod "busybox-privileged-false-d397bb65-5664-4cc6-91a9-3840fa87d337": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007285705s
May 17 06:23:59.577: INFO: Pod "busybox-privileged-false-d397bb65-5664-4cc6-91a9-3840fa87d337" satisfied condition "Succeeded or Failed"
May 17 06:23:59.586: INFO: Got logs for pod "busybox-privileged-false-d397bb65-5664-4cc6-91a9-3840fa87d337": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May 17 06:23:59.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5589" for this suite. 05/17/23 06:23:59.592
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":166,"skipped":3183,"failed":0}
------------------------------
â€¢ [4.061 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:23:55.536
    May 17 06:23:55.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename security-context-test 05/17/23 06:23:55.537
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:23:55.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:23:55.557
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    May 17 06:23:55.570: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-d397bb65-5664-4cc6-91a9-3840fa87d337" in namespace "security-context-test-5589" to be "Succeeded or Failed"
    May 17 06:23:55.573: INFO: Pod "busybox-privileged-false-d397bb65-5664-4cc6-91a9-3840fa87d337": Phase="Pending", Reason="", readiness=false. Elapsed: 3.187687ms
    May 17 06:23:57.577: INFO: Pod "busybox-privileged-false-d397bb65-5664-4cc6-91a9-3840fa87d337": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007568315s
    May 17 06:23:59.577: INFO: Pod "busybox-privileged-false-d397bb65-5664-4cc6-91a9-3840fa87d337": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007285705s
    May 17 06:23:59.577: INFO: Pod "busybox-privileged-false-d397bb65-5664-4cc6-91a9-3840fa87d337" satisfied condition "Succeeded or Failed"
    May 17 06:23:59.586: INFO: Got logs for pod "busybox-privileged-false-d397bb65-5664-4cc6-91a9-3840fa87d337": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May 17 06:23:59.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5589" for this suite. 05/17/23 06:23:59.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:23:59.598
May 17 06:23:59.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 06:23:59.599
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:23:59.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:23:59.619
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 05/17/23 06:23:59.623
STEP: fetching the ConfigMap 05/17/23 06:23:59.627
STEP: patching the ConfigMap 05/17/23 06:23:59.63
STEP: listing all ConfigMaps in all namespaces with a label selector 05/17/23 06:23:59.635
STEP: deleting the ConfigMap by collection with a label selector 05/17/23 06:23:59.643
STEP: listing all ConfigMaps in test namespace 05/17/23 06:23:59.651
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May 17 06:23:59.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-544" for this suite. 05/17/23 06:23:59.659
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":167,"skipped":3203,"failed":0}
------------------------------
â€¢ [0.067 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:23:59.598
    May 17 06:23:59.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 06:23:59.599
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:23:59.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:23:59.619
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 05/17/23 06:23:59.623
    STEP: fetching the ConfigMap 05/17/23 06:23:59.627
    STEP: patching the ConfigMap 05/17/23 06:23:59.63
    STEP: listing all ConfigMaps in all namespaces with a label selector 05/17/23 06:23:59.635
    STEP: deleting the ConfigMap by collection with a label selector 05/17/23 06:23:59.643
    STEP: listing all ConfigMaps in test namespace 05/17/23 06:23:59.651
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 06:23:59.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-544" for this suite. 05/17/23 06:23:59.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:23:59.668
May 17 06:23:59.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 06:23:59.668
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:23:59.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:23:59.687
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/17/23 06:23:59.69
May 17 06:23:59.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:24:09.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:24:42.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9335" for this suite. 05/17/23 06:24:42.33
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":168,"skipped":3269,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.670 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:23:59.668
    May 17 06:23:59.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 06:23:59.668
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:23:59.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:23:59.687
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/17/23 06:23:59.69
    May 17 06:23:59.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:24:09.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:24:42.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9335" for this suite. 05/17/23 06:24:42.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:24:42.339
May 17 06:24:42.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename daemonsets 05/17/23 06:24:42.339
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:24:42.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:24:42.392
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
May 17 06:24:42.427: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 06:24:42.439
May 17 06:24:42.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 06:24:42.451: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 06:24:43.465: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 06:24:43.465: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 06:24:44.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 17 06:24:44.464: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 05/17/23 06:24:44.481
STEP: Check that daemon pods images are updated. 05/17/23 06:24:44.495
May 17 06:24:44.499: INFO: Wrong image for pod: daemon-set-45gtz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May 17 06:24:44.499: INFO: Wrong image for pod: daemon-set-6pt48. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May 17 06:24:44.499: INFO: Wrong image for pod: daemon-set-h5msq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May 17 06:24:45.509: INFO: Wrong image for pod: daemon-set-45gtz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May 17 06:24:45.509: INFO: Wrong image for pod: daemon-set-h5msq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May 17 06:24:46.511: INFO: Wrong image for pod: daemon-set-45gtz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May 17 06:24:46.511: INFO: Pod daemon-set-f2sjv is not available
May 17 06:24:46.511: INFO: Wrong image for pod: daemon-set-h5msq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May 17 06:24:47.511: INFO: Wrong image for pod: daemon-set-h5msq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May 17 06:24:49.511: INFO: Pod daemon-set-k8pbj is not available
STEP: Check that daemon pods are still running on every node of the cluster. 05/17/23 06:24:49.519
May 17 06:24:49.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 06:24:49.531: INFO: Node aks-agentpool-72615086-vmss00000d is running 0 daemon pod, expected 1
May 17 06:24:50.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 17 06:24:50.542: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/17/23 06:24:50.565
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1966, will wait for the garbage collector to delete the pods 05/17/23 06:24:50.565
May 17 06:24:50.631: INFO: Deleting DaemonSet.extensions daemon-set took: 10.538393ms
May 17 06:24:50.732: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.455691ms
May 17 06:24:53.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 06:24:53.437: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 17 06:24:53.442: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1466837"},"items":null}

May 17 06:24:53.447: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1466837"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May 17 06:24:53.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1966" for this suite. 05/17/23 06:24:53.482
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":169,"skipped":3281,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.151 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:24:42.339
    May 17 06:24:42.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename daemonsets 05/17/23 06:24:42.339
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:24:42.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:24:42.392
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    May 17 06:24:42.427: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 06:24:42.439
    May 17 06:24:42.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 06:24:42.451: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 06:24:43.465: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 06:24:43.465: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 06:24:44.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 17 06:24:44.464: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 05/17/23 06:24:44.481
    STEP: Check that daemon pods images are updated. 05/17/23 06:24:44.495
    May 17 06:24:44.499: INFO: Wrong image for pod: daemon-set-45gtz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May 17 06:24:44.499: INFO: Wrong image for pod: daemon-set-6pt48. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May 17 06:24:44.499: INFO: Wrong image for pod: daemon-set-h5msq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May 17 06:24:45.509: INFO: Wrong image for pod: daemon-set-45gtz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May 17 06:24:45.509: INFO: Wrong image for pod: daemon-set-h5msq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May 17 06:24:46.511: INFO: Wrong image for pod: daemon-set-45gtz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May 17 06:24:46.511: INFO: Pod daemon-set-f2sjv is not available
    May 17 06:24:46.511: INFO: Wrong image for pod: daemon-set-h5msq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May 17 06:24:47.511: INFO: Wrong image for pod: daemon-set-h5msq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May 17 06:24:49.511: INFO: Pod daemon-set-k8pbj is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 05/17/23 06:24:49.519
    May 17 06:24:49.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 06:24:49.531: INFO: Node aks-agentpool-72615086-vmss00000d is running 0 daemon pod, expected 1
    May 17 06:24:50.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 17 06:24:50.542: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/17/23 06:24:50.565
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1966, will wait for the garbage collector to delete the pods 05/17/23 06:24:50.565
    May 17 06:24:50.631: INFO: Deleting DaemonSet.extensions daemon-set took: 10.538393ms
    May 17 06:24:50.732: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.455691ms
    May 17 06:24:53.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 06:24:53.437: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 17 06:24:53.442: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1466837"},"items":null}

    May 17 06:24:53.447: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1466837"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May 17 06:24:53.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1966" for this suite. 05/17/23 06:24:53.482
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:24:53.49
May 17 06:24:53.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 06:24:53.491
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:24:53.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:24:53.509
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 05/17/23 06:24:53.514
May 17 06:24:53.529: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68" in namespace "downward-api-9604" to be "Succeeded or Failed"
May 17 06:24:53.533: INFO: Pod "downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.93246ms
May 17 06:24:55.538: INFO: Pod "downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008867018s
May 17 06:24:57.539: INFO: Pod "downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009760211s
STEP: Saw pod success 05/17/23 06:24:57.539
May 17 06:24:57.539: INFO: Pod "downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68" satisfied condition "Succeeded or Failed"
May 17 06:24:57.543: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68 container client-container: <nil>
STEP: delete the pod 05/17/23 06:24:57.553
May 17 06:24:57.565: INFO: Waiting for pod downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68 to disappear
May 17 06:24:57.569: INFO: Pod downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May 17 06:24:57.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9604" for this suite. 05/17/23 06:24:57.577
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":170,"skipped":3282,"failed":0}
------------------------------
â€¢ [4.095 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:24:53.49
    May 17 06:24:53.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 06:24:53.491
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:24:53.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:24:53.509
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 05/17/23 06:24:53.514
    May 17 06:24:53.529: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68" in namespace "downward-api-9604" to be "Succeeded or Failed"
    May 17 06:24:53.533: INFO: Pod "downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.93246ms
    May 17 06:24:55.538: INFO: Pod "downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008867018s
    May 17 06:24:57.539: INFO: Pod "downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009760211s
    STEP: Saw pod success 05/17/23 06:24:57.539
    May 17 06:24:57.539: INFO: Pod "downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68" satisfied condition "Succeeded or Failed"
    May 17 06:24:57.543: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68 container client-container: <nil>
    STEP: delete the pod 05/17/23 06:24:57.553
    May 17 06:24:57.565: INFO: Waiting for pod downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68 to disappear
    May 17 06:24:57.569: INFO: Pod downwardapi-volume-93a00295-3a61-4a93-a3b8-9f5918a19b68 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May 17 06:24:57.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9604" for this suite. 05/17/23 06:24:57.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:24:57.586
May 17 06:24:57.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:24:57.587
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:24:57.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:24:57.647
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 05/17/23 06:24:57.655
STEP: watching for the Service to be added 05/17/23 06:24:57.665
May 17 06:24:57.667: INFO: Found Service test-service-w9t7j in namespace services-4344 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May 17 06:24:57.667: INFO: Service test-service-w9t7j created
STEP: Getting /status 05/17/23 06:24:57.667
May 17 06:24:57.671: INFO: Service test-service-w9t7j has LoadBalancer: {[]}
STEP: patching the ServiceStatus 05/17/23 06:24:57.671
STEP: watching for the Service to be patched 05/17/23 06:24:57.677
May 17 06:24:57.679: INFO: observed Service test-service-w9t7j in namespace services-4344 with annotations: map[] & LoadBalancer: {[]}
May 17 06:24:57.679: INFO: Found Service test-service-w9t7j in namespace services-4344 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May 17 06:24:57.679: INFO: Service test-service-w9t7j has service status patched
STEP: updating the ServiceStatus 05/17/23 06:24:57.679
May 17 06:24:57.688: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 05/17/23 06:24:57.688
May 17 06:24:57.690: INFO: Observed Service test-service-w9t7j in namespace services-4344 with annotations: map[] & Conditions: {[]}
May 17 06:24:57.690: INFO: Observed event: &Service{ObjectMeta:{test-service-w9t7j  services-4344  94cc0bc2-f67e-486e-86da-6adc5f030e71 1466893 0 2023-05-17 06:24:57 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-17 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-17 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.0.146.253,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.0.146.253],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May 17 06:24:57.690: INFO: Found Service test-service-w9t7j in namespace services-4344 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 17 06:24:57.690: INFO: Service test-service-w9t7j has service status updated
STEP: patching the service 05/17/23 06:24:57.69
STEP: watching for the Service to be patched 05/17/23 06:24:57.703
May 17 06:24:57.705: INFO: observed Service test-service-w9t7j in namespace services-4344 with labels: map[test-service-static:true]
May 17 06:24:57.705: INFO: observed Service test-service-w9t7j in namespace services-4344 with labels: map[test-service-static:true]
May 17 06:24:57.705: INFO: observed Service test-service-w9t7j in namespace services-4344 with labels: map[test-service-static:true]
May 17 06:24:57.706: INFO: Found Service test-service-w9t7j in namespace services-4344 with labels: map[test-service:patched test-service-static:true]
May 17 06:24:57.706: INFO: Service test-service-w9t7j patched
STEP: deleting the service 05/17/23 06:24:57.706
STEP: watching for the Service to be deleted 05/17/23 06:24:57.719
May 17 06:24:57.721: INFO: Observed event: ADDED
May 17 06:24:57.721: INFO: Observed event: MODIFIED
May 17 06:24:57.721: INFO: Observed event: MODIFIED
May 17 06:24:57.721: INFO: Observed event: MODIFIED
May 17 06:24:57.721: INFO: Found Service test-service-w9t7j in namespace services-4344 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May 17 06:24:57.721: INFO: Service test-service-w9t7j deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:24:57.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4344" for this suite. 05/17/23 06:24:57.728
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":171,"skipped":3290,"failed":0}
------------------------------
â€¢ [0.148 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:24:57.586
    May 17 06:24:57.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:24:57.587
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:24:57.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:24:57.647
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 05/17/23 06:24:57.655
    STEP: watching for the Service to be added 05/17/23 06:24:57.665
    May 17 06:24:57.667: INFO: Found Service test-service-w9t7j in namespace services-4344 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    May 17 06:24:57.667: INFO: Service test-service-w9t7j created
    STEP: Getting /status 05/17/23 06:24:57.667
    May 17 06:24:57.671: INFO: Service test-service-w9t7j has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 05/17/23 06:24:57.671
    STEP: watching for the Service to be patched 05/17/23 06:24:57.677
    May 17 06:24:57.679: INFO: observed Service test-service-w9t7j in namespace services-4344 with annotations: map[] & LoadBalancer: {[]}
    May 17 06:24:57.679: INFO: Found Service test-service-w9t7j in namespace services-4344 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    May 17 06:24:57.679: INFO: Service test-service-w9t7j has service status patched
    STEP: updating the ServiceStatus 05/17/23 06:24:57.679
    May 17 06:24:57.688: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 05/17/23 06:24:57.688
    May 17 06:24:57.690: INFO: Observed Service test-service-w9t7j in namespace services-4344 with annotations: map[] & Conditions: {[]}
    May 17 06:24:57.690: INFO: Observed event: &Service{ObjectMeta:{test-service-w9t7j  services-4344  94cc0bc2-f67e-486e-86da-6adc5f030e71 1466893 0 2023-05-17 06:24:57 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-17 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-17 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.0.146.253,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.0.146.253],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    May 17 06:24:57.690: INFO: Found Service test-service-w9t7j in namespace services-4344 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May 17 06:24:57.690: INFO: Service test-service-w9t7j has service status updated
    STEP: patching the service 05/17/23 06:24:57.69
    STEP: watching for the Service to be patched 05/17/23 06:24:57.703
    May 17 06:24:57.705: INFO: observed Service test-service-w9t7j in namespace services-4344 with labels: map[test-service-static:true]
    May 17 06:24:57.705: INFO: observed Service test-service-w9t7j in namespace services-4344 with labels: map[test-service-static:true]
    May 17 06:24:57.705: INFO: observed Service test-service-w9t7j in namespace services-4344 with labels: map[test-service-static:true]
    May 17 06:24:57.706: INFO: Found Service test-service-w9t7j in namespace services-4344 with labels: map[test-service:patched test-service-static:true]
    May 17 06:24:57.706: INFO: Service test-service-w9t7j patched
    STEP: deleting the service 05/17/23 06:24:57.706
    STEP: watching for the Service to be deleted 05/17/23 06:24:57.719
    May 17 06:24:57.721: INFO: Observed event: ADDED
    May 17 06:24:57.721: INFO: Observed event: MODIFIED
    May 17 06:24:57.721: INFO: Observed event: MODIFIED
    May 17 06:24:57.721: INFO: Observed event: MODIFIED
    May 17 06:24:57.721: INFO: Found Service test-service-w9t7j in namespace services-4344 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    May 17 06:24:57.721: INFO: Service test-service-w9t7j deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:24:57.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4344" for this suite. 05/17/23 06:24:57.728
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:24:57.735
May 17 06:24:57.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pods 05/17/23 06:24:57.735
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:24:57.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:24:57.758
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 05/17/23 06:24:57.761
STEP: submitting the pod to kubernetes 05/17/23 06:24:57.761
STEP: verifying QOS class is set on the pod 05/17/23 06:24:57.773
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
May 17 06:24:57.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-652" for this suite. 05/17/23 06:24:57.794
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":172,"skipped":3304,"failed":0}
------------------------------
â€¢ [0.066 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:24:57.735
    May 17 06:24:57.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pods 05/17/23 06:24:57.735
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:24:57.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:24:57.758
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 05/17/23 06:24:57.761
    STEP: submitting the pod to kubernetes 05/17/23 06:24:57.761
    STEP: verifying QOS class is set on the pod 05/17/23 06:24:57.773
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    May 17 06:24:57.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-652" for this suite. 05/17/23 06:24:57.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:24:57.802
May 17 06:24:57.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-probe 05/17/23 06:24:57.803
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:24:57.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:24:57.824
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May 17 06:25:57.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6055" for this suite. 05/17/23 06:25:57.85
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":173,"skipped":3332,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.056 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:24:57.802
    May 17 06:24:57.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-probe 05/17/23 06:24:57.803
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:24:57.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:24:57.824
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May 17 06:25:57.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6055" for this suite. 05/17/23 06:25:57.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:25:57.859
May 17 06:25:57.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:25:57.86
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:25:57.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:25:57.88
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-b8461da6-f431-49d4-bc16-b062fbd32c71 05/17/23 06:25:57.883
STEP: Creating a pod to test consume configMaps 05/17/23 06:25:57.888
May 17 06:25:57.900: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023" in namespace "projected-6899" to be "Succeeded or Failed"
May 17 06:25:57.905: INFO: Pod "pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023": Phase="Pending", Reason="", readiness=false. Elapsed: 4.315295ms
May 17 06:25:59.911: INFO: Pod "pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010840148s
May 17 06:26:01.910: INFO: Pod "pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009466439s
STEP: Saw pod success 05/17/23 06:26:01.91
May 17 06:26:01.910: INFO: Pod "pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023" satisfied condition "Succeeded or Failed"
May 17 06:26:01.914: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 06:26:01.924
May 17 06:26:01.939: INFO: Waiting for pod pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023 to disappear
May 17 06:26:01.944: INFO: Pod pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May 17 06:26:01.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6899" for this suite. 05/17/23 06:26:01.952
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":174,"skipped":3365,"failed":0}
------------------------------
â€¢ [4.101 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:25:57.859
    May 17 06:25:57.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:25:57.86
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:25:57.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:25:57.88
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-b8461da6-f431-49d4-bc16-b062fbd32c71 05/17/23 06:25:57.883
    STEP: Creating a pod to test consume configMaps 05/17/23 06:25:57.888
    May 17 06:25:57.900: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023" in namespace "projected-6899" to be "Succeeded or Failed"
    May 17 06:25:57.905: INFO: Pod "pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023": Phase="Pending", Reason="", readiness=false. Elapsed: 4.315295ms
    May 17 06:25:59.911: INFO: Pod "pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010840148s
    May 17 06:26:01.910: INFO: Pod "pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009466439s
    STEP: Saw pod success 05/17/23 06:26:01.91
    May 17 06:26:01.910: INFO: Pod "pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023" satisfied condition "Succeeded or Failed"
    May 17 06:26:01.914: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 06:26:01.924
    May 17 06:26:01.939: INFO: Waiting for pod pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023 to disappear
    May 17 06:26:01.944: INFO: Pod pod-projected-configmaps-78e831b3-991d-471b-99b2-96a7ccc5d023 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May 17 06:26:01.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6899" for this suite. 05/17/23 06:26:01.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:01.961
May 17 06:26:01.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 06:26:01.962
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:01.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:01.985
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/17/23 06:26:02.001
May 17 06:26:02.014: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7556" to be "running and ready"
May 17 06:26:02.018: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.90497ms
May 17 06:26:02.018: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 17 06:26:04.035: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.021816248s
May 17 06:26:04.036: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May 17 06:26:04.036: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 05/17/23 06:26:04.04
May 17 06:26:04.048: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7556" to be "running and ready"
May 17 06:26:04.052: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.26417ms
May 17 06:26:04.052: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 17 06:26:06.061: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013155197s
May 17 06:26:06.061: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
May 17 06:26:06.061: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/17/23 06:26:06.067
May 17 06:26:06.075: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 17 06:26:06.082: INFO: Pod pod-with-prestop-exec-hook still exists
May 17 06:26:08.082: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 17 06:26:08.095: INFO: Pod pod-with-prestop-exec-hook still exists
May 17 06:26:10.082: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 17 06:26:10.090: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 05/17/23 06:26:10.09
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May 17 06:26:10.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7556" for this suite. 05/17/23 06:26:10.107
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":175,"skipped":3373,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.155 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:01.961
    May 17 06:26:01.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 06:26:01.962
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:01.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:01.985
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/17/23 06:26:02.001
    May 17 06:26:02.014: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7556" to be "running and ready"
    May 17 06:26:02.018: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.90497ms
    May 17 06:26:02.018: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:26:04.035: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.021816248s
    May 17 06:26:04.036: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May 17 06:26:04.036: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 05/17/23 06:26:04.04
    May 17 06:26:04.048: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7556" to be "running and ready"
    May 17 06:26:04.052: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.26417ms
    May 17 06:26:04.052: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:26:06.061: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013155197s
    May 17 06:26:06.061: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    May 17 06:26:06.061: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/17/23 06:26:06.067
    May 17 06:26:06.075: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May 17 06:26:06.082: INFO: Pod pod-with-prestop-exec-hook still exists
    May 17 06:26:08.082: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May 17 06:26:08.095: INFO: Pod pod-with-prestop-exec-hook still exists
    May 17 06:26:10.082: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May 17 06:26:10.090: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 05/17/23 06:26:10.09
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May 17 06:26:10.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7556" for this suite. 05/17/23 06:26:10.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:10.117
May 17 06:26:10.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename disruption 05/17/23 06:26:10.118
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:10.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:10.144
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:10.148
May 17 06:26:10.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename disruption-2 05/17/23 06:26:10.148
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:10.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:10.175
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 05/17/23 06:26:10.192
STEP: Waiting for the pdb to be processed 05/17/23 06:26:12.206
STEP: Waiting for the pdb to be processed 05/17/23 06:26:14.219
STEP: listing a collection of PDBs across all namespaces 05/17/23 06:26:16.228
STEP: listing a collection of PDBs in namespace disruption-867 05/17/23 06:26:16.232
STEP: deleting a collection of PDBs 05/17/23 06:26:16.235
STEP: Waiting for the PDB collection to be deleted 05/17/23 06:26:16.245
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
May 17 06:26:16.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-9886" for this suite. 05/17/23 06:26:16.256
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May 17 06:26:16.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-867" for this suite. 05/17/23 06:26:16.27
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":176,"skipped":3399,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.161 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:10.117
    May 17 06:26:10.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename disruption 05/17/23 06:26:10.118
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:10.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:10.144
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:10.148
    May 17 06:26:10.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename disruption-2 05/17/23 06:26:10.148
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:10.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:10.175
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 05/17/23 06:26:10.192
    STEP: Waiting for the pdb to be processed 05/17/23 06:26:12.206
    STEP: Waiting for the pdb to be processed 05/17/23 06:26:14.219
    STEP: listing a collection of PDBs across all namespaces 05/17/23 06:26:16.228
    STEP: listing a collection of PDBs in namespace disruption-867 05/17/23 06:26:16.232
    STEP: deleting a collection of PDBs 05/17/23 06:26:16.235
    STEP: Waiting for the PDB collection to be deleted 05/17/23 06:26:16.245
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    May 17 06:26:16.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-9886" for this suite. 05/17/23 06:26:16.256
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May 17 06:26:16.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-867" for this suite. 05/17/23 06:26:16.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:16.278
May 17 06:26:16.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 06:26:16.279
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:16.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:16.299
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 05/17/23 06:26:16.302
May 17 06:26:16.313: INFO: Waiting up to 5m0s for pod "pod-b7f642c9-df01-462d-b31b-e02581200355" in namespace "emptydir-4565" to be "Succeeded or Failed"
May 17 06:26:16.316: INFO: Pod "pod-b7f642c9-df01-462d-b31b-e02581200355": Phase="Pending", Reason="", readiness=false. Elapsed: 3.55323ms
May 17 06:26:18.322: INFO: Pod "pod-b7f642c9-df01-462d-b31b-e02581200355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009802286s
May 17 06:26:20.323: INFO: Pod "pod-b7f642c9-df01-462d-b31b-e02581200355": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010112799s
STEP: Saw pod success 05/17/23 06:26:20.323
May 17 06:26:20.323: INFO: Pod "pod-b7f642c9-df01-462d-b31b-e02581200355" satisfied condition "Succeeded or Failed"
May 17 06:26:20.327: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-b7f642c9-df01-462d-b31b-e02581200355 container test-container: <nil>
STEP: delete the pod 05/17/23 06:26:20.345
May 17 06:26:20.357: INFO: Waiting for pod pod-b7f642c9-df01-462d-b31b-e02581200355 to disappear
May 17 06:26:20.361: INFO: Pod pod-b7f642c9-df01-462d-b31b-e02581200355 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 06:26:20.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4565" for this suite. 05/17/23 06:26:20.372
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":177,"skipped":3412,"failed":0}
------------------------------
â€¢ [4.101 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:16.278
    May 17 06:26:16.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 06:26:16.279
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:16.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:16.299
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/17/23 06:26:16.302
    May 17 06:26:16.313: INFO: Waiting up to 5m0s for pod "pod-b7f642c9-df01-462d-b31b-e02581200355" in namespace "emptydir-4565" to be "Succeeded or Failed"
    May 17 06:26:16.316: INFO: Pod "pod-b7f642c9-df01-462d-b31b-e02581200355": Phase="Pending", Reason="", readiness=false. Elapsed: 3.55323ms
    May 17 06:26:18.322: INFO: Pod "pod-b7f642c9-df01-462d-b31b-e02581200355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009802286s
    May 17 06:26:20.323: INFO: Pod "pod-b7f642c9-df01-462d-b31b-e02581200355": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010112799s
    STEP: Saw pod success 05/17/23 06:26:20.323
    May 17 06:26:20.323: INFO: Pod "pod-b7f642c9-df01-462d-b31b-e02581200355" satisfied condition "Succeeded or Failed"
    May 17 06:26:20.327: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-b7f642c9-df01-462d-b31b-e02581200355 container test-container: <nil>
    STEP: delete the pod 05/17/23 06:26:20.345
    May 17 06:26:20.357: INFO: Waiting for pod pod-b7f642c9-df01-462d-b31b-e02581200355 to disappear
    May 17 06:26:20.361: INFO: Pod pod-b7f642c9-df01-462d-b31b-e02581200355 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 06:26:20.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4565" for this suite. 05/17/23 06:26:20.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:20.38
May 17 06:26:20.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubelet-test 05/17/23 06:26:20.381
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:20.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:20.404
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May 17 06:26:24.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8639" for this suite. 05/17/23 06:26:24.437
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":178,"skipped":3432,"failed":0}
------------------------------
â€¢ [4.063 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:20.38
    May 17 06:26:20.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubelet-test 05/17/23 06:26:20.381
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:20.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:20.404
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May 17 06:26:24.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8639" for this suite. 05/17/23 06:26:24.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:24.445
May 17 06:26:24.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pods 05/17/23 06:26:24.446
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:24.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:24.466
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
May 17 06:26:24.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: creating the pod 05/17/23 06:26:24.469
STEP: submitting the pod to kubernetes 05/17/23 06:26:24.469
May 17 06:26:24.483: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-dd0a183b-82c3-4c08-803e-3a8d2571c94d" in namespace "pods-1942" to be "running and ready"
May 17 06:26:24.488: INFO: Pod "pod-exec-websocket-dd0a183b-82c3-4c08-803e-3a8d2571c94d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.726752ms
May 17 06:26:24.488: INFO: The phase of Pod pod-exec-websocket-dd0a183b-82c3-4c08-803e-3a8d2571c94d is Pending, waiting for it to be Running (with Ready = true)
May 17 06:26:26.493: INFO: Pod "pod-exec-websocket-dd0a183b-82c3-4c08-803e-3a8d2571c94d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009944517s
May 17 06:26:26.493: INFO: The phase of Pod pod-exec-websocket-dd0a183b-82c3-4c08-803e-3a8d2571c94d is Running (Ready = true)
May 17 06:26:26.493: INFO: Pod "pod-exec-websocket-dd0a183b-82c3-4c08-803e-3a8d2571c94d" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May 17 06:26:26.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1942" for this suite. 05/17/23 06:26:26.607
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":179,"skipped":3473,"failed":0}
------------------------------
â€¢ [2.170 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:24.445
    May 17 06:26:24.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pods 05/17/23 06:26:24.446
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:24.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:24.466
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    May 17 06:26:24.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: creating the pod 05/17/23 06:26:24.469
    STEP: submitting the pod to kubernetes 05/17/23 06:26:24.469
    May 17 06:26:24.483: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-dd0a183b-82c3-4c08-803e-3a8d2571c94d" in namespace "pods-1942" to be "running and ready"
    May 17 06:26:24.488: INFO: Pod "pod-exec-websocket-dd0a183b-82c3-4c08-803e-3a8d2571c94d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.726752ms
    May 17 06:26:24.488: INFO: The phase of Pod pod-exec-websocket-dd0a183b-82c3-4c08-803e-3a8d2571c94d is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:26:26.493: INFO: Pod "pod-exec-websocket-dd0a183b-82c3-4c08-803e-3a8d2571c94d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009944517s
    May 17 06:26:26.493: INFO: The phase of Pod pod-exec-websocket-dd0a183b-82c3-4c08-803e-3a8d2571c94d is Running (Ready = true)
    May 17 06:26:26.493: INFO: Pod "pod-exec-websocket-dd0a183b-82c3-4c08-803e-3a8d2571c94d" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May 17 06:26:26.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1942" for this suite. 05/17/23 06:26:26.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:26.616
May 17 06:26:26.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename dns 05/17/23 06:26:26.617
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:26.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:26.671
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 05/17/23 06:26:26.675
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7269 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7269;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7269 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7269;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7269.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7269.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7269.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7269.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7269.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7269.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7269.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7269.svc;check="$$(dig +notcp +noall +answer +search 177.227.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.227.177_udp@PTR;check="$$(dig +tcp +noall +answer +search 177.227.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.227.177_tcp@PTR;sleep 1; done
 05/17/23 06:26:26.691
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7269 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7269;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7269 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7269;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7269.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7269.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7269.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7269.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7269.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7269.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7269.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7269.svc;check="$$(dig +notcp +noall +answer +search 177.227.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.227.177_udp@PTR;check="$$(dig +tcp +noall +answer +search 177.227.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.227.177_tcp@PTR;sleep 1; done
 05/17/23 06:26:26.691
STEP: creating a pod to probe DNS 05/17/23 06:26:26.691
STEP: submitting the pod to kubernetes 05/17/23 06:26:26.691
May 17 06:26:26.703: INFO: Waiting up to 15m0s for pod "dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246" in namespace "dns-7269" to be "running"
May 17 06:26:26.709: INFO: Pod "dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246": Phase="Pending", Reason="", readiness=false. Elapsed: 5.682638ms
May 17 06:26:28.715: INFO: Pod "dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246": Phase="Running", Reason="", readiness=true. Elapsed: 2.011318895s
May 17 06:26:28.715: INFO: Pod "dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246" satisfied condition "running"
STEP: retrieving the pod 05/17/23 06:26:28.715
STEP: looking for the results for each expected name from probers 05/17/23 06:26:28.719
May 17 06:26:28.731: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.739: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.747: INFO: Unable to read wheezy_udp@dns-test-service.dns-7269 from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.756: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7269 from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.774: INFO: Unable to read wheezy_udp@dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.782: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.789: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.796: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.832: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.839: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.845: INFO: Unable to read jessie_udp@dns-test-service.dns-7269 from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.851: INFO: Unable to read jessie_tcp@dns-test-service.dns-7269 from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.858: INFO: Unable to read jessie_udp@dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.869: INFO: Unable to read jessie_tcp@dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.882: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.889: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
May 17 06:26:28.921: INFO: Lookups using dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7269 wheezy_tcp@dns-test-service.dns-7269 wheezy_udp@dns-test-service.dns-7269.svc wheezy_tcp@dns-test-service.dns-7269.svc wheezy_udp@_http._tcp.dns-test-service.dns-7269.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7269.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7269 jessie_tcp@dns-test-service.dns-7269 jessie_udp@dns-test-service.dns-7269.svc jessie_tcp@dns-test-service.dns-7269.svc jessie_udp@_http._tcp.dns-test-service.dns-7269.svc jessie_tcp@_http._tcp.dns-test-service.dns-7269.svc]

May 17 06:26:34.101: INFO: DNS probes using dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246 succeeded

STEP: deleting the pod 05/17/23 06:26:34.101
STEP: deleting the test service 05/17/23 06:26:34.119
STEP: deleting the test headless service 05/17/23 06:26:34.143
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May 17 06:26:34.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7269" for this suite. 05/17/23 06:26:34.162
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":180,"skipped":3505,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.554 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:26.616
    May 17 06:26:26.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename dns 05/17/23 06:26:26.617
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:26.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:26.671
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 05/17/23 06:26:26.675
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7269 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7269;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7269 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7269;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7269.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7269.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7269.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7269.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7269.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7269.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7269.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7269.svc;check="$$(dig +notcp +noall +answer +search 177.227.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.227.177_udp@PTR;check="$$(dig +tcp +noall +answer +search 177.227.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.227.177_tcp@PTR;sleep 1; done
     05/17/23 06:26:26.691
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7269 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7269;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7269 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7269;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7269.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7269.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7269.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7269.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7269.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7269.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7269.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7269.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7269.svc;check="$$(dig +notcp +noall +answer +search 177.227.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.227.177_udp@PTR;check="$$(dig +tcp +noall +answer +search 177.227.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.227.177_tcp@PTR;sleep 1; done
     05/17/23 06:26:26.691
    STEP: creating a pod to probe DNS 05/17/23 06:26:26.691
    STEP: submitting the pod to kubernetes 05/17/23 06:26:26.691
    May 17 06:26:26.703: INFO: Waiting up to 15m0s for pod "dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246" in namespace "dns-7269" to be "running"
    May 17 06:26:26.709: INFO: Pod "dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246": Phase="Pending", Reason="", readiness=false. Elapsed: 5.682638ms
    May 17 06:26:28.715: INFO: Pod "dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246": Phase="Running", Reason="", readiness=true. Elapsed: 2.011318895s
    May 17 06:26:28.715: INFO: Pod "dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 06:26:28.715
    STEP: looking for the results for each expected name from probers 05/17/23 06:26:28.719
    May 17 06:26:28.731: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.739: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.747: INFO: Unable to read wheezy_udp@dns-test-service.dns-7269 from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.756: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7269 from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.774: INFO: Unable to read wheezy_udp@dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.782: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.789: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.796: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.832: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.839: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.845: INFO: Unable to read jessie_udp@dns-test-service.dns-7269 from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.851: INFO: Unable to read jessie_tcp@dns-test-service.dns-7269 from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.858: INFO: Unable to read jessie_udp@dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.869: INFO: Unable to read jessie_tcp@dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.882: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.889: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7269.svc from pod dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246: the server could not find the requested resource (get pods dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246)
    May 17 06:26:28.921: INFO: Lookups using dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7269 wheezy_tcp@dns-test-service.dns-7269 wheezy_udp@dns-test-service.dns-7269.svc wheezy_tcp@dns-test-service.dns-7269.svc wheezy_udp@_http._tcp.dns-test-service.dns-7269.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7269.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7269 jessie_tcp@dns-test-service.dns-7269 jessie_udp@dns-test-service.dns-7269.svc jessie_tcp@dns-test-service.dns-7269.svc jessie_udp@_http._tcp.dns-test-service.dns-7269.svc jessie_tcp@_http._tcp.dns-test-service.dns-7269.svc]

    May 17 06:26:34.101: INFO: DNS probes using dns-7269/dns-test-3ab5f1cf-b5bd-4052-974f-e1c5ad371246 succeeded

    STEP: deleting the pod 05/17/23 06:26:34.101
    STEP: deleting the test service 05/17/23 06:26:34.119
    STEP: deleting the test headless service 05/17/23 06:26:34.143
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May 17 06:26:34.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7269" for this suite. 05/17/23 06:26:34.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:34.172
May 17 06:26:34.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:26:34.173
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:34.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:34.191
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 05/17/23 06:26:34.195
May 17 06:26:34.207: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4" in namespace "projected-1111" to be "Succeeded or Failed"
May 17 06:26:34.211: INFO: Pod "downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.429827ms
May 17 06:26:36.217: INFO: Pod "downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010272914s
May 17 06:26:38.233: INFO: Pod "downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026732979s
STEP: Saw pod success 05/17/23 06:26:38.233
May 17 06:26:38.233: INFO: Pod "downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4" satisfied condition "Succeeded or Failed"
May 17 06:26:38.243: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4 container client-container: <nil>
STEP: delete the pod 05/17/23 06:26:38.269
May 17 06:26:38.297: INFO: Waiting for pod downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4 to disappear
May 17 06:26:38.305: INFO: Pod downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May 17 06:26:38.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1111" for this suite. 05/17/23 06:26:38.317
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":181,"skipped":3517,"failed":0}
------------------------------
â€¢ [4.156 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:34.172
    May 17 06:26:34.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:26:34.173
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:34.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:34.191
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 05/17/23 06:26:34.195
    May 17 06:26:34.207: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4" in namespace "projected-1111" to be "Succeeded or Failed"
    May 17 06:26:34.211: INFO: Pod "downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.429827ms
    May 17 06:26:36.217: INFO: Pod "downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010272914s
    May 17 06:26:38.233: INFO: Pod "downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026732979s
    STEP: Saw pod success 05/17/23 06:26:38.233
    May 17 06:26:38.233: INFO: Pod "downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4" satisfied condition "Succeeded or Failed"
    May 17 06:26:38.243: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4 container client-container: <nil>
    STEP: delete the pod 05/17/23 06:26:38.269
    May 17 06:26:38.297: INFO: Waiting for pod downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4 to disappear
    May 17 06:26:38.305: INFO: Pod downwardapi-volume-1e555863-3a67-4f2a-9d44-5335096658d4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May 17 06:26:38.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1111" for this suite. 05/17/23 06:26:38.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:38.328
May 17 06:26:38.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:26:38.329
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:38.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:38.357
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 05/17/23 06:26:38.361
May 17 06:26:38.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-1605 create -f -'
May 17 06:26:38.590: INFO: stderr: ""
May 17 06:26:38.590: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/17/23 06:26:38.59
May 17 06:26:39.595: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 06:26:39.595: INFO: Found 1 / 1
May 17 06:26:39.595: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 05/17/23 06:26:39.595
May 17 06:26:39.599: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 06:26:39.599: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 17 06:26:39.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-1605 patch pod agnhost-primary-tkc2b -p {"metadata":{"annotations":{"x":"y"}}}'
May 17 06:26:39.662: INFO: stderr: ""
May 17 06:26:39.662: INFO: stdout: "pod/agnhost-primary-tkc2b patched\n"
STEP: checking annotations 05/17/23 06:26:39.662
May 17 06:26:39.666: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 06:26:39.666: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:26:39.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1605" for this suite. 05/17/23 06:26:39.674
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":182,"skipped":3533,"failed":0}
------------------------------
â€¢ [1.353 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:38.328
    May 17 06:26:38.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:26:38.329
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:38.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:38.357
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 05/17/23 06:26:38.361
    May 17 06:26:38.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-1605 create -f -'
    May 17 06:26:38.590: INFO: stderr: ""
    May 17 06:26:38.590: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/17/23 06:26:38.59
    May 17 06:26:39.595: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 06:26:39.595: INFO: Found 1 / 1
    May 17 06:26:39.595: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 05/17/23 06:26:39.595
    May 17 06:26:39.599: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 06:26:39.599: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May 17 06:26:39.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-1605 patch pod agnhost-primary-tkc2b -p {"metadata":{"annotations":{"x":"y"}}}'
    May 17 06:26:39.662: INFO: stderr: ""
    May 17 06:26:39.662: INFO: stdout: "pod/agnhost-primary-tkc2b patched\n"
    STEP: checking annotations 05/17/23 06:26:39.662
    May 17 06:26:39.666: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 06:26:39.666: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:26:39.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1605" for this suite. 05/17/23 06:26:39.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:39.682
May 17 06:26:39.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename podtemplate 05/17/23 06:26:39.683
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:39.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:39.707
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 05/17/23 06:26:39.711
STEP: Replace a pod template 05/17/23 06:26:39.717
May 17 06:26:39.727: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
May 17 06:26:39.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5745" for this suite. 05/17/23 06:26:39.733
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":183,"skipped":3556,"failed":0}
------------------------------
â€¢ [0.057 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:39.682
    May 17 06:26:39.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename podtemplate 05/17/23 06:26:39.683
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:39.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:39.707
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 05/17/23 06:26:39.711
    STEP: Replace a pod template 05/17/23 06:26:39.717
    May 17 06:26:39.727: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    May 17 06:26:39.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-5745" for this suite. 05/17/23 06:26:39.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:39.74
May 17 06:26:39.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 06:26:39.741
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:39.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:39.764
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 06:26:39.79
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:26:39.995
STEP: Deploying the webhook pod 05/17/23 06:26:40.004
STEP: Wait for the deployment to be ready 05/17/23 06:26:40.018
May 17 06:26:40.031: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/17/23 06:26:42.047
STEP: Verifying the service has paired with the endpoint 05/17/23 06:26:42.059
May 17 06:26:43.059: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 05/17/23 06:26:43.063
STEP: create a pod that should be denied by the webhook 05/17/23 06:26:43.087
STEP: create a pod that causes the webhook to hang 05/17/23 06:26:43.111
STEP: create a configmap that should be denied by the webhook 05/17/23 06:26:53.123
STEP: create a configmap that should be admitted by the webhook 05/17/23 06:26:53.148
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/17/23 06:26:53.162
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/17/23 06:26:53.174
STEP: create a namespace that bypass the webhook 05/17/23 06:26:53.182
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/17/23 06:26:53.192
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:26:53.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9810" for this suite. 05/17/23 06:26:53.234
STEP: Destroying namespace "webhook-9810-markers" for this suite. 05/17/23 06:26:53.244
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":184,"skipped":3570,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.557 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:39.74
    May 17 06:26:39.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 06:26:39.741
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:39.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:39.764
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 06:26:39.79
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:26:39.995
    STEP: Deploying the webhook pod 05/17/23 06:26:40.004
    STEP: Wait for the deployment to be ready 05/17/23 06:26:40.018
    May 17 06:26:40.031: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/17/23 06:26:42.047
    STEP: Verifying the service has paired with the endpoint 05/17/23 06:26:42.059
    May 17 06:26:43.059: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 05/17/23 06:26:43.063
    STEP: create a pod that should be denied by the webhook 05/17/23 06:26:43.087
    STEP: create a pod that causes the webhook to hang 05/17/23 06:26:43.111
    STEP: create a configmap that should be denied by the webhook 05/17/23 06:26:53.123
    STEP: create a configmap that should be admitted by the webhook 05/17/23 06:26:53.148
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/17/23 06:26:53.162
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/17/23 06:26:53.174
    STEP: create a namespace that bypass the webhook 05/17/23 06:26:53.182
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/17/23 06:26:53.192
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:26:53.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9810" for this suite. 05/17/23 06:26:53.234
    STEP: Destroying namespace "webhook-9810-markers" for this suite. 05/17/23 06:26:53.244
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:53.298
May 17 06:26:53.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 06:26:53.298
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:53.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:53.322
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 05/17/23 06:26:53.331
May 17 06:26:53.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5" in namespace "downward-api-956" to be "Succeeded or Failed"
May 17 06:26:53.348: INFO: Pod "downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281121ms
May 17 06:26:55.353: INFO: Pod "downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009446191s
May 17 06:26:57.357: INFO: Pod "downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014097577s
STEP: Saw pod success 05/17/23 06:26:57.357
May 17 06:26:57.358: INFO: Pod "downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5" satisfied condition "Succeeded or Failed"
May 17 06:26:57.362: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5 container client-container: <nil>
STEP: delete the pod 05/17/23 06:26:57.371
May 17 06:26:57.385: INFO: Waiting for pod downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5 to disappear
May 17 06:26:57.389: INFO: Pod downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May 17 06:26:57.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-956" for this suite. 05/17/23 06:26:57.412
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":185,"skipped":3573,"failed":0}
------------------------------
â€¢ [4.122 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:53.298
    May 17 06:26:53.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 06:26:53.298
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:53.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:53.322
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 05/17/23 06:26:53.331
    May 17 06:26:53.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5" in namespace "downward-api-956" to be "Succeeded or Failed"
    May 17 06:26:53.348: INFO: Pod "downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281121ms
    May 17 06:26:55.353: INFO: Pod "downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009446191s
    May 17 06:26:57.357: INFO: Pod "downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014097577s
    STEP: Saw pod success 05/17/23 06:26:57.357
    May 17 06:26:57.358: INFO: Pod "downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5" satisfied condition "Succeeded or Failed"
    May 17 06:26:57.362: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5 container client-container: <nil>
    STEP: delete the pod 05/17/23 06:26:57.371
    May 17 06:26:57.385: INFO: Waiting for pod downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5 to disappear
    May 17 06:26:57.389: INFO: Pod downwardapi-volume-e6ef40a7-2200-46e0-903d-3c1c66c566d5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May 17 06:26:57.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-956" for this suite. 05/17/23 06:26:57.412
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:57.42
May 17 06:26:57.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename certificates 05/17/23 06:26:57.421
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:57.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:57.442
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 05/17/23 06:26:57.691
STEP: getting /apis/certificates.k8s.io 05/17/23 06:26:57.695
STEP: getting /apis/certificates.k8s.io/v1 05/17/23 06:26:57.696
STEP: creating 05/17/23 06:26:57.698
STEP: getting 05/17/23 06:26:57.716
STEP: listing 05/17/23 06:26:57.72
STEP: watching 05/17/23 06:26:57.725
May 17 06:26:57.725: INFO: starting watch
STEP: patching 05/17/23 06:26:57.727
STEP: updating 05/17/23 06:26:57.734
May 17 06:26:57.744: INFO: waiting for watch events with expected annotations
May 17 06:26:57.744: INFO: saw patched and updated annotations
STEP: getting /approval 05/17/23 06:26:57.744
STEP: patching /approval 05/17/23 06:26:57.75
STEP: updating /approval 05/17/23 06:26:57.757
STEP: getting /status 05/17/23 06:26:57.764
STEP: patching /status 05/17/23 06:26:57.769
STEP: updating /status 05/17/23 06:26:57.777
STEP: deleting 05/17/23 06:26:57.785
STEP: deleting a collection 05/17/23 06:26:57.8
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:26:57.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2008" for this suite. 05/17/23 06:26:57.822
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":186,"skipped":3575,"failed":0}
------------------------------
â€¢ [0.409 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:57.42
    May 17 06:26:57.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename certificates 05/17/23 06:26:57.421
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:57.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:57.442
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 05/17/23 06:26:57.691
    STEP: getting /apis/certificates.k8s.io 05/17/23 06:26:57.695
    STEP: getting /apis/certificates.k8s.io/v1 05/17/23 06:26:57.696
    STEP: creating 05/17/23 06:26:57.698
    STEP: getting 05/17/23 06:26:57.716
    STEP: listing 05/17/23 06:26:57.72
    STEP: watching 05/17/23 06:26:57.725
    May 17 06:26:57.725: INFO: starting watch
    STEP: patching 05/17/23 06:26:57.727
    STEP: updating 05/17/23 06:26:57.734
    May 17 06:26:57.744: INFO: waiting for watch events with expected annotations
    May 17 06:26:57.744: INFO: saw patched and updated annotations
    STEP: getting /approval 05/17/23 06:26:57.744
    STEP: patching /approval 05/17/23 06:26:57.75
    STEP: updating /approval 05/17/23 06:26:57.757
    STEP: getting /status 05/17/23 06:26:57.764
    STEP: patching /status 05/17/23 06:26:57.769
    STEP: updating /status 05/17/23 06:26:57.777
    STEP: deleting 05/17/23 06:26:57.785
    STEP: deleting a collection 05/17/23 06:26:57.8
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:26:57.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-2008" for this suite. 05/17/23 06:26:57.822
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:26:57.83
May 17 06:26:57.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 06:26:57.83
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:57.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:57.851
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
May 17 06:26:57.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/17/23 06:27:06.332
May 17 06:27:06.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 create -f -'
May 17 06:27:07.674: INFO: stderr: ""
May 17 06:27:07.674: INFO: stdout: "e2e-test-crd-publish-openapi-1146-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 17 06:27:07.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 delete e2e-test-crd-publish-openapi-1146-crds test-cr'
May 17 06:27:07.819: INFO: stderr: ""
May 17 06:27:07.819: INFO: stdout: "e2e-test-crd-publish-openapi-1146-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 17 06:27:07.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 apply -f -'
May 17 06:27:09.196: INFO: stderr: ""
May 17 06:27:09.196: INFO: stdout: "e2e-test-crd-publish-openapi-1146-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 17 06:27:09.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 delete e2e-test-crd-publish-openapi-1146-crds test-cr'
May 17 06:27:09.272: INFO: stderr: ""
May 17 06:27:09.272: INFO: stdout: "e2e-test-crd-publish-openapi-1146-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 05/17/23 06:27:09.272
May 17 06:27:09.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1026 explain e2e-test-crd-publish-openapi-1146-crds'
May 17 06:27:10.367: INFO: stderr: ""
May 17 06:27:10.367: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1146-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:27:19.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1026" for this suite. 05/17/23 06:27:19.47
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":187,"skipped":3576,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.647 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:26:57.83
    May 17 06:26:57.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 06:26:57.83
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:26:57.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:26:57.851
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    May 17 06:26:57.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/17/23 06:27:06.332
    May 17 06:27:06.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 create -f -'
    May 17 06:27:07.674: INFO: stderr: ""
    May 17 06:27:07.674: INFO: stdout: "e2e-test-crd-publish-openapi-1146-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May 17 06:27:07.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 delete e2e-test-crd-publish-openapi-1146-crds test-cr'
    May 17 06:27:07.819: INFO: stderr: ""
    May 17 06:27:07.819: INFO: stdout: "e2e-test-crd-publish-openapi-1146-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    May 17 06:27:07.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 apply -f -'
    May 17 06:27:09.196: INFO: stderr: ""
    May 17 06:27:09.196: INFO: stdout: "e2e-test-crd-publish-openapi-1146-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May 17 06:27:09.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 delete e2e-test-crd-publish-openapi-1146-crds test-cr'
    May 17 06:27:09.272: INFO: stderr: ""
    May 17 06:27:09.272: INFO: stdout: "e2e-test-crd-publish-openapi-1146-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 05/17/23 06:27:09.272
    May 17 06:27:09.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-1026 explain e2e-test-crd-publish-openapi-1146-crds'
    May 17 06:27:10.367: INFO: stderr: ""
    May 17 06:27:10.367: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1146-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:27:19.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1026" for this suite. 05/17/23 06:27:19.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:27:19.477
May 17 06:27:19.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename var-expansion 05/17/23 06:27:19.478
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:27:19.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:27:19.499
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 05/17/23 06:27:19.504
STEP: waiting for pod running 05/17/23 06:27:19.522
May 17 06:27:19.522: INFO: Waiting up to 2m0s for pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d" in namespace "var-expansion-4608" to be "running"
May 17 06:27:19.528: INFO: Pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059305ms
May 17 06:27:21.534: INFO: Pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012070891s
May 17 06:27:21.534: INFO: Pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d" satisfied condition "running"
STEP: creating a file in subpath 05/17/23 06:27:21.534
May 17 06:27:21.539: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4608 PodName:var-expansion-f7db78b2-283c-4566-afea-e32f8356567d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:27:21.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:27:21.540: INFO: ExecWithOptions: Clientset creation
May 17 06:27:21.540: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/var-expansion-4608/pods/var-expansion-f7db78b2-283c-4566-afea-e32f8356567d/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 05/17/23 06:27:21.67
May 17 06:27:21.674: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4608 PodName:var-expansion-f7db78b2-283c-4566-afea-e32f8356567d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:27:21.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:27:21.675: INFO: ExecWithOptions: Clientset creation
May 17 06:27:21.675: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/var-expansion-4608/pods/var-expansion-f7db78b2-283c-4566-afea-e32f8356567d/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 05/17/23 06:27:21.806
May 17 06:27:22.323: INFO: Successfully updated pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d"
STEP: waiting for annotated pod running 05/17/23 06:27:22.323
May 17 06:27:22.323: INFO: Waiting up to 2m0s for pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d" in namespace "var-expansion-4608" to be "running"
May 17 06:27:22.327: INFO: Pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d": Phase="Running", Reason="", readiness=true. Elapsed: 4.063609ms
May 17 06:27:22.327: INFO: Pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d" satisfied condition "running"
STEP: deleting the pod gracefully 05/17/23 06:27:22.327
May 17 06:27:22.327: INFO: Deleting pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d" in namespace "var-expansion-4608"
May 17 06:27:22.335: INFO: Wait up to 5m0s for pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May 17 06:27:56.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4608" for this suite. 05/17/23 06:27:56.354
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":188,"skipped":3584,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.884 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:27:19.477
    May 17 06:27:19.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename var-expansion 05/17/23 06:27:19.478
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:27:19.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:27:19.499
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 05/17/23 06:27:19.504
    STEP: waiting for pod running 05/17/23 06:27:19.522
    May 17 06:27:19.522: INFO: Waiting up to 2m0s for pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d" in namespace "var-expansion-4608" to be "running"
    May 17 06:27:19.528: INFO: Pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059305ms
    May 17 06:27:21.534: INFO: Pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012070891s
    May 17 06:27:21.534: INFO: Pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d" satisfied condition "running"
    STEP: creating a file in subpath 05/17/23 06:27:21.534
    May 17 06:27:21.539: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4608 PodName:var-expansion-f7db78b2-283c-4566-afea-e32f8356567d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:27:21.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:27:21.540: INFO: ExecWithOptions: Clientset creation
    May 17 06:27:21.540: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/var-expansion-4608/pods/var-expansion-f7db78b2-283c-4566-afea-e32f8356567d/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 05/17/23 06:27:21.67
    May 17 06:27:21.674: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4608 PodName:var-expansion-f7db78b2-283c-4566-afea-e32f8356567d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:27:21.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:27:21.675: INFO: ExecWithOptions: Clientset creation
    May 17 06:27:21.675: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/var-expansion-4608/pods/var-expansion-f7db78b2-283c-4566-afea-e32f8356567d/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 05/17/23 06:27:21.806
    May 17 06:27:22.323: INFO: Successfully updated pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d"
    STEP: waiting for annotated pod running 05/17/23 06:27:22.323
    May 17 06:27:22.323: INFO: Waiting up to 2m0s for pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d" in namespace "var-expansion-4608" to be "running"
    May 17 06:27:22.327: INFO: Pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d": Phase="Running", Reason="", readiness=true. Elapsed: 4.063609ms
    May 17 06:27:22.327: INFO: Pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d" satisfied condition "running"
    STEP: deleting the pod gracefully 05/17/23 06:27:22.327
    May 17 06:27:22.327: INFO: Deleting pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d" in namespace "var-expansion-4608"
    May 17 06:27:22.335: INFO: Wait up to 5m0s for pod "var-expansion-f7db78b2-283c-4566-afea-e32f8356567d" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May 17 06:27:56.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4608" for this suite. 05/17/23 06:27:56.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:27:56.363
May 17 06:27:56.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 06:27:56.364
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:27:56.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:27:56.384
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-dbe06498-43bd-42d2-bcb0-28c8b641e496 05/17/23 06:27:56.388
STEP: Creating a pod to test consume configMaps 05/17/23 06:27:56.394
May 17 06:27:56.405: INFO: Waiting up to 5m0s for pod "pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade" in namespace "configmap-7399" to be "Succeeded or Failed"
May 17 06:27:56.409: INFO: Pod "pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042076ms
May 17 06:27:58.414: INFO: Pod "pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009011775s
May 17 06:28:00.415: INFO: Pod "pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009884912s
STEP: Saw pod success 05/17/23 06:28:00.415
May 17 06:28:00.415: INFO: Pod "pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade" satisfied condition "Succeeded or Failed"
May 17 06:28:00.420: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade container agnhost-container: <nil>
STEP: delete the pod 05/17/23 06:28:00.431
May 17 06:28:00.445: INFO: Waiting for pod pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade to disappear
May 17 06:28:00.449: INFO: Pod pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May 17 06:28:00.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7399" for this suite. 05/17/23 06:28:00.456
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":189,"skipped":3634,"failed":0}
------------------------------
â€¢ [4.101 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:27:56.363
    May 17 06:27:56.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 06:27:56.364
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:27:56.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:27:56.384
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-dbe06498-43bd-42d2-bcb0-28c8b641e496 05/17/23 06:27:56.388
    STEP: Creating a pod to test consume configMaps 05/17/23 06:27:56.394
    May 17 06:27:56.405: INFO: Waiting up to 5m0s for pod "pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade" in namespace "configmap-7399" to be "Succeeded or Failed"
    May 17 06:27:56.409: INFO: Pod "pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042076ms
    May 17 06:27:58.414: INFO: Pod "pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009011775s
    May 17 06:28:00.415: INFO: Pod "pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009884912s
    STEP: Saw pod success 05/17/23 06:28:00.415
    May 17 06:28:00.415: INFO: Pod "pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade" satisfied condition "Succeeded or Failed"
    May 17 06:28:00.420: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 06:28:00.431
    May 17 06:28:00.445: INFO: Waiting for pod pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade to disappear
    May 17 06:28:00.449: INFO: Pod pod-configmaps-fd21aaa8-a9ad-457b-9234-5223ac381ade no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 06:28:00.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7399" for this suite. 05/17/23 06:28:00.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:28:00.464
May 17 06:28:00.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 06:28:00.465
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:00.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:00.485
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-e66f1627-d820-4ba9-9b17-d1e0252b19a8 05/17/23 06:28:00.498
STEP: Creating the pod 05/17/23 06:28:00.506
May 17 06:28:00.517: INFO: Waiting up to 5m0s for pod "pod-configmaps-810623e8-ca93-4136-9b0f-7f720b33d58b" in namespace "configmap-996" to be "running and ready"
May 17 06:28:00.522: INFO: Pod "pod-configmaps-810623e8-ca93-4136-9b0f-7f720b33d58b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.408922ms
May 17 06:28:00.522: INFO: The phase of Pod pod-configmaps-810623e8-ca93-4136-9b0f-7f720b33d58b is Pending, waiting for it to be Running (with Ready = true)
May 17 06:28:02.527: INFO: Pod "pod-configmaps-810623e8-ca93-4136-9b0f-7f720b33d58b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009748532s
May 17 06:28:02.527: INFO: The phase of Pod pod-configmaps-810623e8-ca93-4136-9b0f-7f720b33d58b is Running (Ready = true)
May 17 06:28:02.527: INFO: Pod "pod-configmaps-810623e8-ca93-4136-9b0f-7f720b33d58b" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-e66f1627-d820-4ba9-9b17-d1e0252b19a8 05/17/23 06:28:02.543
STEP: waiting to observe update in volume 05/17/23 06:28:02.549
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May 17 06:28:04.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-996" for this suite. 05/17/23 06:28:04.578
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":190,"skipped":3639,"failed":0}
------------------------------
â€¢ [4.121 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:28:00.464
    May 17 06:28:00.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 06:28:00.465
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:00.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:00.485
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-e66f1627-d820-4ba9-9b17-d1e0252b19a8 05/17/23 06:28:00.498
    STEP: Creating the pod 05/17/23 06:28:00.506
    May 17 06:28:00.517: INFO: Waiting up to 5m0s for pod "pod-configmaps-810623e8-ca93-4136-9b0f-7f720b33d58b" in namespace "configmap-996" to be "running and ready"
    May 17 06:28:00.522: INFO: Pod "pod-configmaps-810623e8-ca93-4136-9b0f-7f720b33d58b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.408922ms
    May 17 06:28:00.522: INFO: The phase of Pod pod-configmaps-810623e8-ca93-4136-9b0f-7f720b33d58b is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:28:02.527: INFO: Pod "pod-configmaps-810623e8-ca93-4136-9b0f-7f720b33d58b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009748532s
    May 17 06:28:02.527: INFO: The phase of Pod pod-configmaps-810623e8-ca93-4136-9b0f-7f720b33d58b is Running (Ready = true)
    May 17 06:28:02.527: INFO: Pod "pod-configmaps-810623e8-ca93-4136-9b0f-7f720b33d58b" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-e66f1627-d820-4ba9-9b17-d1e0252b19a8 05/17/23 06:28:02.543
    STEP: waiting to observe update in volume 05/17/23 06:28:02.549
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 06:28:04.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-996" for this suite. 05/17/23 06:28:04.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:28:04.587
May 17 06:28:04.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pod-network-test 05/17/23 06:28:04.588
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:04.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:04.608
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-6726 05/17/23 06:28:04.612
STEP: creating a selector 05/17/23 06:28:04.612
STEP: Creating the service pods in kubernetes 05/17/23 06:28:04.612
May 17 06:28:04.612: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 17 06:28:04.652: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6726" to be "running and ready"
May 17 06:28:04.657: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359581ms
May 17 06:28:04.657: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 06:28:06.661: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00866887s
May 17 06:28:06.661: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:08.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009791427s
May 17 06:28:08.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:10.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009722566s
May 17 06:28:10.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:12.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01143393s
May 17 06:28:12.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:14.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010042484s
May 17 06:28:14.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:16.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009569608s
May 17 06:28:16.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:18.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01004665s
May 17 06:28:18.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:20.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010419387s
May 17 06:28:20.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:22.661: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009090826s
May 17 06:28:22.661: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:24.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011472535s
May 17 06:28:24.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:26.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009870974s
May 17 06:28:26.662: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May 17 06:28:26.662: INFO: Pod "netserver-0" satisfied condition "running and ready"
May 17 06:28:26.667: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6726" to be "running and ready"
May 17 06:28:26.671: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.644155ms
May 17 06:28:26.671: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May 17 06:28:26.671: INFO: Pod "netserver-1" satisfied condition "running and ready"
May 17 06:28:26.676: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6726" to be "running and ready"
May 17 06:28:26.680: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.392955ms
May 17 06:28:26.680: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May 17 06:28:26.680: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/17/23 06:28:26.689
May 17 06:28:26.706: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6726" to be "running"
May 17 06:28:26.712: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.63271ms
May 17 06:28:28.719: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01227296s
May 17 06:28:28.719: INFO: Pod "test-container-pod" satisfied condition "running"
May 17 06:28:28.723: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6726" to be "running"
May 17 06:28:28.727: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.386352ms
May 17 06:28:28.727: INFO: Pod "host-test-container-pod" satisfied condition "running"
May 17 06:28:28.731: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 17 06:28:28.731: INFO: Going to poll 10.244.0.69 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 17 06:28:28.735: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.69:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6726 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:28:28.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:28:28.736: INFO: ExecWithOptions: Clientset creation
May 17 06:28:28.736: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6726/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.69%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 17 06:28:28.862: INFO: Found all 1 expected endpoints: [netserver-0]
May 17 06:28:28.862: INFO: Going to poll 10.244.2.193 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 17 06:28:28.866: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.193:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6726 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:28:28.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:28:28.867: INFO: ExecWithOptions: Clientset creation
May 17 06:28:28.867: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6726/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.193%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 17 06:28:28.973: INFO: Found all 1 expected endpoints: [netserver-1]
May 17 06:28:28.973: INFO: Going to poll 10.244.1.57 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 17 06:28:28.978: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.57:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6726 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:28:28.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:28:28.979: INFO: ExecWithOptions: Clientset creation
May 17 06:28:28.979: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6726/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.57%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 17 06:28:29.116: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May 17 06:28:29.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6726" for this suite. 05/17/23 06:28:29.124
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":191,"skipped":3699,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.544 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:28:04.587
    May 17 06:28:04.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pod-network-test 05/17/23 06:28:04.588
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:04.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:04.608
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-6726 05/17/23 06:28:04.612
    STEP: creating a selector 05/17/23 06:28:04.612
    STEP: Creating the service pods in kubernetes 05/17/23 06:28:04.612
    May 17 06:28:04.612: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May 17 06:28:04.652: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6726" to be "running and ready"
    May 17 06:28:04.657: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359581ms
    May 17 06:28:04.657: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:28:06.661: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00866887s
    May 17 06:28:06.661: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:08.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009791427s
    May 17 06:28:08.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:10.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009722566s
    May 17 06:28:10.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:12.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01143393s
    May 17 06:28:12.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:14.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010042484s
    May 17 06:28:14.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:16.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009569608s
    May 17 06:28:16.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:18.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01004665s
    May 17 06:28:18.662: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:20.663: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010419387s
    May 17 06:28:20.663: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:22.661: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009090826s
    May 17 06:28:22.661: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:24.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011472535s
    May 17 06:28:24.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:26.662: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009870974s
    May 17 06:28:26.662: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May 17 06:28:26.662: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May 17 06:28:26.667: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6726" to be "running and ready"
    May 17 06:28:26.671: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.644155ms
    May 17 06:28:26.671: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May 17 06:28:26.671: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May 17 06:28:26.676: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6726" to be "running and ready"
    May 17 06:28:26.680: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.392955ms
    May 17 06:28:26.680: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May 17 06:28:26.680: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/17/23 06:28:26.689
    May 17 06:28:26.706: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6726" to be "running"
    May 17 06:28:26.712: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.63271ms
    May 17 06:28:28.719: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01227296s
    May 17 06:28:28.719: INFO: Pod "test-container-pod" satisfied condition "running"
    May 17 06:28:28.723: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6726" to be "running"
    May 17 06:28:28.727: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.386352ms
    May 17 06:28:28.727: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May 17 06:28:28.731: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May 17 06:28:28.731: INFO: Going to poll 10.244.0.69 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May 17 06:28:28.735: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.69:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6726 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:28:28.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:28:28.736: INFO: ExecWithOptions: Clientset creation
    May 17 06:28:28.736: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6726/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.69%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 17 06:28:28.862: INFO: Found all 1 expected endpoints: [netserver-0]
    May 17 06:28:28.862: INFO: Going to poll 10.244.2.193 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May 17 06:28:28.866: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.193:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6726 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:28:28.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:28:28.867: INFO: ExecWithOptions: Clientset creation
    May 17 06:28:28.867: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6726/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.193%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 17 06:28:28.973: INFO: Found all 1 expected endpoints: [netserver-1]
    May 17 06:28:28.973: INFO: Going to poll 10.244.1.57 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May 17 06:28:28.978: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.57:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6726 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:28:28.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:28:28.979: INFO: ExecWithOptions: Clientset creation
    May 17 06:28:28.979: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6726/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.57%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 17 06:28:29.116: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May 17 06:28:29.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6726" for this suite. 05/17/23 06:28:29.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:28:29.132
May 17 06:28:29.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pod-network-test 05/17/23 06:28:29.133
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:29.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:29.152
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6599 05/17/23 06:28:29.155
STEP: creating a selector 05/17/23 06:28:29.155
STEP: Creating the service pods in kubernetes 05/17/23 06:28:29.156
May 17 06:28:29.156: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 17 06:28:29.196: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6599" to be "running and ready"
May 17 06:28:29.201: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.510575ms
May 17 06:28:29.201: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 06:28:31.206: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009822442s
May 17 06:28:31.206: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:33.206: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00921859s
May 17 06:28:33.206: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:35.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012490571s
May 17 06:28:35.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:37.207: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010303194s
May 17 06:28:37.207: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:39.206: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009701415s
May 17 06:28:39.206: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:28:41.206: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.00924001s
May 17 06:28:41.206: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May 17 06:28:41.206: INFO: Pod "netserver-0" satisfied condition "running and ready"
May 17 06:28:41.210: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6599" to be "running and ready"
May 17 06:28:41.216: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.057388ms
May 17 06:28:41.216: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May 17 06:28:41.216: INFO: Pod "netserver-1" satisfied condition "running and ready"
May 17 06:28:41.220: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6599" to be "running and ready"
May 17 06:28:41.224: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.26772ms
May 17 06:28:41.224: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May 17 06:28:41.224: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/17/23 06:28:41.228
May 17 06:28:41.236: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6599" to be "running"
May 17 06:28:41.239: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.412883ms
May 17 06:28:43.246: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009945835s
May 17 06:28:43.246: INFO: Pod "test-container-pod" satisfied condition "running"
May 17 06:28:43.250: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 17 06:28:43.250: INFO: Breadth first check of 10.244.0.70 on host 10.224.0.4...
May 17 06:28:43.253: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.196:9080/dial?request=hostname&protocol=udp&host=10.244.0.70&port=8081&tries=1'] Namespace:pod-network-test-6599 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:28:43.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:28:43.254: INFO: ExecWithOptions: Clientset creation
May 17 06:28:43.254: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6599/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.70%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 17 06:28:43.378: INFO: Waiting for responses: map[]
May 17 06:28:43.378: INFO: reached 10.244.0.70 after 0/1 tries
May 17 06:28:43.378: INFO: Breadth first check of 10.244.2.195 on host 10.224.0.6...
May 17 06:28:43.382: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.196:9080/dial?request=hostname&protocol=udp&host=10.244.2.195&port=8081&tries=1'] Namespace:pod-network-test-6599 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:28:43.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:28:43.382: INFO: ExecWithOptions: Clientset creation
May 17 06:28:43.383: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6599/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.195%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 17 06:28:43.496: INFO: Waiting for responses: map[]
May 17 06:28:43.496: INFO: reached 10.244.2.195 after 0/1 tries
May 17 06:28:43.496: INFO: Breadth first check of 10.244.1.58 on host 10.224.0.5...
May 17 06:28:43.501: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.196:9080/dial?request=hostname&protocol=udp&host=10.244.1.58&port=8081&tries=1'] Namespace:pod-network-test-6599 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:28:43.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:28:43.501: INFO: ExecWithOptions: Clientset creation
May 17 06:28:43.501: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6599/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 17 06:28:43.630: INFO: Waiting for responses: map[]
May 17 06:28:43.630: INFO: reached 10.244.1.58 after 0/1 tries
May 17 06:28:43.630: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May 17 06:28:43.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6599" for this suite. 05/17/23 06:28:43.637
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":192,"skipped":3732,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.513 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:28:29.132
    May 17 06:28:29.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pod-network-test 05/17/23 06:28:29.133
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:29.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:29.152
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6599 05/17/23 06:28:29.155
    STEP: creating a selector 05/17/23 06:28:29.155
    STEP: Creating the service pods in kubernetes 05/17/23 06:28:29.156
    May 17 06:28:29.156: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May 17 06:28:29.196: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6599" to be "running and ready"
    May 17 06:28:29.201: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.510575ms
    May 17 06:28:29.201: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:28:31.206: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009822442s
    May 17 06:28:31.206: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:33.206: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00921859s
    May 17 06:28:33.206: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:35.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012490571s
    May 17 06:28:35.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:37.207: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010303194s
    May 17 06:28:37.207: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:39.206: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009701415s
    May 17 06:28:39.206: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:28:41.206: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.00924001s
    May 17 06:28:41.206: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May 17 06:28:41.206: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May 17 06:28:41.210: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6599" to be "running and ready"
    May 17 06:28:41.216: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.057388ms
    May 17 06:28:41.216: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May 17 06:28:41.216: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May 17 06:28:41.220: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6599" to be "running and ready"
    May 17 06:28:41.224: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.26772ms
    May 17 06:28:41.224: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May 17 06:28:41.224: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/17/23 06:28:41.228
    May 17 06:28:41.236: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6599" to be "running"
    May 17 06:28:41.239: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.412883ms
    May 17 06:28:43.246: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009945835s
    May 17 06:28:43.246: INFO: Pod "test-container-pod" satisfied condition "running"
    May 17 06:28:43.250: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May 17 06:28:43.250: INFO: Breadth first check of 10.244.0.70 on host 10.224.0.4...
    May 17 06:28:43.253: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.196:9080/dial?request=hostname&protocol=udp&host=10.244.0.70&port=8081&tries=1'] Namespace:pod-network-test-6599 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:28:43.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:28:43.254: INFO: ExecWithOptions: Clientset creation
    May 17 06:28:43.254: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6599/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.70%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 17 06:28:43.378: INFO: Waiting for responses: map[]
    May 17 06:28:43.378: INFO: reached 10.244.0.70 after 0/1 tries
    May 17 06:28:43.378: INFO: Breadth first check of 10.244.2.195 on host 10.224.0.6...
    May 17 06:28:43.382: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.196:9080/dial?request=hostname&protocol=udp&host=10.244.2.195&port=8081&tries=1'] Namespace:pod-network-test-6599 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:28:43.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:28:43.382: INFO: ExecWithOptions: Clientset creation
    May 17 06:28:43.383: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6599/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.195%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 17 06:28:43.496: INFO: Waiting for responses: map[]
    May 17 06:28:43.496: INFO: reached 10.244.2.195 after 0/1 tries
    May 17 06:28:43.496: INFO: Breadth first check of 10.244.1.58 on host 10.224.0.5...
    May 17 06:28:43.501: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.196:9080/dial?request=hostname&protocol=udp&host=10.244.1.58&port=8081&tries=1'] Namespace:pod-network-test-6599 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:28:43.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:28:43.501: INFO: ExecWithOptions: Clientset creation
    May 17 06:28:43.501: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6599/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.196%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 17 06:28:43.630: INFO: Waiting for responses: map[]
    May 17 06:28:43.630: INFO: reached 10.244.1.58 after 0/1 tries
    May 17 06:28:43.630: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May 17 06:28:43.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6599" for this suite. 05/17/23 06:28:43.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:28:43.647
May 17 06:28:43.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 06:28:43.648
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:43.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:43.67
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/17/23 06:28:43.674
May 17 06:28:43.684: INFO: Waiting up to 5m0s for pod "pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07" in namespace "emptydir-2998" to be "Succeeded or Failed"
May 17 06:28:43.688: INFO: Pod "pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.769932ms
May 17 06:28:45.694: INFO: Pod "pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009901908s
May 17 06:28:47.694: INFO: Pod "pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009722111s
STEP: Saw pod success 05/17/23 06:28:47.694
May 17 06:28:47.694: INFO: Pod "pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07" satisfied condition "Succeeded or Failed"
May 17 06:28:47.698: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07 container test-container: <nil>
STEP: delete the pod 05/17/23 06:28:47.708
May 17 06:28:47.721: INFO: Waiting for pod pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07 to disappear
May 17 06:28:47.724: INFO: Pod pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 06:28:47.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2998" for this suite. 05/17/23 06:28:47.732
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":193,"skipped":3784,"failed":0}
------------------------------
â€¢ [4.091 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:28:43.647
    May 17 06:28:43.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 06:28:43.648
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:43.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:43.67
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/17/23 06:28:43.674
    May 17 06:28:43.684: INFO: Waiting up to 5m0s for pod "pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07" in namespace "emptydir-2998" to be "Succeeded or Failed"
    May 17 06:28:43.688: INFO: Pod "pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.769932ms
    May 17 06:28:45.694: INFO: Pod "pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009901908s
    May 17 06:28:47.694: INFO: Pod "pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009722111s
    STEP: Saw pod success 05/17/23 06:28:47.694
    May 17 06:28:47.694: INFO: Pod "pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07" satisfied condition "Succeeded or Failed"
    May 17 06:28:47.698: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07 container test-container: <nil>
    STEP: delete the pod 05/17/23 06:28:47.708
    May 17 06:28:47.721: INFO: Waiting for pod pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07 to disappear
    May 17 06:28:47.724: INFO: Pod pod-c72e96cb-7754-4fa8-a0d8-ea21551f0e07 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 06:28:47.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2998" for this suite. 05/17/23 06:28:47.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:28:47.741
May 17 06:28:47.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pods 05/17/23 06:28:47.741
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:47.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:47.757
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 05/17/23 06:28:47.76
STEP: submitting the pod to kubernetes 05/17/23 06:28:47.76
May 17 06:28:47.774: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf" in namespace "pods-4903" to be "running and ready"
May 17 06:28:47.778: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.233678ms
May 17 06:28:47.778: INFO: The phase of Pod pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf is Pending, waiting for it to be Running (with Ready = true)
May 17 06:28:49.784: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009823518s
May 17 06:28:49.784: INFO: The phase of Pod pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf is Running (Ready = true)
May 17 06:28:49.784: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/17/23 06:28:49.787
STEP: updating the pod 05/17/23 06:28:49.792
May 17 06:28:50.308: INFO: Successfully updated pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf"
May 17 06:28:50.308: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf" in namespace "pods-4903" to be "terminated with reason DeadlineExceeded"
May 17 06:28:50.312: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf": Phase="Running", Reason="", readiness=true. Elapsed: 3.943794ms
May 17 06:28:52.319: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.011470665s
May 17 06:28:54.317: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf": Phase="Running", Reason="", readiness=false. Elapsed: 4.008780672s
May 17 06:28:56.318: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.010472258s
May 17 06:28:56.318: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May 17 06:28:56.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4903" for this suite. 05/17/23 06:28:56.326
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":194,"skipped":3841,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.593 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:28:47.741
    May 17 06:28:47.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pods 05/17/23 06:28:47.741
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:47.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:47.757
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 05/17/23 06:28:47.76
    STEP: submitting the pod to kubernetes 05/17/23 06:28:47.76
    May 17 06:28:47.774: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf" in namespace "pods-4903" to be "running and ready"
    May 17 06:28:47.778: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.233678ms
    May 17 06:28:47.778: INFO: The phase of Pod pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:28:49.784: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009823518s
    May 17 06:28:49.784: INFO: The phase of Pod pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf is Running (Ready = true)
    May 17 06:28:49.784: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/17/23 06:28:49.787
    STEP: updating the pod 05/17/23 06:28:49.792
    May 17 06:28:50.308: INFO: Successfully updated pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf"
    May 17 06:28:50.308: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf" in namespace "pods-4903" to be "terminated with reason DeadlineExceeded"
    May 17 06:28:50.312: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf": Phase="Running", Reason="", readiness=true. Elapsed: 3.943794ms
    May 17 06:28:52.319: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.011470665s
    May 17 06:28:54.317: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf": Phase="Running", Reason="", readiness=false. Elapsed: 4.008780672s
    May 17 06:28:56.318: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.010472258s
    May 17 06:28:56.318: INFO: Pod "pod-update-activedeadlineseconds-8919fc67-cf01-4aa7-b9da-2775977320bf" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May 17 06:28:56.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4903" for this suite. 05/17/23 06:28:56.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:28:56.335
May 17 06:28:56.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:28:56.335
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:56.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:56.355
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 05/17/23 06:28:56.358
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:28:56.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6734" for this suite. 05/17/23 06:28:56.373
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":195,"skipped":3856,"failed":0}
------------------------------
â€¢ [0.046 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:28:56.335
    May 17 06:28:56.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:28:56.335
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:56.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:56.355
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 05/17/23 06:28:56.358
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:28:56.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6734" for this suite. 05/17/23 06:28:56.373
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:28:56.381
May 17 06:28:56.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename disruption 05/17/23 06:28:56.382
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:56.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:56.398
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 05/17/23 06:28:56.402
STEP: Waiting for the pdb to be processed 05/17/23 06:28:56.409
STEP: First trying to evict a pod which shouldn't be evictable 05/17/23 06:28:58.424
STEP: Waiting for all pods to be running 05/17/23 06:28:58.424
May 17 06:28:58.429: INFO: pods: 0 < 3
STEP: locating a running pod 05/17/23 06:29:00.434
STEP: Updating the pdb to allow a pod to be evicted 05/17/23 06:29:00.46
STEP: Waiting for the pdb to be processed 05/17/23 06:29:00.471
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/17/23 06:29:02.481
STEP: Waiting for all pods to be running 05/17/23 06:29:02.481
STEP: Waiting for the pdb to observed all healthy pods 05/17/23 06:29:02.486
STEP: Patching the pdb to disallow a pod to be evicted 05/17/23 06:29:02.518
STEP: Waiting for the pdb to be processed 05/17/23 06:29:02.529
STEP: Waiting for all pods to be running 05/17/23 06:29:04.539
STEP: locating a running pod 05/17/23 06:29:04.544
STEP: Deleting the pdb to allow a pod to be evicted 05/17/23 06:29:04.563
STEP: Waiting for the pdb to be deleted 05/17/23 06:29:04.57
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/17/23 06:29:04.574
STEP: Waiting for all pods to be running 05/17/23 06:29:04.574
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May 17 06:29:04.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7996" for this suite. 05/17/23 06:29:04.606
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":196,"skipped":3864,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.231 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:28:56.381
    May 17 06:28:56.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename disruption 05/17/23 06:28:56.382
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:28:56.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:28:56.398
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 05/17/23 06:28:56.402
    STEP: Waiting for the pdb to be processed 05/17/23 06:28:56.409
    STEP: First trying to evict a pod which shouldn't be evictable 05/17/23 06:28:58.424
    STEP: Waiting for all pods to be running 05/17/23 06:28:58.424
    May 17 06:28:58.429: INFO: pods: 0 < 3
    STEP: locating a running pod 05/17/23 06:29:00.434
    STEP: Updating the pdb to allow a pod to be evicted 05/17/23 06:29:00.46
    STEP: Waiting for the pdb to be processed 05/17/23 06:29:00.471
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/17/23 06:29:02.481
    STEP: Waiting for all pods to be running 05/17/23 06:29:02.481
    STEP: Waiting for the pdb to observed all healthy pods 05/17/23 06:29:02.486
    STEP: Patching the pdb to disallow a pod to be evicted 05/17/23 06:29:02.518
    STEP: Waiting for the pdb to be processed 05/17/23 06:29:02.529
    STEP: Waiting for all pods to be running 05/17/23 06:29:04.539
    STEP: locating a running pod 05/17/23 06:29:04.544
    STEP: Deleting the pdb to allow a pod to be evicted 05/17/23 06:29:04.563
    STEP: Waiting for the pdb to be deleted 05/17/23 06:29:04.57
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/17/23 06:29:04.574
    STEP: Waiting for all pods to be running 05/17/23 06:29:04.574
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May 17 06:29:04.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7996" for this suite. 05/17/23 06:29:04.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:29:04.614
May 17 06:29:04.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 06:29:04.615
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:04.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:04.634
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/17/23 06:29:04.638
May 17 06:29:04.652: INFO: Waiting up to 5m0s for pod "pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c" in namespace "emptydir-6705" to be "Succeeded or Failed"
May 17 06:29:04.656: INFO: Pod "pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247466ms
May 17 06:29:06.662: INFO: Pod "pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009549205s
May 17 06:29:08.663: INFO: Pod "pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010543444s
STEP: Saw pod success 05/17/23 06:29:08.663
May 17 06:29:08.663: INFO: Pod "pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c" satisfied condition "Succeeded or Failed"
May 17 06:29:08.671: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c container test-container: <nil>
STEP: delete the pod 05/17/23 06:29:08.68
May 17 06:29:08.694: INFO: Waiting for pod pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c to disappear
May 17 06:29:08.698: INFO: Pod pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 06:29:08.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6705" for this suite. 05/17/23 06:29:08.705
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":197,"skipped":3912,"failed":0}
------------------------------
â€¢ [4.097 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:29:04.614
    May 17 06:29:04.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 06:29:04.615
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:04.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:04.634
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/17/23 06:29:04.638
    May 17 06:29:04.652: INFO: Waiting up to 5m0s for pod "pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c" in namespace "emptydir-6705" to be "Succeeded or Failed"
    May 17 06:29:04.656: INFO: Pod "pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247466ms
    May 17 06:29:06.662: INFO: Pod "pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009549205s
    May 17 06:29:08.663: INFO: Pod "pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010543444s
    STEP: Saw pod success 05/17/23 06:29:08.663
    May 17 06:29:08.663: INFO: Pod "pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c" satisfied condition "Succeeded or Failed"
    May 17 06:29:08.671: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c container test-container: <nil>
    STEP: delete the pod 05/17/23 06:29:08.68
    May 17 06:29:08.694: INFO: Waiting for pod pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c to disappear
    May 17 06:29:08.698: INFO: Pod pod-3d9cd5e6-715a-4c1d-b64f-77c8dea2271c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 06:29:08.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6705" for this suite. 05/17/23 06:29:08.705
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:29:08.712
May 17 06:29:08.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename replication-controller 05/17/23 06:29:08.712
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:08.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:08.731
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 05/17/23 06:29:08.738
May 17 06:29:08.753: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2874" to be "running and ready"
May 17 06:29:08.758: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.221716ms
May 17 06:29:08.758: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May 17 06:29:10.765: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.011332013s
May 17 06:29:10.765: INFO: The phase of Pod pod-adoption is Running (Ready = true)
May 17 06:29:10.765: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 05/17/23 06:29:10.771
STEP: Then the orphan pod is adopted 05/17/23 06:29:10.778
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May 17 06:29:11.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2874" for this suite. 05/17/23 06:29:11.794
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":198,"skipped":3915,"failed":0}
------------------------------
â€¢ [3.089 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:29:08.712
    May 17 06:29:08.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename replication-controller 05/17/23 06:29:08.712
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:08.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:08.731
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 05/17/23 06:29:08.738
    May 17 06:29:08.753: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2874" to be "running and ready"
    May 17 06:29:08.758: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.221716ms
    May 17 06:29:08.758: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:29:10.765: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.011332013s
    May 17 06:29:10.765: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    May 17 06:29:10.765: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 05/17/23 06:29:10.771
    STEP: Then the orphan pod is adopted 05/17/23 06:29:10.778
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May 17 06:29:11.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2874" for this suite. 05/17/23 06:29:11.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:29:11.801
May 17 06:29:11.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 06:29:11.802
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:11.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:11.817
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/17/23 06:29:11.829
May 17 06:29:11.841: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6368" to be "running and ready"
May 17 06:29:11.846: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.151924ms
May 17 06:29:11.846: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 17 06:29:13.850: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008589763s
May 17 06:29:13.850: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May 17 06:29:13.850: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 05/17/23 06:29:13.854
May 17 06:29:13.862: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6368" to be "running and ready"
May 17 06:29:13.867: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.664337ms
May 17 06:29:13.867: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 17 06:29:15.871: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009253877s
May 17 06:29:15.871: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
May 17 06:29:15.871: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/17/23 06:29:15.875
STEP: delete the pod with lifecycle hook 05/17/23 06:29:15.886
May 17 06:29:15.894: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 17 06:29:15.899: INFO: Pod pod-with-poststart-http-hook still exists
May 17 06:29:17.900: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 17 06:29:17.907: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May 17 06:29:17.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6368" for this suite. 05/17/23 06:29:17.914
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":199,"skipped":3926,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.120 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:29:11.801
    May 17 06:29:11.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 06:29:11.802
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:11.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:11.817
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/17/23 06:29:11.829
    May 17 06:29:11.841: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6368" to be "running and ready"
    May 17 06:29:11.846: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.151924ms
    May 17 06:29:11.846: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:29:13.850: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008589763s
    May 17 06:29:13.850: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May 17 06:29:13.850: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 05/17/23 06:29:13.854
    May 17 06:29:13.862: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6368" to be "running and ready"
    May 17 06:29:13.867: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.664337ms
    May 17 06:29:13.867: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:29:15.871: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009253877s
    May 17 06:29:15.871: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    May 17 06:29:15.871: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/17/23 06:29:15.875
    STEP: delete the pod with lifecycle hook 05/17/23 06:29:15.886
    May 17 06:29:15.894: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May 17 06:29:15.899: INFO: Pod pod-with-poststart-http-hook still exists
    May 17 06:29:17.900: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May 17 06:29:17.907: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May 17 06:29:17.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6368" for this suite. 05/17/23 06:29:17.914
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:29:17.922
May 17 06:29:17.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename namespaces 05/17/23 06:29:17.922
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:17.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:17.942
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 05/17/23 06:29:17.945
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:18.01
STEP: Creating a pod in the namespace 05/17/23 06:29:18.014
STEP: Waiting for the pod to have running status 05/17/23 06:29:18.025
May 17 06:29:18.025: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8326" to be "running"
May 17 06:29:18.029: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.981161ms
May 17 06:29:20.039: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013379781s
May 17 06:29:20.039: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 05/17/23 06:29:20.039
STEP: Waiting for the namespace to be removed. 05/17/23 06:29:20.049
STEP: Recreating the namespace 05/17/23 06:29:32.053
STEP: Verifying there are no pods in the namespace 05/17/23 06:29:32.07
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May 17 06:29:32.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3493" for this suite. 05/17/23 06:29:32.081
STEP: Destroying namespace "nsdeletetest-8326" for this suite. 05/17/23 06:29:32.087
May 17 06:29:32.091: INFO: Namespace nsdeletetest-8326 was already deleted
STEP: Destroying namespace "nsdeletetest-7441" for this suite. 05/17/23 06:29:32.091
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":200,"skipped":3930,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.175 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:29:17.922
    May 17 06:29:17.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename namespaces 05/17/23 06:29:17.922
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:17.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:17.942
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 05/17/23 06:29:17.945
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:18.01
    STEP: Creating a pod in the namespace 05/17/23 06:29:18.014
    STEP: Waiting for the pod to have running status 05/17/23 06:29:18.025
    May 17 06:29:18.025: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8326" to be "running"
    May 17 06:29:18.029: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.981161ms
    May 17 06:29:20.039: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013379781s
    May 17 06:29:20.039: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 05/17/23 06:29:20.039
    STEP: Waiting for the namespace to be removed. 05/17/23 06:29:20.049
    STEP: Recreating the namespace 05/17/23 06:29:32.053
    STEP: Verifying there are no pods in the namespace 05/17/23 06:29:32.07
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May 17 06:29:32.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3493" for this suite. 05/17/23 06:29:32.081
    STEP: Destroying namespace "nsdeletetest-8326" for this suite. 05/17/23 06:29:32.087
    May 17 06:29:32.091: INFO: Namespace nsdeletetest-8326 was already deleted
    STEP: Destroying namespace "nsdeletetest-7441" for this suite. 05/17/23 06:29:32.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:29:32.098
May 17 06:29:32.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 06:29:32.099
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:32.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:32.116
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 05/17/23 06:29:32.119
May 17 06:29:32.130: INFO: Waiting up to 5m0s for pod "pod-0e46f4ae-0985-4e6d-909f-9f85496fae91" in namespace "emptydir-6848" to be "Succeeded or Failed"
May 17 06:29:32.134: INFO: Pod "pod-0e46f4ae-0985-4e6d-909f-9f85496fae91": Phase="Pending", Reason="", readiness=false. Elapsed: 3.522726ms
May 17 06:29:34.140: INFO: Pod "pod-0e46f4ae-0985-4e6d-909f-9f85496fae91": Phase="Running", Reason="", readiness=false. Elapsed: 2.00936866s
May 17 06:29:36.141: INFO: Pod "pod-0e46f4ae-0985-4e6d-909f-9f85496fae91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010345918s
STEP: Saw pod success 05/17/23 06:29:36.141
May 17 06:29:36.141: INFO: Pod "pod-0e46f4ae-0985-4e6d-909f-9f85496fae91" satisfied condition "Succeeded or Failed"
May 17 06:29:36.154: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-0e46f4ae-0985-4e6d-909f-9f85496fae91 container test-container: <nil>
STEP: delete the pod 05/17/23 06:29:36.17
May 17 06:29:36.187: INFO: Waiting for pod pod-0e46f4ae-0985-4e6d-909f-9f85496fae91 to disappear
May 17 06:29:36.192: INFO: Pod pod-0e46f4ae-0985-4e6d-909f-9f85496fae91 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 06:29:36.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6848" for this suite. 05/17/23 06:29:36.199
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":201,"skipped":3958,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:29:32.098
    May 17 06:29:32.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 06:29:32.099
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:32.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:32.116
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/17/23 06:29:32.119
    May 17 06:29:32.130: INFO: Waiting up to 5m0s for pod "pod-0e46f4ae-0985-4e6d-909f-9f85496fae91" in namespace "emptydir-6848" to be "Succeeded or Failed"
    May 17 06:29:32.134: INFO: Pod "pod-0e46f4ae-0985-4e6d-909f-9f85496fae91": Phase="Pending", Reason="", readiness=false. Elapsed: 3.522726ms
    May 17 06:29:34.140: INFO: Pod "pod-0e46f4ae-0985-4e6d-909f-9f85496fae91": Phase="Running", Reason="", readiness=false. Elapsed: 2.00936866s
    May 17 06:29:36.141: INFO: Pod "pod-0e46f4ae-0985-4e6d-909f-9f85496fae91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010345918s
    STEP: Saw pod success 05/17/23 06:29:36.141
    May 17 06:29:36.141: INFO: Pod "pod-0e46f4ae-0985-4e6d-909f-9f85496fae91" satisfied condition "Succeeded or Failed"
    May 17 06:29:36.154: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-0e46f4ae-0985-4e6d-909f-9f85496fae91 container test-container: <nil>
    STEP: delete the pod 05/17/23 06:29:36.17
    May 17 06:29:36.187: INFO: Waiting for pod pod-0e46f4ae-0985-4e6d-909f-9f85496fae91 to disappear
    May 17 06:29:36.192: INFO: Pod pod-0e46f4ae-0985-4e6d-909f-9f85496fae91 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 06:29:36.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6848" for this suite. 05/17/23 06:29:36.199
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:29:36.211
May 17 06:29:36.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename replication-controller 05/17/23 06:29:36.211
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:36.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:36.234
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 05/17/23 06:29:36.262
STEP: waiting for RC to be added 05/17/23 06:29:36.272
STEP: waiting for available Replicas 05/17/23 06:29:36.272
STEP: patching ReplicationController 05/17/23 06:29:37.741
STEP: waiting for RC to be modified 05/17/23 06:29:37.75
STEP: patching ReplicationController status 05/17/23 06:29:37.75
STEP: waiting for RC to be modified 05/17/23 06:29:37.756
STEP: waiting for available Replicas 05/17/23 06:29:37.757
STEP: fetching ReplicationController status 05/17/23 06:29:37.764
STEP: patching ReplicationController scale 05/17/23 06:29:37.768
STEP: waiting for RC to be modified 05/17/23 06:29:37.776
STEP: waiting for ReplicationController's scale to be the max amount 05/17/23 06:29:37.776
STEP: fetching ReplicationController; ensuring that it's patched 05/17/23 06:29:38.801
STEP: updating ReplicationController status 05/17/23 06:29:38.806
STEP: waiting for RC to be modified 05/17/23 06:29:38.813
STEP: listing all ReplicationControllers 05/17/23 06:29:38.813
STEP: checking that ReplicationController has expected values 05/17/23 06:29:38.817
STEP: deleting ReplicationControllers by collection 05/17/23 06:29:38.817
STEP: waiting for ReplicationController to have a DELETED watchEvent 05/17/23 06:29:38.826
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May 17 06:29:38.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9458" for this suite. 05/17/23 06:29:38.906
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":202,"skipped":3962,"failed":0}
------------------------------
â€¢ [2.703 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:29:36.211
    May 17 06:29:36.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename replication-controller 05/17/23 06:29:36.211
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:36.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:36.234
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 05/17/23 06:29:36.262
    STEP: waiting for RC to be added 05/17/23 06:29:36.272
    STEP: waiting for available Replicas 05/17/23 06:29:36.272
    STEP: patching ReplicationController 05/17/23 06:29:37.741
    STEP: waiting for RC to be modified 05/17/23 06:29:37.75
    STEP: patching ReplicationController status 05/17/23 06:29:37.75
    STEP: waiting for RC to be modified 05/17/23 06:29:37.756
    STEP: waiting for available Replicas 05/17/23 06:29:37.757
    STEP: fetching ReplicationController status 05/17/23 06:29:37.764
    STEP: patching ReplicationController scale 05/17/23 06:29:37.768
    STEP: waiting for RC to be modified 05/17/23 06:29:37.776
    STEP: waiting for ReplicationController's scale to be the max amount 05/17/23 06:29:37.776
    STEP: fetching ReplicationController; ensuring that it's patched 05/17/23 06:29:38.801
    STEP: updating ReplicationController status 05/17/23 06:29:38.806
    STEP: waiting for RC to be modified 05/17/23 06:29:38.813
    STEP: listing all ReplicationControllers 05/17/23 06:29:38.813
    STEP: checking that ReplicationController has expected values 05/17/23 06:29:38.817
    STEP: deleting ReplicationControllers by collection 05/17/23 06:29:38.817
    STEP: waiting for ReplicationController to have a DELETED watchEvent 05/17/23 06:29:38.826
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May 17 06:29:38.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9458" for this suite. 05/17/23 06:29:38.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:29:38.915
May 17 06:29:38.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 06:29:38.915
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:38.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:38.935
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 05/17/23 06:29:38.938
May 17 06:29:38.951: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb" in namespace "downward-api-5512" to be "Succeeded or Failed"
May 17 06:29:38.955: INFO: Pod "downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891059ms
May 17 06:29:40.960: INFO: Pod "downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008845379s
May 17 06:29:42.960: INFO: Pod "downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008996751s
STEP: Saw pod success 05/17/23 06:29:42.96
May 17 06:29:42.960: INFO: Pod "downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb" satisfied condition "Succeeded or Failed"
May 17 06:29:42.964: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb container client-container: <nil>
STEP: delete the pod 05/17/23 06:29:42.974
May 17 06:29:42.987: INFO: Waiting for pod downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb to disappear
May 17 06:29:42.991: INFO: Pod downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May 17 06:29:42.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5512" for this suite. 05/17/23 06:29:42.998
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":203,"skipped":3976,"failed":0}
------------------------------
â€¢ [4.091 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:29:38.915
    May 17 06:29:38.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 06:29:38.915
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:38.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:38.935
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 05/17/23 06:29:38.938
    May 17 06:29:38.951: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb" in namespace "downward-api-5512" to be "Succeeded or Failed"
    May 17 06:29:38.955: INFO: Pod "downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891059ms
    May 17 06:29:40.960: INFO: Pod "downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008845379s
    May 17 06:29:42.960: INFO: Pod "downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008996751s
    STEP: Saw pod success 05/17/23 06:29:42.96
    May 17 06:29:42.960: INFO: Pod "downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb" satisfied condition "Succeeded or Failed"
    May 17 06:29:42.964: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb container client-container: <nil>
    STEP: delete the pod 05/17/23 06:29:42.974
    May 17 06:29:42.987: INFO: Waiting for pod downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb to disappear
    May 17 06:29:42.991: INFO: Pod downwardapi-volume-0b3cdcb3-a38d-4371-88e4-a43cba01f1fb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May 17 06:29:42.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5512" for this suite. 05/17/23 06:29:42.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:29:43.007
May 17 06:29:43.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename endpointslice 05/17/23 06:29:43.008
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:43.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:43.028
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 05/17/23 06:29:43.031
STEP: getting /apis/discovery.k8s.io 05/17/23 06:29:43.034
STEP: getting /apis/discovery.k8s.iov1 05/17/23 06:29:43.036
STEP: creating 05/17/23 06:29:43.037
STEP: getting 05/17/23 06:29:43.057
STEP: listing 05/17/23 06:29:43.063
STEP: watching 05/17/23 06:29:43.068
May 17 06:29:43.068: INFO: starting watch
STEP: cluster-wide listing 05/17/23 06:29:43.07
STEP: cluster-wide watching 05/17/23 06:29:43.075
May 17 06:29:43.075: INFO: starting watch
STEP: patching 05/17/23 06:29:43.077
STEP: updating 05/17/23 06:29:43.084
May 17 06:29:43.093: INFO: waiting for watch events with expected annotations
May 17 06:29:43.093: INFO: saw patched and updated annotations
STEP: deleting 05/17/23 06:29:43.093
STEP: deleting a collection 05/17/23 06:29:43.11
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May 17 06:29:43.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6655" for this suite. 05/17/23 06:29:43.133
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":204,"skipped":4027,"failed":0}
------------------------------
â€¢ [0.133 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:29:43.007
    May 17 06:29:43.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename endpointslice 05/17/23 06:29:43.008
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:43.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:43.028
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 05/17/23 06:29:43.031
    STEP: getting /apis/discovery.k8s.io 05/17/23 06:29:43.034
    STEP: getting /apis/discovery.k8s.iov1 05/17/23 06:29:43.036
    STEP: creating 05/17/23 06:29:43.037
    STEP: getting 05/17/23 06:29:43.057
    STEP: listing 05/17/23 06:29:43.063
    STEP: watching 05/17/23 06:29:43.068
    May 17 06:29:43.068: INFO: starting watch
    STEP: cluster-wide listing 05/17/23 06:29:43.07
    STEP: cluster-wide watching 05/17/23 06:29:43.075
    May 17 06:29:43.075: INFO: starting watch
    STEP: patching 05/17/23 06:29:43.077
    STEP: updating 05/17/23 06:29:43.084
    May 17 06:29:43.093: INFO: waiting for watch events with expected annotations
    May 17 06:29:43.093: INFO: saw patched and updated annotations
    STEP: deleting 05/17/23 06:29:43.093
    STEP: deleting a collection 05/17/23 06:29:43.11
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May 17 06:29:43.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6655" for this suite. 05/17/23 06:29:43.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:29:43.141
May 17 06:29:43.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:29:43.141
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:43.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:43.164
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 05/17/23 06:29:43.168
May 17 06:29:43.168: INFO: namespace kubectl-6782
May 17 06:29:43.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-6782 create -f -'
May 17 06:29:44.456: INFO: stderr: ""
May 17 06:29:44.456: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/17/23 06:29:44.456
May 17 06:29:45.461: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 06:29:45.461: INFO: Found 0 / 1
May 17 06:29:46.462: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 06:29:46.462: INFO: Found 1 / 1
May 17 06:29:46.462: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 17 06:29:46.466: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 06:29:46.466: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 17 06:29:46.466: INFO: wait on agnhost-primary startup in kubectl-6782 
May 17 06:29:46.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-6782 logs agnhost-primary-dzvnq agnhost-primary'
May 17 06:29:46.564: INFO: stderr: ""
May 17 06:29:46.564: INFO: stdout: "Paused\n"
STEP: exposing RC 05/17/23 06:29:46.564
May 17 06:29:46.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-6782 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 17 06:29:46.639: INFO: stderr: ""
May 17 06:29:46.639: INFO: stdout: "service/rm2 exposed\n"
May 17 06:29:46.643: INFO: Service rm2 in namespace kubectl-6782 found.
STEP: exposing service 05/17/23 06:29:48.652
May 17 06:29:48.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-6782 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 17 06:29:48.730: INFO: stderr: ""
May 17 06:29:48.730: INFO: stdout: "service/rm3 exposed\n"
May 17 06:29:48.734: INFO: Service rm3 in namespace kubectl-6782 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:29:50.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6782" for this suite. 05/17/23 06:29:50.752
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":205,"skipped":4050,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.619 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:29:43.141
    May 17 06:29:43.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:29:43.141
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:43.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:43.164
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 05/17/23 06:29:43.168
    May 17 06:29:43.168: INFO: namespace kubectl-6782
    May 17 06:29:43.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-6782 create -f -'
    May 17 06:29:44.456: INFO: stderr: ""
    May 17 06:29:44.456: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/17/23 06:29:44.456
    May 17 06:29:45.461: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 06:29:45.461: INFO: Found 0 / 1
    May 17 06:29:46.462: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 06:29:46.462: INFO: Found 1 / 1
    May 17 06:29:46.462: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May 17 06:29:46.466: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 06:29:46.466: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May 17 06:29:46.466: INFO: wait on agnhost-primary startup in kubectl-6782 
    May 17 06:29:46.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-6782 logs agnhost-primary-dzvnq agnhost-primary'
    May 17 06:29:46.564: INFO: stderr: ""
    May 17 06:29:46.564: INFO: stdout: "Paused\n"
    STEP: exposing RC 05/17/23 06:29:46.564
    May 17 06:29:46.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-6782 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    May 17 06:29:46.639: INFO: stderr: ""
    May 17 06:29:46.639: INFO: stdout: "service/rm2 exposed\n"
    May 17 06:29:46.643: INFO: Service rm2 in namespace kubectl-6782 found.
    STEP: exposing service 05/17/23 06:29:48.652
    May 17 06:29:48.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-6782 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    May 17 06:29:48.730: INFO: stderr: ""
    May 17 06:29:48.730: INFO: stdout: "service/rm3 exposed\n"
    May 17 06:29:48.734: INFO: Service rm3 in namespace kubectl-6782 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:29:50.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6782" for this suite. 05/17/23 06:29:50.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:29:50.763
May 17 06:29:50.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 06:29:50.763
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:50.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:50.784
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 06:29:50.802
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:29:50.975
STEP: Deploying the webhook pod 05/17/23 06:29:50.981
STEP: Wait for the deployment to be ready 05/17/23 06:29:50.992
May 17 06:29:51.001: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/17/23 06:29:53.017
STEP: Verifying the service has paired with the endpoint 05/17/23 06:29:53.027
May 17 06:29:54.027: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
May 17 06:29:54.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2509-crds.webhook.example.com via the AdmissionRegistration API 05/17/23 06:29:54.544
STEP: Creating a custom resource while v1 is storage version 05/17/23 06:29:54.565
STEP: Patching Custom Resource Definition to set v2 as storage 05/17/23 06:29:56.759
STEP: Patching the custom resource while v2 is storage version 05/17/23 06:29:56.789
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:29:57.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3907" for this suite. 05/17/23 06:29:57.595
STEP: Destroying namespace "webhook-3907-markers" for this suite. 05/17/23 06:29:57.603
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":206,"skipped":4118,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.884 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:29:50.763
    May 17 06:29:50.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 06:29:50.763
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:50.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:50.784
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 06:29:50.802
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:29:50.975
    STEP: Deploying the webhook pod 05/17/23 06:29:50.981
    STEP: Wait for the deployment to be ready 05/17/23 06:29:50.992
    May 17 06:29:51.001: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/17/23 06:29:53.017
    STEP: Verifying the service has paired with the endpoint 05/17/23 06:29:53.027
    May 17 06:29:54.027: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    May 17 06:29:54.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2509-crds.webhook.example.com via the AdmissionRegistration API 05/17/23 06:29:54.544
    STEP: Creating a custom resource while v1 is storage version 05/17/23 06:29:54.565
    STEP: Patching Custom Resource Definition to set v2 as storage 05/17/23 06:29:56.759
    STEP: Patching the custom resource while v2 is storage version 05/17/23 06:29:56.789
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:29:57.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3907" for this suite. 05/17/23 06:29:57.595
    STEP: Destroying namespace "webhook-3907-markers" for this suite. 05/17/23 06:29:57.603
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:29:57.648
May 17 06:29:57.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:29:57.648
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:57.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:57.674
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a3072ac6-0071-44bd-8af6-d9d1ee139fd9 05/17/23 06:29:57.685
STEP: Creating the pod 05/17/23 06:29:57.691
May 17 06:29:57.706: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1ae89e60-0156-44c9-aaf6-2ca6e99e4a24" in namespace "projected-1082" to be "running and ready"
May 17 06:29:57.709: INFO: Pod "pod-projected-configmaps-1ae89e60-0156-44c9-aaf6-2ca6e99e4a24": Phase="Pending", Reason="", readiness=false. Elapsed: 3.926253ms
May 17 06:29:57.709: INFO: The phase of Pod pod-projected-configmaps-1ae89e60-0156-44c9-aaf6-2ca6e99e4a24 is Pending, waiting for it to be Running (with Ready = true)
May 17 06:29:59.715: INFO: Pod "pod-projected-configmaps-1ae89e60-0156-44c9-aaf6-2ca6e99e4a24": Phase="Running", Reason="", readiness=true. Elapsed: 2.009781217s
May 17 06:29:59.715: INFO: The phase of Pod pod-projected-configmaps-1ae89e60-0156-44c9-aaf6-2ca6e99e4a24 is Running (Ready = true)
May 17 06:29:59.715: INFO: Pod "pod-projected-configmaps-1ae89e60-0156-44c9-aaf6-2ca6e99e4a24" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-a3072ac6-0071-44bd-8af6-d9d1ee139fd9 05/17/23 06:29:59.73
STEP: waiting to observe update in volume 05/17/23 06:29:59.736
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May 17 06:30:01.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1082" for this suite. 05/17/23 06:30:01.765
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":207,"skipped":4141,"failed":0}
------------------------------
â€¢ [4.124 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:29:57.648
    May 17 06:29:57.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:29:57.648
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:29:57.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:29:57.674
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-a3072ac6-0071-44bd-8af6-d9d1ee139fd9 05/17/23 06:29:57.685
    STEP: Creating the pod 05/17/23 06:29:57.691
    May 17 06:29:57.706: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1ae89e60-0156-44c9-aaf6-2ca6e99e4a24" in namespace "projected-1082" to be "running and ready"
    May 17 06:29:57.709: INFO: Pod "pod-projected-configmaps-1ae89e60-0156-44c9-aaf6-2ca6e99e4a24": Phase="Pending", Reason="", readiness=false. Elapsed: 3.926253ms
    May 17 06:29:57.709: INFO: The phase of Pod pod-projected-configmaps-1ae89e60-0156-44c9-aaf6-2ca6e99e4a24 is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:29:59.715: INFO: Pod "pod-projected-configmaps-1ae89e60-0156-44c9-aaf6-2ca6e99e4a24": Phase="Running", Reason="", readiness=true. Elapsed: 2.009781217s
    May 17 06:29:59.715: INFO: The phase of Pod pod-projected-configmaps-1ae89e60-0156-44c9-aaf6-2ca6e99e4a24 is Running (Ready = true)
    May 17 06:29:59.715: INFO: Pod "pod-projected-configmaps-1ae89e60-0156-44c9-aaf6-2ca6e99e4a24" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-a3072ac6-0071-44bd-8af6-d9d1ee139fd9 05/17/23 06:29:59.73
    STEP: waiting to observe update in volume 05/17/23 06:29:59.736
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May 17 06:30:01.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1082" for this suite. 05/17/23 06:30:01.765
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:30:01.772
May 17 06:30:01.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:30:01.773
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:30:01.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:30:01.794
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-614 05/17/23 06:30:01.797
STEP: creating service affinity-clusterip-transition in namespace services-614 05/17/23 06:30:01.797
STEP: creating replication controller affinity-clusterip-transition in namespace services-614 05/17/23 06:30:01.813
I0517 06:30:01.820435      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-614, replica count: 3
I0517 06:30:04.871644      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 06:30:04.884: INFO: Creating new exec pod
May 17 06:30:04.896: INFO: Waiting up to 5m0s for pod "execpod-affinityzjljj" in namespace "services-614" to be "running"
May 17 06:30:04.899: INFO: Pod "execpod-affinityzjljj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.884781ms
May 17 06:30:06.906: INFO: Pod "execpod-affinityzjljj": Phase="Running", Reason="", readiness=true. Elapsed: 2.010842937s
May 17 06:30:06.906: INFO: Pod "execpod-affinityzjljj" satisfied condition "running"
May 17 06:30:07.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-614 exec execpod-affinityzjljj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
May 17 06:30:08.105: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 17 06:30:08.105: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:30:08.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-614 exec execpod-affinityzjljj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.35.51 80'
May 17 06:30:08.267: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.35.51 80\nConnection to 10.0.35.51 80 port [tcp/http] succeeded!\n"
May 17 06:30:08.267: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:30:08.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-614 exec execpod-affinityzjljj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.35.51:80/ ; done'
May 17 06:30:08.516: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n"
May 17 06:30:08.516: INFO: stdout: "\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-qzqq9\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-qzqq9\naffinity-clusterip-transition-qzqq9\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-qzqq9\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-qzqq9\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-hzqdl"
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-qzqq9
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-qzqq9
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-qzqq9
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-qzqq9
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-qzqq9
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-614 exec execpod-affinityzjljj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.35.51:80/ ; done'
May 17 06:30:08.802: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n"
May 17 06:30:08.802: INFO: stdout: "\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl"
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
May 17 06:30:08.802: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-614, will wait for the garbage collector to delete the pods 05/17/23 06:30:08.816
May 17 06:30:08.878: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.114995ms
May 17 06:30:08.979: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.416804ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:30:10.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-614" for this suite. 05/17/23 06:30:10.905
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":208,"skipped":4142,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.140 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:30:01.772
    May 17 06:30:01.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:30:01.773
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:30:01.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:30:01.794
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-614 05/17/23 06:30:01.797
    STEP: creating service affinity-clusterip-transition in namespace services-614 05/17/23 06:30:01.797
    STEP: creating replication controller affinity-clusterip-transition in namespace services-614 05/17/23 06:30:01.813
    I0517 06:30:01.820435      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-614, replica count: 3
    I0517 06:30:04.871644      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 06:30:04.884: INFO: Creating new exec pod
    May 17 06:30:04.896: INFO: Waiting up to 5m0s for pod "execpod-affinityzjljj" in namespace "services-614" to be "running"
    May 17 06:30:04.899: INFO: Pod "execpod-affinityzjljj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.884781ms
    May 17 06:30:06.906: INFO: Pod "execpod-affinityzjljj": Phase="Running", Reason="", readiness=true. Elapsed: 2.010842937s
    May 17 06:30:06.906: INFO: Pod "execpod-affinityzjljj" satisfied condition "running"
    May 17 06:30:07.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-614 exec execpod-affinityzjljj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    May 17 06:30:08.105: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    May 17 06:30:08.105: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:30:08.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-614 exec execpod-affinityzjljj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.35.51 80'
    May 17 06:30:08.267: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.35.51 80\nConnection to 10.0.35.51 80 port [tcp/http] succeeded!\n"
    May 17 06:30:08.267: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:30:08.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-614 exec execpod-affinityzjljj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.35.51:80/ ; done'
    May 17 06:30:08.516: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n"
    May 17 06:30:08.516: INFO: stdout: "\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-qzqq9\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-qzqq9\naffinity-clusterip-transition-qzqq9\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-qzqq9\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-qzqq9\naffinity-clusterip-transition-4w9kk\naffinity-clusterip-transition-hzqdl"
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-qzqq9
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-qzqq9
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-qzqq9
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-qzqq9
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-qzqq9
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-4w9kk
    May 17 06:30:08.516: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-614 exec execpod-affinityzjljj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.35.51:80/ ; done'
    May 17 06:30:08.802: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.51:80/\n"
    May 17 06:30:08.802: INFO: stdout: "\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl\naffinity-clusterip-transition-hzqdl"
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Received response from host: affinity-clusterip-transition-hzqdl
    May 17 06:30:08.802: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-614, will wait for the garbage collector to delete the pods 05/17/23 06:30:08.816
    May 17 06:30:08.878: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.114995ms
    May 17 06:30:08.979: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.416804ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:30:10.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-614" for this suite. 05/17/23 06:30:10.905
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:30:10.913
May 17 06:30:10.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:30:10.914
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:30:10.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:30:10.931
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-e1db5f09-96a0-4145-8042-a038b94c79fe 05/17/23 06:30:10.942
STEP: Creating secret with name s-test-opt-upd-f303d8f7-7561-42b6-b78d-74517568b61a 05/17/23 06:30:10.948
STEP: Creating the pod 05/17/23 06:30:10.953
May 17 06:30:10.965: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b3548cfe-ff7c-497f-89f1-1894a64eca1c" in namespace "projected-9192" to be "running and ready"
May 17 06:30:10.970: INFO: Pod "pod-projected-secrets-b3548cfe-ff7c-497f-89f1-1894a64eca1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.916443ms
May 17 06:30:10.970: INFO: The phase of Pod pod-projected-secrets-b3548cfe-ff7c-497f-89f1-1894a64eca1c is Pending, waiting for it to be Running (with Ready = true)
May 17 06:30:12.977: INFO: Pod "pod-projected-secrets-b3548cfe-ff7c-497f-89f1-1894a64eca1c": Phase="Running", Reason="", readiness=true. Elapsed: 2.011781589s
May 17 06:30:12.977: INFO: The phase of Pod pod-projected-secrets-b3548cfe-ff7c-497f-89f1-1894a64eca1c is Running (Ready = true)
May 17 06:30:12.977: INFO: Pod "pod-projected-secrets-b3548cfe-ff7c-497f-89f1-1894a64eca1c" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-e1db5f09-96a0-4145-8042-a038b94c79fe 05/17/23 06:30:13.01
STEP: Updating secret s-test-opt-upd-f303d8f7-7561-42b6-b78d-74517568b61a 05/17/23 06:30:13.018
STEP: Creating secret with name s-test-opt-create-3e8ac909-3b8d-4e59-8b60-6ca7f913fe1d 05/17/23 06:30:13.024
STEP: waiting to observe update in volume 05/17/23 06:30:13.029
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May 17 06:31:37.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9192" for this suite. 05/17/23 06:31:37.702
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":209,"skipped":4182,"failed":0}
------------------------------
â€¢ [SLOW TEST] [86.799 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:30:10.913
    May 17 06:30:10.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:30:10.914
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:30:10.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:30:10.931
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-e1db5f09-96a0-4145-8042-a038b94c79fe 05/17/23 06:30:10.942
    STEP: Creating secret with name s-test-opt-upd-f303d8f7-7561-42b6-b78d-74517568b61a 05/17/23 06:30:10.948
    STEP: Creating the pod 05/17/23 06:30:10.953
    May 17 06:30:10.965: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b3548cfe-ff7c-497f-89f1-1894a64eca1c" in namespace "projected-9192" to be "running and ready"
    May 17 06:30:10.970: INFO: Pod "pod-projected-secrets-b3548cfe-ff7c-497f-89f1-1894a64eca1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.916443ms
    May 17 06:30:10.970: INFO: The phase of Pod pod-projected-secrets-b3548cfe-ff7c-497f-89f1-1894a64eca1c is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:30:12.977: INFO: Pod "pod-projected-secrets-b3548cfe-ff7c-497f-89f1-1894a64eca1c": Phase="Running", Reason="", readiness=true. Elapsed: 2.011781589s
    May 17 06:30:12.977: INFO: The phase of Pod pod-projected-secrets-b3548cfe-ff7c-497f-89f1-1894a64eca1c is Running (Ready = true)
    May 17 06:30:12.977: INFO: Pod "pod-projected-secrets-b3548cfe-ff7c-497f-89f1-1894a64eca1c" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-e1db5f09-96a0-4145-8042-a038b94c79fe 05/17/23 06:30:13.01
    STEP: Updating secret s-test-opt-upd-f303d8f7-7561-42b6-b78d-74517568b61a 05/17/23 06:30:13.018
    STEP: Creating secret with name s-test-opt-create-3e8ac909-3b8d-4e59-8b60-6ca7f913fe1d 05/17/23 06:30:13.024
    STEP: waiting to observe update in volume 05/17/23 06:30:13.029
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May 17 06:31:37.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9192" for this suite. 05/17/23 06:31:37.702
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:31:37.713
May 17 06:31:37.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename svc-latency 05/17/23 06:31:37.713
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:31:37.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:31:37.739
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
May 17 06:31:37.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3612 05/17/23 06:31:37.744
I0517 06:31:37.752545      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3612, replica count: 1
I0517 06:31:38.803527      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0517 06:31:39.804251      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 06:31:39.920: INFO: Created: latency-svc-kqdpv
May 17 06:31:39.941: INFO: Got endpoints: latency-svc-kqdpv [36.202131ms]
May 17 06:31:39.956: INFO: Created: latency-svc-8vv4t
May 17 06:31:39.961: INFO: Got endpoints: latency-svc-8vv4t [20.728693ms]
May 17 06:31:39.965: INFO: Created: latency-svc-g289m
May 17 06:31:39.970: INFO: Got endpoints: latency-svc-g289m [28.99166ms]
May 17 06:31:39.972: INFO: Created: latency-svc-dkdnj
May 17 06:31:39.977: INFO: Got endpoints: latency-svc-dkdnj [36.460864ms]
May 17 06:31:39.981: INFO: Created: latency-svc-k6wln
May 17 06:31:39.985: INFO: Got endpoints: latency-svc-k6wln [44.270805ms]
May 17 06:31:39.988: INFO: Created: latency-svc-ndzpz
May 17 06:31:39.995: INFO: Created: latency-svc-rwp5z
May 17 06:31:40.000: INFO: Got endpoints: latency-svc-ndzpz [58.705831ms]
May 17 06:31:40.000: INFO: Got endpoints: latency-svc-rwp5z [59.041057ms]
May 17 06:31:40.003: INFO: Created: latency-svc-ck9w9
May 17 06:31:40.008: INFO: Got endpoints: latency-svc-ck9w9 [66.897501ms]
May 17 06:31:40.032: INFO: Created: latency-svc-r4n4s
May 17 06:31:40.040: INFO: Created: latency-svc-s6257
May 17 06:31:40.047: INFO: Got endpoints: latency-svc-r4n4s [105.505976ms]
May 17 06:31:40.047: INFO: Got endpoints: latency-svc-s6257 [105.469247ms]
May 17 06:31:40.048: INFO: Created: latency-svc-wzcrs
May 17 06:31:40.054: INFO: Got endpoints: latency-svc-wzcrs [112.479783ms]
May 17 06:31:40.057: INFO: Created: latency-svc-799cz
May 17 06:31:40.066: INFO: Got endpoints: latency-svc-799cz [124.415538ms]
May 17 06:31:40.066: INFO: Created: latency-svc-7xc77
May 17 06:31:40.086: INFO: Created: latency-svc-ngtt2
May 17 06:31:40.086: INFO: Created: latency-svc-th5p2
May 17 06:31:40.086: INFO: Got endpoints: latency-svc-7xc77 [145.309219ms]
May 17 06:31:40.087: INFO: Got endpoints: latency-svc-ngtt2 [145.312985ms]
May 17 06:31:40.090: INFO: Got endpoints: latency-svc-th5p2 [149.248851ms]
May 17 06:31:40.093: INFO: Created: latency-svc-dv9hm
May 17 06:31:40.097: INFO: Got endpoints: latency-svc-dv9hm [155.650244ms]
May 17 06:31:40.101: INFO: Created: latency-svc-m7xch
May 17 06:31:40.106: INFO: Got endpoints: latency-svc-m7xch [144.16475ms]
May 17 06:31:40.109: INFO: Created: latency-svc-wvrbg
May 17 06:31:40.115: INFO: Created: latency-svc-vmgdp
May 17 06:31:40.121: INFO: Got endpoints: latency-svc-wvrbg [151.134697ms]
May 17 06:31:40.122: INFO: Got endpoints: latency-svc-vmgdp [144.265386ms]
May 17 06:31:40.124: INFO: Created: latency-svc-hpsft
May 17 06:31:40.130: INFO: Got endpoints: latency-svc-hpsft [144.518611ms]
May 17 06:31:40.133: INFO: Created: latency-svc-tzxxh
May 17 06:31:40.139: INFO: Got endpoints: latency-svc-tzxxh [139.154812ms]
May 17 06:31:40.141: INFO: Created: latency-svc-w9ktv
May 17 06:31:40.147: INFO: Got endpoints: latency-svc-w9ktv [146.823739ms]
May 17 06:31:40.150: INFO: Created: latency-svc-hzzr2
May 17 06:31:40.157: INFO: Got endpoints: latency-svc-hzzr2 [148.686021ms]
May 17 06:31:40.158: INFO: Created: latency-svc-4x465
May 17 06:31:40.165: INFO: Got endpoints: latency-svc-4x465 [117.910125ms]
May 17 06:31:40.168: INFO: Created: latency-svc-2m9r4
May 17 06:31:40.173: INFO: Got endpoints: latency-svc-2m9r4 [126.620577ms]
May 17 06:31:40.176: INFO: Created: latency-svc-t6jtn
May 17 06:31:40.183: INFO: Got endpoints: latency-svc-t6jtn [129.741101ms]
May 17 06:31:40.186: INFO: Created: latency-svc-whwd8
May 17 06:31:40.191: INFO: Got endpoints: latency-svc-whwd8 [125.736948ms]
May 17 06:31:40.195: INFO: Created: latency-svc-m8zk4
May 17 06:31:40.201: INFO: Got endpoints: latency-svc-m8zk4 [114.757172ms]
May 17 06:31:40.203: INFO: Created: latency-svc-vp7j9
May 17 06:31:40.209: INFO: Got endpoints: latency-svc-vp7j9 [122.958266ms]
May 17 06:31:40.212: INFO: Created: latency-svc-5cbnq
May 17 06:31:40.223: INFO: Got endpoints: latency-svc-5cbnq [132.895223ms]
May 17 06:31:40.223: INFO: Created: latency-svc-pgjcc
May 17 06:31:40.235: INFO: Got endpoints: latency-svc-pgjcc [138.004459ms]
May 17 06:31:40.240: INFO: Created: latency-svc-tqgh5
May 17 06:31:40.247: INFO: Got endpoints: latency-svc-tqgh5 [141.578693ms]
May 17 06:31:40.251: INFO: Created: latency-svc-grfg6
May 17 06:31:40.258: INFO: Got endpoints: latency-svc-grfg6 [136.790994ms]
May 17 06:31:40.261: INFO: Created: latency-svc-lsjtn
May 17 06:31:40.267: INFO: Got endpoints: latency-svc-lsjtn [145.008957ms]
May 17 06:31:40.273: INFO: Created: latency-svc-79p2v
May 17 06:31:40.279: INFO: Got endpoints: latency-svc-79p2v [149.068105ms]
May 17 06:31:40.284: INFO: Created: latency-svc-4b9tj
May 17 06:31:40.290: INFO: Got endpoints: latency-svc-4b9tj [151.035005ms]
May 17 06:31:40.293: INFO: Created: latency-svc-9ntbb
May 17 06:31:40.307: INFO: Got endpoints: latency-svc-9ntbb [160.387315ms]
May 17 06:31:40.310: INFO: Created: latency-svc-ln52b
May 17 06:31:40.317: INFO: Got endpoints: latency-svc-ln52b [160.563482ms]
May 17 06:31:40.320: INFO: Created: latency-svc-s8fxz
May 17 06:31:40.328: INFO: Got endpoints: latency-svc-s8fxz [163.429322ms]
May 17 06:31:40.333: INFO: Created: latency-svc-bxczn
May 17 06:31:40.338: INFO: Got endpoints: latency-svc-bxczn [164.357751ms]
May 17 06:31:40.341: INFO: Created: latency-svc-6htsj
May 17 06:31:40.347: INFO: Got endpoints: latency-svc-6htsj [163.386906ms]
May 17 06:31:40.349: INFO: Created: latency-svc-d82p8
May 17 06:31:40.354: INFO: Got endpoints: latency-svc-d82p8 [162.95645ms]
May 17 06:31:40.357: INFO: Created: latency-svc-p75d6
May 17 06:31:40.369: INFO: Got endpoints: latency-svc-p75d6 [167.677465ms]
May 17 06:31:40.370: INFO: Created: latency-svc-wlmj4
May 17 06:31:40.375: INFO: Got endpoints: latency-svc-wlmj4 [165.5844ms]
May 17 06:31:40.378: INFO: Created: latency-svc-dvwn2
May 17 06:31:40.383: INFO: Got endpoints: latency-svc-dvwn2 [160.388327ms]
May 17 06:31:40.387: INFO: Created: latency-svc-vsjds
May 17 06:31:40.392: INFO: Got endpoints: latency-svc-vsjds [157.531529ms]
May 17 06:31:40.396: INFO: Created: latency-svc-rcg8n
May 17 06:31:40.401: INFO: Got endpoints: latency-svc-rcg8n [153.283169ms]
May 17 06:31:40.404: INFO: Created: latency-svc-9xnhl
May 17 06:31:40.409: INFO: Got endpoints: latency-svc-9xnhl [151.399454ms]
May 17 06:31:40.414: INFO: Created: latency-svc-hz9hf
May 17 06:31:40.420: INFO: Got endpoints: latency-svc-hz9hf [153.494775ms]
May 17 06:31:40.422: INFO: Created: latency-svc-nms5f
May 17 06:31:40.427: INFO: Got endpoints: latency-svc-nms5f [147.765987ms]
May 17 06:31:40.430: INFO: Created: latency-svc-8crcd
May 17 06:31:40.438: INFO: Got endpoints: latency-svc-8crcd [148.283378ms]
May 17 06:31:40.438: INFO: Created: latency-svc-8pk6q
May 17 06:31:40.444: INFO: Got endpoints: latency-svc-8pk6q [136.287776ms]
May 17 06:31:40.448: INFO: Created: latency-svc-7rkv4
May 17 06:31:40.453: INFO: Got endpoints: latency-svc-7rkv4 [135.387465ms]
May 17 06:31:40.455: INFO: Created: latency-svc-vsvsj
May 17 06:31:40.460: INFO: Got endpoints: latency-svc-vsvsj [132.327675ms]
May 17 06:31:40.463: INFO: Created: latency-svc-qdbgl
May 17 06:31:40.468: INFO: Got endpoints: latency-svc-qdbgl [130.261396ms]
May 17 06:31:40.472: INFO: Created: latency-svc-trgn4
May 17 06:31:40.478: INFO: Got endpoints: latency-svc-trgn4 [130.877609ms]
May 17 06:31:40.479: INFO: Created: latency-svc-p456b
May 17 06:31:40.486: INFO: Got endpoints: latency-svc-p456b [131.803111ms]
May 17 06:31:40.490: INFO: Created: latency-svc-92qfg
May 17 06:31:40.495: INFO: Got endpoints: latency-svc-92qfg [125.711999ms]
May 17 06:31:40.501: INFO: Created: latency-svc-gz8dk
May 17 06:31:40.508: INFO: Got endpoints: latency-svc-gz8dk [132.474687ms]
May 17 06:31:40.510: INFO: Created: latency-svc-nslgf
May 17 06:31:40.515: INFO: Got endpoints: latency-svc-nslgf [131.995403ms]
May 17 06:31:40.519: INFO: Created: latency-svc-vhms6
May 17 06:31:40.524: INFO: Got endpoints: latency-svc-vhms6 [131.809583ms]
May 17 06:31:40.526: INFO: Created: latency-svc-8n4bw
May 17 06:31:40.532: INFO: Got endpoints: latency-svc-8n4bw [131.838584ms]
May 17 06:31:40.535: INFO: Created: latency-svc-lk929
May 17 06:31:40.540: INFO: Got endpoints: latency-svc-lk929 [131.053961ms]
May 17 06:31:40.542: INFO: Created: latency-svc-4tzws
May 17 06:31:40.547: INFO: Got endpoints: latency-svc-4tzws [127.0978ms]
May 17 06:31:40.553: INFO: Created: latency-svc-nhb99
May 17 06:31:40.558: INFO: Got endpoints: latency-svc-nhb99 [131.324852ms]
May 17 06:31:40.562: INFO: Created: latency-svc-2wzzc
May 17 06:31:40.568: INFO: Got endpoints: latency-svc-2wzzc [129.56242ms]
May 17 06:31:40.569: INFO: Created: latency-svc-bjtsk
May 17 06:31:40.575: INFO: Got endpoints: latency-svc-bjtsk [131.594198ms]
May 17 06:31:40.578: INFO: Created: latency-svc-km48r
May 17 06:31:40.584: INFO: Got endpoints: latency-svc-km48r [130.672334ms]
May 17 06:31:40.587: INFO: Created: latency-svc-ff6zx
May 17 06:31:40.593: INFO: Got endpoints: latency-svc-ff6zx [132.318791ms]
May 17 06:31:40.596: INFO: Created: latency-svc-d4tp9
May 17 06:31:40.601: INFO: Got endpoints: latency-svc-d4tp9 [133.041224ms]
May 17 06:31:40.606: INFO: Created: latency-svc-gj692
May 17 06:31:40.612: INFO: Got endpoints: latency-svc-gj692 [134.02897ms]
May 17 06:31:40.616: INFO: Created: latency-svc-d54gl
May 17 06:31:40.622: INFO: Got endpoints: latency-svc-d54gl [135.543658ms]
May 17 06:31:40.623: INFO: Created: latency-svc-r85t7
May 17 06:31:40.629: INFO: Got endpoints: latency-svc-r85t7 [134.277257ms]
May 17 06:31:40.633: INFO: Created: latency-svc-765jm
May 17 06:31:40.638: INFO: Got endpoints: latency-svc-765jm [130.774433ms]
May 17 06:31:40.643: INFO: Created: latency-svc-g47v7
May 17 06:31:40.653: INFO: Got endpoints: latency-svc-g47v7 [137.347543ms]
May 17 06:31:40.657: INFO: Created: latency-svc-b28ng
May 17 06:31:40.664: INFO: Got endpoints: latency-svc-b28ng [140.029727ms]
May 17 06:31:40.666: INFO: Created: latency-svc-pdd5k
May 17 06:31:40.671: INFO: Got endpoints: latency-svc-pdd5k [138.17969ms]
May 17 06:31:40.674: INFO: Created: latency-svc-mjvxc
May 17 06:31:40.680: INFO: Got endpoints: latency-svc-mjvxc [139.725029ms]
May 17 06:31:40.683: INFO: Created: latency-svc-cqvdg
May 17 06:31:40.688: INFO: Got endpoints: latency-svc-cqvdg [140.30315ms]
May 17 06:31:40.693: INFO: Created: latency-svc-khll7
May 17 06:31:40.698: INFO: Got endpoints: latency-svc-khll7 [139.889087ms]
May 17 06:31:40.700: INFO: Created: latency-svc-fmlfn
May 17 06:31:40.706: INFO: Got endpoints: latency-svc-fmlfn [138.47566ms]
May 17 06:31:40.709: INFO: Created: latency-svc-mz8c6
May 17 06:31:40.716: INFO: Got endpoints: latency-svc-mz8c6 [141.077929ms]
May 17 06:31:40.717: INFO: Created: latency-svc-948zc
May 17 06:31:40.722: INFO: Got endpoints: latency-svc-948zc [138.639482ms]
May 17 06:31:40.726: INFO: Created: latency-svc-gp9lm
May 17 06:31:40.730: INFO: Got endpoints: latency-svc-gp9lm [137.334111ms]
May 17 06:31:40.737: INFO: Created: latency-svc-l4f4k
May 17 06:31:40.743: INFO: Created: latency-svc-zch9v
May 17 06:31:40.744: INFO: Got endpoints: latency-svc-l4f4k [142.753022ms]
May 17 06:31:40.748: INFO: Got endpoints: latency-svc-zch9v [136.058051ms]
May 17 06:31:40.751: INFO: Created: latency-svc-s6kn6
May 17 06:31:40.759: INFO: Got endpoints: latency-svc-s6kn6 [137.588735ms]
May 17 06:31:40.761: INFO: Created: latency-svc-9ghnv
May 17 06:31:40.768: INFO: Got endpoints: latency-svc-9ghnv [138.467887ms]
May 17 06:31:40.770: INFO: Created: latency-svc-j77wb
May 17 06:31:40.777: INFO: Got endpoints: latency-svc-j77wb [138.332005ms]
May 17 06:31:40.779: INFO: Created: latency-svc-9g8v4
May 17 06:31:40.787: INFO: Got endpoints: latency-svc-9g8v4 [134.133813ms]
May 17 06:31:40.788: INFO: Created: latency-svc-wjp4d
May 17 06:31:40.793: INFO: Got endpoints: latency-svc-wjp4d [129.095419ms]
May 17 06:31:40.798: INFO: Created: latency-svc-bh9f6
May 17 06:31:40.804: INFO: Got endpoints: latency-svc-bh9f6 [133.383872ms]
May 17 06:31:40.807: INFO: Created: latency-svc-llwp4
May 17 06:31:40.814: INFO: Got endpoints: latency-svc-llwp4 [133.879928ms]
May 17 06:31:40.820: INFO: Created: latency-svc-vljlc
May 17 06:31:40.824: INFO: Got endpoints: latency-svc-vljlc [136.36245ms]
May 17 06:31:40.827: INFO: Created: latency-svc-ldgq9
May 17 06:31:40.834: INFO: Got endpoints: latency-svc-ldgq9 [135.864009ms]
May 17 06:31:40.836: INFO: Created: latency-svc-c5rql
May 17 06:31:40.842: INFO: Got endpoints: latency-svc-c5rql [135.631279ms]
May 17 06:31:40.846: INFO: Created: latency-svc-2cs2m
May 17 06:31:40.851: INFO: Got endpoints: latency-svc-2cs2m [134.683011ms]
May 17 06:31:40.854: INFO: Created: latency-svc-9hd2k
May 17 06:31:40.860: INFO: Got endpoints: latency-svc-9hd2k [138.097335ms]
May 17 06:31:40.861: INFO: Created: latency-svc-hhl47
May 17 06:31:40.868: INFO: Got endpoints: latency-svc-hhl47 [137.834785ms]
May 17 06:31:40.870: INFO: Created: latency-svc-74jl7
May 17 06:31:40.877: INFO: Got endpoints: latency-svc-74jl7 [132.784079ms]
May 17 06:31:40.878: INFO: Created: latency-svc-r92d5
May 17 06:31:40.885: INFO: Got endpoints: latency-svc-r92d5 [137.672122ms]
May 17 06:31:40.887: INFO: Created: latency-svc-tnzxx
May 17 06:31:40.893: INFO: Got endpoints: latency-svc-tnzxx [133.356329ms]
May 17 06:31:40.895: INFO: Created: latency-svc-dmrp2
May 17 06:31:40.900: INFO: Got endpoints: latency-svc-dmrp2 [132.252654ms]
May 17 06:31:40.904: INFO: Created: latency-svc-d7r5k
May 17 06:31:40.909: INFO: Got endpoints: latency-svc-d7r5k [132.404519ms]
May 17 06:31:40.912: INFO: Created: latency-svc-m7wxg
May 17 06:31:40.917: INFO: Got endpoints: latency-svc-m7wxg [129.62429ms]
May 17 06:31:40.920: INFO: Created: latency-svc-llprh
May 17 06:31:40.925: INFO: Got endpoints: latency-svc-llprh [131.981002ms]
May 17 06:31:40.928: INFO: Created: latency-svc-rqn54
May 17 06:31:40.933: INFO: Got endpoints: latency-svc-rqn54 [129.323638ms]
May 17 06:31:40.936: INFO: Created: latency-svc-4xhdb
May 17 06:31:40.942: INFO: Got endpoints: latency-svc-4xhdb [127.484238ms]
May 17 06:31:40.945: INFO: Created: latency-svc-t5vfb
May 17 06:31:40.952: INFO: Got endpoints: latency-svc-t5vfb [128.064773ms]
May 17 06:31:40.961: INFO: Created: latency-svc-fjz6w
May 17 06:31:40.968: INFO: Got endpoints: latency-svc-fjz6w [133.859336ms]
May 17 06:31:40.968: INFO: Created: latency-svc-xw2fc
May 17 06:31:40.976: INFO: Got endpoints: latency-svc-xw2fc [134.366031ms]
May 17 06:31:40.980: INFO: Created: latency-svc-d2pd5
May 17 06:31:40.985: INFO: Got endpoints: latency-svc-d2pd5 [134.237653ms]
May 17 06:31:40.989: INFO: Created: latency-svc-j7pxg
May 17 06:31:40.994: INFO: Got endpoints: latency-svc-j7pxg [133.721116ms]
May 17 06:31:40.997: INFO: Created: latency-svc-hxcl6
May 17 06:31:41.002: INFO: Got endpoints: latency-svc-hxcl6 [133.755611ms]
May 17 06:31:41.005: INFO: Created: latency-svc-j5dd9
May 17 06:31:41.011: INFO: Got endpoints: latency-svc-j5dd9 [134.119685ms]
May 17 06:31:41.013: INFO: Created: latency-svc-4trd5
May 17 06:31:41.017: INFO: Got endpoints: latency-svc-4trd5 [131.797373ms]
May 17 06:31:41.020: INFO: Created: latency-svc-4rpvq
May 17 06:31:41.025: INFO: Got endpoints: latency-svc-4rpvq [132.274297ms]
May 17 06:31:41.028: INFO: Created: latency-svc-xd9xx
May 17 06:31:41.033: INFO: Got endpoints: latency-svc-xd9xx [133.436325ms]
May 17 06:31:41.035: INFO: Created: latency-svc-qw4ql
May 17 06:31:41.040: INFO: Got endpoints: latency-svc-qw4ql [131.170054ms]
May 17 06:31:41.043: INFO: Created: latency-svc-7lvlh
May 17 06:31:41.048: INFO: Got endpoints: latency-svc-7lvlh [131.799041ms]
May 17 06:31:41.051: INFO: Created: latency-svc-hfzzz
May 17 06:31:41.058: INFO: Got endpoints: latency-svc-hfzzz [132.356566ms]
May 17 06:31:41.059: INFO: Created: latency-svc-5tqd6
May 17 06:31:41.066: INFO: Got endpoints: latency-svc-5tqd6 [132.543715ms]
May 17 06:31:41.069: INFO: Created: latency-svc-n5lz8
May 17 06:31:41.085: INFO: Got endpoints: latency-svc-n5lz8 [143.162226ms]
May 17 06:31:41.085: INFO: Created: latency-svc-ptvj8
May 17 06:31:41.099: INFO: Got endpoints: latency-svc-ptvj8 [146.238988ms]
May 17 06:31:41.101: INFO: Created: latency-svc-v9zkm
May 17 06:31:41.106: INFO: Got endpoints: latency-svc-v9zkm [138.412095ms]
May 17 06:31:41.110: INFO: Created: latency-svc-cspvs
May 17 06:31:41.120: INFO: Got endpoints: latency-svc-cspvs [143.435017ms]
May 17 06:31:41.120: INFO: Created: latency-svc-llp6r
May 17 06:31:41.126: INFO: Got endpoints: latency-svc-llp6r [140.834866ms]
May 17 06:31:41.130: INFO: Created: latency-svc-nbv2c
May 17 06:31:41.135: INFO: Got endpoints: latency-svc-nbv2c [140.712864ms]
May 17 06:31:41.139: INFO: Created: latency-svc-pv2vx
May 17 06:31:41.145: INFO: Got endpoints: latency-svc-pv2vx [143.021241ms]
May 17 06:31:41.151: INFO: Created: latency-svc-xbxxh
May 17 06:31:41.155: INFO: Got endpoints: latency-svc-xbxxh [144.114075ms]
May 17 06:31:41.160: INFO: Created: latency-svc-wxx57
May 17 06:31:41.165: INFO: Got endpoints: latency-svc-wxx57 [148.166561ms]
May 17 06:31:41.169: INFO: Created: latency-svc-czjxs
May 17 06:31:41.174: INFO: Got endpoints: latency-svc-czjxs [148.842345ms]
May 17 06:31:41.178: INFO: Created: latency-svc-q5wcs
May 17 06:31:41.185: INFO: Got endpoints: latency-svc-q5wcs [151.375901ms]
May 17 06:31:41.189: INFO: Created: latency-svc-jmwr2
May 17 06:31:41.194: INFO: Got endpoints: latency-svc-jmwr2 [153.91544ms]
May 17 06:31:41.198: INFO: Created: latency-svc-wvmtn
May 17 06:31:41.204: INFO: Got endpoints: latency-svc-wvmtn [155.036855ms]
May 17 06:31:41.208: INFO: Created: latency-svc-h9x6f
May 17 06:31:41.216: INFO: Got endpoints: latency-svc-h9x6f [157.721404ms]
May 17 06:31:41.218: INFO: Created: latency-svc-qd68f
May 17 06:31:41.225: INFO: Got endpoints: latency-svc-qd68f [159.031442ms]
May 17 06:31:41.226: INFO: Created: latency-svc-7rxtj
May 17 06:31:41.232: INFO: Got endpoints: latency-svc-7rxtj [146.824281ms]
May 17 06:31:41.235: INFO: Created: latency-svc-whlz5
May 17 06:31:41.239: INFO: Got endpoints: latency-svc-whlz5 [140.932076ms]
May 17 06:31:41.243: INFO: Created: latency-svc-z2dp2
May 17 06:31:41.248: INFO: Got endpoints: latency-svc-z2dp2 [141.981877ms]
May 17 06:31:41.252: INFO: Created: latency-svc-b7qcl
May 17 06:31:41.257: INFO: Got endpoints: latency-svc-b7qcl [137.140527ms]
May 17 06:31:41.261: INFO: Created: latency-svc-4jgkm
May 17 06:31:41.267: INFO: Got endpoints: latency-svc-4jgkm [140.194056ms]
May 17 06:31:41.270: INFO: Created: latency-svc-dh48n
May 17 06:31:41.275: INFO: Got endpoints: latency-svc-dh48n [140.170983ms]
May 17 06:31:41.278: INFO: Created: latency-svc-kcj5d
May 17 06:31:41.284: INFO: Got endpoints: latency-svc-kcj5d [139.213651ms]
May 17 06:31:41.287: INFO: Created: latency-svc-szb8l
May 17 06:31:41.292: INFO: Got endpoints: latency-svc-szb8l [136.978743ms]
May 17 06:31:41.296: INFO: Created: latency-svc-wh6nd
May 17 06:31:41.301: INFO: Got endpoints: latency-svc-wh6nd [135.94426ms]
May 17 06:31:41.308: INFO: Created: latency-svc-xzqb8
May 17 06:31:41.334: INFO: Got endpoints: latency-svc-xzqb8 [160.112923ms]
May 17 06:31:41.336: INFO: Created: latency-svc-2nmcr
May 17 06:31:41.342: INFO: Got endpoints: latency-svc-2nmcr [157.637126ms]
May 17 06:31:41.346: INFO: Created: latency-svc-g86pm
May 17 06:31:41.352: INFO: Got endpoints: latency-svc-g86pm [157.393738ms]
May 17 06:31:41.357: INFO: Created: latency-svc-64dmf
May 17 06:31:41.362: INFO: Got endpoints: latency-svc-64dmf [158.893564ms]
May 17 06:31:41.365: INFO: Created: latency-svc-58gl4
May 17 06:31:41.370: INFO: Got endpoints: latency-svc-58gl4 [154.375616ms]
May 17 06:31:41.376: INFO: Created: latency-svc-fcgkt
May 17 06:31:41.381: INFO: Got endpoints: latency-svc-fcgkt [155.804697ms]
May 17 06:31:41.386: INFO: Created: latency-svc-qhlsd
May 17 06:31:41.393: INFO: Got endpoints: latency-svc-qhlsd [161.407673ms]
May 17 06:31:41.396: INFO: Created: latency-svc-4s6jm
May 17 06:31:41.404: INFO: Got endpoints: latency-svc-4s6jm [164.601705ms]
May 17 06:31:41.406: INFO: Created: latency-svc-kkhdt
May 17 06:31:41.412: INFO: Got endpoints: latency-svc-kkhdt [163.386103ms]
May 17 06:31:41.421: INFO: Created: latency-svc-bjstf
May 17 06:31:41.425: INFO: Got endpoints: latency-svc-bjstf [167.907726ms]
May 17 06:31:41.426: INFO: Created: latency-svc-27c52
May 17 06:31:41.432: INFO: Got endpoints: latency-svc-27c52 [165.827244ms]
May 17 06:31:41.434: INFO: Created: latency-svc-tbzfc
May 17 06:31:41.443: INFO: Got endpoints: latency-svc-tbzfc [167.722311ms]
May 17 06:31:41.447: INFO: Created: latency-svc-chb8x
May 17 06:31:41.454: INFO: Got endpoints: latency-svc-chb8x [169.902607ms]
May 17 06:31:41.460: INFO: Created: latency-svc-g4nhs
May 17 06:31:41.466: INFO: Got endpoints: latency-svc-g4nhs [173.441443ms]
May 17 06:31:41.468: INFO: Created: latency-svc-6fl5b
May 17 06:31:41.472: INFO: Got endpoints: latency-svc-6fl5b [170.91013ms]
May 17 06:31:41.475: INFO: Created: latency-svc-m76ww
May 17 06:31:41.482: INFO: Got endpoints: latency-svc-m76ww [147.73254ms]
May 17 06:31:41.486: INFO: Created: latency-svc-qpflh
May 17 06:31:41.493: INFO: Got endpoints: latency-svc-qpflh [150.204869ms]
May 17 06:31:41.494: INFO: Created: latency-svc-xd6qn
May 17 06:31:41.501: INFO: Got endpoints: latency-svc-xd6qn [149.73233ms]
May 17 06:31:41.503: INFO: Created: latency-svc-sj9pj
May 17 06:31:41.510: INFO: Got endpoints: latency-svc-sj9pj [147.294118ms]
May 17 06:31:41.514: INFO: Created: latency-svc-b65pz
May 17 06:31:41.520: INFO: Got endpoints: latency-svc-b65pz [149.967118ms]
May 17 06:31:41.521: INFO: Created: latency-svc-l7qxd
May 17 06:31:41.527: INFO: Got endpoints: latency-svc-l7qxd [145.953947ms]
May 17 06:31:41.530: INFO: Created: latency-svc-wd5lr
May 17 06:31:41.536: INFO: Got endpoints: latency-svc-wd5lr [143.054946ms]
May 17 06:31:41.540: INFO: Created: latency-svc-jx6hw
May 17 06:31:41.545: INFO: Got endpoints: latency-svc-jx6hw [140.772328ms]
May 17 06:31:41.547: INFO: Created: latency-svc-m8ltf
May 17 06:31:41.553: INFO: Got endpoints: latency-svc-m8ltf [140.911497ms]
May 17 06:31:41.557: INFO: Created: latency-svc-s5gr4
May 17 06:31:41.562: INFO: Got endpoints: latency-svc-s5gr4 [137.282423ms]
May 17 06:31:41.566: INFO: Created: latency-svc-qrjgw
May 17 06:31:41.571: INFO: Got endpoints: latency-svc-qrjgw [138.197585ms]
May 17 06:31:41.574: INFO: Created: latency-svc-j957l
May 17 06:31:41.581: INFO: Got endpoints: latency-svc-j957l [138.338379ms]
May 17 06:31:41.583: INFO: Created: latency-svc-gsvjf
May 17 06:31:41.588: INFO: Got endpoints: latency-svc-gsvjf [133.735605ms]
May 17 06:31:41.592: INFO: Created: latency-svc-km59k
May 17 06:31:41.596: INFO: Got endpoints: latency-svc-km59k [130.689608ms]
May 17 06:31:41.599: INFO: Created: latency-svc-g8c8w
May 17 06:31:41.604: INFO: Got endpoints: latency-svc-g8c8w [132.134008ms]
May 17 06:31:41.608: INFO: Created: latency-svc-tzbss
May 17 06:31:41.613: INFO: Got endpoints: latency-svc-tzbss [131.532185ms]
May 17 06:31:41.615: INFO: Created: latency-svc-bq5dp
May 17 06:31:41.620: INFO: Got endpoints: latency-svc-bq5dp [127.260066ms]
May 17 06:31:41.624: INFO: Created: latency-svc-8zsdm
May 17 06:31:41.629: INFO: Got endpoints: latency-svc-8zsdm [127.616141ms]
May 17 06:31:41.632: INFO: Created: latency-svc-pwhgg
May 17 06:31:41.637: INFO: Got endpoints: latency-svc-pwhgg [127.193672ms]
May 17 06:31:41.640: INFO: Created: latency-svc-hhz5m
May 17 06:31:41.647: INFO: Got endpoints: latency-svc-hhz5m [127.248935ms]
May 17 06:31:41.649: INFO: Created: latency-svc-brs4z
May 17 06:31:41.654: INFO: Got endpoints: latency-svc-brs4z [127.015026ms]
May 17 06:31:41.657: INFO: Created: latency-svc-5kkl6
May 17 06:31:41.662: INFO: Got endpoints: latency-svc-5kkl6 [126.248352ms]
May 17 06:31:41.666: INFO: Created: latency-svc-wjzvn
May 17 06:31:41.671: INFO: Got endpoints: latency-svc-wjzvn [126.473055ms]
May 17 06:31:41.673: INFO: Created: latency-svc-76srp
May 17 06:31:41.678: INFO: Got endpoints: latency-svc-76srp [125.316722ms]
May 17 06:31:41.681: INFO: Created: latency-svc-w9hzb
May 17 06:31:41.686: INFO: Got endpoints: latency-svc-w9hzb [124.319115ms]
May 17 06:31:41.690: INFO: Created: latency-svc-jcdwc
May 17 06:31:41.695: INFO: Got endpoints: latency-svc-jcdwc [124.1385ms]
May 17 06:31:41.698: INFO: Created: latency-svc-xvhx8
May 17 06:31:41.703: INFO: Got endpoints: latency-svc-xvhx8 [122.32096ms]
May 17 06:31:41.706: INFO: Created: latency-svc-d2llg
May 17 06:31:41.712: INFO: Got endpoints: latency-svc-d2llg [124.659972ms]
May 17 06:31:41.714: INFO: Created: latency-svc-455f8
May 17 06:31:41.721: INFO: Got endpoints: latency-svc-455f8 [125.216001ms]
May 17 06:31:41.723: INFO: Created: latency-svc-5xdr5
May 17 06:31:41.728: INFO: Got endpoints: latency-svc-5xdr5 [123.50274ms]
May 17 06:31:41.731: INFO: Created: latency-svc-k9jvn
May 17 06:31:41.737: INFO: Got endpoints: latency-svc-k9jvn [123.333855ms]
May 17 06:31:41.739: INFO: Created: latency-svc-gl95z
May 17 06:31:41.746: INFO: Got endpoints: latency-svc-gl95z [126.408345ms]
May 17 06:31:41.747: INFO: Created: latency-svc-zjxwg
May 17 06:31:41.752: INFO: Got endpoints: latency-svc-zjxwg [123.360044ms]
May 17 06:31:41.755: INFO: Created: latency-svc-qljkh
May 17 06:31:41.760: INFO: Got endpoints: latency-svc-qljkh [123.38886ms]
May 17 06:31:41.764: INFO: Created: latency-svc-jfvh6
May 17 06:31:41.770: INFO: Got endpoints: latency-svc-jfvh6 [122.915124ms]
May 17 06:31:41.772: INFO: Created: latency-svc-5dfwt
May 17 06:31:41.776: INFO: Got endpoints: latency-svc-5dfwt [122.577131ms]
May 17 06:31:41.780: INFO: Created: latency-svc-g4sgl
May 17 06:31:41.787: INFO: Got endpoints: latency-svc-g4sgl [124.293102ms]
May 17 06:31:41.788: INFO: Created: latency-svc-gxfrs
May 17 06:31:41.794: INFO: Got endpoints: latency-svc-gxfrs [122.7326ms]
May 17 06:31:41.797: INFO: Created: latency-svc-9zx6s
May 17 06:31:41.802: INFO: Got endpoints: latency-svc-9zx6s [124.075063ms]
May 17 06:31:41.805: INFO: Created: latency-svc-ccpv7
May 17 06:31:41.810: INFO: Got endpoints: latency-svc-ccpv7 [123.467716ms]
May 17 06:31:41.810: INFO: Latencies: [20.728693ms 28.99166ms 36.460864ms 44.270805ms 58.705831ms 59.041057ms 66.897501ms 105.469247ms 105.505976ms 112.479783ms 114.757172ms 117.910125ms 122.32096ms 122.577131ms 122.7326ms 122.915124ms 122.958266ms 123.333855ms 123.360044ms 123.38886ms 123.467716ms 123.50274ms 124.075063ms 124.1385ms 124.293102ms 124.319115ms 124.415538ms 124.659972ms 125.216001ms 125.316722ms 125.711999ms 125.736948ms 126.248352ms 126.408345ms 126.473055ms 126.620577ms 127.015026ms 127.0978ms 127.193672ms 127.248935ms 127.260066ms 127.484238ms 127.616141ms 128.064773ms 129.095419ms 129.323638ms 129.56242ms 129.62429ms 129.741101ms 130.261396ms 130.672334ms 130.689608ms 130.774433ms 130.877609ms 131.053961ms 131.170054ms 131.324852ms 131.532185ms 131.594198ms 131.797373ms 131.799041ms 131.803111ms 131.809583ms 131.838584ms 131.981002ms 131.995403ms 132.134008ms 132.252654ms 132.274297ms 132.318791ms 132.327675ms 132.356566ms 132.404519ms 132.474687ms 132.543715ms 132.784079ms 132.895223ms 133.041224ms 133.356329ms 133.383872ms 133.436325ms 133.721116ms 133.735605ms 133.755611ms 133.859336ms 133.879928ms 134.02897ms 134.119685ms 134.133813ms 134.237653ms 134.277257ms 134.366031ms 134.683011ms 135.387465ms 135.543658ms 135.631279ms 135.864009ms 135.94426ms 136.058051ms 136.287776ms 136.36245ms 136.790994ms 136.978743ms 137.140527ms 137.282423ms 137.334111ms 137.347543ms 137.588735ms 137.672122ms 137.834785ms 138.004459ms 138.097335ms 138.17969ms 138.197585ms 138.332005ms 138.338379ms 138.412095ms 138.467887ms 138.47566ms 138.639482ms 139.154812ms 139.213651ms 139.725029ms 139.889087ms 140.029727ms 140.170983ms 140.194056ms 140.30315ms 140.712864ms 140.772328ms 140.834866ms 140.911497ms 140.932076ms 141.077929ms 141.578693ms 141.981877ms 142.753022ms 143.021241ms 143.054946ms 143.162226ms 143.435017ms 144.114075ms 144.16475ms 144.265386ms 144.518611ms 145.008957ms 145.309219ms 145.312985ms 145.953947ms 146.238988ms 146.823739ms 146.824281ms 147.294118ms 147.73254ms 147.765987ms 148.166561ms 148.283378ms 148.686021ms 148.842345ms 149.068105ms 149.248851ms 149.73233ms 149.967118ms 150.204869ms 151.035005ms 151.134697ms 151.375901ms 151.399454ms 153.283169ms 153.494775ms 153.91544ms 154.375616ms 155.036855ms 155.650244ms 155.804697ms 157.393738ms 157.531529ms 157.637126ms 157.721404ms 158.893564ms 159.031442ms 160.112923ms 160.387315ms 160.388327ms 160.563482ms 161.407673ms 162.95645ms 163.386103ms 163.386906ms 163.429322ms 164.357751ms 164.601705ms 165.5844ms 165.827244ms 167.677465ms 167.722311ms 167.907726ms 169.902607ms 170.91013ms 173.441443ms]
May 17 06:31:41.810: INFO: 50 %ile: 136.36245ms
May 17 06:31:41.810: INFO: 90 %ile: 159.031442ms
May 17 06:31:41.810: INFO: 99 %ile: 170.91013ms
May 17 06:31:41.810: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
May 17 06:31:41.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3612" for this suite. 05/17/23 06:31:41.825
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":210,"skipped":4183,"failed":0}
------------------------------
â€¢ [4.123 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:31:37.713
    May 17 06:31:37.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename svc-latency 05/17/23 06:31:37.713
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:31:37.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:31:37.739
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    May 17 06:31:37.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-3612 05/17/23 06:31:37.744
    I0517 06:31:37.752545      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3612, replica count: 1
    I0517 06:31:38.803527      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0517 06:31:39.804251      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 06:31:39.920: INFO: Created: latency-svc-kqdpv
    May 17 06:31:39.941: INFO: Got endpoints: latency-svc-kqdpv [36.202131ms]
    May 17 06:31:39.956: INFO: Created: latency-svc-8vv4t
    May 17 06:31:39.961: INFO: Got endpoints: latency-svc-8vv4t [20.728693ms]
    May 17 06:31:39.965: INFO: Created: latency-svc-g289m
    May 17 06:31:39.970: INFO: Got endpoints: latency-svc-g289m [28.99166ms]
    May 17 06:31:39.972: INFO: Created: latency-svc-dkdnj
    May 17 06:31:39.977: INFO: Got endpoints: latency-svc-dkdnj [36.460864ms]
    May 17 06:31:39.981: INFO: Created: latency-svc-k6wln
    May 17 06:31:39.985: INFO: Got endpoints: latency-svc-k6wln [44.270805ms]
    May 17 06:31:39.988: INFO: Created: latency-svc-ndzpz
    May 17 06:31:39.995: INFO: Created: latency-svc-rwp5z
    May 17 06:31:40.000: INFO: Got endpoints: latency-svc-ndzpz [58.705831ms]
    May 17 06:31:40.000: INFO: Got endpoints: latency-svc-rwp5z [59.041057ms]
    May 17 06:31:40.003: INFO: Created: latency-svc-ck9w9
    May 17 06:31:40.008: INFO: Got endpoints: latency-svc-ck9w9 [66.897501ms]
    May 17 06:31:40.032: INFO: Created: latency-svc-r4n4s
    May 17 06:31:40.040: INFO: Created: latency-svc-s6257
    May 17 06:31:40.047: INFO: Got endpoints: latency-svc-r4n4s [105.505976ms]
    May 17 06:31:40.047: INFO: Got endpoints: latency-svc-s6257 [105.469247ms]
    May 17 06:31:40.048: INFO: Created: latency-svc-wzcrs
    May 17 06:31:40.054: INFO: Got endpoints: latency-svc-wzcrs [112.479783ms]
    May 17 06:31:40.057: INFO: Created: latency-svc-799cz
    May 17 06:31:40.066: INFO: Got endpoints: latency-svc-799cz [124.415538ms]
    May 17 06:31:40.066: INFO: Created: latency-svc-7xc77
    May 17 06:31:40.086: INFO: Created: latency-svc-ngtt2
    May 17 06:31:40.086: INFO: Created: latency-svc-th5p2
    May 17 06:31:40.086: INFO: Got endpoints: latency-svc-7xc77 [145.309219ms]
    May 17 06:31:40.087: INFO: Got endpoints: latency-svc-ngtt2 [145.312985ms]
    May 17 06:31:40.090: INFO: Got endpoints: latency-svc-th5p2 [149.248851ms]
    May 17 06:31:40.093: INFO: Created: latency-svc-dv9hm
    May 17 06:31:40.097: INFO: Got endpoints: latency-svc-dv9hm [155.650244ms]
    May 17 06:31:40.101: INFO: Created: latency-svc-m7xch
    May 17 06:31:40.106: INFO: Got endpoints: latency-svc-m7xch [144.16475ms]
    May 17 06:31:40.109: INFO: Created: latency-svc-wvrbg
    May 17 06:31:40.115: INFO: Created: latency-svc-vmgdp
    May 17 06:31:40.121: INFO: Got endpoints: latency-svc-wvrbg [151.134697ms]
    May 17 06:31:40.122: INFO: Got endpoints: latency-svc-vmgdp [144.265386ms]
    May 17 06:31:40.124: INFO: Created: latency-svc-hpsft
    May 17 06:31:40.130: INFO: Got endpoints: latency-svc-hpsft [144.518611ms]
    May 17 06:31:40.133: INFO: Created: latency-svc-tzxxh
    May 17 06:31:40.139: INFO: Got endpoints: latency-svc-tzxxh [139.154812ms]
    May 17 06:31:40.141: INFO: Created: latency-svc-w9ktv
    May 17 06:31:40.147: INFO: Got endpoints: latency-svc-w9ktv [146.823739ms]
    May 17 06:31:40.150: INFO: Created: latency-svc-hzzr2
    May 17 06:31:40.157: INFO: Got endpoints: latency-svc-hzzr2 [148.686021ms]
    May 17 06:31:40.158: INFO: Created: latency-svc-4x465
    May 17 06:31:40.165: INFO: Got endpoints: latency-svc-4x465 [117.910125ms]
    May 17 06:31:40.168: INFO: Created: latency-svc-2m9r4
    May 17 06:31:40.173: INFO: Got endpoints: latency-svc-2m9r4 [126.620577ms]
    May 17 06:31:40.176: INFO: Created: latency-svc-t6jtn
    May 17 06:31:40.183: INFO: Got endpoints: latency-svc-t6jtn [129.741101ms]
    May 17 06:31:40.186: INFO: Created: latency-svc-whwd8
    May 17 06:31:40.191: INFO: Got endpoints: latency-svc-whwd8 [125.736948ms]
    May 17 06:31:40.195: INFO: Created: latency-svc-m8zk4
    May 17 06:31:40.201: INFO: Got endpoints: latency-svc-m8zk4 [114.757172ms]
    May 17 06:31:40.203: INFO: Created: latency-svc-vp7j9
    May 17 06:31:40.209: INFO: Got endpoints: latency-svc-vp7j9 [122.958266ms]
    May 17 06:31:40.212: INFO: Created: latency-svc-5cbnq
    May 17 06:31:40.223: INFO: Got endpoints: latency-svc-5cbnq [132.895223ms]
    May 17 06:31:40.223: INFO: Created: latency-svc-pgjcc
    May 17 06:31:40.235: INFO: Got endpoints: latency-svc-pgjcc [138.004459ms]
    May 17 06:31:40.240: INFO: Created: latency-svc-tqgh5
    May 17 06:31:40.247: INFO: Got endpoints: latency-svc-tqgh5 [141.578693ms]
    May 17 06:31:40.251: INFO: Created: latency-svc-grfg6
    May 17 06:31:40.258: INFO: Got endpoints: latency-svc-grfg6 [136.790994ms]
    May 17 06:31:40.261: INFO: Created: latency-svc-lsjtn
    May 17 06:31:40.267: INFO: Got endpoints: latency-svc-lsjtn [145.008957ms]
    May 17 06:31:40.273: INFO: Created: latency-svc-79p2v
    May 17 06:31:40.279: INFO: Got endpoints: latency-svc-79p2v [149.068105ms]
    May 17 06:31:40.284: INFO: Created: latency-svc-4b9tj
    May 17 06:31:40.290: INFO: Got endpoints: latency-svc-4b9tj [151.035005ms]
    May 17 06:31:40.293: INFO: Created: latency-svc-9ntbb
    May 17 06:31:40.307: INFO: Got endpoints: latency-svc-9ntbb [160.387315ms]
    May 17 06:31:40.310: INFO: Created: latency-svc-ln52b
    May 17 06:31:40.317: INFO: Got endpoints: latency-svc-ln52b [160.563482ms]
    May 17 06:31:40.320: INFO: Created: latency-svc-s8fxz
    May 17 06:31:40.328: INFO: Got endpoints: latency-svc-s8fxz [163.429322ms]
    May 17 06:31:40.333: INFO: Created: latency-svc-bxczn
    May 17 06:31:40.338: INFO: Got endpoints: latency-svc-bxczn [164.357751ms]
    May 17 06:31:40.341: INFO: Created: latency-svc-6htsj
    May 17 06:31:40.347: INFO: Got endpoints: latency-svc-6htsj [163.386906ms]
    May 17 06:31:40.349: INFO: Created: latency-svc-d82p8
    May 17 06:31:40.354: INFO: Got endpoints: latency-svc-d82p8 [162.95645ms]
    May 17 06:31:40.357: INFO: Created: latency-svc-p75d6
    May 17 06:31:40.369: INFO: Got endpoints: latency-svc-p75d6 [167.677465ms]
    May 17 06:31:40.370: INFO: Created: latency-svc-wlmj4
    May 17 06:31:40.375: INFO: Got endpoints: latency-svc-wlmj4 [165.5844ms]
    May 17 06:31:40.378: INFO: Created: latency-svc-dvwn2
    May 17 06:31:40.383: INFO: Got endpoints: latency-svc-dvwn2 [160.388327ms]
    May 17 06:31:40.387: INFO: Created: latency-svc-vsjds
    May 17 06:31:40.392: INFO: Got endpoints: latency-svc-vsjds [157.531529ms]
    May 17 06:31:40.396: INFO: Created: latency-svc-rcg8n
    May 17 06:31:40.401: INFO: Got endpoints: latency-svc-rcg8n [153.283169ms]
    May 17 06:31:40.404: INFO: Created: latency-svc-9xnhl
    May 17 06:31:40.409: INFO: Got endpoints: latency-svc-9xnhl [151.399454ms]
    May 17 06:31:40.414: INFO: Created: latency-svc-hz9hf
    May 17 06:31:40.420: INFO: Got endpoints: latency-svc-hz9hf [153.494775ms]
    May 17 06:31:40.422: INFO: Created: latency-svc-nms5f
    May 17 06:31:40.427: INFO: Got endpoints: latency-svc-nms5f [147.765987ms]
    May 17 06:31:40.430: INFO: Created: latency-svc-8crcd
    May 17 06:31:40.438: INFO: Got endpoints: latency-svc-8crcd [148.283378ms]
    May 17 06:31:40.438: INFO: Created: latency-svc-8pk6q
    May 17 06:31:40.444: INFO: Got endpoints: latency-svc-8pk6q [136.287776ms]
    May 17 06:31:40.448: INFO: Created: latency-svc-7rkv4
    May 17 06:31:40.453: INFO: Got endpoints: latency-svc-7rkv4 [135.387465ms]
    May 17 06:31:40.455: INFO: Created: latency-svc-vsvsj
    May 17 06:31:40.460: INFO: Got endpoints: latency-svc-vsvsj [132.327675ms]
    May 17 06:31:40.463: INFO: Created: latency-svc-qdbgl
    May 17 06:31:40.468: INFO: Got endpoints: latency-svc-qdbgl [130.261396ms]
    May 17 06:31:40.472: INFO: Created: latency-svc-trgn4
    May 17 06:31:40.478: INFO: Got endpoints: latency-svc-trgn4 [130.877609ms]
    May 17 06:31:40.479: INFO: Created: latency-svc-p456b
    May 17 06:31:40.486: INFO: Got endpoints: latency-svc-p456b [131.803111ms]
    May 17 06:31:40.490: INFO: Created: latency-svc-92qfg
    May 17 06:31:40.495: INFO: Got endpoints: latency-svc-92qfg [125.711999ms]
    May 17 06:31:40.501: INFO: Created: latency-svc-gz8dk
    May 17 06:31:40.508: INFO: Got endpoints: latency-svc-gz8dk [132.474687ms]
    May 17 06:31:40.510: INFO: Created: latency-svc-nslgf
    May 17 06:31:40.515: INFO: Got endpoints: latency-svc-nslgf [131.995403ms]
    May 17 06:31:40.519: INFO: Created: latency-svc-vhms6
    May 17 06:31:40.524: INFO: Got endpoints: latency-svc-vhms6 [131.809583ms]
    May 17 06:31:40.526: INFO: Created: latency-svc-8n4bw
    May 17 06:31:40.532: INFO: Got endpoints: latency-svc-8n4bw [131.838584ms]
    May 17 06:31:40.535: INFO: Created: latency-svc-lk929
    May 17 06:31:40.540: INFO: Got endpoints: latency-svc-lk929 [131.053961ms]
    May 17 06:31:40.542: INFO: Created: latency-svc-4tzws
    May 17 06:31:40.547: INFO: Got endpoints: latency-svc-4tzws [127.0978ms]
    May 17 06:31:40.553: INFO: Created: latency-svc-nhb99
    May 17 06:31:40.558: INFO: Got endpoints: latency-svc-nhb99 [131.324852ms]
    May 17 06:31:40.562: INFO: Created: latency-svc-2wzzc
    May 17 06:31:40.568: INFO: Got endpoints: latency-svc-2wzzc [129.56242ms]
    May 17 06:31:40.569: INFO: Created: latency-svc-bjtsk
    May 17 06:31:40.575: INFO: Got endpoints: latency-svc-bjtsk [131.594198ms]
    May 17 06:31:40.578: INFO: Created: latency-svc-km48r
    May 17 06:31:40.584: INFO: Got endpoints: latency-svc-km48r [130.672334ms]
    May 17 06:31:40.587: INFO: Created: latency-svc-ff6zx
    May 17 06:31:40.593: INFO: Got endpoints: latency-svc-ff6zx [132.318791ms]
    May 17 06:31:40.596: INFO: Created: latency-svc-d4tp9
    May 17 06:31:40.601: INFO: Got endpoints: latency-svc-d4tp9 [133.041224ms]
    May 17 06:31:40.606: INFO: Created: latency-svc-gj692
    May 17 06:31:40.612: INFO: Got endpoints: latency-svc-gj692 [134.02897ms]
    May 17 06:31:40.616: INFO: Created: latency-svc-d54gl
    May 17 06:31:40.622: INFO: Got endpoints: latency-svc-d54gl [135.543658ms]
    May 17 06:31:40.623: INFO: Created: latency-svc-r85t7
    May 17 06:31:40.629: INFO: Got endpoints: latency-svc-r85t7 [134.277257ms]
    May 17 06:31:40.633: INFO: Created: latency-svc-765jm
    May 17 06:31:40.638: INFO: Got endpoints: latency-svc-765jm [130.774433ms]
    May 17 06:31:40.643: INFO: Created: latency-svc-g47v7
    May 17 06:31:40.653: INFO: Got endpoints: latency-svc-g47v7 [137.347543ms]
    May 17 06:31:40.657: INFO: Created: latency-svc-b28ng
    May 17 06:31:40.664: INFO: Got endpoints: latency-svc-b28ng [140.029727ms]
    May 17 06:31:40.666: INFO: Created: latency-svc-pdd5k
    May 17 06:31:40.671: INFO: Got endpoints: latency-svc-pdd5k [138.17969ms]
    May 17 06:31:40.674: INFO: Created: latency-svc-mjvxc
    May 17 06:31:40.680: INFO: Got endpoints: latency-svc-mjvxc [139.725029ms]
    May 17 06:31:40.683: INFO: Created: latency-svc-cqvdg
    May 17 06:31:40.688: INFO: Got endpoints: latency-svc-cqvdg [140.30315ms]
    May 17 06:31:40.693: INFO: Created: latency-svc-khll7
    May 17 06:31:40.698: INFO: Got endpoints: latency-svc-khll7 [139.889087ms]
    May 17 06:31:40.700: INFO: Created: latency-svc-fmlfn
    May 17 06:31:40.706: INFO: Got endpoints: latency-svc-fmlfn [138.47566ms]
    May 17 06:31:40.709: INFO: Created: latency-svc-mz8c6
    May 17 06:31:40.716: INFO: Got endpoints: latency-svc-mz8c6 [141.077929ms]
    May 17 06:31:40.717: INFO: Created: latency-svc-948zc
    May 17 06:31:40.722: INFO: Got endpoints: latency-svc-948zc [138.639482ms]
    May 17 06:31:40.726: INFO: Created: latency-svc-gp9lm
    May 17 06:31:40.730: INFO: Got endpoints: latency-svc-gp9lm [137.334111ms]
    May 17 06:31:40.737: INFO: Created: latency-svc-l4f4k
    May 17 06:31:40.743: INFO: Created: latency-svc-zch9v
    May 17 06:31:40.744: INFO: Got endpoints: latency-svc-l4f4k [142.753022ms]
    May 17 06:31:40.748: INFO: Got endpoints: latency-svc-zch9v [136.058051ms]
    May 17 06:31:40.751: INFO: Created: latency-svc-s6kn6
    May 17 06:31:40.759: INFO: Got endpoints: latency-svc-s6kn6 [137.588735ms]
    May 17 06:31:40.761: INFO: Created: latency-svc-9ghnv
    May 17 06:31:40.768: INFO: Got endpoints: latency-svc-9ghnv [138.467887ms]
    May 17 06:31:40.770: INFO: Created: latency-svc-j77wb
    May 17 06:31:40.777: INFO: Got endpoints: latency-svc-j77wb [138.332005ms]
    May 17 06:31:40.779: INFO: Created: latency-svc-9g8v4
    May 17 06:31:40.787: INFO: Got endpoints: latency-svc-9g8v4 [134.133813ms]
    May 17 06:31:40.788: INFO: Created: latency-svc-wjp4d
    May 17 06:31:40.793: INFO: Got endpoints: latency-svc-wjp4d [129.095419ms]
    May 17 06:31:40.798: INFO: Created: latency-svc-bh9f6
    May 17 06:31:40.804: INFO: Got endpoints: latency-svc-bh9f6 [133.383872ms]
    May 17 06:31:40.807: INFO: Created: latency-svc-llwp4
    May 17 06:31:40.814: INFO: Got endpoints: latency-svc-llwp4 [133.879928ms]
    May 17 06:31:40.820: INFO: Created: latency-svc-vljlc
    May 17 06:31:40.824: INFO: Got endpoints: latency-svc-vljlc [136.36245ms]
    May 17 06:31:40.827: INFO: Created: latency-svc-ldgq9
    May 17 06:31:40.834: INFO: Got endpoints: latency-svc-ldgq9 [135.864009ms]
    May 17 06:31:40.836: INFO: Created: latency-svc-c5rql
    May 17 06:31:40.842: INFO: Got endpoints: latency-svc-c5rql [135.631279ms]
    May 17 06:31:40.846: INFO: Created: latency-svc-2cs2m
    May 17 06:31:40.851: INFO: Got endpoints: latency-svc-2cs2m [134.683011ms]
    May 17 06:31:40.854: INFO: Created: latency-svc-9hd2k
    May 17 06:31:40.860: INFO: Got endpoints: latency-svc-9hd2k [138.097335ms]
    May 17 06:31:40.861: INFO: Created: latency-svc-hhl47
    May 17 06:31:40.868: INFO: Got endpoints: latency-svc-hhl47 [137.834785ms]
    May 17 06:31:40.870: INFO: Created: latency-svc-74jl7
    May 17 06:31:40.877: INFO: Got endpoints: latency-svc-74jl7 [132.784079ms]
    May 17 06:31:40.878: INFO: Created: latency-svc-r92d5
    May 17 06:31:40.885: INFO: Got endpoints: latency-svc-r92d5 [137.672122ms]
    May 17 06:31:40.887: INFO: Created: latency-svc-tnzxx
    May 17 06:31:40.893: INFO: Got endpoints: latency-svc-tnzxx [133.356329ms]
    May 17 06:31:40.895: INFO: Created: latency-svc-dmrp2
    May 17 06:31:40.900: INFO: Got endpoints: latency-svc-dmrp2 [132.252654ms]
    May 17 06:31:40.904: INFO: Created: latency-svc-d7r5k
    May 17 06:31:40.909: INFO: Got endpoints: latency-svc-d7r5k [132.404519ms]
    May 17 06:31:40.912: INFO: Created: latency-svc-m7wxg
    May 17 06:31:40.917: INFO: Got endpoints: latency-svc-m7wxg [129.62429ms]
    May 17 06:31:40.920: INFO: Created: latency-svc-llprh
    May 17 06:31:40.925: INFO: Got endpoints: latency-svc-llprh [131.981002ms]
    May 17 06:31:40.928: INFO: Created: latency-svc-rqn54
    May 17 06:31:40.933: INFO: Got endpoints: latency-svc-rqn54 [129.323638ms]
    May 17 06:31:40.936: INFO: Created: latency-svc-4xhdb
    May 17 06:31:40.942: INFO: Got endpoints: latency-svc-4xhdb [127.484238ms]
    May 17 06:31:40.945: INFO: Created: latency-svc-t5vfb
    May 17 06:31:40.952: INFO: Got endpoints: latency-svc-t5vfb [128.064773ms]
    May 17 06:31:40.961: INFO: Created: latency-svc-fjz6w
    May 17 06:31:40.968: INFO: Got endpoints: latency-svc-fjz6w [133.859336ms]
    May 17 06:31:40.968: INFO: Created: latency-svc-xw2fc
    May 17 06:31:40.976: INFO: Got endpoints: latency-svc-xw2fc [134.366031ms]
    May 17 06:31:40.980: INFO: Created: latency-svc-d2pd5
    May 17 06:31:40.985: INFO: Got endpoints: latency-svc-d2pd5 [134.237653ms]
    May 17 06:31:40.989: INFO: Created: latency-svc-j7pxg
    May 17 06:31:40.994: INFO: Got endpoints: latency-svc-j7pxg [133.721116ms]
    May 17 06:31:40.997: INFO: Created: latency-svc-hxcl6
    May 17 06:31:41.002: INFO: Got endpoints: latency-svc-hxcl6 [133.755611ms]
    May 17 06:31:41.005: INFO: Created: latency-svc-j5dd9
    May 17 06:31:41.011: INFO: Got endpoints: latency-svc-j5dd9 [134.119685ms]
    May 17 06:31:41.013: INFO: Created: latency-svc-4trd5
    May 17 06:31:41.017: INFO: Got endpoints: latency-svc-4trd5 [131.797373ms]
    May 17 06:31:41.020: INFO: Created: latency-svc-4rpvq
    May 17 06:31:41.025: INFO: Got endpoints: latency-svc-4rpvq [132.274297ms]
    May 17 06:31:41.028: INFO: Created: latency-svc-xd9xx
    May 17 06:31:41.033: INFO: Got endpoints: latency-svc-xd9xx [133.436325ms]
    May 17 06:31:41.035: INFO: Created: latency-svc-qw4ql
    May 17 06:31:41.040: INFO: Got endpoints: latency-svc-qw4ql [131.170054ms]
    May 17 06:31:41.043: INFO: Created: latency-svc-7lvlh
    May 17 06:31:41.048: INFO: Got endpoints: latency-svc-7lvlh [131.799041ms]
    May 17 06:31:41.051: INFO: Created: latency-svc-hfzzz
    May 17 06:31:41.058: INFO: Got endpoints: latency-svc-hfzzz [132.356566ms]
    May 17 06:31:41.059: INFO: Created: latency-svc-5tqd6
    May 17 06:31:41.066: INFO: Got endpoints: latency-svc-5tqd6 [132.543715ms]
    May 17 06:31:41.069: INFO: Created: latency-svc-n5lz8
    May 17 06:31:41.085: INFO: Got endpoints: latency-svc-n5lz8 [143.162226ms]
    May 17 06:31:41.085: INFO: Created: latency-svc-ptvj8
    May 17 06:31:41.099: INFO: Got endpoints: latency-svc-ptvj8 [146.238988ms]
    May 17 06:31:41.101: INFO: Created: latency-svc-v9zkm
    May 17 06:31:41.106: INFO: Got endpoints: latency-svc-v9zkm [138.412095ms]
    May 17 06:31:41.110: INFO: Created: latency-svc-cspvs
    May 17 06:31:41.120: INFO: Got endpoints: latency-svc-cspvs [143.435017ms]
    May 17 06:31:41.120: INFO: Created: latency-svc-llp6r
    May 17 06:31:41.126: INFO: Got endpoints: latency-svc-llp6r [140.834866ms]
    May 17 06:31:41.130: INFO: Created: latency-svc-nbv2c
    May 17 06:31:41.135: INFO: Got endpoints: latency-svc-nbv2c [140.712864ms]
    May 17 06:31:41.139: INFO: Created: latency-svc-pv2vx
    May 17 06:31:41.145: INFO: Got endpoints: latency-svc-pv2vx [143.021241ms]
    May 17 06:31:41.151: INFO: Created: latency-svc-xbxxh
    May 17 06:31:41.155: INFO: Got endpoints: latency-svc-xbxxh [144.114075ms]
    May 17 06:31:41.160: INFO: Created: latency-svc-wxx57
    May 17 06:31:41.165: INFO: Got endpoints: latency-svc-wxx57 [148.166561ms]
    May 17 06:31:41.169: INFO: Created: latency-svc-czjxs
    May 17 06:31:41.174: INFO: Got endpoints: latency-svc-czjxs [148.842345ms]
    May 17 06:31:41.178: INFO: Created: latency-svc-q5wcs
    May 17 06:31:41.185: INFO: Got endpoints: latency-svc-q5wcs [151.375901ms]
    May 17 06:31:41.189: INFO: Created: latency-svc-jmwr2
    May 17 06:31:41.194: INFO: Got endpoints: latency-svc-jmwr2 [153.91544ms]
    May 17 06:31:41.198: INFO: Created: latency-svc-wvmtn
    May 17 06:31:41.204: INFO: Got endpoints: latency-svc-wvmtn [155.036855ms]
    May 17 06:31:41.208: INFO: Created: latency-svc-h9x6f
    May 17 06:31:41.216: INFO: Got endpoints: latency-svc-h9x6f [157.721404ms]
    May 17 06:31:41.218: INFO: Created: latency-svc-qd68f
    May 17 06:31:41.225: INFO: Got endpoints: latency-svc-qd68f [159.031442ms]
    May 17 06:31:41.226: INFO: Created: latency-svc-7rxtj
    May 17 06:31:41.232: INFO: Got endpoints: latency-svc-7rxtj [146.824281ms]
    May 17 06:31:41.235: INFO: Created: latency-svc-whlz5
    May 17 06:31:41.239: INFO: Got endpoints: latency-svc-whlz5 [140.932076ms]
    May 17 06:31:41.243: INFO: Created: latency-svc-z2dp2
    May 17 06:31:41.248: INFO: Got endpoints: latency-svc-z2dp2 [141.981877ms]
    May 17 06:31:41.252: INFO: Created: latency-svc-b7qcl
    May 17 06:31:41.257: INFO: Got endpoints: latency-svc-b7qcl [137.140527ms]
    May 17 06:31:41.261: INFO: Created: latency-svc-4jgkm
    May 17 06:31:41.267: INFO: Got endpoints: latency-svc-4jgkm [140.194056ms]
    May 17 06:31:41.270: INFO: Created: latency-svc-dh48n
    May 17 06:31:41.275: INFO: Got endpoints: latency-svc-dh48n [140.170983ms]
    May 17 06:31:41.278: INFO: Created: latency-svc-kcj5d
    May 17 06:31:41.284: INFO: Got endpoints: latency-svc-kcj5d [139.213651ms]
    May 17 06:31:41.287: INFO: Created: latency-svc-szb8l
    May 17 06:31:41.292: INFO: Got endpoints: latency-svc-szb8l [136.978743ms]
    May 17 06:31:41.296: INFO: Created: latency-svc-wh6nd
    May 17 06:31:41.301: INFO: Got endpoints: latency-svc-wh6nd [135.94426ms]
    May 17 06:31:41.308: INFO: Created: latency-svc-xzqb8
    May 17 06:31:41.334: INFO: Got endpoints: latency-svc-xzqb8 [160.112923ms]
    May 17 06:31:41.336: INFO: Created: latency-svc-2nmcr
    May 17 06:31:41.342: INFO: Got endpoints: latency-svc-2nmcr [157.637126ms]
    May 17 06:31:41.346: INFO: Created: latency-svc-g86pm
    May 17 06:31:41.352: INFO: Got endpoints: latency-svc-g86pm [157.393738ms]
    May 17 06:31:41.357: INFO: Created: latency-svc-64dmf
    May 17 06:31:41.362: INFO: Got endpoints: latency-svc-64dmf [158.893564ms]
    May 17 06:31:41.365: INFO: Created: latency-svc-58gl4
    May 17 06:31:41.370: INFO: Got endpoints: latency-svc-58gl4 [154.375616ms]
    May 17 06:31:41.376: INFO: Created: latency-svc-fcgkt
    May 17 06:31:41.381: INFO: Got endpoints: latency-svc-fcgkt [155.804697ms]
    May 17 06:31:41.386: INFO: Created: latency-svc-qhlsd
    May 17 06:31:41.393: INFO: Got endpoints: latency-svc-qhlsd [161.407673ms]
    May 17 06:31:41.396: INFO: Created: latency-svc-4s6jm
    May 17 06:31:41.404: INFO: Got endpoints: latency-svc-4s6jm [164.601705ms]
    May 17 06:31:41.406: INFO: Created: latency-svc-kkhdt
    May 17 06:31:41.412: INFO: Got endpoints: latency-svc-kkhdt [163.386103ms]
    May 17 06:31:41.421: INFO: Created: latency-svc-bjstf
    May 17 06:31:41.425: INFO: Got endpoints: latency-svc-bjstf [167.907726ms]
    May 17 06:31:41.426: INFO: Created: latency-svc-27c52
    May 17 06:31:41.432: INFO: Got endpoints: latency-svc-27c52 [165.827244ms]
    May 17 06:31:41.434: INFO: Created: latency-svc-tbzfc
    May 17 06:31:41.443: INFO: Got endpoints: latency-svc-tbzfc [167.722311ms]
    May 17 06:31:41.447: INFO: Created: latency-svc-chb8x
    May 17 06:31:41.454: INFO: Got endpoints: latency-svc-chb8x [169.902607ms]
    May 17 06:31:41.460: INFO: Created: latency-svc-g4nhs
    May 17 06:31:41.466: INFO: Got endpoints: latency-svc-g4nhs [173.441443ms]
    May 17 06:31:41.468: INFO: Created: latency-svc-6fl5b
    May 17 06:31:41.472: INFO: Got endpoints: latency-svc-6fl5b [170.91013ms]
    May 17 06:31:41.475: INFO: Created: latency-svc-m76ww
    May 17 06:31:41.482: INFO: Got endpoints: latency-svc-m76ww [147.73254ms]
    May 17 06:31:41.486: INFO: Created: latency-svc-qpflh
    May 17 06:31:41.493: INFO: Got endpoints: latency-svc-qpflh [150.204869ms]
    May 17 06:31:41.494: INFO: Created: latency-svc-xd6qn
    May 17 06:31:41.501: INFO: Got endpoints: latency-svc-xd6qn [149.73233ms]
    May 17 06:31:41.503: INFO: Created: latency-svc-sj9pj
    May 17 06:31:41.510: INFO: Got endpoints: latency-svc-sj9pj [147.294118ms]
    May 17 06:31:41.514: INFO: Created: latency-svc-b65pz
    May 17 06:31:41.520: INFO: Got endpoints: latency-svc-b65pz [149.967118ms]
    May 17 06:31:41.521: INFO: Created: latency-svc-l7qxd
    May 17 06:31:41.527: INFO: Got endpoints: latency-svc-l7qxd [145.953947ms]
    May 17 06:31:41.530: INFO: Created: latency-svc-wd5lr
    May 17 06:31:41.536: INFO: Got endpoints: latency-svc-wd5lr [143.054946ms]
    May 17 06:31:41.540: INFO: Created: latency-svc-jx6hw
    May 17 06:31:41.545: INFO: Got endpoints: latency-svc-jx6hw [140.772328ms]
    May 17 06:31:41.547: INFO: Created: latency-svc-m8ltf
    May 17 06:31:41.553: INFO: Got endpoints: latency-svc-m8ltf [140.911497ms]
    May 17 06:31:41.557: INFO: Created: latency-svc-s5gr4
    May 17 06:31:41.562: INFO: Got endpoints: latency-svc-s5gr4 [137.282423ms]
    May 17 06:31:41.566: INFO: Created: latency-svc-qrjgw
    May 17 06:31:41.571: INFO: Got endpoints: latency-svc-qrjgw [138.197585ms]
    May 17 06:31:41.574: INFO: Created: latency-svc-j957l
    May 17 06:31:41.581: INFO: Got endpoints: latency-svc-j957l [138.338379ms]
    May 17 06:31:41.583: INFO: Created: latency-svc-gsvjf
    May 17 06:31:41.588: INFO: Got endpoints: latency-svc-gsvjf [133.735605ms]
    May 17 06:31:41.592: INFO: Created: latency-svc-km59k
    May 17 06:31:41.596: INFO: Got endpoints: latency-svc-km59k [130.689608ms]
    May 17 06:31:41.599: INFO: Created: latency-svc-g8c8w
    May 17 06:31:41.604: INFO: Got endpoints: latency-svc-g8c8w [132.134008ms]
    May 17 06:31:41.608: INFO: Created: latency-svc-tzbss
    May 17 06:31:41.613: INFO: Got endpoints: latency-svc-tzbss [131.532185ms]
    May 17 06:31:41.615: INFO: Created: latency-svc-bq5dp
    May 17 06:31:41.620: INFO: Got endpoints: latency-svc-bq5dp [127.260066ms]
    May 17 06:31:41.624: INFO: Created: latency-svc-8zsdm
    May 17 06:31:41.629: INFO: Got endpoints: latency-svc-8zsdm [127.616141ms]
    May 17 06:31:41.632: INFO: Created: latency-svc-pwhgg
    May 17 06:31:41.637: INFO: Got endpoints: latency-svc-pwhgg [127.193672ms]
    May 17 06:31:41.640: INFO: Created: latency-svc-hhz5m
    May 17 06:31:41.647: INFO: Got endpoints: latency-svc-hhz5m [127.248935ms]
    May 17 06:31:41.649: INFO: Created: latency-svc-brs4z
    May 17 06:31:41.654: INFO: Got endpoints: latency-svc-brs4z [127.015026ms]
    May 17 06:31:41.657: INFO: Created: latency-svc-5kkl6
    May 17 06:31:41.662: INFO: Got endpoints: latency-svc-5kkl6 [126.248352ms]
    May 17 06:31:41.666: INFO: Created: latency-svc-wjzvn
    May 17 06:31:41.671: INFO: Got endpoints: latency-svc-wjzvn [126.473055ms]
    May 17 06:31:41.673: INFO: Created: latency-svc-76srp
    May 17 06:31:41.678: INFO: Got endpoints: latency-svc-76srp [125.316722ms]
    May 17 06:31:41.681: INFO: Created: latency-svc-w9hzb
    May 17 06:31:41.686: INFO: Got endpoints: latency-svc-w9hzb [124.319115ms]
    May 17 06:31:41.690: INFO: Created: latency-svc-jcdwc
    May 17 06:31:41.695: INFO: Got endpoints: latency-svc-jcdwc [124.1385ms]
    May 17 06:31:41.698: INFO: Created: latency-svc-xvhx8
    May 17 06:31:41.703: INFO: Got endpoints: latency-svc-xvhx8 [122.32096ms]
    May 17 06:31:41.706: INFO: Created: latency-svc-d2llg
    May 17 06:31:41.712: INFO: Got endpoints: latency-svc-d2llg [124.659972ms]
    May 17 06:31:41.714: INFO: Created: latency-svc-455f8
    May 17 06:31:41.721: INFO: Got endpoints: latency-svc-455f8 [125.216001ms]
    May 17 06:31:41.723: INFO: Created: latency-svc-5xdr5
    May 17 06:31:41.728: INFO: Got endpoints: latency-svc-5xdr5 [123.50274ms]
    May 17 06:31:41.731: INFO: Created: latency-svc-k9jvn
    May 17 06:31:41.737: INFO: Got endpoints: latency-svc-k9jvn [123.333855ms]
    May 17 06:31:41.739: INFO: Created: latency-svc-gl95z
    May 17 06:31:41.746: INFO: Got endpoints: latency-svc-gl95z [126.408345ms]
    May 17 06:31:41.747: INFO: Created: latency-svc-zjxwg
    May 17 06:31:41.752: INFO: Got endpoints: latency-svc-zjxwg [123.360044ms]
    May 17 06:31:41.755: INFO: Created: latency-svc-qljkh
    May 17 06:31:41.760: INFO: Got endpoints: latency-svc-qljkh [123.38886ms]
    May 17 06:31:41.764: INFO: Created: latency-svc-jfvh6
    May 17 06:31:41.770: INFO: Got endpoints: latency-svc-jfvh6 [122.915124ms]
    May 17 06:31:41.772: INFO: Created: latency-svc-5dfwt
    May 17 06:31:41.776: INFO: Got endpoints: latency-svc-5dfwt [122.577131ms]
    May 17 06:31:41.780: INFO: Created: latency-svc-g4sgl
    May 17 06:31:41.787: INFO: Got endpoints: latency-svc-g4sgl [124.293102ms]
    May 17 06:31:41.788: INFO: Created: latency-svc-gxfrs
    May 17 06:31:41.794: INFO: Got endpoints: latency-svc-gxfrs [122.7326ms]
    May 17 06:31:41.797: INFO: Created: latency-svc-9zx6s
    May 17 06:31:41.802: INFO: Got endpoints: latency-svc-9zx6s [124.075063ms]
    May 17 06:31:41.805: INFO: Created: latency-svc-ccpv7
    May 17 06:31:41.810: INFO: Got endpoints: latency-svc-ccpv7 [123.467716ms]
    May 17 06:31:41.810: INFO: Latencies: [20.728693ms 28.99166ms 36.460864ms 44.270805ms 58.705831ms 59.041057ms 66.897501ms 105.469247ms 105.505976ms 112.479783ms 114.757172ms 117.910125ms 122.32096ms 122.577131ms 122.7326ms 122.915124ms 122.958266ms 123.333855ms 123.360044ms 123.38886ms 123.467716ms 123.50274ms 124.075063ms 124.1385ms 124.293102ms 124.319115ms 124.415538ms 124.659972ms 125.216001ms 125.316722ms 125.711999ms 125.736948ms 126.248352ms 126.408345ms 126.473055ms 126.620577ms 127.015026ms 127.0978ms 127.193672ms 127.248935ms 127.260066ms 127.484238ms 127.616141ms 128.064773ms 129.095419ms 129.323638ms 129.56242ms 129.62429ms 129.741101ms 130.261396ms 130.672334ms 130.689608ms 130.774433ms 130.877609ms 131.053961ms 131.170054ms 131.324852ms 131.532185ms 131.594198ms 131.797373ms 131.799041ms 131.803111ms 131.809583ms 131.838584ms 131.981002ms 131.995403ms 132.134008ms 132.252654ms 132.274297ms 132.318791ms 132.327675ms 132.356566ms 132.404519ms 132.474687ms 132.543715ms 132.784079ms 132.895223ms 133.041224ms 133.356329ms 133.383872ms 133.436325ms 133.721116ms 133.735605ms 133.755611ms 133.859336ms 133.879928ms 134.02897ms 134.119685ms 134.133813ms 134.237653ms 134.277257ms 134.366031ms 134.683011ms 135.387465ms 135.543658ms 135.631279ms 135.864009ms 135.94426ms 136.058051ms 136.287776ms 136.36245ms 136.790994ms 136.978743ms 137.140527ms 137.282423ms 137.334111ms 137.347543ms 137.588735ms 137.672122ms 137.834785ms 138.004459ms 138.097335ms 138.17969ms 138.197585ms 138.332005ms 138.338379ms 138.412095ms 138.467887ms 138.47566ms 138.639482ms 139.154812ms 139.213651ms 139.725029ms 139.889087ms 140.029727ms 140.170983ms 140.194056ms 140.30315ms 140.712864ms 140.772328ms 140.834866ms 140.911497ms 140.932076ms 141.077929ms 141.578693ms 141.981877ms 142.753022ms 143.021241ms 143.054946ms 143.162226ms 143.435017ms 144.114075ms 144.16475ms 144.265386ms 144.518611ms 145.008957ms 145.309219ms 145.312985ms 145.953947ms 146.238988ms 146.823739ms 146.824281ms 147.294118ms 147.73254ms 147.765987ms 148.166561ms 148.283378ms 148.686021ms 148.842345ms 149.068105ms 149.248851ms 149.73233ms 149.967118ms 150.204869ms 151.035005ms 151.134697ms 151.375901ms 151.399454ms 153.283169ms 153.494775ms 153.91544ms 154.375616ms 155.036855ms 155.650244ms 155.804697ms 157.393738ms 157.531529ms 157.637126ms 157.721404ms 158.893564ms 159.031442ms 160.112923ms 160.387315ms 160.388327ms 160.563482ms 161.407673ms 162.95645ms 163.386103ms 163.386906ms 163.429322ms 164.357751ms 164.601705ms 165.5844ms 165.827244ms 167.677465ms 167.722311ms 167.907726ms 169.902607ms 170.91013ms 173.441443ms]
    May 17 06:31:41.810: INFO: 50 %ile: 136.36245ms
    May 17 06:31:41.810: INFO: 90 %ile: 159.031442ms
    May 17 06:31:41.810: INFO: 99 %ile: 170.91013ms
    May 17 06:31:41.810: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    May 17 06:31:41.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-3612" for this suite. 05/17/23 06:31:41.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:31:41.838
May 17 06:31:41.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-probe 05/17/23 06:31:41.838
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:31:41.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:31:41.864
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a in namespace container-probe-9002 05/17/23 06:31:41.868
May 17 06:31:41.887: INFO: Waiting up to 5m0s for pod "busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a" in namespace "container-probe-9002" to be "not pending"
May 17 06:31:41.893: INFO: Pod "busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.944142ms
May 17 06:31:43.900: INFO: Pod "busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a": Phase="Running", Reason="", readiness=true. Elapsed: 2.012915322s
May 17 06:31:43.900: INFO: Pod "busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a" satisfied condition "not pending"
May 17 06:31:43.900: INFO: Started pod busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a in namespace container-probe-9002
STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 06:31:43.9
May 17 06:31:43.906: INFO: Initial restart count of pod busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a is 0
May 17 06:32:34.135: INFO: Restart count of pod container-probe-9002/busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a is now 1 (50.229579419s elapsed)
STEP: deleting the pod 05/17/23 06:32:34.135
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May 17 06:32:34.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9002" for this suite. 05/17/23 06:32:34.164
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":211,"skipped":4228,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.336 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:31:41.838
    May 17 06:31:41.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-probe 05/17/23 06:31:41.838
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:31:41.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:31:41.864
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a in namespace container-probe-9002 05/17/23 06:31:41.868
    May 17 06:31:41.887: INFO: Waiting up to 5m0s for pod "busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a" in namespace "container-probe-9002" to be "not pending"
    May 17 06:31:41.893: INFO: Pod "busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.944142ms
    May 17 06:31:43.900: INFO: Pod "busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a": Phase="Running", Reason="", readiness=true. Elapsed: 2.012915322s
    May 17 06:31:43.900: INFO: Pod "busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a" satisfied condition "not pending"
    May 17 06:31:43.900: INFO: Started pod busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a in namespace container-probe-9002
    STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 06:31:43.9
    May 17 06:31:43.906: INFO: Initial restart count of pod busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a is 0
    May 17 06:32:34.135: INFO: Restart count of pod container-probe-9002/busybox-f34dd77e-b2a9-4678-baf0-50fdcf4b629a is now 1 (50.229579419s elapsed)
    STEP: deleting the pod 05/17/23 06:32:34.135
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May 17 06:32:34.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9002" for this suite. 05/17/23 06:32:34.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:32:34.175
May 17 06:32:34.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename watch 05/17/23 06:32:34.175
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:32:34.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:32:34.203
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 05/17/23 06:32:34.208
STEP: modifying the configmap once 05/17/23 06:32:34.219
STEP: modifying the configmap a second time 05/17/23 06:32:34.232
STEP: deleting the configmap 05/17/23 06:32:34.244
STEP: creating a watch on configmaps from the resource version returned by the first update 05/17/23 06:32:34.257
STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/17/23 06:32:34.259
May 17 06:32:34.259: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1164  bc0f5140-d389-4eae-a154-480fff2c4cd7 1474725 0 2023-05-17 06:32:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-17 06:32:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 06:32:34.260: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1164  bc0f5140-d389-4eae-a154-480fff2c4cd7 1474726 0 2023-05-17 06:32:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-17 06:32:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May 17 06:32:34.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1164" for this suite. 05/17/23 06:32:34.268
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":212,"skipped":4239,"failed":0}
------------------------------
â€¢ [0.105 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:32:34.175
    May 17 06:32:34.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename watch 05/17/23 06:32:34.175
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:32:34.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:32:34.203
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 05/17/23 06:32:34.208
    STEP: modifying the configmap once 05/17/23 06:32:34.219
    STEP: modifying the configmap a second time 05/17/23 06:32:34.232
    STEP: deleting the configmap 05/17/23 06:32:34.244
    STEP: creating a watch on configmaps from the resource version returned by the first update 05/17/23 06:32:34.257
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/17/23 06:32:34.259
    May 17 06:32:34.259: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1164  bc0f5140-d389-4eae-a154-480fff2c4cd7 1474725 0 2023-05-17 06:32:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-17 06:32:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 06:32:34.260: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1164  bc0f5140-d389-4eae-a154-480fff2c4cd7 1474726 0 2023-05-17 06:32:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-17 06:32:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May 17 06:32:34.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1164" for this suite. 05/17/23 06:32:34.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:32:34.28
May 17 06:32:34.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:32:34.28
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:32:34.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:32:34.303
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 05/17/23 06:32:34.307
May 17 06:32:34.322: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d" in namespace "projected-546" to be "Succeeded or Failed"
May 17 06:32:34.327: INFO: Pod "downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.293193ms
May 17 06:32:36.334: INFO: Pod "downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012490041s
May 17 06:32:38.334: INFO: Pod "downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012616032s
STEP: Saw pod success 05/17/23 06:32:38.334
May 17 06:32:38.334: INFO: Pod "downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d" satisfied condition "Succeeded or Failed"
May 17 06:32:38.341: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d container client-container: <nil>
STEP: delete the pod 05/17/23 06:32:38.355
May 17 06:32:38.370: INFO: Waiting for pod downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d to disappear
May 17 06:32:38.376: INFO: Pod downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May 17 06:32:38.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-546" for this suite. 05/17/23 06:32:38.386
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":213,"skipped":4245,"failed":0}
------------------------------
â€¢ [4.117 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:32:34.28
    May 17 06:32:34.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:32:34.28
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:32:34.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:32:34.303
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 05/17/23 06:32:34.307
    May 17 06:32:34.322: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d" in namespace "projected-546" to be "Succeeded or Failed"
    May 17 06:32:34.327: INFO: Pod "downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.293193ms
    May 17 06:32:36.334: INFO: Pod "downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012490041s
    May 17 06:32:38.334: INFO: Pod "downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012616032s
    STEP: Saw pod success 05/17/23 06:32:38.334
    May 17 06:32:38.334: INFO: Pod "downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d" satisfied condition "Succeeded or Failed"
    May 17 06:32:38.341: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d container client-container: <nil>
    STEP: delete the pod 05/17/23 06:32:38.355
    May 17 06:32:38.370: INFO: Waiting for pod downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d to disappear
    May 17 06:32:38.376: INFO: Pod downwardapi-volume-44623b38-b1a2-4d3d-8106-7386a982da6d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May 17 06:32:38.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-546" for this suite. 05/17/23 06:32:38.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:32:38.398
May 17 06:32:38.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 06:32:38.399
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:32:38.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:32:38.423
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-a76d7813-2faa-456c-b805-f18b35188777 05/17/23 06:32:38.432
STEP: Creating a pod to test consume secrets 05/17/23 06:32:38.438
May 17 06:32:38.451: INFO: Waiting up to 5m0s for pod "pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145" in namespace "secrets-3856" to be "Succeeded or Failed"
May 17 06:32:38.459: INFO: Pod "pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145": Phase="Pending", Reason="", readiness=false. Elapsed: 7.67718ms
May 17 06:32:40.467: INFO: Pod "pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145": Phase="Running", Reason="", readiness=false. Elapsed: 2.015395093s
May 17 06:32:42.466: INFO: Pod "pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015094342s
STEP: Saw pod success 05/17/23 06:32:42.466
May 17 06:32:42.466: INFO: Pod "pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145" satisfied condition "Succeeded or Failed"
May 17 06:32:42.472: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145 container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 06:32:42.485
May 17 06:32:42.506: INFO: Waiting for pod pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145 to disappear
May 17 06:32:42.512: INFO: Pod pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May 17 06:32:42.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3856" for this suite. 05/17/23 06:32:42.523
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":214,"skipped":4280,"failed":0}
------------------------------
â€¢ [4.137 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:32:38.398
    May 17 06:32:38.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 06:32:38.399
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:32:38.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:32:38.423
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-a76d7813-2faa-456c-b805-f18b35188777 05/17/23 06:32:38.432
    STEP: Creating a pod to test consume secrets 05/17/23 06:32:38.438
    May 17 06:32:38.451: INFO: Waiting up to 5m0s for pod "pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145" in namespace "secrets-3856" to be "Succeeded or Failed"
    May 17 06:32:38.459: INFO: Pod "pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145": Phase="Pending", Reason="", readiness=false. Elapsed: 7.67718ms
    May 17 06:32:40.467: INFO: Pod "pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145": Phase="Running", Reason="", readiness=false. Elapsed: 2.015395093s
    May 17 06:32:42.466: INFO: Pod "pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015094342s
    STEP: Saw pod success 05/17/23 06:32:42.466
    May 17 06:32:42.466: INFO: Pod "pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145" satisfied condition "Succeeded or Failed"
    May 17 06:32:42.472: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145 container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 06:32:42.485
    May 17 06:32:42.506: INFO: Waiting for pod pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145 to disappear
    May 17 06:32:42.512: INFO: Pod pod-secrets-5c3ba8e0-59e9-48ba-b2f6-9aa9d67de145 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May 17 06:32:42.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3856" for this suite. 05/17/23 06:32:42.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:32:42.536
May 17 06:32:42.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename subpath 05/17/23 06:32:42.537
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:32:42.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:32:42.56
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/17/23 06:32:42.565
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-2kxx 05/17/23 06:32:42.582
STEP: Creating a pod to test atomic-volume-subpath 05/17/23 06:32:42.582
May 17 06:32:42.598: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2kxx" in namespace "subpath-7467" to be "Succeeded or Failed"
May 17 06:32:42.610: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Pending", Reason="", readiness=false. Elapsed: 11.885468ms
May 17 06:32:44.617: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018389648s
May 17 06:32:46.618: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 4.019151768s
May 17 06:32:48.617: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 6.018684277s
May 17 06:32:50.619: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 8.020258008s
May 17 06:32:52.617: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 10.018365018s
May 17 06:32:54.618: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 12.019984258s
May 17 06:32:56.620: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 14.021085761s
May 17 06:32:58.621: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 16.022384025s
May 17 06:33:00.618: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 18.019341212s
May 17 06:33:02.619: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 20.020975166s
May 17 06:33:04.618: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=false. Elapsed: 22.019616327s
May 17 06:33:06.618: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=false. Elapsed: 24.019853036s
May 17 06:33:08.619: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.020312731s
STEP: Saw pod success 05/17/23 06:33:08.619
May 17 06:33:08.619: INFO: Pod "pod-subpath-test-configmap-2kxx" satisfied condition "Succeeded or Failed"
May 17 06:33:08.627: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-subpath-test-configmap-2kxx container test-container-subpath-configmap-2kxx: <nil>
STEP: delete the pod 05/17/23 06:33:08.641
May 17 06:33:08.661: INFO: Waiting for pod pod-subpath-test-configmap-2kxx to disappear
May 17 06:33:08.667: INFO: Pod pod-subpath-test-configmap-2kxx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-2kxx 05/17/23 06:33:08.667
May 17 06:33:08.667: INFO: Deleting pod "pod-subpath-test-configmap-2kxx" in namespace "subpath-7467"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May 17 06:33:08.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7467" for this suite. 05/17/23 06:33:08.683
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":215,"skipped":4298,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.157 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:32:42.536
    May 17 06:32:42.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename subpath 05/17/23 06:32:42.537
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:32:42.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:32:42.56
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/17/23 06:32:42.565
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-2kxx 05/17/23 06:32:42.582
    STEP: Creating a pod to test atomic-volume-subpath 05/17/23 06:32:42.582
    May 17 06:32:42.598: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2kxx" in namespace "subpath-7467" to be "Succeeded or Failed"
    May 17 06:32:42.610: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Pending", Reason="", readiness=false. Elapsed: 11.885468ms
    May 17 06:32:44.617: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018389648s
    May 17 06:32:46.618: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 4.019151768s
    May 17 06:32:48.617: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 6.018684277s
    May 17 06:32:50.619: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 8.020258008s
    May 17 06:32:52.617: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 10.018365018s
    May 17 06:32:54.618: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 12.019984258s
    May 17 06:32:56.620: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 14.021085761s
    May 17 06:32:58.621: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 16.022384025s
    May 17 06:33:00.618: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 18.019341212s
    May 17 06:33:02.619: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=true. Elapsed: 20.020975166s
    May 17 06:33:04.618: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=false. Elapsed: 22.019616327s
    May 17 06:33:06.618: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Running", Reason="", readiness=false. Elapsed: 24.019853036s
    May 17 06:33:08.619: INFO: Pod "pod-subpath-test-configmap-2kxx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.020312731s
    STEP: Saw pod success 05/17/23 06:33:08.619
    May 17 06:33:08.619: INFO: Pod "pod-subpath-test-configmap-2kxx" satisfied condition "Succeeded or Failed"
    May 17 06:33:08.627: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-subpath-test-configmap-2kxx container test-container-subpath-configmap-2kxx: <nil>
    STEP: delete the pod 05/17/23 06:33:08.641
    May 17 06:33:08.661: INFO: Waiting for pod pod-subpath-test-configmap-2kxx to disappear
    May 17 06:33:08.667: INFO: Pod pod-subpath-test-configmap-2kxx no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-2kxx 05/17/23 06:33:08.667
    May 17 06:33:08.667: INFO: Deleting pod "pod-subpath-test-configmap-2kxx" in namespace "subpath-7467"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May 17 06:33:08.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7467" for this suite. 05/17/23 06:33:08.683
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:33:08.694
May 17 06:33:08.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename namespaces 05/17/23 06:33:08.694
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:33:08.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:33:08.716
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 05/17/23 06:33:08.729
STEP: patching the Namespace 05/17/23 06:33:08.748
STEP: get the Namespace and ensuring it has the label 05/17/23 06:33:08.758
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May 17 06:33:08.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3051" for this suite. 05/17/23 06:33:08.772
STEP: Destroying namespace "nspatchtest-7beacf01-5ee8-4fc5-af49-61e4abd793a9-2170" for this suite. 05/17/23 06:33:08.783
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":216,"skipped":4299,"failed":0}
------------------------------
â€¢ [0.099 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:33:08.694
    May 17 06:33:08.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename namespaces 05/17/23 06:33:08.694
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:33:08.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:33:08.716
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 05/17/23 06:33:08.729
    STEP: patching the Namespace 05/17/23 06:33:08.748
    STEP: get the Namespace and ensuring it has the label 05/17/23 06:33:08.758
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May 17 06:33:08.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3051" for this suite. 05/17/23 06:33:08.772
    STEP: Destroying namespace "nspatchtest-7beacf01-5ee8-4fc5-af49-61e4abd793a9-2170" for this suite. 05/17/23 06:33:08.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:33:08.794
May 17 06:33:08.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename cronjob 05/17/23 06:33:08.794
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:33:08.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:33:08.817
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 05/17/23 06:33:08.822
STEP: Ensuring a job is scheduled 05/17/23 06:33:08.83
STEP: Ensuring exactly one is scheduled 05/17/23 06:34:00.836
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/17/23 06:34:00.842
STEP: Ensuring the job is replaced with a new one 05/17/23 06:34:00.849
STEP: Removing cronjob 05/17/23 06:35:00.858
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May 17 06:35:00.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2640" for this suite. 05/17/23 06:35:00.881
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":217,"skipped":4316,"failed":0}
------------------------------
â€¢ [SLOW TEST] [112.101 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:33:08.794
    May 17 06:33:08.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename cronjob 05/17/23 06:33:08.794
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:33:08.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:33:08.817
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 05/17/23 06:33:08.822
    STEP: Ensuring a job is scheduled 05/17/23 06:33:08.83
    STEP: Ensuring exactly one is scheduled 05/17/23 06:34:00.836
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/17/23 06:34:00.842
    STEP: Ensuring the job is replaced with a new one 05/17/23 06:34:00.849
    STEP: Removing cronjob 05/17/23 06:35:00.858
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May 17 06:35:00.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2640" for this suite. 05/17/23 06:35:00.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:35:00.897
May 17 06:35:00.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:35:00.898
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:00.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:00.925
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
May 17 06:35:00.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 create -f -'
May 17 06:35:01.135: INFO: stderr: ""
May 17 06:35:01.135: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 17 06:35:01.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 create -f -'
May 17 06:35:01.348: INFO: stderr: ""
May 17 06:35:01.348: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/17/23 06:35:01.348
May 17 06:35:02.356: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 06:35:02.356: INFO: Found 1 / 1
May 17 06:35:02.356: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 17 06:35:02.362: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 06:35:02.362: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 17 06:35:02.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 describe pod agnhost-primary-tvhvl'
May 17 06:35:02.445: INFO: stderr: ""
May 17 06:35:02.445: INFO: stdout: "Name:             agnhost-primary-tvhvl\nNamespace:        kubectl-8790\nPriority:         0\nService Account:  default\nNode:             aks-agentpool-72615086-vmss00000c/10.224.0.6\nStart Time:       Wed, 17 May 2023 06:35:01 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.2.220\nIPs:\n  IP:           10.244.2.220\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://2942adfc29089fcfdf5a0aac24b146acd22f4ad1aed6c80c8867739bcf6ffdad\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 17 May 2023 06:35:01 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nm8jh (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-nm8jh:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-8790/agnhost-primary-tvhvl to aks-agentpool-72615086-vmss00000c\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
May 17 06:35:02.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 describe rc agnhost-primary'
May 17 06:35:02.535: INFO: stderr: ""
May 17 06:35:02.535: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8790\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-tvhvl\n"
May 17 06:35:02.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 describe service agnhost-primary'
May 17 06:35:02.613: INFO: stderr: ""
May 17 06:35:02.613: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8790\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.0.209.60\nIPs:               10.0.209.60\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.2.220:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 17 06:35:02.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 describe node aks-agentpool-72615086-vmss00000b'
May 17 06:35:02.769: INFO: stderr: ""
May 17 06:35:02.769: INFO: stdout: "Name:               aks-agentpool-72615086-vmss00000b\nRoles:              agent\nLabels:             agentpool=agentpool\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_D4as_v5\n                    beta.kubernetes.io/os=linux\n                    cpu-feature.node.kubevirt.io/3dnowprefetch=true\n                    cpu-feature.node.kubevirt.io/abm=true\n                    cpu-feature.node.kubevirt.io/adx=true\n                    cpu-feature.node.kubevirt.io/aes=true\n                    cpu-feature.node.kubevirt.io/apic=true\n                    cpu-feature.node.kubevirt.io/arat=true\n                    cpu-feature.node.kubevirt.io/arch-capabilities=true\n                    cpu-feature.node.kubevirt.io/avx=true\n                    cpu-feature.node.kubevirt.io/avx2=true\n                    cpu-feature.node.kubevirt.io/bmi1=true\n                    cpu-feature.node.kubevirt.io/bmi2=true\n                    cpu-feature.node.kubevirt.io/clflush=true\n                    cpu-feature.node.kubevirt.io/clflushopt=true\n                    cpu-feature.node.kubevirt.io/clwb=true\n                    cpu-feature.node.kubevirt.io/clzero=true\n                    cpu-feature.node.kubevirt.io/cmov=true\n                    cpu-feature.node.kubevirt.io/cmp_legacy=true\n                    cpu-feature.node.kubevirt.io/cr8legacy=true\n                    cpu-feature.node.kubevirt.io/cx16=true\n                    cpu-feature.node.kubevirt.io/cx8=true\n                    cpu-feature.node.kubevirt.io/de=true\n                    cpu-feature.node.kubevirt.io/erms=true\n                    cpu-feature.node.kubevirt.io/f16c=true\n                    cpu-feature.node.kubevirt.io/fma=true\n                    cpu-feature.node.kubevirt.io/fpu=true\n                    cpu-feature.node.kubevirt.io/fsgsbase=true\n                    cpu-feature.node.kubevirt.io/fsrm=true\n                    cpu-feature.node.kubevirt.io/fxsr=true\n                    cpu-feature.node.kubevirt.io/fxsr_opt=true\n                    cpu-feature.node.kubevirt.io/hypervisor=true\n                    cpu-feature.node.kubevirt.io/invpcid=true\n                    cpu-feature.node.kubevirt.io/invtsc=true\n                    cpu-feature.node.kubevirt.io/lahf_lm=true\n                    cpu-feature.node.kubevirt.io/lm=true\n                    cpu-feature.node.kubevirt.io/mca=true\n                    cpu-feature.node.kubevirt.io/mce=true\n                    cpu-feature.node.kubevirt.io/mds-no=true\n                    cpu-feature.node.kubevirt.io/misalignsse=true\n                    cpu-feature.node.kubevirt.io/mmx=true\n                    cpu-feature.node.kubevirt.io/mmxext=true\n                    cpu-feature.node.kubevirt.io/movbe=true\n                    cpu-feature.node.kubevirt.io/msr=true\n                    cpu-feature.node.kubevirt.io/mtrr=true\n                    cpu-feature.node.kubevirt.io/npt=true\n                    cpu-feature.node.kubevirt.io/nrip-save=true\n                    cpu-feature.node.kubevirt.io/nx=true\n                    cpu-feature.node.kubevirt.io/osvw=true\n                    cpu-feature.node.kubevirt.io/pae=true\n                    cpu-feature.node.kubevirt.io/pat=true\n                    cpu-feature.node.kubevirt.io/pcid=true\n                    cpu-feature.node.kubevirt.io/pclmuldq=true\n                    cpu-feature.node.kubevirt.io/pdpe1gb=true\n                    cpu-feature.node.kubevirt.io/pge=true\n                    cpu-feature.node.kubevirt.io/pni=true\n                    cpu-feature.node.kubevirt.io/popcnt=true\n                    cpu-feature.node.kubevirt.io/pschange-mc-no=true\n                    cpu-feature.node.kubevirt.io/pse=true\n                    cpu-feature.node.kubevirt.io/pse36=true\n                    cpu-feature.node.kubevirt.io/rdctl-no=true\n                    cpu-feature.node.kubevirt.io/rdpid=true\n                    cpu-feature.node.kubevirt.io/rdrand=true\n                    cpu-feature.node.kubevirt.io/rdseed=true\n                    cpu-feature.node.kubevirt.io/rdtscp=true\n                    cpu-feature.node.kubevirt.io/sep=true\n                    cpu-feature.node.kubevirt.io/sha-ni=true\n                    cpu-feature.node.kubevirt.io/skip-l1dfl-vmentry=true\n                    cpu-feature.node.kubevirt.io/smap=true\n                    cpu-feature.node.kubevirt.io/smep=true\n                    cpu-feature.node.kubevirt.io/sse=true\n                    cpu-feature.node.kubevirt.io/sse2=true\n                    cpu-feature.node.kubevirt.io/sse4.1=true\n                    cpu-feature.node.kubevirt.io/sse4.2=true\n                    cpu-feature.node.kubevirt.io/sse4a=true\n                    cpu-feature.node.kubevirt.io/ssse3=true\n                    cpu-feature.node.kubevirt.io/svm=true\n                    cpu-feature.node.kubevirt.io/svme-addr-chk=true\n                    cpu-feature.node.kubevirt.io/syscall=true\n                    cpu-feature.node.kubevirt.io/tsc=true\n                    cpu-feature.node.kubevirt.io/tsc-deadline=true\n                    cpu-feature.node.kubevirt.io/tsc_adjust=true\n                    cpu-feature.node.kubevirt.io/umip=true\n                    cpu-feature.node.kubevirt.io/vaes=true\n                    cpu-feature.node.kubevirt.io/vme=true\n                    cpu-feature.node.kubevirt.io/vpclmulqdq=true\n                    cpu-feature.node.kubevirt.io/x2apic=true\n                    cpu-feature.node.kubevirt.io/xgetbv1=true\n                    cpu-feature.node.kubevirt.io/xsave=true\n                    cpu-feature.node.kubevirt.io/xsavec=true\n                    cpu-feature.node.kubevirt.io/xsaveerptr=true\n                    cpu-feature.node.kubevirt.io/xsaveopt=true\n                    cpu-feature.node.kubevirt.io/xsaves=true\n                    cpu-model-migration.node.kubevirt.io/Broadwell-noTSX=true\n                    cpu-model-migration.node.kubevirt.io/EPYC-Milan=true\n                    cpu-model-migration.node.kubevirt.io/Haswell-noTSX=true\n                    cpu-model-migration.node.kubevirt.io/IvyBridge=true\n                    cpu-model-migration.node.kubevirt.io/Nehalem=true\n                    cpu-model-migration.node.kubevirt.io/Opteron_G1=true\n                    cpu-model-migration.node.kubevirt.io/Opteron_G2=true\n                    cpu-model-migration.node.kubevirt.io/Penryn=true\n                    cpu-model-migration.node.kubevirt.io/SandyBridge=true\n                    cpu-model-migration.node.kubevirt.io/Westmere=true\n                    cpu-model.node.kubevirt.io/Broadwell-noTSX=true\n                    cpu-model.node.kubevirt.io/Haswell-noTSX=true\n                    cpu-model.node.kubevirt.io/IvyBridge=true\n                    cpu-model.node.kubevirt.io/Nehalem=true\n                    cpu-model.node.kubevirt.io/Opteron_G1=true\n                    cpu-model.node.kubevirt.io/Opteron_G2=true\n                    cpu-model.node.kubevirt.io/Penryn=true\n                    cpu-model.node.kubevirt.io/SandyBridge=true\n                    cpu-model.node.kubevirt.io/Westmere=true\n                    cpu-timer.node.kubevirt.io/tsc-frequency=2445431000\n                    cpu-timer.node.kubevirt.io/tsc-scalable=true\n                    cpu-vendor.node.kubevirt.io/AMD=true\n                    cpumanager=false\n                    failure-domain.beta.kubernetes.io/region=eastus\n                    failure-domain.beta.kubernetes.io/zone=eastus-1\n                    host-model-cpu.node.kubevirt.io/EPYC-Milan=true\n                    host-model-required-features.node.kubevirt.io/arch-capabilities=true\n                    host-model-required-features.node.kubevirt.io/cmp_legacy=true\n                    host-model-required-features.node.kubevirt.io/hypervisor=true\n                    host-model-required-features.node.kubevirt.io/invtsc=true\n                    host-model-required-features.node.kubevirt.io/mds-no=true\n                    host-model-required-features.node.kubevirt.io/pschange-mc-no=true\n                    host-model-required-features.node.kubevirt.io/rdctl-no=true\n                    host-model-required-features.node.kubevirt.io/skip-l1dfl-vmentry=true\n                    host-model-required-features.node.kubevirt.io/tsc-deadline=true\n                    host-model-required-features.node.kubevirt.io/tsc_adjust=true\n                    host-model-required-features.node.kubevirt.io/vaes=true\n                    host-model-required-features.node.kubevirt.io/vpclmulqdq=true\n                    host-model-required-features.node.kubevirt.io/x2apic=true\n                    hyperv.node.kubevirt.io/base=true\n                    hyperv.node.kubevirt.io/frequencies=true\n                    hyperv.node.kubevirt.io/ipi=true\n                    hyperv.node.kubevirt.io/reenlightenment=true\n                    hyperv.node.kubevirt.io/reset=true\n                    hyperv.node.kubevirt.io/runtime=true\n                    hyperv.node.kubevirt.io/synic=true\n                    hyperv.node.kubevirt.io/synic2=true\n                    hyperv.node.kubevirt.io/synictimer=true\n                    hyperv.node.kubevirt.io/time=true\n                    hyperv.node.kubevirt.io/tlbflush=true\n                    hyperv.node.kubevirt.io/vpindex=true\n                    kubernetes.azure.com/agentpool=agentpool\n                    kubernetes.azure.com/cluster=MC_BH-AKS-RG_bh-aks-kubevirt-cluster_eastus\n                    kubernetes.azure.com/kubelet-identity-client-id=27f8cc40-bb45-4825-87db-a7f61e297246\n                    kubernetes.azure.com/mode=system\n                    kubernetes.azure.com/node-image-version=AKSUbuntu-2204gen2containerd-202304.20.0\n                    kubernetes.azure.com/os-sku=Ubuntu\n                    kubernetes.azure.com/role=agent\n                    kubernetes.azure.com/storageprofile=managed\n                    kubernetes.azure.com/storagetier=Premium_LRS\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=aks-agentpool-72615086-vmss00000b\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=agent\n                    kubevirt.io/schedulable=true\n                    node-role.kubernetes.io/agent=\n                    node.kubernetes.io/instance-type=Standard_D4as_v5\n                    scheduling.node.kubevirt.io/tsc-frequency-2445427000=true\n                    scheduling.node.kubevirt.io/tsc-frequency-2445431000=true\n                    storageprofile=managed\n                    storagetier=Premium_LRS\n                    topology.disk.csi.azure.com/zone=eastus-1\n                    topology.kubernetes.io/region=eastus\n                    topology.kubernetes.io/zone=eastus-1\nAnnotations:        csi.volume.kubernetes.io/nodeid:\n                      {\"disk.csi.azure.com\":\"aks-agentpool-72615086-vmss00000b\",\"file.csi.azure.com\":\"aks-agentpool-72615086-vmss00000b\"}\n                    kubevirt.io/heartbeat: 2023-05-17T06:33:59Z\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 17 May 2023 04:32:22 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  aks-agentpool-72615086-vmss00000b\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 17 May 2023 06:34:54 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  VMEventScheduled              False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:33:26 +0000   NoVMEventScheduled              VM has no scheduled event\n  FrequentDockerRestart         False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   NoFrequentDockerRestart         docker is functioning properly\n  KernelDeadlock                False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  FrequentKubeletRestart        False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  ContainerRuntimeProblem       False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   ContainerRuntimeIsUp            container runtime service is up\n  FilesystemCorruptionProblem   False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   FilesystemIsOK                  Filesystem is healthy\n  ReadonlyFilesystem            False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentContainerdRestart     False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  FrequentUnregisterNetDevice   False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  KubeletProblem                False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   KubeletIsUp                     kubelet service is up\n  NetworkUnavailable            False   Wed, 17 May 2023 04:33:10 +0000   Wed, 17 May 2023 04:33:10 +0000   RouteCreated                    RouteController created a route\n  MemoryPressure                False   Wed, 17 May 2023 06:32:38 +0000   Wed, 17 May 2023 04:32:22 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Wed, 17 May 2023 06:32:38 +0000   Wed, 17 May 2023 04:32:22 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Wed, 17 May 2023 06:32:38 +0000   Wed, 17 May 2023 04:32:22 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Wed, 17 May 2023 06:32:38 +0000   Wed, 17 May 2023 04:32:23 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.224.0.4\n  Hostname:    aks-agentpool-72615086-vmss00000b\nCapacity:\n  cpu:                            4\n  devices.kubevirt.io/kvm:        1k\n  devices.kubevirt.io/tun:        1k\n  devices.kubevirt.io/vhost-net:  1k\n  ephemeral-storage:              129886128Ki\n  hugepages-1Gi:                  0\n  hugepages-2Mi:                  0\n  memory:                         16386876Ki\n  pods:                           110\nAllocatable:\n  cpu:                            3860m\n  devices.kubevirt.io/kvm:        1k\n  devices.kubevirt.io/tun:        1k\n  devices.kubevirt.io/vhost-net:  1k\n  ephemeral-storage:              119703055367\n  hugepages-1Gi:                  0\n  hugepages-2Mi:                  0\n  memory:                         12892988Ki\n  pods:                           110\nSystem Info:\n  Machine ID:                       6a81ea1e01f2445995289c7d11e6bad9\n  System UUID:                      8e51f35e-ab7a-4956-98b1-bb28db00762f\n  Boot ID:                          72bee15f-ffa2-41dd-a4e1-fe87e359f142\n  Kernel Version:                   5.15.0-1036-azure\n  OS Image:                         Ubuntu 22.04.2 LTS\n  Operating System:                 linux\n  Architecture:                     amd64\n  Container Runtime Version:        containerd://1.6.18+azure-1\n  Kubelet Version:                  v1.25.6\n  Kube-Proxy Version:               v1.25.6\nPodCIDR:                            10.244.0.0/24\nPodCIDRs:                           10.244.0.0/24\nProviderID:                         azure:///subscriptions/bc01b223-3861-435b-a64e-eb575176570c/resourceGroups/mc_bh-aks-rg_bh-aks-kubevirt-cluster_eastus/providers/Microsoft.Compute/virtualMachineScaleSets/aks-agentpool-72615086-vmss/virtualMachines/11\nNon-terminated Pods:                (37 in total)\n  Namespace                         Name                                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                         ----                                                         ------------  ----------  ---------------  -------------  ---\n  cdi                               cdi-apiserver-656bdddf7b-t4q89                               10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         122m\n  cdi                               cdi-deployment-59b94f4bcf-6b8js                              10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         122m\n  cdi                               cdi-operator-7f58c444b8-cnf5v                                10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         122m\n  cdi                               cdi-uploadproxy-7787fffb9b-g7dzg                             10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         122m\n  cert-manager                      cert-manager-96f7799d6-csgfn                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  cert-manager                      cert-manager-cainjector-6f79489644-w5ggx                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  cert-manager                      cert-manager-webhook-7dfbf7bff6-zk657                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  cluster-6458ddaada42cb67ae77e285  cluster-management-agent-lite-758b6c7957-74gwr               100m (2%)     500m (12%)  100Mi (0%)       512Mi (4%)     122m\n  cluster-6458ddaada42cb67ae77e285  palette-lite-controller-manager-6cd9fb7bd-h4j4p              100m (2%)     500m (12%)  100Mi (0%)       1500Mi (11%)   122m\n  cluster-6458ddaada42cb67ae77e285  terraform-controller-848679f679-6zmxw                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  kube-system                       ama-logs-87zkj                                               150m (3%)     1 (25%)     550Mi (4%)       1774Mi (14%)   122m\n  kube-system                       ama-logs-rs-68c99469fb-bzwjp                                 150m (3%)     1 (25%)     250Mi (1%)       1536Mi (12%)   122m\n  kube-system                       azure-ip-masq-agent-928nn                                    100m (2%)     500m (12%)  50Mi (0%)        250Mi (1%)     122m\n  kube-system                       cloud-node-manager-x7fjk                                     50m (1%)      0 (0%)      50Mi (0%)        512Mi (4%)     122m\n  kube-system                       coredns-75bbfcbc66-mkbkm                                     100m (2%)     3 (77%)     70Mi (0%)        500Mi (3%)     122m\n  kube-system                       coredns-75bbfcbc66-vjv2g                                     100m (2%)     3 (77%)     70Mi (0%)        500Mi (3%)     122m\n  kube-system                       coredns-autoscaler-7879846967-wvdn8                          20m (0%)      200m (5%)   10Mi (0%)        1G (7%)        122m\n  kube-system                       csi-azuredisk-node-pccm7                                     30m (0%)      0 (0%)      60Mi (0%)        400Mi (3%)     122m\n  kube-system                       csi-azurefile-node-bzf7d                                     30m (0%)      0 (0%)      60Mi (0%)        600Mi (4%)     122m\n  kube-system                       konnectivity-agent-6859749db4-r746q                          20m (0%)      1 (25%)     20Mi (0%)        1Gi (8%)       122m\n  kube-system                       konnectivity-agent-6859749db4-vbj9c                          20m (0%)      1 (25%)     20Mi (0%)        1Gi (8%)       122m\n  kube-system                       kube-proxy-mzbkj                                             100m (2%)     0 (0%)      0 (0%)           0 (0%)         122m\n  kubevirt                          virt-api-6c4f849c9d-2jwc2                                    5m (0%)       0 (0%)      500Mi (3%)       0 (0%)         122m\n  kubevirt                          virt-api-6c4f849c9d-2x4kv                                    5m (0%)       0 (0%)      500Mi (3%)       0 (0%)         122m\n  kubevirt                          virt-controller-67b95d99d5-2cnzp                             10m (0%)      0 (0%)      275Mi (2%)       0 (0%)         122m\n  kubevirt                          virt-controller-67b95d99d5-8wv5s                             10m (0%)      0 (0%)      275Mi (2%)       0 (0%)         122m\n  kubevirt                          virt-handler-hnwhl                                           10m (0%)      0 (0%)      325Mi (2%)       0 (0%)         121m\n  kubevirt                          virt-operator-798f64bdf6-gx4sg                               10m (0%)      0 (0%)      450Mi (3%)       0 (0%)         122m\n  kubevirt                          virt-operator-798f64bdf6-x5z2b                               10m (0%)      0 (0%)      450Mi (3%)       0 (0%)         122m\n  sonobuoy                          sonobuoy-systemd-logs-daemon-set-880aaedda6954705-5jttt      0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\n  trilio-system                     k8s-triliovault-admission-webhook-77b46b96cd-9lksn           700m (18%)    0 (0%)      512Mi (4%)       0 (0%)         122m\n  trilio-system                     k8s-triliovault-control-plane-6fcbd76b99-8kgx9               800m (20%)    0 (0%)      1Gi (8%)         0 (0%)         122m\n  trilio-system                     k8s-triliovault-exporter-59c8c8dcdc-kwbxw                    50m (1%)      0 (0%)      512Mi (4%)       0 (0%)         122m\n  trilio-system                     k8s-triliovault-ingress-nginx-controller-76b7885d96-rr66q    10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         122m\n  trilio-system                     k8s-triliovault-operator-66747bd477-pmljl                    10m (0%)      200m (5%)   256Mi (2%)       512Mi (4%)     122m\n  trilio-system                     k8s-triliovault-web-75bdc67d7d-l8t7z                         10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         122m\n  trilio-system                     k8s-triliovault-web-backend-54b448979f-98rfz                 400m (10%)    0 (0%)      1Gi (8%)         0 (0%)         122m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests      Limits\n  --------                       --------      ------\n  cpu                            3150m (81%)   11900m (308%)\n  memory                         8133Mi (64%)  12161042944 (92%)\n  ephemeral-storage              64Mi (0%)     512Mi (0%)\n  hugepages-1Gi                  0 (0%)        0 (0%)\n  hugepages-2Mi                  0 (0%)        0 (0%)\n  devices.kubevirt.io/kvm        0             0\n  devices.kubevirt.io/tun        0             0\n  devices.kubevirt.io/vhost-net  0             0\nEvents:                          <none>\n"
May 17 06:35:02.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 describe namespace kubectl-8790'
May 17 06:35:02.850: INFO: stderr: ""
May 17 06:35:02.850: INFO: stdout: "Name:         kubectl-8790\nLabels:       e2e-framework=kubectl\n              e2e-run=01622574-b831-410b-947c-38c822a8716f\n              kubernetes.io/metadata.name=kubectl-8790\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:35:02.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8790" for this suite. 05/17/23 06:35:02.86
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":218,"skipped":4365,"failed":0}
------------------------------
â€¢ [1.973 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:35:00.897
    May 17 06:35:00.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:35:00.898
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:00.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:00.925
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    May 17 06:35:00.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 create -f -'
    May 17 06:35:01.135: INFO: stderr: ""
    May 17 06:35:01.135: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    May 17 06:35:01.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 create -f -'
    May 17 06:35:01.348: INFO: stderr: ""
    May 17 06:35:01.348: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/17/23 06:35:01.348
    May 17 06:35:02.356: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 06:35:02.356: INFO: Found 1 / 1
    May 17 06:35:02.356: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May 17 06:35:02.362: INFO: Selector matched 1 pods for map[app:agnhost]
    May 17 06:35:02.362: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May 17 06:35:02.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 describe pod agnhost-primary-tvhvl'
    May 17 06:35:02.445: INFO: stderr: ""
    May 17 06:35:02.445: INFO: stdout: "Name:             agnhost-primary-tvhvl\nNamespace:        kubectl-8790\nPriority:         0\nService Account:  default\nNode:             aks-agentpool-72615086-vmss00000c/10.224.0.6\nStart Time:       Wed, 17 May 2023 06:35:01 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.2.220\nIPs:\n  IP:           10.244.2.220\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://2942adfc29089fcfdf5a0aac24b146acd22f4ad1aed6c80c8867739bcf6ffdad\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 17 May 2023 06:35:01 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nm8jh (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-nm8jh:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-8790/agnhost-primary-tvhvl to aks-agentpool-72615086-vmss00000c\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    May 17 06:35:02.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 describe rc agnhost-primary'
    May 17 06:35:02.535: INFO: stderr: ""
    May 17 06:35:02.535: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8790\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-tvhvl\n"
    May 17 06:35:02.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 describe service agnhost-primary'
    May 17 06:35:02.613: INFO: stderr: ""
    May 17 06:35:02.613: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8790\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.0.209.60\nIPs:               10.0.209.60\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.2.220:6379\nSession Affinity:  None\nEvents:            <none>\n"
    May 17 06:35:02.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 describe node aks-agentpool-72615086-vmss00000b'
    May 17 06:35:02.769: INFO: stderr: ""
    May 17 06:35:02.769: INFO: stdout: "Name:               aks-agentpool-72615086-vmss00000b\nRoles:              agent\nLabels:             agentpool=agentpool\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_D4as_v5\n                    beta.kubernetes.io/os=linux\n                    cpu-feature.node.kubevirt.io/3dnowprefetch=true\n                    cpu-feature.node.kubevirt.io/abm=true\n                    cpu-feature.node.kubevirt.io/adx=true\n                    cpu-feature.node.kubevirt.io/aes=true\n                    cpu-feature.node.kubevirt.io/apic=true\n                    cpu-feature.node.kubevirt.io/arat=true\n                    cpu-feature.node.kubevirt.io/arch-capabilities=true\n                    cpu-feature.node.kubevirt.io/avx=true\n                    cpu-feature.node.kubevirt.io/avx2=true\n                    cpu-feature.node.kubevirt.io/bmi1=true\n                    cpu-feature.node.kubevirt.io/bmi2=true\n                    cpu-feature.node.kubevirt.io/clflush=true\n                    cpu-feature.node.kubevirt.io/clflushopt=true\n                    cpu-feature.node.kubevirt.io/clwb=true\n                    cpu-feature.node.kubevirt.io/clzero=true\n                    cpu-feature.node.kubevirt.io/cmov=true\n                    cpu-feature.node.kubevirt.io/cmp_legacy=true\n                    cpu-feature.node.kubevirt.io/cr8legacy=true\n                    cpu-feature.node.kubevirt.io/cx16=true\n                    cpu-feature.node.kubevirt.io/cx8=true\n                    cpu-feature.node.kubevirt.io/de=true\n                    cpu-feature.node.kubevirt.io/erms=true\n                    cpu-feature.node.kubevirt.io/f16c=true\n                    cpu-feature.node.kubevirt.io/fma=true\n                    cpu-feature.node.kubevirt.io/fpu=true\n                    cpu-feature.node.kubevirt.io/fsgsbase=true\n                    cpu-feature.node.kubevirt.io/fsrm=true\n                    cpu-feature.node.kubevirt.io/fxsr=true\n                    cpu-feature.node.kubevirt.io/fxsr_opt=true\n                    cpu-feature.node.kubevirt.io/hypervisor=true\n                    cpu-feature.node.kubevirt.io/invpcid=true\n                    cpu-feature.node.kubevirt.io/invtsc=true\n                    cpu-feature.node.kubevirt.io/lahf_lm=true\n                    cpu-feature.node.kubevirt.io/lm=true\n                    cpu-feature.node.kubevirt.io/mca=true\n                    cpu-feature.node.kubevirt.io/mce=true\n                    cpu-feature.node.kubevirt.io/mds-no=true\n                    cpu-feature.node.kubevirt.io/misalignsse=true\n                    cpu-feature.node.kubevirt.io/mmx=true\n                    cpu-feature.node.kubevirt.io/mmxext=true\n                    cpu-feature.node.kubevirt.io/movbe=true\n                    cpu-feature.node.kubevirt.io/msr=true\n                    cpu-feature.node.kubevirt.io/mtrr=true\n                    cpu-feature.node.kubevirt.io/npt=true\n                    cpu-feature.node.kubevirt.io/nrip-save=true\n                    cpu-feature.node.kubevirt.io/nx=true\n                    cpu-feature.node.kubevirt.io/osvw=true\n                    cpu-feature.node.kubevirt.io/pae=true\n                    cpu-feature.node.kubevirt.io/pat=true\n                    cpu-feature.node.kubevirt.io/pcid=true\n                    cpu-feature.node.kubevirt.io/pclmuldq=true\n                    cpu-feature.node.kubevirt.io/pdpe1gb=true\n                    cpu-feature.node.kubevirt.io/pge=true\n                    cpu-feature.node.kubevirt.io/pni=true\n                    cpu-feature.node.kubevirt.io/popcnt=true\n                    cpu-feature.node.kubevirt.io/pschange-mc-no=true\n                    cpu-feature.node.kubevirt.io/pse=true\n                    cpu-feature.node.kubevirt.io/pse36=true\n                    cpu-feature.node.kubevirt.io/rdctl-no=true\n                    cpu-feature.node.kubevirt.io/rdpid=true\n                    cpu-feature.node.kubevirt.io/rdrand=true\n                    cpu-feature.node.kubevirt.io/rdseed=true\n                    cpu-feature.node.kubevirt.io/rdtscp=true\n                    cpu-feature.node.kubevirt.io/sep=true\n                    cpu-feature.node.kubevirt.io/sha-ni=true\n                    cpu-feature.node.kubevirt.io/skip-l1dfl-vmentry=true\n                    cpu-feature.node.kubevirt.io/smap=true\n                    cpu-feature.node.kubevirt.io/smep=true\n                    cpu-feature.node.kubevirt.io/sse=true\n                    cpu-feature.node.kubevirt.io/sse2=true\n                    cpu-feature.node.kubevirt.io/sse4.1=true\n                    cpu-feature.node.kubevirt.io/sse4.2=true\n                    cpu-feature.node.kubevirt.io/sse4a=true\n                    cpu-feature.node.kubevirt.io/ssse3=true\n                    cpu-feature.node.kubevirt.io/svm=true\n                    cpu-feature.node.kubevirt.io/svme-addr-chk=true\n                    cpu-feature.node.kubevirt.io/syscall=true\n                    cpu-feature.node.kubevirt.io/tsc=true\n                    cpu-feature.node.kubevirt.io/tsc-deadline=true\n                    cpu-feature.node.kubevirt.io/tsc_adjust=true\n                    cpu-feature.node.kubevirt.io/umip=true\n                    cpu-feature.node.kubevirt.io/vaes=true\n                    cpu-feature.node.kubevirt.io/vme=true\n                    cpu-feature.node.kubevirt.io/vpclmulqdq=true\n                    cpu-feature.node.kubevirt.io/x2apic=true\n                    cpu-feature.node.kubevirt.io/xgetbv1=true\n                    cpu-feature.node.kubevirt.io/xsave=true\n                    cpu-feature.node.kubevirt.io/xsavec=true\n                    cpu-feature.node.kubevirt.io/xsaveerptr=true\n                    cpu-feature.node.kubevirt.io/xsaveopt=true\n                    cpu-feature.node.kubevirt.io/xsaves=true\n                    cpu-model-migration.node.kubevirt.io/Broadwell-noTSX=true\n                    cpu-model-migration.node.kubevirt.io/EPYC-Milan=true\n                    cpu-model-migration.node.kubevirt.io/Haswell-noTSX=true\n                    cpu-model-migration.node.kubevirt.io/IvyBridge=true\n                    cpu-model-migration.node.kubevirt.io/Nehalem=true\n                    cpu-model-migration.node.kubevirt.io/Opteron_G1=true\n                    cpu-model-migration.node.kubevirt.io/Opteron_G2=true\n                    cpu-model-migration.node.kubevirt.io/Penryn=true\n                    cpu-model-migration.node.kubevirt.io/SandyBridge=true\n                    cpu-model-migration.node.kubevirt.io/Westmere=true\n                    cpu-model.node.kubevirt.io/Broadwell-noTSX=true\n                    cpu-model.node.kubevirt.io/Haswell-noTSX=true\n                    cpu-model.node.kubevirt.io/IvyBridge=true\n                    cpu-model.node.kubevirt.io/Nehalem=true\n                    cpu-model.node.kubevirt.io/Opteron_G1=true\n                    cpu-model.node.kubevirt.io/Opteron_G2=true\n                    cpu-model.node.kubevirt.io/Penryn=true\n                    cpu-model.node.kubevirt.io/SandyBridge=true\n                    cpu-model.node.kubevirt.io/Westmere=true\n                    cpu-timer.node.kubevirt.io/tsc-frequency=2445431000\n                    cpu-timer.node.kubevirt.io/tsc-scalable=true\n                    cpu-vendor.node.kubevirt.io/AMD=true\n                    cpumanager=false\n                    failure-domain.beta.kubernetes.io/region=eastus\n                    failure-domain.beta.kubernetes.io/zone=eastus-1\n                    host-model-cpu.node.kubevirt.io/EPYC-Milan=true\n                    host-model-required-features.node.kubevirt.io/arch-capabilities=true\n                    host-model-required-features.node.kubevirt.io/cmp_legacy=true\n                    host-model-required-features.node.kubevirt.io/hypervisor=true\n                    host-model-required-features.node.kubevirt.io/invtsc=true\n                    host-model-required-features.node.kubevirt.io/mds-no=true\n                    host-model-required-features.node.kubevirt.io/pschange-mc-no=true\n                    host-model-required-features.node.kubevirt.io/rdctl-no=true\n                    host-model-required-features.node.kubevirt.io/skip-l1dfl-vmentry=true\n                    host-model-required-features.node.kubevirt.io/tsc-deadline=true\n                    host-model-required-features.node.kubevirt.io/tsc_adjust=true\n                    host-model-required-features.node.kubevirt.io/vaes=true\n                    host-model-required-features.node.kubevirt.io/vpclmulqdq=true\n                    host-model-required-features.node.kubevirt.io/x2apic=true\n                    hyperv.node.kubevirt.io/base=true\n                    hyperv.node.kubevirt.io/frequencies=true\n                    hyperv.node.kubevirt.io/ipi=true\n                    hyperv.node.kubevirt.io/reenlightenment=true\n                    hyperv.node.kubevirt.io/reset=true\n                    hyperv.node.kubevirt.io/runtime=true\n                    hyperv.node.kubevirt.io/synic=true\n                    hyperv.node.kubevirt.io/synic2=true\n                    hyperv.node.kubevirt.io/synictimer=true\n                    hyperv.node.kubevirt.io/time=true\n                    hyperv.node.kubevirt.io/tlbflush=true\n                    hyperv.node.kubevirt.io/vpindex=true\n                    kubernetes.azure.com/agentpool=agentpool\n                    kubernetes.azure.com/cluster=MC_BH-AKS-RG_bh-aks-kubevirt-cluster_eastus\n                    kubernetes.azure.com/kubelet-identity-client-id=27f8cc40-bb45-4825-87db-a7f61e297246\n                    kubernetes.azure.com/mode=system\n                    kubernetes.azure.com/node-image-version=AKSUbuntu-2204gen2containerd-202304.20.0\n                    kubernetes.azure.com/os-sku=Ubuntu\n                    kubernetes.azure.com/role=agent\n                    kubernetes.azure.com/storageprofile=managed\n                    kubernetes.azure.com/storagetier=Premium_LRS\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=aks-agentpool-72615086-vmss00000b\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=agent\n                    kubevirt.io/schedulable=true\n                    node-role.kubernetes.io/agent=\n                    node.kubernetes.io/instance-type=Standard_D4as_v5\n                    scheduling.node.kubevirt.io/tsc-frequency-2445427000=true\n                    scheduling.node.kubevirt.io/tsc-frequency-2445431000=true\n                    storageprofile=managed\n                    storagetier=Premium_LRS\n                    topology.disk.csi.azure.com/zone=eastus-1\n                    topology.kubernetes.io/region=eastus\n                    topology.kubernetes.io/zone=eastus-1\nAnnotations:        csi.volume.kubernetes.io/nodeid:\n                      {\"disk.csi.azure.com\":\"aks-agentpool-72615086-vmss00000b\",\"file.csi.azure.com\":\"aks-agentpool-72615086-vmss00000b\"}\n                    kubevirt.io/heartbeat: 2023-05-17T06:33:59Z\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 17 May 2023 04:32:22 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  aks-agentpool-72615086-vmss00000b\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 17 May 2023 06:34:54 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  VMEventScheduled              False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:33:26 +0000   NoVMEventScheduled              VM has no scheduled event\n  FrequentDockerRestart         False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   NoFrequentDockerRestart         docker is functioning properly\n  KernelDeadlock                False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  FrequentKubeletRestart        False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  ContainerRuntimeProblem       False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   ContainerRuntimeIsUp            container runtime service is up\n  FilesystemCorruptionProblem   False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   FilesystemIsOK                  Filesystem is healthy\n  ReadonlyFilesystem            False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentContainerdRestart     False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  FrequentUnregisterNetDevice   False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  KubeletProblem                False   Wed, 17 May 2023 06:33:37 +0000   Wed, 17 May 2023 04:32:41 +0000   KubeletIsUp                     kubelet service is up\n  NetworkUnavailable            False   Wed, 17 May 2023 04:33:10 +0000   Wed, 17 May 2023 04:33:10 +0000   RouteCreated                    RouteController created a route\n  MemoryPressure                False   Wed, 17 May 2023 06:32:38 +0000   Wed, 17 May 2023 04:32:22 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Wed, 17 May 2023 06:32:38 +0000   Wed, 17 May 2023 04:32:22 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Wed, 17 May 2023 06:32:38 +0000   Wed, 17 May 2023 04:32:22 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Wed, 17 May 2023 06:32:38 +0000   Wed, 17 May 2023 04:32:23 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.224.0.4\n  Hostname:    aks-agentpool-72615086-vmss00000b\nCapacity:\n  cpu:                            4\n  devices.kubevirt.io/kvm:        1k\n  devices.kubevirt.io/tun:        1k\n  devices.kubevirt.io/vhost-net:  1k\n  ephemeral-storage:              129886128Ki\n  hugepages-1Gi:                  0\n  hugepages-2Mi:                  0\n  memory:                         16386876Ki\n  pods:                           110\nAllocatable:\n  cpu:                            3860m\n  devices.kubevirt.io/kvm:        1k\n  devices.kubevirt.io/tun:        1k\n  devices.kubevirt.io/vhost-net:  1k\n  ephemeral-storage:              119703055367\n  hugepages-1Gi:                  0\n  hugepages-2Mi:                  0\n  memory:                         12892988Ki\n  pods:                           110\nSystem Info:\n  Machine ID:                       6a81ea1e01f2445995289c7d11e6bad9\n  System UUID:                      8e51f35e-ab7a-4956-98b1-bb28db00762f\n  Boot ID:                          72bee15f-ffa2-41dd-a4e1-fe87e359f142\n  Kernel Version:                   5.15.0-1036-azure\n  OS Image:                         Ubuntu 22.04.2 LTS\n  Operating System:                 linux\n  Architecture:                     amd64\n  Container Runtime Version:        containerd://1.6.18+azure-1\n  Kubelet Version:                  v1.25.6\n  Kube-Proxy Version:               v1.25.6\nPodCIDR:                            10.244.0.0/24\nPodCIDRs:                           10.244.0.0/24\nProviderID:                         azure:///subscriptions/bc01b223-3861-435b-a64e-eb575176570c/resourceGroups/mc_bh-aks-rg_bh-aks-kubevirt-cluster_eastus/providers/Microsoft.Compute/virtualMachineScaleSets/aks-agentpool-72615086-vmss/virtualMachines/11\nNon-terminated Pods:                (37 in total)\n  Namespace                         Name                                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                         ----                                                         ------------  ----------  ---------------  -------------  ---\n  cdi                               cdi-apiserver-656bdddf7b-t4q89                               10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         122m\n  cdi                               cdi-deployment-59b94f4bcf-6b8js                              10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         122m\n  cdi                               cdi-operator-7f58c444b8-cnf5v                                10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         122m\n  cdi                               cdi-uploadproxy-7787fffb9b-g7dzg                             10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         122m\n  cert-manager                      cert-manager-96f7799d6-csgfn                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  cert-manager                      cert-manager-cainjector-6f79489644-w5ggx                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  cert-manager                      cert-manager-webhook-7dfbf7bff6-zk657                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  cluster-6458ddaada42cb67ae77e285  cluster-management-agent-lite-758b6c7957-74gwr               100m (2%)     500m (12%)  100Mi (0%)       512Mi (4%)     122m\n  cluster-6458ddaada42cb67ae77e285  palette-lite-controller-manager-6cd9fb7bd-h4j4p              100m (2%)     500m (12%)  100Mi (0%)       1500Mi (11%)   122m\n  cluster-6458ddaada42cb67ae77e285  terraform-controller-848679f679-6zmxw                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m\n  kube-system                       ama-logs-87zkj                                               150m (3%)     1 (25%)     550Mi (4%)       1774Mi (14%)   122m\n  kube-system                       ama-logs-rs-68c99469fb-bzwjp                                 150m (3%)     1 (25%)     250Mi (1%)       1536Mi (12%)   122m\n  kube-system                       azure-ip-masq-agent-928nn                                    100m (2%)     500m (12%)  50Mi (0%)        250Mi (1%)     122m\n  kube-system                       cloud-node-manager-x7fjk                                     50m (1%)      0 (0%)      50Mi (0%)        512Mi (4%)     122m\n  kube-system                       coredns-75bbfcbc66-mkbkm                                     100m (2%)     3 (77%)     70Mi (0%)        500Mi (3%)     122m\n  kube-system                       coredns-75bbfcbc66-vjv2g                                     100m (2%)     3 (77%)     70Mi (0%)        500Mi (3%)     122m\n  kube-system                       coredns-autoscaler-7879846967-wvdn8                          20m (0%)      200m (5%)   10Mi (0%)        1G (7%)        122m\n  kube-system                       csi-azuredisk-node-pccm7                                     30m (0%)      0 (0%)      60Mi (0%)        400Mi (3%)     122m\n  kube-system                       csi-azurefile-node-bzf7d                                     30m (0%)      0 (0%)      60Mi (0%)        600Mi (4%)     122m\n  kube-system                       konnectivity-agent-6859749db4-r746q                          20m (0%)      1 (25%)     20Mi (0%)        1Gi (8%)       122m\n  kube-system                       konnectivity-agent-6859749db4-vbj9c                          20m (0%)      1 (25%)     20Mi (0%)        1Gi (8%)       122m\n  kube-system                       kube-proxy-mzbkj                                             100m (2%)     0 (0%)      0 (0%)           0 (0%)         122m\n  kubevirt                          virt-api-6c4f849c9d-2jwc2                                    5m (0%)       0 (0%)      500Mi (3%)       0 (0%)         122m\n  kubevirt                          virt-api-6c4f849c9d-2x4kv                                    5m (0%)       0 (0%)      500Mi (3%)       0 (0%)         122m\n  kubevirt                          virt-controller-67b95d99d5-2cnzp                             10m (0%)      0 (0%)      275Mi (2%)       0 (0%)         122m\n  kubevirt                          virt-controller-67b95d99d5-8wv5s                             10m (0%)      0 (0%)      275Mi (2%)       0 (0%)         122m\n  kubevirt                          virt-handler-hnwhl                                           10m (0%)      0 (0%)      325Mi (2%)       0 (0%)         121m\n  kubevirt                          virt-operator-798f64bdf6-gx4sg                               10m (0%)      0 (0%)      450Mi (3%)       0 (0%)         122m\n  kubevirt                          virt-operator-798f64bdf6-x5z2b                               10m (0%)      0 (0%)      450Mi (3%)       0 (0%)         122m\n  sonobuoy                          sonobuoy-systemd-logs-daemon-set-880aaedda6954705-5jttt      0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\n  trilio-system                     k8s-triliovault-admission-webhook-77b46b96cd-9lksn           700m (18%)    0 (0%)      512Mi (4%)       0 (0%)         122m\n  trilio-system                     k8s-triliovault-control-plane-6fcbd76b99-8kgx9               800m (20%)    0 (0%)      1Gi (8%)         0 (0%)         122m\n  trilio-system                     k8s-triliovault-exporter-59c8c8dcdc-kwbxw                    50m (1%)      0 (0%)      512Mi (4%)       0 (0%)         122m\n  trilio-system                     k8s-triliovault-ingress-nginx-controller-76b7885d96-rr66q    10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         122m\n  trilio-system                     k8s-triliovault-operator-66747bd477-pmljl                    10m (0%)      200m (5%)   256Mi (2%)       512Mi (4%)     122m\n  trilio-system                     k8s-triliovault-web-75bdc67d7d-l8t7z                         10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         122m\n  trilio-system                     k8s-triliovault-web-backend-54b448979f-98rfz                 400m (10%)    0 (0%)      1Gi (8%)         0 (0%)         122m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests      Limits\n  --------                       --------      ------\n  cpu                            3150m (81%)   11900m (308%)\n  memory                         8133Mi (64%)  12161042944 (92%)\n  ephemeral-storage              64Mi (0%)     512Mi (0%)\n  hugepages-1Gi                  0 (0%)        0 (0%)\n  hugepages-2Mi                  0 (0%)        0 (0%)\n  devices.kubevirt.io/kvm        0             0\n  devices.kubevirt.io/tun        0             0\n  devices.kubevirt.io/vhost-net  0             0\nEvents:                          <none>\n"
    May 17 06:35:02.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8790 describe namespace kubectl-8790'
    May 17 06:35:02.850: INFO: stderr: ""
    May 17 06:35:02.850: INFO: stdout: "Name:         kubectl-8790\nLabels:       e2e-framework=kubectl\n              e2e-run=01622574-b831-410b-947c-38c822a8716f\n              kubernetes.io/metadata.name=kubectl-8790\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:35:02.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8790" for this suite. 05/17/23 06:35:02.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:35:02.871
May 17 06:35:02.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename sched-pred 05/17/23 06:35:02.872
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:02.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:02.901
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May 17 06:35:02.906: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 06:35:02.922: INFO: Waiting for terminating namespaces to be deleted...
May 17 06:35:02.927: INFO: 
Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000b before test
May 17 06:35:02.946: INFO: cdi-apiserver-656bdddf7b-t4q89 from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.946: INFO: 	Container cdi-apiserver ready: true, restart count 0
May 17 06:35:02.946: INFO: cdi-deployment-59b94f4bcf-6b8js from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.946: INFO: 	Container cdi-controller ready: true, restart count 0
May 17 06:35:02.946: INFO: cdi-operator-7f58c444b8-cnf5v from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.946: INFO: 	Container cdi-operator ready: true, restart count 0
May 17 06:35:02.946: INFO: cdi-uploadproxy-7787fffb9b-g7dzg from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.946: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
May 17 06:35:02.946: INFO: cert-manager-96f7799d6-csgfn from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.946: INFO: 	Container cert-manager-controller ready: true, restart count 0
May 17 06:35:02.946: INFO: cert-manager-cainjector-6f79489644-w5ggx from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.946: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
May 17 06:35:02.946: INFO: cert-manager-webhook-7dfbf7bff6-zk657 from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.946: INFO: 	Container cert-manager-webhook ready: true, restart count 0
May 17 06:35:02.946: INFO: cluster-management-agent-lite-758b6c7957-74gwr from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.946: INFO: 	Container cluster-management-agent-lite ready: true, restart count 0
May 17 06:35:02.946: INFO: palette-lite-controller-manager-6cd9fb7bd-h4j4p from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.946: INFO: 	Container manager ready: true, restart count 0
May 17 06:35:02.946: INFO: terraform-controller-848679f679-6zmxw from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.946: INFO: 	Container terraform-controller ready: true, restart count 0
May 17 06:35:02.946: INFO: ama-logs-87zkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (2 container statuses recorded)
May 17 06:35:02.946: INFO: 	Container ama-logs ready: true, restart count 0
May 17 06:35:02.946: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
May 17 06:35:02.946: INFO: ama-logs-rs-68c99469fb-bzwjp from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.946: INFO: 	Container ama-logs ready: true, restart count 0
May 17 06:35:02.946: INFO: azure-ip-masq-agent-928nn from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 17 06:35:02.947: INFO: cloud-node-manager-x7fjk from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container cloud-node-manager ready: true, restart count 0
May 17 06:35:02.947: INFO: coredns-75bbfcbc66-mkbkm from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container coredns ready: true, restart count 0
May 17 06:35:02.947: INFO: coredns-75bbfcbc66-vjv2g from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container coredns ready: true, restart count 0
May 17 06:35:02.947: INFO: coredns-autoscaler-7879846967-wvdn8 from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container autoscaler ready: true, restart count 0
May 17 06:35:02.947: INFO: csi-azuredisk-node-pccm7 from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container azuredisk ready: true, restart count 0
May 17 06:35:02.947: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 06:35:02.947: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 06:35:02.947: INFO: csi-azurefile-node-bzf7d from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container azurefile ready: true, restart count 0
May 17 06:35:02.947: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 06:35:02.947: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 06:35:02.947: INFO: konnectivity-agent-6859749db4-r746q from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container konnectivity-agent ready: true, restart count 0
May 17 06:35:02.947: INFO: konnectivity-agent-6859749db4-vbj9c from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container konnectivity-agent ready: true, restart count 0
May 17 06:35:02.947: INFO: kube-proxy-mzbkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 06:35:02.947: INFO: virt-api-6c4f849c9d-2jwc2 from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container virt-api ready: true, restart count 0
May 17 06:35:02.947: INFO: virt-api-6c4f849c9d-2x4kv from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container virt-api ready: true, restart count 0
May 17 06:35:02.947: INFO: virt-controller-67b95d99d5-2cnzp from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container virt-controller ready: true, restart count 0
May 17 06:35:02.947: INFO: virt-controller-67b95d99d5-8wv5s from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container virt-controller ready: true, restart count 0
May 17 06:35:02.947: INFO: virt-handler-hnwhl from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container virt-handler ready: true, restart count 0
May 17 06:35:02.947: INFO: virt-operator-798f64bdf6-gx4sg from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container virt-operator ready: true, restart count 0
May 17 06:35:02.947: INFO: virt-operator-798f64bdf6-x5z2b from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container virt-operator ready: true, restart count 0
May 17 06:35:02.947: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-5jttt from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 06:35:02.947: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 06:35:02.947: INFO: k8s-triliovault-admission-webhook-77b46b96cd-9lksn from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container triliovault-admission-webhook ready: true, restart count 0
May 17 06:35:02.947: INFO: k8s-triliovault-control-plane-6fcbd76b99-8kgx9 from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container triliovault-analyzer ready: true, restart count 0
May 17 06:35:02.947: INFO: 	Container triliovault-control-plane ready: true, restart count 0
May 17 06:35:02.947: INFO: k8s-triliovault-exporter-59c8c8dcdc-kwbxw from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container triliovault-exporter ready: true, restart count 0
May 17 06:35:02.947: INFO: k8s-triliovault-ingress-nginx-controller-76b7885d96-rr66q from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container controller ready: true, restart count 0
May 17 06:35:02.947: INFO: k8s-triliovault-operator-66747bd477-pmljl from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container k8s-triliovault-operator ready: true, restart count 0
May 17 06:35:02.947: INFO: k8s-triliovault-web-75bdc67d7d-l8t7z from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container triliovault-web ready: true, restart count 0
May 17 06:35:02.947: INFO: k8s-triliovault-web-backend-54b448979f-98rfz from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.947: INFO: 	Container triliovault-web-backend ready: true, restart count 0
May 17 06:35:02.947: INFO: 
Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000c before test
May 17 06:35:02.963: INFO: replace-28071754-c42cp from cronjob-2640 started at 2023-05-17 06:34:00 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container c ready: true, restart count 0
May 17 06:35:02.963: INFO: replace-28071755-9c9sx from cronjob-2640 started at 2023-05-17 06:35:00 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container c ready: true, restart count 0
May 17 06:35:02.963: INFO: ama-logs-xntrk from kube-system started at 2023-05-17 04:32:30 +0000 UTC (2 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container ama-logs ready: true, restart count 0
May 17 06:35:02.963: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
May 17 06:35:02.963: INFO: azure-ip-masq-agent-ns2hc from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 17 06:35:02.963: INFO: cloud-node-manager-hhrc6 from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container cloud-node-manager ready: true, restart count 0
May 17 06:35:02.963: INFO: csi-azuredisk-node-dms8v from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container azuredisk ready: true, restart count 0
May 17 06:35:02.963: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 06:35:02.963: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 06:35:02.963: INFO: csi-azurefile-node-whbbj from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container azurefile ready: true, restart count 0
May 17 06:35:02.963: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 06:35:02.963: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 06:35:02.963: INFO: kube-proxy-lpnpx from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 06:35:02.963: INFO: metrics-server-85c9977f87-495cd from kube-system started at 2023-05-17 06:19:50 +0000 UTC (2 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container metrics-server ready: true, restart count 0
May 17 06:35:02.963: INFO: 	Container metrics-server-vpa ready: true, restart count 0
May 17 06:35:02.963: INFO: agnhost-primary-tvhvl from kubectl-8790 started at 2023-05-17 06:35:01 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container agnhost-primary ready: true, restart count 0
May 17 06:35:02.963: INFO: virt-handler-n79f2 from kubevirt started at 2023-05-17 06:11:14 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container virt-handler ready: true, restart count 0
May 17 06:35:02.963: INFO: sonobuoy from sonobuoy started at 2023-05-17 05:36:06 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 06:35:02.963: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-6hdss from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 06:35:02.963: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 06:35:02.963: INFO: k8s-triliovault-resource-cleaner-28071750-xsfk6 from trilio-system started at 2023-05-17 06:30:00 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.963: INFO: 	Container resource-cleaner ready: false, restart count 0
May 17 06:35:02.963: INFO: 
Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000d before test
May 17 06:35:02.978: INFO: virt-launcher-cirros-vm-x6gh2 from default started at 2023-05-17 05:47:31 +0000 UTC (2 container statuses recorded)
May 17 06:35:02.978: INFO: 	Container compute ready: true, restart count 0
May 17 06:35:02.978: INFO: 	Container volumecontainerdisk ready: true, restart count 0
May 17 06:35:02.978: INFO: ama-logs-bmxmm from kube-system started at 2023-05-17 04:32:24 +0000 UTC (2 container statuses recorded)
May 17 06:35:02.978: INFO: 	Container ama-logs ready: true, restart count 0
May 17 06:35:02.978: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
May 17 06:35:02.978: INFO: azure-ip-masq-agent-vm9n2 from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.978: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 17 06:35:02.978: INFO: cloud-node-manager-6mv9z from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.978: INFO: 	Container cloud-node-manager ready: true, restart count 0
May 17 06:35:02.978: INFO: csi-azuredisk-node-rg26q from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
May 17 06:35:02.978: INFO: 	Container azuredisk ready: true, restart count 0
May 17 06:35:02.978: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 06:35:02.978: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 06:35:02.978: INFO: csi-azurefile-node-zlpwj from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
May 17 06:35:02.978: INFO: 	Container azurefile ready: true, restart count 0
May 17 06:35:02.978: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 06:35:02.978: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 06:35:02.978: INFO: kube-proxy-nsx8v from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.978: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 06:35:02.978: INFO: metrics-server-85c9977f87-st4cg from kube-system started at 2023-05-17 06:19:50 +0000 UTC (2 container statuses recorded)
May 17 06:35:02.978: INFO: 	Container metrics-server ready: true, restart count 0
May 17 06:35:02.978: INFO: 	Container metrics-server-vpa ready: true, restart count 0
May 17 06:35:02.978: INFO: virt-handler-9qq9z from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.978: INFO: 	Container virt-handler ready: true, restart count 0
May 17 06:35:02.978: INFO: mysql-qa-64f5b55869-877ll from mysql started at 2023-05-17 04:33:14 +0000 UTC (1 container statuses recorded)
May 17 06:35:02.978: INFO: 	Container mysql-qa ready: true, restart count 0
May 17 06:35:02.978: INFO: sonobuoy-e2e-job-a0b627fa1b44421a from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 06:35:02.978: INFO: 	Container e2e ready: true, restart count 0
May 17 06:35:02.978: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 06:35:02.978: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-tnssv from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 06:35:02.978: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 06:35:02.978: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node aks-agentpool-72615086-vmss00000b 05/17/23 06:35:03.037
STEP: verifying the node has the label node aks-agentpool-72615086-vmss00000c 05/17/23 06:35:03.076
STEP: verifying the node has the label node aks-agentpool-72615086-vmss00000d 05/17/23 06:35:03.122
May 17 06:35:03.163: INFO: Pod cdi-apiserver-656bdddf7b-t4q89 requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod cdi-deployment-59b94f4bcf-6b8js requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod cdi-operator-7f58c444b8-cnf5v requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod cdi-uploadproxy-7787fffb9b-g7dzg requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod cert-manager-96f7799d6-csgfn requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod cert-manager-cainjector-6f79489644-w5ggx requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod cert-manager-webhook-7dfbf7bff6-zk657 requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod cluster-management-agent-lite-758b6c7957-74gwr requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod palette-lite-controller-manager-6cd9fb7bd-h4j4p requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod terraform-controller-848679f679-6zmxw requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod replace-28071754-c42cp requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod replace-28071755-9c9sx requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod virt-launcher-cirros-vm-x6gh2 requesting resource cpu=101m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.163: INFO: Pod ama-logs-87zkj requesting resource cpu=150m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod ama-logs-bmxmm requesting resource cpu=150m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.163: INFO: Pod ama-logs-rs-68c99469fb-bzwjp requesting resource cpu=150m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod ama-logs-xntrk requesting resource cpu=150m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod azure-ip-masq-agent-928nn requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod azure-ip-masq-agent-ns2hc requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod azure-ip-masq-agent-vm9n2 requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.163: INFO: Pod cloud-node-manager-6mv9z requesting resource cpu=50m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.163: INFO: Pod cloud-node-manager-hhrc6 requesting resource cpu=50m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod cloud-node-manager-x7fjk requesting resource cpu=50m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod coredns-75bbfcbc66-mkbkm requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod coredns-75bbfcbc66-vjv2g requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod coredns-autoscaler-7879846967-wvdn8 requesting resource cpu=20m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod csi-azuredisk-node-dms8v requesting resource cpu=30m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod csi-azuredisk-node-pccm7 requesting resource cpu=30m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod csi-azuredisk-node-rg26q requesting resource cpu=30m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.163: INFO: Pod csi-azurefile-node-bzf7d requesting resource cpu=30m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod csi-azurefile-node-whbbj requesting resource cpu=30m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod csi-azurefile-node-zlpwj requesting resource cpu=30m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.163: INFO: Pod konnectivity-agent-6859749db4-r746q requesting resource cpu=20m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod konnectivity-agent-6859749db4-vbj9c requesting resource cpu=20m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod kube-proxy-lpnpx requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod kube-proxy-mzbkj requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod kube-proxy-nsx8v requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.163: INFO: Pod metrics-server-85c9977f87-495cd requesting resource cpu=51m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod metrics-server-85c9977f87-st4cg requesting resource cpu=51m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.163: INFO: Pod agnhost-primary-tvhvl requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod virt-api-6c4f849c9d-2jwc2 requesting resource cpu=5m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod virt-api-6c4f849c9d-2x4kv requesting resource cpu=5m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod virt-controller-67b95d99d5-2cnzp requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod virt-controller-67b95d99d5-8wv5s requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod virt-handler-9qq9z requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.163: INFO: Pod virt-handler-hnwhl requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod virt-handler-n79f2 requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod virt-operator-798f64bdf6-gx4sg requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod virt-operator-798f64bdf6-x5z2b requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod mysql-qa-64f5b55869-877ll requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.163: INFO: Pod sonobuoy requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod sonobuoy-e2e-job-a0b627fa1b44421a requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.163: INFO: Pod sonobuoy-systemd-logs-daemon-set-880aaedda6954705-5jttt requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod sonobuoy-systemd-logs-daemon-set-880aaedda6954705-6hdss requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.163: INFO: Pod sonobuoy-systemd-logs-daemon-set-880aaedda6954705-tnssv requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.163: INFO: Pod k8s-triliovault-admission-webhook-77b46b96cd-9lksn requesting resource cpu=700m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod k8s-triliovault-control-plane-6fcbd76b99-8kgx9 requesting resource cpu=800m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod k8s-triliovault-exporter-59c8c8dcdc-kwbxw requesting resource cpu=50m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod k8s-triliovault-ingress-nginx-controller-76b7885d96-rr66q requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod k8s-triliovault-operator-66747bd477-pmljl requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod k8s-triliovault-web-75bdc67d7d-l8t7z requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.163: INFO: Pod k8s-triliovault-web-backend-54b448979f-98rfz requesting resource cpu=400m on Node aks-agentpool-72615086-vmss00000b
STEP: Starting Pods to consume most of the cluster CPU. 05/17/23 06:35:03.163
May 17 06:35:03.163: INFO: Creating a pod which consumes cpu=497m on Node aks-agentpool-72615086-vmss00000b
May 17 06:35:03.183: INFO: Creating a pod which consumes cpu=2337m on Node aks-agentpool-72615086-vmss00000c
May 17 06:35:03.193: INFO: Creating a pod which consumes cpu=2196m on Node aks-agentpool-72615086-vmss00000d
May 17 06:35:03.202: INFO: Waiting up to 5m0s for pod "filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251" in namespace "sched-pred-4782" to be "running"
May 17 06:35:03.209: INFO: Pod "filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251": Phase="Pending", Reason="", readiness=false. Elapsed: 6.950955ms
May 17 06:35:05.217: INFO: Pod "filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251": Phase="Running", Reason="", readiness=true. Elapsed: 2.015130686s
May 17 06:35:05.217: INFO: Pod "filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251" satisfied condition "running"
May 17 06:35:05.217: INFO: Waiting up to 5m0s for pod "filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0" in namespace "sched-pred-4782" to be "running"
May 17 06:35:05.224: INFO: Pod "filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0": Phase="Running", Reason="", readiness=true. Elapsed: 6.193839ms
May 17 06:35:05.224: INFO: Pod "filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0" satisfied condition "running"
May 17 06:35:05.224: INFO: Waiting up to 5m0s for pod "filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044" in namespace "sched-pred-4782" to be "running"
May 17 06:35:05.230: INFO: Pod "filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044": Phase="Running", Reason="", readiness=true. Elapsed: 6.04449ms
May 17 06:35:05.230: INFO: Pod "filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 05/17/23 06:35:05.23
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251.175fdabfda249023], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4782/filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251 to aks-agentpool-72615086-vmss00000b] 05/17/23 06:35:05.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251.175fdabff63360e2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/17/23 06:35:05.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251.175fdabff7f55091], Reason = [Created], Message = [Created container filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251] 05/17/23 06:35:05.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251.175fdabffd43c298], Reason = [Started], Message = [Started container filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251] 05/17/23 06:35:05.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0.175fdabfdaa87a3d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4782/filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0 to aks-agentpool-72615086-vmss00000c] 05/17/23 06:35:05.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0.175fdabff4f9854c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/17/23 06:35:05.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0.175fdabff7160800], Reason = [Created], Message = [Created container filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0] 05/17/23 06:35:05.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0.175fdabffa9b433e], Reason = [Started], Message = [Started container filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0] 05/17/23 06:35:05.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044.175fdabfdb3cf428], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4782/filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044 to aks-agentpool-72615086-vmss00000d] 05/17/23 06:35:05.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044.175fdabff8e23b07], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/17/23 06:35:05.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044.175fdabffb57fe1e], Reason = [Created], Message = [Created container filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044] 05/17/23 06:35:05.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044.175fdac0005252fe], Reason = [Started], Message = [Started container filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044] 05/17/23 06:35:05.237
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.175fdac054b49bb6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 05/17/23 06:35:05.255
STEP: removing the label node off the node aks-agentpool-72615086-vmss00000d 05/17/23 06:35:06.258
STEP: verifying the node doesn't have the label node 05/17/23 06:35:06.297
STEP: removing the label node off the node aks-agentpool-72615086-vmss00000b 05/17/23 06:35:06.304
STEP: verifying the node doesn't have the label node 05/17/23 06:35:06.341
STEP: removing the label node off the node aks-agentpool-72615086-vmss00000c 05/17/23 06:35:06.348
STEP: verifying the node doesn't have the label node 05/17/23 06:35:06.387
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May 17 06:35:06.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4782" for this suite. 05/17/23 06:35:06.402
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":219,"skipped":4373,"failed":0}
------------------------------
â€¢ [3.543 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:35:02.871
    May 17 06:35:02.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename sched-pred 05/17/23 06:35:02.872
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:02.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:02.901
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May 17 06:35:02.906: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May 17 06:35:02.922: INFO: Waiting for terminating namespaces to be deleted...
    May 17 06:35:02.927: INFO: 
    Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000b before test
    May 17 06:35:02.946: INFO: cdi-apiserver-656bdddf7b-t4q89 from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.946: INFO: 	Container cdi-apiserver ready: true, restart count 0
    May 17 06:35:02.946: INFO: cdi-deployment-59b94f4bcf-6b8js from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.946: INFO: 	Container cdi-controller ready: true, restart count 0
    May 17 06:35:02.946: INFO: cdi-operator-7f58c444b8-cnf5v from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.946: INFO: 	Container cdi-operator ready: true, restart count 0
    May 17 06:35:02.946: INFO: cdi-uploadproxy-7787fffb9b-g7dzg from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.946: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
    May 17 06:35:02.946: INFO: cert-manager-96f7799d6-csgfn from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.946: INFO: 	Container cert-manager-controller ready: true, restart count 0
    May 17 06:35:02.946: INFO: cert-manager-cainjector-6f79489644-w5ggx from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.946: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
    May 17 06:35:02.946: INFO: cert-manager-webhook-7dfbf7bff6-zk657 from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.946: INFO: 	Container cert-manager-webhook ready: true, restart count 0
    May 17 06:35:02.946: INFO: cluster-management-agent-lite-758b6c7957-74gwr from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.946: INFO: 	Container cluster-management-agent-lite ready: true, restart count 0
    May 17 06:35:02.946: INFO: palette-lite-controller-manager-6cd9fb7bd-h4j4p from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.946: INFO: 	Container manager ready: true, restart count 0
    May 17 06:35:02.946: INFO: terraform-controller-848679f679-6zmxw from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.946: INFO: 	Container terraform-controller ready: true, restart count 0
    May 17 06:35:02.946: INFO: ama-logs-87zkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (2 container statuses recorded)
    May 17 06:35:02.946: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 06:35:02.946: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
    May 17 06:35:02.946: INFO: ama-logs-rs-68c99469fb-bzwjp from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.946: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 06:35:02.946: INFO: azure-ip-masq-agent-928nn from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    May 17 06:35:02.947: INFO: cloud-node-manager-x7fjk from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container cloud-node-manager ready: true, restart count 0
    May 17 06:35:02.947: INFO: coredns-75bbfcbc66-mkbkm from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container coredns ready: true, restart count 0
    May 17 06:35:02.947: INFO: coredns-75bbfcbc66-vjv2g from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container coredns ready: true, restart count 0
    May 17 06:35:02.947: INFO: coredns-autoscaler-7879846967-wvdn8 from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container autoscaler ready: true, restart count 0
    May 17 06:35:02.947: INFO: csi-azuredisk-node-pccm7 from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container azuredisk ready: true, restart count 0
    May 17 06:35:02.947: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 06:35:02.947: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 06:35:02.947: INFO: csi-azurefile-node-bzf7d from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container azurefile ready: true, restart count 0
    May 17 06:35:02.947: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 06:35:02.947: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 06:35:02.947: INFO: konnectivity-agent-6859749db4-r746q from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container konnectivity-agent ready: true, restart count 0
    May 17 06:35:02.947: INFO: konnectivity-agent-6859749db4-vbj9c from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container konnectivity-agent ready: true, restart count 0
    May 17 06:35:02.947: INFO: kube-proxy-mzbkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 06:35:02.947: INFO: virt-api-6c4f849c9d-2jwc2 from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container virt-api ready: true, restart count 0
    May 17 06:35:02.947: INFO: virt-api-6c4f849c9d-2x4kv from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container virt-api ready: true, restart count 0
    May 17 06:35:02.947: INFO: virt-controller-67b95d99d5-2cnzp from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container virt-controller ready: true, restart count 0
    May 17 06:35:02.947: INFO: virt-controller-67b95d99d5-8wv5s from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container virt-controller ready: true, restart count 0
    May 17 06:35:02.947: INFO: virt-handler-hnwhl from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container virt-handler ready: true, restart count 0
    May 17 06:35:02.947: INFO: virt-operator-798f64bdf6-gx4sg from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container virt-operator ready: true, restart count 0
    May 17 06:35:02.947: INFO: virt-operator-798f64bdf6-x5z2b from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container virt-operator ready: true, restart count 0
    May 17 06:35:02.947: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-5jttt from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 06:35:02.947: INFO: 	Container systemd-logs ready: true, restart count 0
    May 17 06:35:02.947: INFO: k8s-triliovault-admission-webhook-77b46b96cd-9lksn from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container triliovault-admission-webhook ready: true, restart count 0
    May 17 06:35:02.947: INFO: k8s-triliovault-control-plane-6fcbd76b99-8kgx9 from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container triliovault-analyzer ready: true, restart count 0
    May 17 06:35:02.947: INFO: 	Container triliovault-control-plane ready: true, restart count 0
    May 17 06:35:02.947: INFO: k8s-triliovault-exporter-59c8c8dcdc-kwbxw from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container triliovault-exporter ready: true, restart count 0
    May 17 06:35:02.947: INFO: k8s-triliovault-ingress-nginx-controller-76b7885d96-rr66q from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container controller ready: true, restart count 0
    May 17 06:35:02.947: INFO: k8s-triliovault-operator-66747bd477-pmljl from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container k8s-triliovault-operator ready: true, restart count 0
    May 17 06:35:02.947: INFO: k8s-triliovault-web-75bdc67d7d-l8t7z from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container triliovault-web ready: true, restart count 0
    May 17 06:35:02.947: INFO: k8s-triliovault-web-backend-54b448979f-98rfz from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.947: INFO: 	Container triliovault-web-backend ready: true, restart count 0
    May 17 06:35:02.947: INFO: 
    Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000c before test
    May 17 06:35:02.963: INFO: replace-28071754-c42cp from cronjob-2640 started at 2023-05-17 06:34:00 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container c ready: true, restart count 0
    May 17 06:35:02.963: INFO: replace-28071755-9c9sx from cronjob-2640 started at 2023-05-17 06:35:00 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container c ready: true, restart count 0
    May 17 06:35:02.963: INFO: ama-logs-xntrk from kube-system started at 2023-05-17 04:32:30 +0000 UTC (2 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 06:35:02.963: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
    May 17 06:35:02.963: INFO: azure-ip-masq-agent-ns2hc from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    May 17 06:35:02.963: INFO: cloud-node-manager-hhrc6 from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container cloud-node-manager ready: true, restart count 0
    May 17 06:35:02.963: INFO: csi-azuredisk-node-dms8v from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container azuredisk ready: true, restart count 0
    May 17 06:35:02.963: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 06:35:02.963: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 06:35:02.963: INFO: csi-azurefile-node-whbbj from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container azurefile ready: true, restart count 0
    May 17 06:35:02.963: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 06:35:02.963: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 06:35:02.963: INFO: kube-proxy-lpnpx from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 06:35:02.963: INFO: metrics-server-85c9977f87-495cd from kube-system started at 2023-05-17 06:19:50 +0000 UTC (2 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container metrics-server ready: true, restart count 0
    May 17 06:35:02.963: INFO: 	Container metrics-server-vpa ready: true, restart count 0
    May 17 06:35:02.963: INFO: agnhost-primary-tvhvl from kubectl-8790 started at 2023-05-17 06:35:01 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container agnhost-primary ready: true, restart count 0
    May 17 06:35:02.963: INFO: virt-handler-n79f2 from kubevirt started at 2023-05-17 06:11:14 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container virt-handler ready: true, restart count 0
    May 17 06:35:02.963: INFO: sonobuoy from sonobuoy started at 2023-05-17 05:36:06 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May 17 06:35:02.963: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-6hdss from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 06:35:02.963: INFO: 	Container systemd-logs ready: true, restart count 0
    May 17 06:35:02.963: INFO: k8s-triliovault-resource-cleaner-28071750-xsfk6 from trilio-system started at 2023-05-17 06:30:00 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.963: INFO: 	Container resource-cleaner ready: false, restart count 0
    May 17 06:35:02.963: INFO: 
    Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000d before test
    May 17 06:35:02.978: INFO: virt-launcher-cirros-vm-x6gh2 from default started at 2023-05-17 05:47:31 +0000 UTC (2 container statuses recorded)
    May 17 06:35:02.978: INFO: 	Container compute ready: true, restart count 0
    May 17 06:35:02.978: INFO: 	Container volumecontainerdisk ready: true, restart count 0
    May 17 06:35:02.978: INFO: ama-logs-bmxmm from kube-system started at 2023-05-17 04:32:24 +0000 UTC (2 container statuses recorded)
    May 17 06:35:02.978: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 06:35:02.978: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
    May 17 06:35:02.978: INFO: azure-ip-masq-agent-vm9n2 from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.978: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    May 17 06:35:02.978: INFO: cloud-node-manager-6mv9z from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.978: INFO: 	Container cloud-node-manager ready: true, restart count 0
    May 17 06:35:02.978: INFO: csi-azuredisk-node-rg26q from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
    May 17 06:35:02.978: INFO: 	Container azuredisk ready: true, restart count 0
    May 17 06:35:02.978: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 06:35:02.978: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 06:35:02.978: INFO: csi-azurefile-node-zlpwj from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
    May 17 06:35:02.978: INFO: 	Container azurefile ready: true, restart count 0
    May 17 06:35:02.978: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 06:35:02.978: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 06:35:02.978: INFO: kube-proxy-nsx8v from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.978: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 06:35:02.978: INFO: metrics-server-85c9977f87-st4cg from kube-system started at 2023-05-17 06:19:50 +0000 UTC (2 container statuses recorded)
    May 17 06:35:02.978: INFO: 	Container metrics-server ready: true, restart count 0
    May 17 06:35:02.978: INFO: 	Container metrics-server-vpa ready: true, restart count 0
    May 17 06:35:02.978: INFO: virt-handler-9qq9z from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.978: INFO: 	Container virt-handler ready: true, restart count 0
    May 17 06:35:02.978: INFO: mysql-qa-64f5b55869-877ll from mysql started at 2023-05-17 04:33:14 +0000 UTC (1 container statuses recorded)
    May 17 06:35:02.978: INFO: 	Container mysql-qa ready: true, restart count 0
    May 17 06:35:02.978: INFO: sonobuoy-e2e-job-a0b627fa1b44421a from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 06:35:02.978: INFO: 	Container e2e ready: true, restart count 0
    May 17 06:35:02.978: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 06:35:02.978: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-tnssv from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 06:35:02.978: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 06:35:02.978: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node aks-agentpool-72615086-vmss00000b 05/17/23 06:35:03.037
    STEP: verifying the node has the label node aks-agentpool-72615086-vmss00000c 05/17/23 06:35:03.076
    STEP: verifying the node has the label node aks-agentpool-72615086-vmss00000d 05/17/23 06:35:03.122
    May 17 06:35:03.163: INFO: Pod cdi-apiserver-656bdddf7b-t4q89 requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod cdi-deployment-59b94f4bcf-6b8js requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod cdi-operator-7f58c444b8-cnf5v requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod cdi-uploadproxy-7787fffb9b-g7dzg requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod cert-manager-96f7799d6-csgfn requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod cert-manager-cainjector-6f79489644-w5ggx requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod cert-manager-webhook-7dfbf7bff6-zk657 requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod cluster-management-agent-lite-758b6c7957-74gwr requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod palette-lite-controller-manager-6cd9fb7bd-h4j4p requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod terraform-controller-848679f679-6zmxw requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod replace-28071754-c42cp requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod replace-28071755-9c9sx requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod virt-launcher-cirros-vm-x6gh2 requesting resource cpu=101m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.163: INFO: Pod ama-logs-87zkj requesting resource cpu=150m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod ama-logs-bmxmm requesting resource cpu=150m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.163: INFO: Pod ama-logs-rs-68c99469fb-bzwjp requesting resource cpu=150m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod ama-logs-xntrk requesting resource cpu=150m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod azure-ip-masq-agent-928nn requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod azure-ip-masq-agent-ns2hc requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod azure-ip-masq-agent-vm9n2 requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.163: INFO: Pod cloud-node-manager-6mv9z requesting resource cpu=50m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.163: INFO: Pod cloud-node-manager-hhrc6 requesting resource cpu=50m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod cloud-node-manager-x7fjk requesting resource cpu=50m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod coredns-75bbfcbc66-mkbkm requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod coredns-75bbfcbc66-vjv2g requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod coredns-autoscaler-7879846967-wvdn8 requesting resource cpu=20m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod csi-azuredisk-node-dms8v requesting resource cpu=30m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod csi-azuredisk-node-pccm7 requesting resource cpu=30m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod csi-azuredisk-node-rg26q requesting resource cpu=30m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.163: INFO: Pod csi-azurefile-node-bzf7d requesting resource cpu=30m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod csi-azurefile-node-whbbj requesting resource cpu=30m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod csi-azurefile-node-zlpwj requesting resource cpu=30m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.163: INFO: Pod konnectivity-agent-6859749db4-r746q requesting resource cpu=20m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod konnectivity-agent-6859749db4-vbj9c requesting resource cpu=20m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod kube-proxy-lpnpx requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod kube-proxy-mzbkj requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod kube-proxy-nsx8v requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.163: INFO: Pod metrics-server-85c9977f87-495cd requesting resource cpu=51m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod metrics-server-85c9977f87-st4cg requesting resource cpu=51m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.163: INFO: Pod agnhost-primary-tvhvl requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod virt-api-6c4f849c9d-2jwc2 requesting resource cpu=5m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod virt-api-6c4f849c9d-2x4kv requesting resource cpu=5m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod virt-controller-67b95d99d5-2cnzp requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod virt-controller-67b95d99d5-8wv5s requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod virt-handler-9qq9z requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.163: INFO: Pod virt-handler-hnwhl requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod virt-handler-n79f2 requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod virt-operator-798f64bdf6-gx4sg requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod virt-operator-798f64bdf6-x5z2b requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod mysql-qa-64f5b55869-877ll requesting resource cpu=100m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.163: INFO: Pod sonobuoy requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod sonobuoy-e2e-job-a0b627fa1b44421a requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.163: INFO: Pod sonobuoy-systemd-logs-daemon-set-880aaedda6954705-5jttt requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod sonobuoy-systemd-logs-daemon-set-880aaedda6954705-6hdss requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.163: INFO: Pod sonobuoy-systemd-logs-daemon-set-880aaedda6954705-tnssv requesting resource cpu=0m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.163: INFO: Pod k8s-triliovault-admission-webhook-77b46b96cd-9lksn requesting resource cpu=700m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod k8s-triliovault-control-plane-6fcbd76b99-8kgx9 requesting resource cpu=800m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod k8s-triliovault-exporter-59c8c8dcdc-kwbxw requesting resource cpu=50m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod k8s-triliovault-ingress-nginx-controller-76b7885d96-rr66q requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod k8s-triliovault-operator-66747bd477-pmljl requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod k8s-triliovault-web-75bdc67d7d-l8t7z requesting resource cpu=10m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.163: INFO: Pod k8s-triliovault-web-backend-54b448979f-98rfz requesting resource cpu=400m on Node aks-agentpool-72615086-vmss00000b
    STEP: Starting Pods to consume most of the cluster CPU. 05/17/23 06:35:03.163
    May 17 06:35:03.163: INFO: Creating a pod which consumes cpu=497m on Node aks-agentpool-72615086-vmss00000b
    May 17 06:35:03.183: INFO: Creating a pod which consumes cpu=2337m on Node aks-agentpool-72615086-vmss00000c
    May 17 06:35:03.193: INFO: Creating a pod which consumes cpu=2196m on Node aks-agentpool-72615086-vmss00000d
    May 17 06:35:03.202: INFO: Waiting up to 5m0s for pod "filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251" in namespace "sched-pred-4782" to be "running"
    May 17 06:35:03.209: INFO: Pod "filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251": Phase="Pending", Reason="", readiness=false. Elapsed: 6.950955ms
    May 17 06:35:05.217: INFO: Pod "filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251": Phase="Running", Reason="", readiness=true. Elapsed: 2.015130686s
    May 17 06:35:05.217: INFO: Pod "filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251" satisfied condition "running"
    May 17 06:35:05.217: INFO: Waiting up to 5m0s for pod "filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0" in namespace "sched-pred-4782" to be "running"
    May 17 06:35:05.224: INFO: Pod "filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0": Phase="Running", Reason="", readiness=true. Elapsed: 6.193839ms
    May 17 06:35:05.224: INFO: Pod "filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0" satisfied condition "running"
    May 17 06:35:05.224: INFO: Waiting up to 5m0s for pod "filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044" in namespace "sched-pred-4782" to be "running"
    May 17 06:35:05.230: INFO: Pod "filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044": Phase="Running", Reason="", readiness=true. Elapsed: 6.04449ms
    May 17 06:35:05.230: INFO: Pod "filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 05/17/23 06:35:05.23
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251.175fdabfda249023], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4782/filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251 to aks-agentpool-72615086-vmss00000b] 05/17/23 06:35:05.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251.175fdabff63360e2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/17/23 06:35:05.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251.175fdabff7f55091], Reason = [Created], Message = [Created container filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251] 05/17/23 06:35:05.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251.175fdabffd43c298], Reason = [Started], Message = [Started container filler-pod-31ebe4a3-f820-4242-93c7-555f3f83e251] 05/17/23 06:35:05.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0.175fdabfdaa87a3d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4782/filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0 to aks-agentpool-72615086-vmss00000c] 05/17/23 06:35:05.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0.175fdabff4f9854c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/17/23 06:35:05.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0.175fdabff7160800], Reason = [Created], Message = [Created container filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0] 05/17/23 06:35:05.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0.175fdabffa9b433e], Reason = [Started], Message = [Started container filler-pod-f2a19d0f-8995-4059-a76c-3b8b07de9ed0] 05/17/23 06:35:05.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044.175fdabfdb3cf428], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4782/filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044 to aks-agentpool-72615086-vmss00000d] 05/17/23 06:35:05.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044.175fdabff8e23b07], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/17/23 06:35:05.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044.175fdabffb57fe1e], Reason = [Created], Message = [Created container filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044] 05/17/23 06:35:05.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044.175fdac0005252fe], Reason = [Started], Message = [Started container filler-pod-f60fc6f9-03fa-45f3-9032-d4c4d99cb044] 05/17/23 06:35:05.237
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.175fdac054b49bb6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 05/17/23 06:35:05.255
    STEP: removing the label node off the node aks-agentpool-72615086-vmss00000d 05/17/23 06:35:06.258
    STEP: verifying the node doesn't have the label node 05/17/23 06:35:06.297
    STEP: removing the label node off the node aks-agentpool-72615086-vmss00000b 05/17/23 06:35:06.304
    STEP: verifying the node doesn't have the label node 05/17/23 06:35:06.341
    STEP: removing the label node off the node aks-agentpool-72615086-vmss00000c 05/17/23 06:35:06.348
    STEP: verifying the node doesn't have the label node 05/17/23 06:35:06.387
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May 17 06:35:06.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4782" for this suite. 05/17/23 06:35:06.402
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:35:06.414
May 17 06:35:06.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:35:06.415
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:06.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:06.44
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9671 05/17/23 06:35:06.445
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/17/23 06:35:06.468
STEP: creating service externalsvc in namespace services-9671 05/17/23 06:35:06.468
STEP: creating replication controller externalsvc in namespace services-9671 05/17/23 06:35:06.484
I0517 06:35:06.493944      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9671, replica count: 2
I0517 06:35:09.545082      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 05/17/23 06:35:09.55
May 17 06:35:09.579: INFO: Creating new exec pod
May 17 06:35:09.592: INFO: Waiting up to 5m0s for pod "execpodr2sll" in namespace "services-9671" to be "running"
May 17 06:35:09.598: INFO: Pod "execpodr2sll": Phase="Pending", Reason="", readiness=false. Elapsed: 6.499222ms
May 17 06:35:11.605: INFO: Pod "execpodr2sll": Phase="Running", Reason="", readiness=true. Elapsed: 2.012893667s
May 17 06:35:11.605: INFO: Pod "execpodr2sll" satisfied condition "running"
May 17 06:35:11.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9671 exec execpodr2sll -- /bin/sh -x -c nslookup nodeport-service.services-9671.svc.cluster.local'
May 17 06:35:11.781: INFO: stderr: "+ nslookup nodeport-service.services-9671.svc.cluster.local\n"
May 17 06:35:11.781: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-9671.svc.cluster.local\tcanonical name = externalsvc.services-9671.svc.cluster.local.\nName:\texternalsvc.services-9671.svc.cluster.local\nAddress: 10.0.133.249\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9671, will wait for the garbage collector to delete the pods 05/17/23 06:35:11.781
May 17 06:35:11.847: INFO: Deleting ReplicationController externalsvc took: 9.737526ms
May 17 06:35:11.947: INFO: Terminating ReplicationController externalsvc pods took: 100.151395ms
May 17 06:35:13.474: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:35:13.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9671" for this suite. 05/17/23 06:35:13.505
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":220,"skipped":4373,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.101 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:35:06.414
    May 17 06:35:06.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:35:06.415
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:06.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:06.44
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9671 05/17/23 06:35:06.445
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/17/23 06:35:06.468
    STEP: creating service externalsvc in namespace services-9671 05/17/23 06:35:06.468
    STEP: creating replication controller externalsvc in namespace services-9671 05/17/23 06:35:06.484
    I0517 06:35:06.493944      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9671, replica count: 2
    I0517 06:35:09.545082      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 05/17/23 06:35:09.55
    May 17 06:35:09.579: INFO: Creating new exec pod
    May 17 06:35:09.592: INFO: Waiting up to 5m0s for pod "execpodr2sll" in namespace "services-9671" to be "running"
    May 17 06:35:09.598: INFO: Pod "execpodr2sll": Phase="Pending", Reason="", readiness=false. Elapsed: 6.499222ms
    May 17 06:35:11.605: INFO: Pod "execpodr2sll": Phase="Running", Reason="", readiness=true. Elapsed: 2.012893667s
    May 17 06:35:11.605: INFO: Pod "execpodr2sll" satisfied condition "running"
    May 17 06:35:11.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9671 exec execpodr2sll -- /bin/sh -x -c nslookup nodeport-service.services-9671.svc.cluster.local'
    May 17 06:35:11.781: INFO: stderr: "+ nslookup nodeport-service.services-9671.svc.cluster.local\n"
    May 17 06:35:11.781: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-9671.svc.cluster.local\tcanonical name = externalsvc.services-9671.svc.cluster.local.\nName:\texternalsvc.services-9671.svc.cluster.local\nAddress: 10.0.133.249\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9671, will wait for the garbage collector to delete the pods 05/17/23 06:35:11.781
    May 17 06:35:11.847: INFO: Deleting ReplicationController externalsvc took: 9.737526ms
    May 17 06:35:11.947: INFO: Terminating ReplicationController externalsvc pods took: 100.151395ms
    May 17 06:35:13.474: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:35:13.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9671" for this suite. 05/17/23 06:35:13.505
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:35:13.516
May 17 06:35:13.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:35:13.517
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:13.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:13.547
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 05/17/23 06:35:13.551
May 17 06:35:13.551: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 17 06:35:13.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 create -f -'
May 17 06:35:13.766: INFO: stderr: ""
May 17 06:35:13.766: INFO: stdout: "service/agnhost-replica created\n"
May 17 06:35:13.766: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 17 06:35:13.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 create -f -'
May 17 06:35:13.977: INFO: stderr: ""
May 17 06:35:13.977: INFO: stdout: "service/agnhost-primary created\n"
May 17 06:35:13.977: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 17 06:35:13.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 create -f -'
May 17 06:35:14.183: INFO: stderr: ""
May 17 06:35:14.183: INFO: stdout: "service/frontend created\n"
May 17 06:35:14.183: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 17 06:35:14.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 create -f -'
May 17 06:35:14.380: INFO: stderr: ""
May 17 06:35:14.380: INFO: stdout: "deployment.apps/frontend created\n"
May 17 06:35:14.380: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 17 06:35:14.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 create -f -'
May 17 06:35:14.615: INFO: stderr: ""
May 17 06:35:14.615: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 17 06:35:14.616: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 17 06:35:14.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 create -f -'
May 17 06:35:14.826: INFO: stderr: ""
May 17 06:35:14.826: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 05/17/23 06:35:14.826
May 17 06:35:14.826: INFO: Waiting for all frontend pods to be Running.
May 17 06:35:19.877: INFO: Waiting for frontend to serve content.
May 17 06:35:19.896: INFO: Trying to add a new entry to the guestbook.
May 17 06:35:19.923: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 05/17/23 06:35:19.943
May 17 06:35:19.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 delete --grace-period=0 --force -f -'
May 17 06:35:20.047: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 06:35:20.047: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 05/17/23 06:35:20.047
May 17 06:35:20.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 delete --grace-period=0 --force -f -'
May 17 06:35:20.163: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 06:35:20.163: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/17/23 06:35:20.163
May 17 06:35:20.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 delete --grace-period=0 --force -f -'
May 17 06:35:20.243: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 06:35:20.243: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/17/23 06:35:20.243
May 17 06:35:20.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 delete --grace-period=0 --force -f -'
May 17 06:35:20.321: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 06:35:20.321: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/17/23 06:35:20.321
May 17 06:35:20.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 delete --grace-period=0 --force -f -'
May 17 06:35:20.389: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 06:35:20.389: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/17/23 06:35:20.39
May 17 06:35:20.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 delete --grace-period=0 --force -f -'
May 17 06:35:20.459: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 06:35:20.459: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:35:20.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9715" for this suite. 05/17/23 06:35:20.47
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":221,"skipped":4400,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.967 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:35:13.516
    May 17 06:35:13.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:35:13.517
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:13.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:13.547
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 05/17/23 06:35:13.551
    May 17 06:35:13.551: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    May 17 06:35:13.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 create -f -'
    May 17 06:35:13.766: INFO: stderr: ""
    May 17 06:35:13.766: INFO: stdout: "service/agnhost-replica created\n"
    May 17 06:35:13.766: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    May 17 06:35:13.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 create -f -'
    May 17 06:35:13.977: INFO: stderr: ""
    May 17 06:35:13.977: INFO: stdout: "service/agnhost-primary created\n"
    May 17 06:35:13.977: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    May 17 06:35:13.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 create -f -'
    May 17 06:35:14.183: INFO: stderr: ""
    May 17 06:35:14.183: INFO: stdout: "service/frontend created\n"
    May 17 06:35:14.183: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    May 17 06:35:14.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 create -f -'
    May 17 06:35:14.380: INFO: stderr: ""
    May 17 06:35:14.380: INFO: stdout: "deployment.apps/frontend created\n"
    May 17 06:35:14.380: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May 17 06:35:14.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 create -f -'
    May 17 06:35:14.615: INFO: stderr: ""
    May 17 06:35:14.615: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    May 17 06:35:14.616: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May 17 06:35:14.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 create -f -'
    May 17 06:35:14.826: INFO: stderr: ""
    May 17 06:35:14.826: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 05/17/23 06:35:14.826
    May 17 06:35:14.826: INFO: Waiting for all frontend pods to be Running.
    May 17 06:35:19.877: INFO: Waiting for frontend to serve content.
    May 17 06:35:19.896: INFO: Trying to add a new entry to the guestbook.
    May 17 06:35:19.923: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 05/17/23 06:35:19.943
    May 17 06:35:19.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 delete --grace-period=0 --force -f -'
    May 17 06:35:20.047: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 06:35:20.047: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 05/17/23 06:35:20.047
    May 17 06:35:20.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 delete --grace-period=0 --force -f -'
    May 17 06:35:20.163: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 06:35:20.163: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/17/23 06:35:20.163
    May 17 06:35:20.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 delete --grace-period=0 --force -f -'
    May 17 06:35:20.243: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 06:35:20.243: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/17/23 06:35:20.243
    May 17 06:35:20.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 delete --grace-period=0 --force -f -'
    May 17 06:35:20.321: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 06:35:20.321: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/17/23 06:35:20.321
    May 17 06:35:20.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 delete --grace-period=0 --force -f -'
    May 17 06:35:20.389: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 06:35:20.389: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/17/23 06:35:20.39
    May 17 06:35:20.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-9715 delete --grace-period=0 --force -f -'
    May 17 06:35:20.459: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 06:35:20.459: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:35:20.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9715" for this suite. 05/17/23 06:35:20.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:35:20.483
May 17 06:35:20.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-runtime 05/17/23 06:35:20.484
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:20.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:20.509
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 05/17/23 06:35:20.514
STEP: wait for the container to reach Succeeded 05/17/23 06:35:20.529
STEP: get the container status 05/17/23 06:35:23.555
STEP: the container should be terminated 05/17/23 06:35:23.56
STEP: the termination message should be set 05/17/23 06:35:23.56
May 17 06:35:23.560: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 05/17/23 06:35:23.56
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May 17 06:35:23.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-108" for this suite. 05/17/23 06:35:23.591
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":222,"skipped":4411,"failed":0}
------------------------------
â€¢ [3.118 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:35:20.483
    May 17 06:35:20.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-runtime 05/17/23 06:35:20.484
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:20.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:20.509
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 05/17/23 06:35:20.514
    STEP: wait for the container to reach Succeeded 05/17/23 06:35:20.529
    STEP: get the container status 05/17/23 06:35:23.555
    STEP: the container should be terminated 05/17/23 06:35:23.56
    STEP: the termination message should be set 05/17/23 06:35:23.56
    May 17 06:35:23.560: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 05/17/23 06:35:23.56
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May 17 06:35:23.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-108" for this suite. 05/17/23 06:35:23.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:35:23.603
May 17 06:35:23.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-runtime 05/17/23 06:35:23.603
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:23.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:23.632
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/17/23 06:35:23.65
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/17/23 06:35:40.782
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/17/23 06:35:40.788
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/17/23 06:35:40.799
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/17/23 06:35:40.799
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/17/23 06:35:40.835
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/17/23 06:35:43.861
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/17/23 06:35:45.879
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/17/23 06:35:45.89
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/17/23 06:35:45.89
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/17/23 06:35:45.932
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/17/23 06:35:46.947
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/17/23 06:35:49.972
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/17/23 06:35:50.01
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/17/23 06:35:50.01
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May 17 06:35:50.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8271" for this suite. 05/17/23 06:35:50.06
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":223,"skipped":4457,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.468 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:35:23.603
    May 17 06:35:23.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-runtime 05/17/23 06:35:23.603
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:23.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:23.632
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/17/23 06:35:23.65
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/17/23 06:35:40.782
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/17/23 06:35:40.788
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/17/23 06:35:40.799
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/17/23 06:35:40.799
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/17/23 06:35:40.835
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/17/23 06:35:43.861
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/17/23 06:35:45.879
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/17/23 06:35:45.89
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/17/23 06:35:45.89
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/17/23 06:35:45.932
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/17/23 06:35:46.947
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/17/23 06:35:49.972
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/17/23 06:35:50.01
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/17/23 06:35:50.01
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May 17 06:35:50.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8271" for this suite. 05/17/23 06:35:50.06
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:35:50.071
May 17 06:35:50.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename endpointslice 05/17/23 06:35:50.072
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:50.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:50.098
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May 17 06:35:52.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6385" for this suite. 05/17/23 06:35:52.189
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":224,"skipped":4459,"failed":0}
------------------------------
â€¢ [2.128 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:35:50.071
    May 17 06:35:50.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename endpointslice 05/17/23 06:35:50.072
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:50.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:50.098
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May 17 06:35:52.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6385" for this suite. 05/17/23 06:35:52.189
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:35:52.199
May 17 06:35:52.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:35:52.199
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:52.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:52.225
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-1612 05/17/23 06:35:52.23
May 17 06:35:52.244: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1612" to be "running and ready"
May 17 06:35:52.250: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 5.588284ms
May 17 06:35:52.250: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 17 06:35:54.256: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.011679346s
May 17 06:35:54.256: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
May 17 06:35:54.256: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
May 17 06:35:54.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 17 06:35:54.452: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 17 06:35:54.452: INFO: stdout: "iptables"
May 17 06:35:54.452: INFO: proxyMode: iptables
May 17 06:35:54.468: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 17 06:35:54.474: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1612 05/17/23 06:35:54.474
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1612 05/17/23 06:35:54.489
I0517 06:35:54.497447      23 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1612, replica count: 3
I0517 06:35:57.548956      23 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 06:35:57.560: INFO: Creating new exec pod
May 17 06:35:57.570: INFO: Waiting up to 5m0s for pod "execpod-affinitydcn6r" in namespace "services-1612" to be "running"
May 17 06:35:57.575: INFO: Pod "execpod-affinitydcn6r": Phase="Pending", Reason="", readiness=false. Elapsed: 5.467421ms
May 17 06:35:59.581: INFO: Pod "execpod-affinitydcn6r": Phase="Running", Reason="", readiness=true. Elapsed: 2.011486417s
May 17 06:35:59.581: INFO: Pod "execpod-affinitydcn6r" satisfied condition "running"
May 17 06:36:00.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec execpod-affinitydcn6r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
May 17 06:36:00.781: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 17 06:36:00.781: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:36:00.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec execpod-affinitydcn6r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.235.95 80'
May 17 06:36:00.943: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.235.95 80\nConnection to 10.0.235.95 80 port [tcp/http] succeeded!\n"
May 17 06:36:00.943: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:36:00.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec execpod-affinitydcn6r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.235.95:80/ ; done'
May 17 06:36:01.162: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n"
May 17 06:36:01.162: INFO: stdout: "\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw"
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
May 17 06:36:01.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec execpod-affinitydcn6r -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.235.95:80/'
May 17 06:36:01.328: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n"
May 17 06:36:01.328: INFO: stdout: "affinity-clusterip-timeout-dc4lw"
May 17 06:36:21.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec execpod-affinitydcn6r -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.235.95:80/'
May 17 06:36:21.532: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n"
May 17 06:36:21.532: INFO: stdout: "affinity-clusterip-timeout-dc4lw"
May 17 06:36:41.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec execpod-affinitydcn6r -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.235.95:80/'
May 17 06:36:41.722: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n"
May 17 06:36:41.722: INFO: stdout: "affinity-clusterip-timeout-qsf7h"
May 17 06:36:41.722: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1612, will wait for the garbage collector to delete the pods 05/17/23 06:36:41.741
May 17 06:36:41.806: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 9.64558ms
May 17 06:36:41.907: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.630254ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:36:44.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1612" for this suite. 05/17/23 06:36:44.945
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":225,"skipped":4459,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.757 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:35:52.199
    May 17 06:35:52.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:35:52.199
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:35:52.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:35:52.225
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-1612 05/17/23 06:35:52.23
    May 17 06:35:52.244: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-1612" to be "running and ready"
    May 17 06:35:52.250: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 5.588284ms
    May 17 06:35:52.250: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:35:54.256: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.011679346s
    May 17 06:35:54.256: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    May 17 06:35:54.256: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    May 17 06:35:54.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    May 17 06:35:54.452: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    May 17 06:35:54.452: INFO: stdout: "iptables"
    May 17 06:35:54.452: INFO: proxyMode: iptables
    May 17 06:35:54.468: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    May 17 06:35:54.474: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-1612 05/17/23 06:35:54.474
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-1612 05/17/23 06:35:54.489
    I0517 06:35:54.497447      23 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1612, replica count: 3
    I0517 06:35:57.548956      23 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 06:35:57.560: INFO: Creating new exec pod
    May 17 06:35:57.570: INFO: Waiting up to 5m0s for pod "execpod-affinitydcn6r" in namespace "services-1612" to be "running"
    May 17 06:35:57.575: INFO: Pod "execpod-affinitydcn6r": Phase="Pending", Reason="", readiness=false. Elapsed: 5.467421ms
    May 17 06:35:59.581: INFO: Pod "execpod-affinitydcn6r": Phase="Running", Reason="", readiness=true. Elapsed: 2.011486417s
    May 17 06:35:59.581: INFO: Pod "execpod-affinitydcn6r" satisfied condition "running"
    May 17 06:36:00.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec execpod-affinitydcn6r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    May 17 06:36:00.781: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    May 17 06:36:00.781: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:36:00.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec execpod-affinitydcn6r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.235.95 80'
    May 17 06:36:00.943: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.235.95 80\nConnection to 10.0.235.95 80 port [tcp/http] succeeded!\n"
    May 17 06:36:00.943: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:36:00.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec execpod-affinitydcn6r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.235.95:80/ ; done'
    May 17 06:36:01.162: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n"
    May 17 06:36:01.162: INFO: stdout: "\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw\naffinity-clusterip-timeout-dc4lw"
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Received response from host: affinity-clusterip-timeout-dc4lw
    May 17 06:36:01.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec execpod-affinitydcn6r -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.235.95:80/'
    May 17 06:36:01.328: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n"
    May 17 06:36:01.328: INFO: stdout: "affinity-clusterip-timeout-dc4lw"
    May 17 06:36:21.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec execpod-affinitydcn6r -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.235.95:80/'
    May 17 06:36:21.532: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n"
    May 17 06:36:21.532: INFO: stdout: "affinity-clusterip-timeout-dc4lw"
    May 17 06:36:41.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-1612 exec execpod-affinitydcn6r -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.235.95:80/'
    May 17 06:36:41.722: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.235.95:80/\n"
    May 17 06:36:41.722: INFO: stdout: "affinity-clusterip-timeout-qsf7h"
    May 17 06:36:41.722: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1612, will wait for the garbage collector to delete the pods 05/17/23 06:36:41.741
    May 17 06:36:41.806: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 9.64558ms
    May 17 06:36:41.907: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.630254ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:36:44.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1612" for this suite. 05/17/23 06:36:44.945
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:36:44.957
May 17 06:36:44.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename resourcequota 05/17/23 06:36:44.958
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:36:44.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:36:44.984
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 05/17/23 06:36:44.989
STEP: Counting existing ResourceQuota 05/17/23 06:36:50.008
STEP: Creating a ResourceQuota 05/17/23 06:36:55.015
STEP: Ensuring resource quota status is calculated 05/17/23 06:36:55.023
STEP: Creating a Secret 05/17/23 06:36:57.03
STEP: Ensuring resource quota status captures secret creation 05/17/23 06:36:57.045
STEP: Deleting a secret 05/17/23 06:36:59.053
STEP: Ensuring resource quota status released usage 05/17/23 06:36:59.065
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May 17 06:37:01.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2892" for this suite. 05/17/23 06:37:01.083
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":226,"skipped":4477,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.137 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:36:44.957
    May 17 06:36:44.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename resourcequota 05/17/23 06:36:44.958
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:36:44.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:36:44.984
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 05/17/23 06:36:44.989
    STEP: Counting existing ResourceQuota 05/17/23 06:36:50.008
    STEP: Creating a ResourceQuota 05/17/23 06:36:55.015
    STEP: Ensuring resource quota status is calculated 05/17/23 06:36:55.023
    STEP: Creating a Secret 05/17/23 06:36:57.03
    STEP: Ensuring resource quota status captures secret creation 05/17/23 06:36:57.045
    STEP: Deleting a secret 05/17/23 06:36:59.053
    STEP: Ensuring resource quota status released usage 05/17/23 06:36:59.065
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May 17 06:37:01.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2892" for this suite. 05/17/23 06:37:01.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:01.095
May 17 06:37:01.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename ingressclass 05/17/23 06:37:01.095
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:01.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:01.121
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 05/17/23 06:37:01.126
STEP: getting /apis/networking.k8s.io 05/17/23 06:37:01.129
STEP: getting /apis/networking.k8s.iov1 05/17/23 06:37:01.131
STEP: creating 05/17/23 06:37:01.134
STEP: getting 05/17/23 06:37:01.153
STEP: listing 05/17/23 06:37:01.158
STEP: watching 05/17/23 06:37:01.164
May 17 06:37:01.164: INFO: starting watch
STEP: patching 05/17/23 06:37:01.166
STEP: updating 05/17/23 06:37:01.174
May 17 06:37:01.181: INFO: waiting for watch events with expected annotations
May 17 06:37:01.181: INFO: saw patched and updated annotations
STEP: deleting 05/17/23 06:37:01.181
STEP: deleting a collection 05/17/23 06:37:01.202
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
May 17 06:37:01.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-5826" for this suite. 05/17/23 06:37:01.231
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":227,"skipped":4492,"failed":0}
------------------------------
â€¢ [0.146 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:01.095
    May 17 06:37:01.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename ingressclass 05/17/23 06:37:01.095
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:01.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:01.121
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 05/17/23 06:37:01.126
    STEP: getting /apis/networking.k8s.io 05/17/23 06:37:01.129
    STEP: getting /apis/networking.k8s.iov1 05/17/23 06:37:01.131
    STEP: creating 05/17/23 06:37:01.134
    STEP: getting 05/17/23 06:37:01.153
    STEP: listing 05/17/23 06:37:01.158
    STEP: watching 05/17/23 06:37:01.164
    May 17 06:37:01.164: INFO: starting watch
    STEP: patching 05/17/23 06:37:01.166
    STEP: updating 05/17/23 06:37:01.174
    May 17 06:37:01.181: INFO: waiting for watch events with expected annotations
    May 17 06:37:01.181: INFO: saw patched and updated annotations
    STEP: deleting 05/17/23 06:37:01.181
    STEP: deleting a collection 05/17/23 06:37:01.202
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    May 17 06:37:01.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-5826" for this suite. 05/17/23 06:37:01.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:01.241
May 17 06:37:01.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:37:01.242
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:01.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:01.27
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 05/17/23 06:37:01.274
May 17 06:37:01.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4876 api-versions'
May 17 06:37:01.340: INFO: stderr: ""
May 17 06:37:01.340: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncdi.kubevirt.io/v1beta1\ncert-manager.io/v1\ncertificates.k8s.io/v1\nclone.kubevirt.io/v1alpha1\ncluster.spectrocloud.com/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nexport.kubevirt.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninstancetype.kubevirt.io/v1alpha1\ninstancetype.kubevirt.io/v1alpha2\nkubevirt.io/v1\nkubevirt.io/v1alpha3\nmetrics.k8s.io/v1beta1\nmigrations.kubevirt.io/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\npool.kubevirt.io/v1alpha1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.kubevirt.io/v1alpha1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nsubresources.kubevirt.io/v1\nsubresources.kubevirt.io/v1alpha3\nterraform.core.oam.dev/v1beta1\ntriliovault.trilio.io/v1\nupload.cdi.kubevirt.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:37:01.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4876" for this suite. 05/17/23 06:37:01.35
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":228,"skipped":4506,"failed":0}
------------------------------
â€¢ [0.119 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:01.241
    May 17 06:37:01.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:37:01.242
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:01.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:01.27
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 05/17/23 06:37:01.274
    May 17 06:37:01.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-4876 api-versions'
    May 17 06:37:01.340: INFO: stderr: ""
    May 17 06:37:01.340: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncdi.kubevirt.io/v1beta1\ncert-manager.io/v1\ncertificates.k8s.io/v1\nclone.kubevirt.io/v1alpha1\ncluster.spectrocloud.com/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nexport.kubevirt.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninstancetype.kubevirt.io/v1alpha1\ninstancetype.kubevirt.io/v1alpha2\nkubevirt.io/v1\nkubevirt.io/v1alpha3\nmetrics.k8s.io/v1beta1\nmigrations.kubevirt.io/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\npool.kubevirt.io/v1alpha1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.kubevirt.io/v1alpha1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nsubresources.kubevirt.io/v1\nsubresources.kubevirt.io/v1alpha3\nterraform.core.oam.dev/v1beta1\ntriliovault.trilio.io/v1\nupload.cdi.kubevirt.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:37:01.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4876" for this suite. 05/17/23 06:37:01.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:01.361
May 17 06:37:01.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename var-expansion 05/17/23 06:37:01.361
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:01.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:01.388
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
May 17 06:37:01.406: INFO: Waiting up to 2m0s for pod "var-expansion-0b785f04-059b-4593-b048-c62f00ef7f39" in namespace "var-expansion-9074" to be "container 0 failed with reason CreateContainerConfigError"
May 17 06:37:01.411: INFO: Pod "var-expansion-0b785f04-059b-4593-b048-c62f00ef7f39": Phase="Pending", Reason="", readiness=false. Elapsed: 5.597704ms
May 17 06:37:03.420: INFO: Pod "var-expansion-0b785f04-059b-4593-b048-c62f00ef7f39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014115088s
May 17 06:37:03.420: INFO: Pod "var-expansion-0b785f04-059b-4593-b048-c62f00ef7f39" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May 17 06:37:03.420: INFO: Deleting pod "var-expansion-0b785f04-059b-4593-b048-c62f00ef7f39" in namespace "var-expansion-9074"
May 17 06:37:03.431: INFO: Wait up to 5m0s for pod "var-expansion-0b785f04-059b-4593-b048-c62f00ef7f39" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May 17 06:37:05.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9074" for this suite. 05/17/23 06:37:05.455
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":229,"skipped":4523,"failed":0}
------------------------------
â€¢ [4.105 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:01.361
    May 17 06:37:01.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename var-expansion 05/17/23 06:37:01.361
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:01.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:01.388
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    May 17 06:37:01.406: INFO: Waiting up to 2m0s for pod "var-expansion-0b785f04-059b-4593-b048-c62f00ef7f39" in namespace "var-expansion-9074" to be "container 0 failed with reason CreateContainerConfigError"
    May 17 06:37:01.411: INFO: Pod "var-expansion-0b785f04-059b-4593-b048-c62f00ef7f39": Phase="Pending", Reason="", readiness=false. Elapsed: 5.597704ms
    May 17 06:37:03.420: INFO: Pod "var-expansion-0b785f04-059b-4593-b048-c62f00ef7f39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014115088s
    May 17 06:37:03.420: INFO: Pod "var-expansion-0b785f04-059b-4593-b048-c62f00ef7f39" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May 17 06:37:03.420: INFO: Deleting pod "var-expansion-0b785f04-059b-4593-b048-c62f00ef7f39" in namespace "var-expansion-9074"
    May 17 06:37:03.431: INFO: Wait up to 5m0s for pod "var-expansion-0b785f04-059b-4593-b048-c62f00ef7f39" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May 17 06:37:05.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9074" for this suite. 05/17/23 06:37:05.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:05.467
May 17 06:37:05.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename runtimeclass 05/17/23 06:37:05.467
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:05.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:05.493
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 05/17/23 06:37:05.498
STEP: getting /apis/node.k8s.io 05/17/23 06:37:05.502
STEP: getting /apis/node.k8s.io/v1 05/17/23 06:37:05.504
STEP: creating 05/17/23 06:37:05.506
STEP: watching 05/17/23 06:37:05.528
May 17 06:37:05.528: INFO: starting watch
STEP: getting 05/17/23 06:37:05.538
STEP: listing 05/17/23 06:37:05.543
STEP: patching 05/17/23 06:37:05.549
STEP: updating 05/17/23 06:37:05.556
May 17 06:37:05.564: INFO: waiting for watch events with expected annotations
STEP: deleting 05/17/23 06:37:05.564
STEP: deleting a collection 05/17/23 06:37:05.585
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May 17 06:37:05.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2798" for this suite. 05/17/23 06:37:05.614
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":230,"skipped":4537,"failed":0}
------------------------------
â€¢ [0.157 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:05.467
    May 17 06:37:05.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename runtimeclass 05/17/23 06:37:05.467
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:05.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:05.493
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 05/17/23 06:37:05.498
    STEP: getting /apis/node.k8s.io 05/17/23 06:37:05.502
    STEP: getting /apis/node.k8s.io/v1 05/17/23 06:37:05.504
    STEP: creating 05/17/23 06:37:05.506
    STEP: watching 05/17/23 06:37:05.528
    May 17 06:37:05.528: INFO: starting watch
    STEP: getting 05/17/23 06:37:05.538
    STEP: listing 05/17/23 06:37:05.543
    STEP: patching 05/17/23 06:37:05.549
    STEP: updating 05/17/23 06:37:05.556
    May 17 06:37:05.564: INFO: waiting for watch events with expected annotations
    STEP: deleting 05/17/23 06:37:05.564
    STEP: deleting a collection 05/17/23 06:37:05.585
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May 17 06:37:05.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2798" for this suite. 05/17/23 06:37:05.614
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:05.625
May 17 06:37:05.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 06:37:05.625
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:05.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:05.653
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May 17 06:37:05.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7870" for this suite. 05/17/23 06:37:05.732
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":231,"skipped":4541,"failed":0}
------------------------------
â€¢ [0.117 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:05.625
    May 17 06:37:05.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 06:37:05.625
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:05.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:05.653
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May 17 06:37:05.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7870" for this suite. 05/17/23 06:37:05.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:05.742
May 17 06:37:05.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename resourcequota 05/17/23 06:37:05.742
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:05.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:05.769
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 05/17/23 06:37:05.774
STEP: Creating a ResourceQuota 05/17/23 06:37:10.782
STEP: Ensuring resource quota status is calculated 05/17/23 06:37:10.79
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May 17 06:37:12.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4870" for this suite. 05/17/23 06:37:12.811
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":232,"skipped":4546,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.079 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:05.742
    May 17 06:37:05.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename resourcequota 05/17/23 06:37:05.742
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:05.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:05.769
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 05/17/23 06:37:05.774
    STEP: Creating a ResourceQuota 05/17/23 06:37:10.782
    STEP: Ensuring resource quota status is calculated 05/17/23 06:37:10.79
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May 17 06:37:12.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4870" for this suite. 05/17/23 06:37:12.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:12.821
May 17 06:37:12.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 06:37:12.821
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:12.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:12.848
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
May 17 06:37:12.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:37:19.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7158" for this suite. 05/17/23 06:37:19.57
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":233,"skipped":4552,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.759 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:12.821
    May 17 06:37:12.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 06:37:12.821
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:12.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:12.848
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    May 17 06:37:12.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:37:19.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7158" for this suite. 05/17/23 06:37:19.57
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:19.58
May 17 06:37:19.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 06:37:19.581
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:19.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:19.613
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 06:37:19.637
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:37:19.814
STEP: Deploying the webhook pod 05/17/23 06:37:19.826
STEP: Wait for the deployment to be ready 05/17/23 06:37:19.842
May 17 06:37:19.852: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 06:37:21.867
STEP: Verifying the service has paired with the endpoint 05/17/23 06:37:21.882
May 17 06:37:22.883: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 05/17/23 06:37:22.889
STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 06:37:22.913
STEP: Updating a validating webhook configuration's rules to not include the create operation 05/17/23 06:37:22.932
STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 06:37:22.948
STEP: Patching a validating webhook configuration's rules to include the create operation 05/17/23 06:37:22.966
STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 06:37:22.975
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:37:22.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1963" for this suite. 05/17/23 06:37:23.003
STEP: Destroying namespace "webhook-1963-markers" for this suite. 05/17/23 06:37:23.013
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":234,"skipped":4552,"failed":0}
------------------------------
â€¢ [3.496 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:19.58
    May 17 06:37:19.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 06:37:19.581
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:19.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:19.613
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 06:37:19.637
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:37:19.814
    STEP: Deploying the webhook pod 05/17/23 06:37:19.826
    STEP: Wait for the deployment to be ready 05/17/23 06:37:19.842
    May 17 06:37:19.852: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 06:37:21.867
    STEP: Verifying the service has paired with the endpoint 05/17/23 06:37:21.882
    May 17 06:37:22.883: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 05/17/23 06:37:22.889
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 06:37:22.913
    STEP: Updating a validating webhook configuration's rules to not include the create operation 05/17/23 06:37:22.932
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 06:37:22.948
    STEP: Patching a validating webhook configuration's rules to include the create operation 05/17/23 06:37:22.966
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 06:37:22.975
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:37:22.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1963" for this suite. 05/17/23 06:37:23.003
    STEP: Destroying namespace "webhook-1963-markers" for this suite. 05/17/23 06:37:23.013
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:23.078
May 17 06:37:23.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:37:23.078
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:23.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:23.11
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-5876 05/17/23 06:37:23.114
STEP: creating service affinity-nodeport in namespace services-5876 05/17/23 06:37:23.114
STEP: creating replication controller affinity-nodeport in namespace services-5876 05/17/23 06:37:23.14
I0517 06:37:23.150689      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5876, replica count: 3
I0517 06:37:26.201273      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 06:37:26.224: INFO: Creating new exec pod
May 17 06:37:26.238: INFO: Waiting up to 5m0s for pod "execpod-affinitynn5wx" in namespace "services-5876" to be "running"
May 17 06:37:26.245: INFO: Pod "execpod-affinitynn5wx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.170634ms
May 17 06:37:28.259: INFO: Pod "execpod-affinitynn5wx": Phase="Running", Reason="", readiness=true. Elapsed: 2.02094161s
May 17 06:37:28.259: INFO: Pod "execpod-affinitynn5wx" satisfied condition "running"
May 17 06:37:29.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5876 exec execpod-affinitynn5wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
May 17 06:37:29.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 17 06:37:29.461: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:37:29.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5876 exec execpod-affinitynn5wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.45.97 80'
May 17 06:37:29.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.45.97 80\nConnection to 10.0.45.97 80 port [tcp/http] succeeded!\n"
May 17 06:37:29.639: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:37:29.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5876 exec execpod-affinitynn5wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.6 32178'
May 17 06:37:29.818: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.6 32178\nConnection to 10.224.0.6 32178 port [tcp/*] succeeded!\n"
May 17 06:37:29.818: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:37:29.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5876 exec execpod-affinitynn5wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.4 32178'
May 17 06:37:29.975: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.4 32178\nConnection to 10.224.0.4 32178 port [tcp/*] succeeded!\n"
May 17 06:37:29.975: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:37:29.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5876 exec execpod-affinitynn5wx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.224.0.4:32178/ ; done'
May 17 06:37:30.250: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n"
May 17 06:37:30.250: INFO: stdout: "\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7"
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
May 17 06:37:30.250: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5876, will wait for the garbage collector to delete the pods 05/17/23 06:37:30.269
May 17 06:37:30.336: INFO: Deleting ReplicationController affinity-nodeport took: 11.263525ms
May 17 06:37:30.437: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.619355ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:37:32.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5876" for this suite. 05/17/23 06:37:32.682
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":235,"skipped":4570,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.614 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:23.078
    May 17 06:37:23.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:37:23.078
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:23.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:23.11
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-5876 05/17/23 06:37:23.114
    STEP: creating service affinity-nodeport in namespace services-5876 05/17/23 06:37:23.114
    STEP: creating replication controller affinity-nodeport in namespace services-5876 05/17/23 06:37:23.14
    I0517 06:37:23.150689      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5876, replica count: 3
    I0517 06:37:26.201273      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 06:37:26.224: INFO: Creating new exec pod
    May 17 06:37:26.238: INFO: Waiting up to 5m0s for pod "execpod-affinitynn5wx" in namespace "services-5876" to be "running"
    May 17 06:37:26.245: INFO: Pod "execpod-affinitynn5wx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.170634ms
    May 17 06:37:28.259: INFO: Pod "execpod-affinitynn5wx": Phase="Running", Reason="", readiness=true. Elapsed: 2.02094161s
    May 17 06:37:28.259: INFO: Pod "execpod-affinitynn5wx" satisfied condition "running"
    May 17 06:37:29.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5876 exec execpod-affinitynn5wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    May 17 06:37:29.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    May 17 06:37:29.461: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:37:29.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5876 exec execpod-affinitynn5wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.45.97 80'
    May 17 06:37:29.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.45.97 80\nConnection to 10.0.45.97 80 port [tcp/http] succeeded!\n"
    May 17 06:37:29.639: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:37:29.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5876 exec execpod-affinitynn5wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.6 32178'
    May 17 06:37:29.818: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.6 32178\nConnection to 10.224.0.6 32178 port [tcp/*] succeeded!\n"
    May 17 06:37:29.818: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:37:29.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5876 exec execpod-affinitynn5wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.4 32178'
    May 17 06:37:29.975: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.4 32178\nConnection to 10.224.0.4 32178 port [tcp/*] succeeded!\n"
    May 17 06:37:29.975: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:37:29.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5876 exec execpod-affinitynn5wx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.224.0.4:32178/ ; done'
    May 17 06:37:30.250: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:32178/\n"
    May 17 06:37:30.250: INFO: stdout: "\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7\naffinity-nodeport-kwzg7"
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Received response from host: affinity-nodeport-kwzg7
    May 17 06:37:30.250: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-5876, will wait for the garbage collector to delete the pods 05/17/23 06:37:30.269
    May 17 06:37:30.336: INFO: Deleting ReplicationController affinity-nodeport took: 11.263525ms
    May 17 06:37:30.437: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.619355ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:37:32.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5876" for this suite. 05/17/23 06:37:32.682
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:32.692
May 17 06:37:32.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename resourcequota 05/17/23 06:37:32.693
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:32.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:32.722
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 05/17/23 06:37:32.727
STEP: Creating a ResourceQuota 05/17/23 06:37:37.734
STEP: Ensuring resource quota status is calculated 05/17/23 06:37:37.742
STEP: Creating a Pod that fits quota 05/17/23 06:37:39.75
STEP: Ensuring ResourceQuota status captures the pod usage 05/17/23 06:37:39.785
STEP: Not allowing a pod to be created that exceeds remaining quota 05/17/23 06:37:41.791
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/17/23 06:37:41.797
STEP: Ensuring a pod cannot update its resource requirements 05/17/23 06:37:41.801
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/17/23 06:37:41.809
STEP: Deleting the pod 05/17/23 06:37:43.816
STEP: Ensuring resource quota status released the pod usage 05/17/23 06:37:43.834
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May 17 06:37:45.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2469" for this suite. 05/17/23 06:37:45.852
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":236,"skipped":4596,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.170 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:32.692
    May 17 06:37:32.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename resourcequota 05/17/23 06:37:32.693
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:32.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:32.722
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 05/17/23 06:37:32.727
    STEP: Creating a ResourceQuota 05/17/23 06:37:37.734
    STEP: Ensuring resource quota status is calculated 05/17/23 06:37:37.742
    STEP: Creating a Pod that fits quota 05/17/23 06:37:39.75
    STEP: Ensuring ResourceQuota status captures the pod usage 05/17/23 06:37:39.785
    STEP: Not allowing a pod to be created that exceeds remaining quota 05/17/23 06:37:41.791
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/17/23 06:37:41.797
    STEP: Ensuring a pod cannot update its resource requirements 05/17/23 06:37:41.801
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/17/23 06:37:41.809
    STEP: Deleting the pod 05/17/23 06:37:43.816
    STEP: Ensuring resource quota status released the pod usage 05/17/23 06:37:43.834
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May 17 06:37:45.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2469" for this suite. 05/17/23 06:37:45.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:45.863
May 17 06:37:45.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename resourcequota 05/17/23 06:37:45.864
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:45.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:45.885
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 05/17/23 06:37:45.89
STEP: Getting a ResourceQuota 05/17/23 06:37:45.897
STEP: Updating a ResourceQuota 05/17/23 06:37:45.902
STEP: Verifying a ResourceQuota was modified 05/17/23 06:37:45.909
STEP: Deleting a ResourceQuota 05/17/23 06:37:45.914
STEP: Verifying the deleted ResourceQuota 05/17/23 06:37:45.924
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May 17 06:37:45.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7131" for this suite. 05/17/23 06:37:45.938
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":237,"skipped":4615,"failed":0}
------------------------------
â€¢ [0.085 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:45.863
    May 17 06:37:45.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename resourcequota 05/17/23 06:37:45.864
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:45.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:45.885
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 05/17/23 06:37:45.89
    STEP: Getting a ResourceQuota 05/17/23 06:37:45.897
    STEP: Updating a ResourceQuota 05/17/23 06:37:45.902
    STEP: Verifying a ResourceQuota was modified 05/17/23 06:37:45.909
    STEP: Deleting a ResourceQuota 05/17/23 06:37:45.914
    STEP: Verifying the deleted ResourceQuota 05/17/23 06:37:45.924
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May 17 06:37:45.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7131" for this suite. 05/17/23 06:37:45.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:45.949
May 17 06:37:45.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename disruption 05/17/23 06:37:45.949
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:45.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:45.971
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 05/17/23 06:37:45.985
STEP: Updating PodDisruptionBudget status 05/17/23 06:37:48.022
STEP: Waiting for all pods to be running 05/17/23 06:37:48.037
May 17 06:37:48.043: INFO: running pods: 0 < 1
STEP: locating a running pod 05/17/23 06:37:50.05
STEP: Waiting for the pdb to be processed 05/17/23 06:37:50.07
STEP: Patching PodDisruptionBudget status 05/17/23 06:37:50.081
STEP: Waiting for the pdb to be processed 05/17/23 06:37:50.096
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May 17 06:37:50.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6841" for this suite. 05/17/23 06:37:50.112
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":238,"skipped":4620,"failed":0}
------------------------------
â€¢ [4.173 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:45.949
    May 17 06:37:45.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename disruption 05/17/23 06:37:45.949
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:45.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:45.971
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 05/17/23 06:37:45.985
    STEP: Updating PodDisruptionBudget status 05/17/23 06:37:48.022
    STEP: Waiting for all pods to be running 05/17/23 06:37:48.037
    May 17 06:37:48.043: INFO: running pods: 0 < 1
    STEP: locating a running pod 05/17/23 06:37:50.05
    STEP: Waiting for the pdb to be processed 05/17/23 06:37:50.07
    STEP: Patching PodDisruptionBudget status 05/17/23 06:37:50.081
    STEP: Waiting for the pdb to be processed 05/17/23 06:37:50.096
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May 17 06:37:50.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6841" for this suite. 05/17/23 06:37:50.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:50.123
May 17 06:37:50.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename podtemplate 05/17/23 06:37:50.123
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:50.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:50.144
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
May 17 06:37:50.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4629" for this suite. 05/17/23 06:37:50.207
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":239,"skipped":4642,"failed":0}
------------------------------
â€¢ [0.096 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:50.123
    May 17 06:37:50.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename podtemplate 05/17/23 06:37:50.123
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:50.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:50.144
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    May 17 06:37:50.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-4629" for this suite. 05/17/23 06:37:50.207
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:37:50.219
May 17 06:37:50.219: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename dns 05/17/23 06:37:50.219
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:50.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:50.242
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 05/17/23 06:37:50.247
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4761.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4761.svc.cluster.local; sleep 1; done
 05/17/23 06:37:50.254
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4761.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4761.svc.cluster.local; sleep 1; done
 05/17/23 06:37:50.254
STEP: creating a pod to probe DNS 05/17/23 06:37:50.254
STEP: submitting the pod to kubernetes 05/17/23 06:37:50.254
May 17 06:37:50.268: INFO: Waiting up to 15m0s for pod "dns-test-95fca6e0-4c16-4648-b15a-4dc1dde7b4f5" in namespace "dns-4761" to be "running"
May 17 06:37:50.275: INFO: Pod "dns-test-95fca6e0-4c16-4648-b15a-4dc1dde7b4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.59846ms
May 17 06:37:52.282: INFO: Pod "dns-test-95fca6e0-4c16-4648-b15a-4dc1dde7b4f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013844726s
May 17 06:37:52.282: INFO: Pod "dns-test-95fca6e0-4c16-4648-b15a-4dc1dde7b4f5" satisfied condition "running"
STEP: retrieving the pod 05/17/23 06:37:52.282
STEP: looking for the results for each expected name from probers 05/17/23 06:37:52.288
May 17 06:37:52.310: INFO: DNS probes using dns-test-95fca6e0-4c16-4648-b15a-4dc1dde7b4f5 succeeded

STEP: deleting the pod 05/17/23 06:37:52.31
STEP: changing the externalName to bar.example.com 05/17/23 06:37:52.33
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4761.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4761.svc.cluster.local; sleep 1; done
 05/17/23 06:37:52.343
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4761.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4761.svc.cluster.local; sleep 1; done
 05/17/23 06:37:52.343
STEP: creating a second pod to probe DNS 05/17/23 06:37:52.343
STEP: submitting the pod to kubernetes 05/17/23 06:37:52.344
May 17 06:37:52.353: INFO: Waiting up to 15m0s for pod "dns-test-b3a296a0-da88-4801-af8c-97004e805fd5" in namespace "dns-4761" to be "running"
May 17 06:37:52.359: INFO: Pod "dns-test-b3a296a0-da88-4801-af8c-97004e805fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.665598ms
May 17 06:37:54.367: INFO: Pod "dns-test-b3a296a0-da88-4801-af8c-97004e805fd5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013281546s
May 17 06:37:54.367: INFO: Pod "dns-test-b3a296a0-da88-4801-af8c-97004e805fd5" satisfied condition "running"
STEP: retrieving the pod 05/17/23 06:37:54.367
STEP: looking for the results for each expected name from probers 05/17/23 06:37:54.373
May 17 06:37:54.386: INFO: File wheezy_udp@dns-test-service-3.dns-4761.svc.cluster.local from pod  dns-4761/dns-test-b3a296a0-da88-4801-af8c-97004e805fd5 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 06:37:54.394: INFO: File jessie_udp@dns-test-service-3.dns-4761.svc.cluster.local from pod  dns-4761/dns-test-b3a296a0-da88-4801-af8c-97004e805fd5 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 06:37:54.394: INFO: Lookups using dns-4761/dns-test-b3a296a0-da88-4801-af8c-97004e805fd5 failed for: [wheezy_udp@dns-test-service-3.dns-4761.svc.cluster.local jessie_udp@dns-test-service-3.dns-4761.svc.cluster.local]

May 17 06:37:59.427: INFO: DNS probes using dns-test-b3a296a0-da88-4801-af8c-97004e805fd5 succeeded

STEP: deleting the pod 05/17/23 06:37:59.427
STEP: changing the service to type=ClusterIP 05/17/23 06:37:59.446
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4761.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4761.svc.cluster.local; sleep 1; done
 05/17/23 06:37:59.469
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4761.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4761.svc.cluster.local; sleep 1; done
 05/17/23 06:37:59.469
STEP: creating a third pod to probe DNS 05/17/23 06:37:59.469
STEP: submitting the pod to kubernetes 05/17/23 06:37:59.474
May 17 06:37:59.485: INFO: Waiting up to 15m0s for pod "dns-test-60ec48bc-e9cd-402c-b976-8fc88f300ecb" in namespace "dns-4761" to be "running"
May 17 06:37:59.493: INFO: Pod "dns-test-60ec48bc-e9cd-402c-b976-8fc88f300ecb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.555055ms
May 17 06:38:01.499: INFO: Pod "dns-test-60ec48bc-e9cd-402c-b976-8fc88f300ecb": Phase="Running", Reason="", readiness=true. Elapsed: 2.013986037s
May 17 06:38:01.499: INFO: Pod "dns-test-60ec48bc-e9cd-402c-b976-8fc88f300ecb" satisfied condition "running"
STEP: retrieving the pod 05/17/23 06:38:01.499
STEP: looking for the results for each expected name from probers 05/17/23 06:38:01.507
May 17 06:38:01.529: INFO: DNS probes using dns-test-60ec48bc-e9cd-402c-b976-8fc88f300ecb succeeded

STEP: deleting the pod 05/17/23 06:38:01.529
STEP: deleting the test externalName service 05/17/23 06:38:01.552
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May 17 06:38:01.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4761" for this suite. 05/17/23 06:38:01.584
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":240,"skipped":4643,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.375 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:37:50.219
    May 17 06:37:50.219: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename dns 05/17/23 06:37:50.219
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:37:50.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:37:50.242
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 05/17/23 06:37:50.247
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4761.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4761.svc.cluster.local; sleep 1; done
     05/17/23 06:37:50.254
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4761.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4761.svc.cluster.local; sleep 1; done
     05/17/23 06:37:50.254
    STEP: creating a pod to probe DNS 05/17/23 06:37:50.254
    STEP: submitting the pod to kubernetes 05/17/23 06:37:50.254
    May 17 06:37:50.268: INFO: Waiting up to 15m0s for pod "dns-test-95fca6e0-4c16-4648-b15a-4dc1dde7b4f5" in namespace "dns-4761" to be "running"
    May 17 06:37:50.275: INFO: Pod "dns-test-95fca6e0-4c16-4648-b15a-4dc1dde7b4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.59846ms
    May 17 06:37:52.282: INFO: Pod "dns-test-95fca6e0-4c16-4648-b15a-4dc1dde7b4f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013844726s
    May 17 06:37:52.282: INFO: Pod "dns-test-95fca6e0-4c16-4648-b15a-4dc1dde7b4f5" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 06:37:52.282
    STEP: looking for the results for each expected name from probers 05/17/23 06:37:52.288
    May 17 06:37:52.310: INFO: DNS probes using dns-test-95fca6e0-4c16-4648-b15a-4dc1dde7b4f5 succeeded

    STEP: deleting the pod 05/17/23 06:37:52.31
    STEP: changing the externalName to bar.example.com 05/17/23 06:37:52.33
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4761.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4761.svc.cluster.local; sleep 1; done
     05/17/23 06:37:52.343
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4761.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4761.svc.cluster.local; sleep 1; done
     05/17/23 06:37:52.343
    STEP: creating a second pod to probe DNS 05/17/23 06:37:52.343
    STEP: submitting the pod to kubernetes 05/17/23 06:37:52.344
    May 17 06:37:52.353: INFO: Waiting up to 15m0s for pod "dns-test-b3a296a0-da88-4801-af8c-97004e805fd5" in namespace "dns-4761" to be "running"
    May 17 06:37:52.359: INFO: Pod "dns-test-b3a296a0-da88-4801-af8c-97004e805fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.665598ms
    May 17 06:37:54.367: INFO: Pod "dns-test-b3a296a0-da88-4801-af8c-97004e805fd5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013281546s
    May 17 06:37:54.367: INFO: Pod "dns-test-b3a296a0-da88-4801-af8c-97004e805fd5" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 06:37:54.367
    STEP: looking for the results for each expected name from probers 05/17/23 06:37:54.373
    May 17 06:37:54.386: INFO: File wheezy_udp@dns-test-service-3.dns-4761.svc.cluster.local from pod  dns-4761/dns-test-b3a296a0-da88-4801-af8c-97004e805fd5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 06:37:54.394: INFO: File jessie_udp@dns-test-service-3.dns-4761.svc.cluster.local from pod  dns-4761/dns-test-b3a296a0-da88-4801-af8c-97004e805fd5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 17 06:37:54.394: INFO: Lookups using dns-4761/dns-test-b3a296a0-da88-4801-af8c-97004e805fd5 failed for: [wheezy_udp@dns-test-service-3.dns-4761.svc.cluster.local jessie_udp@dns-test-service-3.dns-4761.svc.cluster.local]

    May 17 06:37:59.427: INFO: DNS probes using dns-test-b3a296a0-da88-4801-af8c-97004e805fd5 succeeded

    STEP: deleting the pod 05/17/23 06:37:59.427
    STEP: changing the service to type=ClusterIP 05/17/23 06:37:59.446
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4761.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4761.svc.cluster.local; sleep 1; done
     05/17/23 06:37:59.469
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4761.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4761.svc.cluster.local; sleep 1; done
     05/17/23 06:37:59.469
    STEP: creating a third pod to probe DNS 05/17/23 06:37:59.469
    STEP: submitting the pod to kubernetes 05/17/23 06:37:59.474
    May 17 06:37:59.485: INFO: Waiting up to 15m0s for pod "dns-test-60ec48bc-e9cd-402c-b976-8fc88f300ecb" in namespace "dns-4761" to be "running"
    May 17 06:37:59.493: INFO: Pod "dns-test-60ec48bc-e9cd-402c-b976-8fc88f300ecb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.555055ms
    May 17 06:38:01.499: INFO: Pod "dns-test-60ec48bc-e9cd-402c-b976-8fc88f300ecb": Phase="Running", Reason="", readiness=true. Elapsed: 2.013986037s
    May 17 06:38:01.499: INFO: Pod "dns-test-60ec48bc-e9cd-402c-b976-8fc88f300ecb" satisfied condition "running"
    STEP: retrieving the pod 05/17/23 06:38:01.499
    STEP: looking for the results for each expected name from probers 05/17/23 06:38:01.507
    May 17 06:38:01.529: INFO: DNS probes using dns-test-60ec48bc-e9cd-402c-b976-8fc88f300ecb succeeded

    STEP: deleting the pod 05/17/23 06:38:01.529
    STEP: deleting the test externalName service 05/17/23 06:38:01.552
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May 17 06:38:01.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4761" for this suite. 05/17/23 06:38:01.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:38:01.595
May 17 06:38:01.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename var-expansion 05/17/23 06:38:01.595
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:38:01.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:38:01.621
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 05/17/23 06:38:01.625
May 17 06:38:01.640: INFO: Waiting up to 5m0s for pod "var-expansion-2255d97b-cb25-4367-86a4-eb099388851f" in namespace "var-expansion-1589" to be "Succeeded or Failed"
May 17 06:38:01.648: INFO: Pod "var-expansion-2255d97b-cb25-4367-86a4-eb099388851f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.493498ms
May 17 06:38:03.655: INFO: Pod "var-expansion-2255d97b-cb25-4367-86a4-eb099388851f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014742623s
May 17 06:38:05.656: INFO: Pod "var-expansion-2255d97b-cb25-4367-86a4-eb099388851f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015470317s
STEP: Saw pod success 05/17/23 06:38:05.656
May 17 06:38:05.656: INFO: Pod "var-expansion-2255d97b-cb25-4367-86a4-eb099388851f" satisfied condition "Succeeded or Failed"
May 17 06:38:05.662: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod var-expansion-2255d97b-cb25-4367-86a4-eb099388851f container dapi-container: <nil>
STEP: delete the pod 05/17/23 06:38:05.675
May 17 06:38:05.697: INFO: Waiting for pod var-expansion-2255d97b-cb25-4367-86a4-eb099388851f to disappear
May 17 06:38:05.703: INFO: Pod var-expansion-2255d97b-cb25-4367-86a4-eb099388851f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May 17 06:38:05.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1589" for this suite. 05/17/23 06:38:05.714
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":241,"skipped":4651,"failed":0}
------------------------------
â€¢ [4.129 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:38:01.595
    May 17 06:38:01.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename var-expansion 05/17/23 06:38:01.595
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:38:01.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:38:01.621
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 05/17/23 06:38:01.625
    May 17 06:38:01.640: INFO: Waiting up to 5m0s for pod "var-expansion-2255d97b-cb25-4367-86a4-eb099388851f" in namespace "var-expansion-1589" to be "Succeeded or Failed"
    May 17 06:38:01.648: INFO: Pod "var-expansion-2255d97b-cb25-4367-86a4-eb099388851f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.493498ms
    May 17 06:38:03.655: INFO: Pod "var-expansion-2255d97b-cb25-4367-86a4-eb099388851f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014742623s
    May 17 06:38:05.656: INFO: Pod "var-expansion-2255d97b-cb25-4367-86a4-eb099388851f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015470317s
    STEP: Saw pod success 05/17/23 06:38:05.656
    May 17 06:38:05.656: INFO: Pod "var-expansion-2255d97b-cb25-4367-86a4-eb099388851f" satisfied condition "Succeeded or Failed"
    May 17 06:38:05.662: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod var-expansion-2255d97b-cb25-4367-86a4-eb099388851f container dapi-container: <nil>
    STEP: delete the pod 05/17/23 06:38:05.675
    May 17 06:38:05.697: INFO: Waiting for pod var-expansion-2255d97b-cb25-4367-86a4-eb099388851f to disappear
    May 17 06:38:05.703: INFO: Pod var-expansion-2255d97b-cb25-4367-86a4-eb099388851f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May 17 06:38:05.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1589" for this suite. 05/17/23 06:38:05.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:38:05.724
May 17 06:38:05.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename job 05/17/23 06:38:05.725
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:38:05.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:38:05.751
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 05/17/23 06:38:05.756
STEP: Ensuring active pods == parallelism 05/17/23 06:38:05.766
STEP: Orphaning one of the Job's Pods 05/17/23 06:38:07.773
May 17 06:38:08.297: INFO: Successfully updated pod "adopt-release-9w924"
STEP: Checking that the Job readopts the Pod 05/17/23 06:38:08.297
May 17 06:38:08.297: INFO: Waiting up to 15m0s for pod "adopt-release-9w924" in namespace "job-3962" to be "adopted"
May 17 06:38:08.303: INFO: Pod "adopt-release-9w924": Phase="Running", Reason="", readiness=true. Elapsed: 5.970861ms
May 17 06:38:10.310: INFO: Pod "adopt-release-9w924": Phase="Running", Reason="", readiness=true. Elapsed: 2.013221859s
May 17 06:38:10.310: INFO: Pod "adopt-release-9w924" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 05/17/23 06:38:10.31
May 17 06:38:10.827: INFO: Successfully updated pod "adopt-release-9w924"
STEP: Checking that the Job releases the Pod 05/17/23 06:38:10.827
May 17 06:38:10.827: INFO: Waiting up to 15m0s for pod "adopt-release-9w924" in namespace "job-3962" to be "released"
May 17 06:38:10.833: INFO: Pod "adopt-release-9w924": Phase="Running", Reason="", readiness=true. Elapsed: 6.185679ms
May 17 06:38:12.841: INFO: Pod "adopt-release-9w924": Phase="Running", Reason="", readiness=true. Elapsed: 2.013774095s
May 17 06:38:12.841: INFO: Pod "adopt-release-9w924" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May 17 06:38:12.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3962" for this suite. 05/17/23 06:38:12.853
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":242,"skipped":4665,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.139 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:38:05.724
    May 17 06:38:05.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename job 05/17/23 06:38:05.725
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:38:05.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:38:05.751
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 05/17/23 06:38:05.756
    STEP: Ensuring active pods == parallelism 05/17/23 06:38:05.766
    STEP: Orphaning one of the Job's Pods 05/17/23 06:38:07.773
    May 17 06:38:08.297: INFO: Successfully updated pod "adopt-release-9w924"
    STEP: Checking that the Job readopts the Pod 05/17/23 06:38:08.297
    May 17 06:38:08.297: INFO: Waiting up to 15m0s for pod "adopt-release-9w924" in namespace "job-3962" to be "adopted"
    May 17 06:38:08.303: INFO: Pod "adopt-release-9w924": Phase="Running", Reason="", readiness=true. Elapsed: 5.970861ms
    May 17 06:38:10.310: INFO: Pod "adopt-release-9w924": Phase="Running", Reason="", readiness=true. Elapsed: 2.013221859s
    May 17 06:38:10.310: INFO: Pod "adopt-release-9w924" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 05/17/23 06:38:10.31
    May 17 06:38:10.827: INFO: Successfully updated pod "adopt-release-9w924"
    STEP: Checking that the Job releases the Pod 05/17/23 06:38:10.827
    May 17 06:38:10.827: INFO: Waiting up to 15m0s for pod "adopt-release-9w924" in namespace "job-3962" to be "released"
    May 17 06:38:10.833: INFO: Pod "adopt-release-9w924": Phase="Running", Reason="", readiness=true. Elapsed: 6.185679ms
    May 17 06:38:12.841: INFO: Pod "adopt-release-9w924": Phase="Running", Reason="", readiness=true. Elapsed: 2.013774095s
    May 17 06:38:12.841: INFO: Pod "adopt-release-9w924" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May 17 06:38:12.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3962" for this suite. 05/17/23 06:38:12.853
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:38:12.864
May 17 06:38:12.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:38:12.865
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:38:12.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:38:12.899
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-b4b4c8bf-7a3e-4039-a482-130491e6b507 05/17/23 06:38:12.904
STEP: Creating a pod to test consume secrets 05/17/23 06:38:12.911
May 17 06:38:12.926: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a" in namespace "projected-6736" to be "Succeeded or Failed"
May 17 06:38:12.932: INFO: Pod "pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.556386ms
May 17 06:38:14.940: INFO: Pod "pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014484767s
May 17 06:38:16.938: INFO: Pod "pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012701916s
STEP: Saw pod success 05/17/23 06:38:16.938
May 17 06:38:16.939: INFO: Pod "pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a" satisfied condition "Succeeded or Failed"
May 17 06:38:16.945: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 06:38:16.963
May 17 06:38:16.984: INFO: Waiting for pod pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a to disappear
May 17 06:38:16.995: INFO: Pod pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May 17 06:38:16.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6736" for this suite. 05/17/23 06:38:17.006
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":243,"skipped":4667,"failed":0}
------------------------------
â€¢ [4.152 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:38:12.864
    May 17 06:38:12.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:38:12.865
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:38:12.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:38:12.899
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-b4b4c8bf-7a3e-4039-a482-130491e6b507 05/17/23 06:38:12.904
    STEP: Creating a pod to test consume secrets 05/17/23 06:38:12.911
    May 17 06:38:12.926: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a" in namespace "projected-6736" to be "Succeeded or Failed"
    May 17 06:38:12.932: INFO: Pod "pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.556386ms
    May 17 06:38:14.940: INFO: Pod "pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014484767s
    May 17 06:38:16.938: INFO: Pod "pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012701916s
    STEP: Saw pod success 05/17/23 06:38:16.938
    May 17 06:38:16.939: INFO: Pod "pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a" satisfied condition "Succeeded or Failed"
    May 17 06:38:16.945: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 06:38:16.963
    May 17 06:38:16.984: INFO: Waiting for pod pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a to disappear
    May 17 06:38:16.995: INFO: Pod pod-projected-secrets-03ae68bb-5e0e-48d3-b9ce-99848145c33a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May 17 06:38:16.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6736" for this suite. 05/17/23 06:38:17.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:38:17.016
May 17 06:38:17.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:38:17.017
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:38:17.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:38:17.044
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 05/17/23 06:38:17.05
May 17 06:38:17.065: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0" in namespace "projected-8204" to be "Succeeded or Failed"
May 17 06:38:17.071: INFO: Pod "downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.157076ms
May 17 06:38:19.078: INFO: Pod "downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013528302s
May 17 06:38:21.078: INFO: Pod "downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013019167s
STEP: Saw pod success 05/17/23 06:38:21.078
May 17 06:38:21.078: INFO: Pod "downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0" satisfied condition "Succeeded or Failed"
May 17 06:38:21.083: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000d pod downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0 container client-container: <nil>
STEP: delete the pod 05/17/23 06:38:21.098
May 17 06:38:21.113: INFO: Waiting for pod downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0 to disappear
May 17 06:38:21.119: INFO: Pod downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May 17 06:38:21.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8204" for this suite. 05/17/23 06:38:21.129
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":244,"skipped":4677,"failed":0}
------------------------------
â€¢ [4.123 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:38:17.016
    May 17 06:38:17.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:38:17.017
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:38:17.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:38:17.044
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 05/17/23 06:38:17.05
    May 17 06:38:17.065: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0" in namespace "projected-8204" to be "Succeeded or Failed"
    May 17 06:38:17.071: INFO: Pod "downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.157076ms
    May 17 06:38:19.078: INFO: Pod "downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013528302s
    May 17 06:38:21.078: INFO: Pod "downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013019167s
    STEP: Saw pod success 05/17/23 06:38:21.078
    May 17 06:38:21.078: INFO: Pod "downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0" satisfied condition "Succeeded or Failed"
    May 17 06:38:21.083: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000d pod downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0 container client-container: <nil>
    STEP: delete the pod 05/17/23 06:38:21.098
    May 17 06:38:21.113: INFO: Waiting for pod downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0 to disappear
    May 17 06:38:21.119: INFO: Pod downwardapi-volume-6f03ab4a-243e-4448-8679-7c5f678db4b0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May 17 06:38:21.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8204" for this suite. 05/17/23 06:38:21.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:38:21.139
May 17 06:38:21.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename statefulset 05/17/23 06:38:21.14
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:38:21.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:38:21.163
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4439 05/17/23 06:38:21.168
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 05/17/23 06:38:21.174
May 17 06:38:21.192: INFO: Found 0 stateful pods, waiting for 3
May 17 06:38:31.200: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 06:38:31.200: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 06:38:31.200: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 17 06:38:31.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-4439 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 06:38:31.426: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 06:38:31.426: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 06:38:31.426: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/17/23 06:38:41.452
May 17 06:38:41.477: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/17/23 06:38:41.477
STEP: Updating Pods in reverse ordinal order 05/17/23 06:38:51.505
May 17 06:38:51.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-4439 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 06:38:51.699: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 06:38:51.699: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 06:38:51.699: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 05/17/23 06:39:01.737
May 17 06:39:01.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-4439 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 06:39:01.938: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 06:39:01.939: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 06:39:01.939: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 06:39:11.991: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 05/17/23 06:39:22.021
May 17 06:39:22.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-4439 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 06:39:22.227: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 06:39:22.227: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 06:39:22.227: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May 17 06:39:32.266: INFO: Deleting all statefulset in ns statefulset-4439
May 17 06:39:32.273: INFO: Scaling statefulset ss2 to 0
May 17 06:39:42.301: INFO: Waiting for statefulset status.replicas updated to 0
May 17 06:39:42.307: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May 17 06:39:42.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4439" for this suite. 05/17/23 06:39:42.34
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":245,"skipped":4686,"failed":0}
------------------------------
â€¢ [SLOW TEST] [81.211 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:38:21.139
    May 17 06:38:21.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename statefulset 05/17/23 06:38:21.14
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:38:21.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:38:21.163
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4439 05/17/23 06:38:21.168
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 05/17/23 06:38:21.174
    May 17 06:38:21.192: INFO: Found 0 stateful pods, waiting for 3
    May 17 06:38:31.200: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May 17 06:38:31.200: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May 17 06:38:31.200: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    May 17 06:38:31.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-4439 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 06:38:31.426: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 06:38:31.426: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 06:38:31.426: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/17/23 06:38:41.452
    May 17 06:38:41.477: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/17/23 06:38:41.477
    STEP: Updating Pods in reverse ordinal order 05/17/23 06:38:51.505
    May 17 06:38:51.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-4439 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 06:38:51.699: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 06:38:51.699: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 06:38:51.699: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 05/17/23 06:39:01.737
    May 17 06:39:01.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-4439 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 06:39:01.938: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 06:39:01.939: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 06:39:01.939: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 06:39:11.991: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 05/17/23 06:39:22.021
    May 17 06:39:22.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-4439 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 06:39:22.227: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 06:39:22.227: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 06:39:22.227: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May 17 06:39:32.266: INFO: Deleting all statefulset in ns statefulset-4439
    May 17 06:39:32.273: INFO: Scaling statefulset ss2 to 0
    May 17 06:39:42.301: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 06:39:42.307: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May 17 06:39:42.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4439" for this suite. 05/17/23 06:39:42.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:39:42.351
May 17 06:39:42.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:39:42.352
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:39:42.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:39:42.375
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-4410 05/17/23 06:39:42.38
STEP: creating service affinity-clusterip in namespace services-4410 05/17/23 06:39:42.38
STEP: creating replication controller affinity-clusterip in namespace services-4410 05/17/23 06:39:42.398
I0517 06:39:42.406188      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4410, replica count: 3
I0517 06:39:45.457432      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 06:39:45.468: INFO: Creating new exec pod
May 17 06:39:45.477: INFO: Waiting up to 5m0s for pod "execpod-affinity6rcgl" in namespace "services-4410" to be "running"
May 17 06:39:45.483: INFO: Pod "execpod-affinity6rcgl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.614855ms
May 17 06:39:47.489: INFO: Pod "execpod-affinity6rcgl": Phase="Running", Reason="", readiness=true. Elapsed: 2.011962313s
May 17 06:39:47.489: INFO: Pod "execpod-affinity6rcgl" satisfied condition "running"
May 17 06:39:48.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4410 exec execpod-affinity6rcgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
May 17 06:39:48.728: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 17 06:39:48.728: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:39:48.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4410 exec execpod-affinity6rcgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.195.150 80'
May 17 06:39:48.923: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.195.150 80\nConnection to 10.0.195.150 80 port [tcp/http] succeeded!\n"
May 17 06:39:48.923: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:39:48.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4410 exec execpod-affinity6rcgl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.195.150:80/ ; done'
May 17 06:39:49.152: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n"
May 17 06:39:49.152: INFO: stdout: "\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv"
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
May 17 06:39:49.152: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4410, will wait for the garbage collector to delete the pods 05/17/23 06:39:49.172
May 17 06:39:49.239: INFO: Deleting ReplicationController affinity-clusterip took: 10.592449ms
May 17 06:39:49.340: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.944825ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:39:51.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4410" for this suite. 05/17/23 06:39:51.077
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":246,"skipped":4713,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.736 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:39:42.351
    May 17 06:39:42.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:39:42.352
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:39:42.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:39:42.375
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-4410 05/17/23 06:39:42.38
    STEP: creating service affinity-clusterip in namespace services-4410 05/17/23 06:39:42.38
    STEP: creating replication controller affinity-clusterip in namespace services-4410 05/17/23 06:39:42.398
    I0517 06:39:42.406188      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4410, replica count: 3
    I0517 06:39:45.457432      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 06:39:45.468: INFO: Creating new exec pod
    May 17 06:39:45.477: INFO: Waiting up to 5m0s for pod "execpod-affinity6rcgl" in namespace "services-4410" to be "running"
    May 17 06:39:45.483: INFO: Pod "execpod-affinity6rcgl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.614855ms
    May 17 06:39:47.489: INFO: Pod "execpod-affinity6rcgl": Phase="Running", Reason="", readiness=true. Elapsed: 2.011962313s
    May 17 06:39:47.489: INFO: Pod "execpod-affinity6rcgl" satisfied condition "running"
    May 17 06:39:48.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4410 exec execpod-affinity6rcgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    May 17 06:39:48.728: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    May 17 06:39:48.728: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:39:48.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4410 exec execpod-affinity6rcgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.195.150 80'
    May 17 06:39:48.923: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.195.150 80\nConnection to 10.0.195.150 80 port [tcp/http] succeeded!\n"
    May 17 06:39:48.923: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:39:48.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4410 exec execpod-affinity6rcgl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.195.150:80/ ; done'
    May 17 06:39:49.152: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.195.150:80/\n"
    May 17 06:39:49.152: INFO: stdout: "\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv\naffinity-clusterip-r7rkv"
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Received response from host: affinity-clusterip-r7rkv
    May 17 06:39:49.152: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-4410, will wait for the garbage collector to delete the pods 05/17/23 06:39:49.172
    May 17 06:39:49.239: INFO: Deleting ReplicationController affinity-clusterip took: 10.592449ms
    May 17 06:39:49.340: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.944825ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:39:51.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4410" for this suite. 05/17/23 06:39:51.077
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:39:51.089
May 17 06:39:51.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename gc 05/17/23 06:39:51.089
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:39:51.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:39:51.113
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 05/17/23 06:39:51.13
STEP: create the rc2 05/17/23 06:39:51.138
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/17/23 06:39:56.152
STEP: delete the rc simpletest-rc-to-be-deleted 05/17/23 06:39:56.609
STEP: wait for the rc to be deleted 05/17/23 06:39:56.619
STEP: Gathering metrics 05/17/23 06:40:01.638
W0517 06:40:01.650339      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 17 06:40:01.650: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 17 06:40:01.650: INFO: Deleting pod "simpletest-rc-to-be-deleted-28srm" in namespace "gc-3326"
May 17 06:40:01.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mbwd" in namespace "gc-3326"
May 17 06:40:01.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vnz9" in namespace "gc-3326"
May 17 06:40:01.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-2z98w" in namespace "gc-3326"
May 17 06:40:01.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-45fcr" in namespace "gc-3326"
May 17 06:40:01.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fddw" in namespace "gc-3326"
May 17 06:40:01.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lq52" in namespace "gc-3326"
May 17 06:40:01.768: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xst5" in namespace "gc-3326"
May 17 06:40:01.785: INFO: Deleting pod "simpletest-rc-to-be-deleted-5r9db" in namespace "gc-3326"
May 17 06:40:01.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-5v4wr" in namespace "gc-3326"
May 17 06:40:01.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vk27" in namespace "gc-3326"
May 17 06:40:01.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xsvz" in namespace "gc-3326"
May 17 06:40:01.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-6f4xx" in namespace "gc-3326"
May 17 06:40:01.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fdkq" in namespace "gc-3326"
May 17 06:40:01.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wlll" in namespace "gc-3326"
May 17 06:40:01.909: INFO: Deleting pod "simpletest-rc-to-be-deleted-78rp4" in namespace "gc-3326"
May 17 06:40:01.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dpf6" in namespace "gc-3326"
May 17 06:40:01.943: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tn6s" in namespace "gc-3326"
May 17 06:40:01.964: INFO: Deleting pod "simpletest-rc-to-be-deleted-85hlm" in namespace "gc-3326"
May 17 06:40:01.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jdrd" in namespace "gc-3326"
May 17 06:40:01.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qd7h" in namespace "gc-3326"
May 17 06:40:02.017: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wv6x" in namespace "gc-3326"
May 17 06:40:02.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-92n27" in namespace "gc-3326"
May 17 06:40:02.051: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qcts" in namespace "gc-3326"
May 17 06:40:02.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zdmr" in namespace "gc-3326"
May 17 06:40:02.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm8xg" in namespace "gc-3326"
May 17 06:40:02.108: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvs2x" in namespace "gc-3326"
May 17 06:40:02.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4zc4" in namespace "gc-3326"
May 17 06:40:02.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccj2w" in namespace "gc-3326"
May 17 06:40:02.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-clln6" in namespace "gc-3326"
May 17 06:40:02.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-crmqq" in namespace "gc-3326"
May 17 06:40:02.217: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx428" in namespace "gc-3326"
May 17 06:40:02.237: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxzfm" in namespace "gc-3326"
May 17 06:40:02.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkj6j" in namespace "gc-3326"
May 17 06:40:02.268: INFO: Deleting pod "simpletest-rc-to-be-deleted-dr79d" in namespace "gc-3326"
May 17 06:40:02.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-drcrg" in namespace "gc-3326"
May 17 06:40:02.307: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsb7d" in namespace "gc-3326"
May 17 06:40:02.323: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5xt7" in namespace "gc-3326"
May 17 06:40:02.341: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjjc4" in namespace "gc-3326"
May 17 06:40:02.358: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmqbv" in namespace "gc-3326"
May 17 06:40:02.376: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft6ng" in namespace "gc-3326"
May 17 06:40:02.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2x4f" in namespace "gc-3326"
May 17 06:40:02.409: INFO: Deleting pod "simpletest-rc-to-be-deleted-g678h" in namespace "gc-3326"
May 17 06:40:02.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7b7d" in namespace "gc-3326"
May 17 06:40:02.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8d85" in namespace "gc-3326"
May 17 06:40:02.461: INFO: Deleting pod "simpletest-rc-to-be-deleted-gw494" in namespace "gc-3326"
May 17 06:40:02.475: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqzrk" in namespace "gc-3326"
May 17 06:40:02.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwmjc" in namespace "gc-3326"
May 17 06:40:02.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-hznfp" in namespace "gc-3326"
May 17 06:40:02.520: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5nt7" in namespace "gc-3326"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May 17 06:40:02.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3326" for this suite. 05/17/23 06:40:02.55
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":247,"skipped":4737,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.471 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:39:51.089
    May 17 06:39:51.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename gc 05/17/23 06:39:51.089
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:39:51.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:39:51.113
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 05/17/23 06:39:51.13
    STEP: create the rc2 05/17/23 06:39:51.138
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/17/23 06:39:56.152
    STEP: delete the rc simpletest-rc-to-be-deleted 05/17/23 06:39:56.609
    STEP: wait for the rc to be deleted 05/17/23 06:39:56.619
    STEP: Gathering metrics 05/17/23 06:40:01.638
    W0517 06:40:01.650339      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May 17 06:40:01.650: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May 17 06:40:01.650: INFO: Deleting pod "simpletest-rc-to-be-deleted-28srm" in namespace "gc-3326"
    May 17 06:40:01.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mbwd" in namespace "gc-3326"
    May 17 06:40:01.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vnz9" in namespace "gc-3326"
    May 17 06:40:01.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-2z98w" in namespace "gc-3326"
    May 17 06:40:01.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-45fcr" in namespace "gc-3326"
    May 17 06:40:01.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fddw" in namespace "gc-3326"
    May 17 06:40:01.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lq52" in namespace "gc-3326"
    May 17 06:40:01.768: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xst5" in namespace "gc-3326"
    May 17 06:40:01.785: INFO: Deleting pod "simpletest-rc-to-be-deleted-5r9db" in namespace "gc-3326"
    May 17 06:40:01.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-5v4wr" in namespace "gc-3326"
    May 17 06:40:01.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vk27" in namespace "gc-3326"
    May 17 06:40:01.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xsvz" in namespace "gc-3326"
    May 17 06:40:01.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-6f4xx" in namespace "gc-3326"
    May 17 06:40:01.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fdkq" in namespace "gc-3326"
    May 17 06:40:01.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wlll" in namespace "gc-3326"
    May 17 06:40:01.909: INFO: Deleting pod "simpletest-rc-to-be-deleted-78rp4" in namespace "gc-3326"
    May 17 06:40:01.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dpf6" in namespace "gc-3326"
    May 17 06:40:01.943: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tn6s" in namespace "gc-3326"
    May 17 06:40:01.964: INFO: Deleting pod "simpletest-rc-to-be-deleted-85hlm" in namespace "gc-3326"
    May 17 06:40:01.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jdrd" in namespace "gc-3326"
    May 17 06:40:01.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qd7h" in namespace "gc-3326"
    May 17 06:40:02.017: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wv6x" in namespace "gc-3326"
    May 17 06:40:02.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-92n27" in namespace "gc-3326"
    May 17 06:40:02.051: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qcts" in namespace "gc-3326"
    May 17 06:40:02.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zdmr" in namespace "gc-3326"
    May 17 06:40:02.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm8xg" in namespace "gc-3326"
    May 17 06:40:02.108: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvs2x" in namespace "gc-3326"
    May 17 06:40:02.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4zc4" in namespace "gc-3326"
    May 17 06:40:02.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccj2w" in namespace "gc-3326"
    May 17 06:40:02.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-clln6" in namespace "gc-3326"
    May 17 06:40:02.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-crmqq" in namespace "gc-3326"
    May 17 06:40:02.217: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx428" in namespace "gc-3326"
    May 17 06:40:02.237: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxzfm" in namespace "gc-3326"
    May 17 06:40:02.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkj6j" in namespace "gc-3326"
    May 17 06:40:02.268: INFO: Deleting pod "simpletest-rc-to-be-deleted-dr79d" in namespace "gc-3326"
    May 17 06:40:02.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-drcrg" in namespace "gc-3326"
    May 17 06:40:02.307: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsb7d" in namespace "gc-3326"
    May 17 06:40:02.323: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5xt7" in namespace "gc-3326"
    May 17 06:40:02.341: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjjc4" in namespace "gc-3326"
    May 17 06:40:02.358: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmqbv" in namespace "gc-3326"
    May 17 06:40:02.376: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft6ng" in namespace "gc-3326"
    May 17 06:40:02.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2x4f" in namespace "gc-3326"
    May 17 06:40:02.409: INFO: Deleting pod "simpletest-rc-to-be-deleted-g678h" in namespace "gc-3326"
    May 17 06:40:02.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7b7d" in namespace "gc-3326"
    May 17 06:40:02.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8d85" in namespace "gc-3326"
    May 17 06:40:02.461: INFO: Deleting pod "simpletest-rc-to-be-deleted-gw494" in namespace "gc-3326"
    May 17 06:40:02.475: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqzrk" in namespace "gc-3326"
    May 17 06:40:02.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwmjc" in namespace "gc-3326"
    May 17 06:40:02.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-hznfp" in namespace "gc-3326"
    May 17 06:40:02.520: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5nt7" in namespace "gc-3326"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May 17 06:40:02.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3326" for this suite. 05/17/23 06:40:02.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:40:02.561
May 17 06:40:02.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:40:02.561
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:02.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:02.587
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 05/17/23 06:40:02.592
May 17 06:40:02.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-6917 cluster-info'
May 17 06:40:02.685: INFO: stderr: ""
May 17 06:40:02.685: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:40:02.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6917" for this suite. 05/17/23 06:40:02.693
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":248,"skipped":4744,"failed":0}
------------------------------
â€¢ [0.143 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:40:02.561
    May 17 06:40:02.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:40:02.561
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:02.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:02.587
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 05/17/23 06:40:02.592
    May 17 06:40:02.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-6917 cluster-info'
    May 17 06:40:02.685: INFO: stderr: ""
    May 17 06:40:02.685: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:40:02.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6917" for this suite. 05/17/23 06:40:02.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:40:02.704
May 17 06:40:02.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename namespaces 05/17/23 06:40:02.705
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:02.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:02.731
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 05/17/23 06:40:02.737
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:02.76
STEP: Creating a service in the namespace 05/17/23 06:40:02.765
STEP: Deleting the namespace 05/17/23 06:40:02.78
STEP: Waiting for the namespace to be removed. 05/17/23 06:40:02.79
STEP: Recreating the namespace 05/17/23 06:40:08.797
STEP: Verifying there is no service in the namespace 05/17/23 06:40:08.818
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May 17 06:40:08.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8553" for this suite. 05/17/23 06:40:08.835
STEP: Destroying namespace "nsdeletetest-3980" for this suite. 05/17/23 06:40:08.848
May 17 06:40:08.854: INFO: Namespace nsdeletetest-3980 was already deleted
STEP: Destroying namespace "nsdeletetest-5798" for this suite. 05/17/23 06:40:08.854
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":249,"skipped":4760,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.160 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:40:02.704
    May 17 06:40:02.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename namespaces 05/17/23 06:40:02.705
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:02.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:02.731
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 05/17/23 06:40:02.737
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:02.76
    STEP: Creating a service in the namespace 05/17/23 06:40:02.765
    STEP: Deleting the namespace 05/17/23 06:40:02.78
    STEP: Waiting for the namespace to be removed. 05/17/23 06:40:02.79
    STEP: Recreating the namespace 05/17/23 06:40:08.797
    STEP: Verifying there is no service in the namespace 05/17/23 06:40:08.818
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May 17 06:40:08.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8553" for this suite. 05/17/23 06:40:08.835
    STEP: Destroying namespace "nsdeletetest-3980" for this suite. 05/17/23 06:40:08.848
    May 17 06:40:08.854: INFO: Namespace nsdeletetest-3980 was already deleted
    STEP: Destroying namespace "nsdeletetest-5798" for this suite. 05/17/23 06:40:08.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:40:08.865
May 17 06:40:08.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename daemonsets 05/17/23 06:40:08.865
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:08.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:08.891
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 05/17/23 06:40:08.941
STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 06:40:08.95
May 17 06:40:08.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 06:40:08.967: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 06:40:10.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 06:40:10.007: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 06:40:10.984: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 06:40:10.984: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 06:40:11.987: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 17 06:40:11.987: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 05/17/23 06:40:11.993
May 17 06:40:11.999: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 05/17/23 06:40:11.999
May 17 06:40:12.027: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 05/17/23 06:40:12.027
May 17 06:40:12.030: INFO: Observed &DaemonSet event: ADDED
May 17 06:40:12.030: INFO: Observed &DaemonSet event: MODIFIED
May 17 06:40:12.030: INFO: Observed &DaemonSet event: MODIFIED
May 17 06:40:12.030: INFO: Observed &DaemonSet event: MODIFIED
May 17 06:40:12.030: INFO: Observed &DaemonSet event: MODIFIED
May 17 06:40:12.030: INFO: Found daemon set daemon-set in namespace daemonsets-9539 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 17 06:40:12.030: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 05/17/23 06:40:12.03
STEP: watching for the daemon set status to be patched 05/17/23 06:40:12.04
May 17 06:40:12.042: INFO: Observed &DaemonSet event: ADDED
May 17 06:40:12.042: INFO: Observed &DaemonSet event: MODIFIED
May 17 06:40:12.043: INFO: Observed &DaemonSet event: MODIFIED
May 17 06:40:12.043: INFO: Observed &DaemonSet event: MODIFIED
May 17 06:40:12.043: INFO: Observed &DaemonSet event: MODIFIED
May 17 06:40:12.043: INFO: Observed daemon set daemon-set in namespace daemonsets-9539 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 17 06:40:12.043: INFO: Observed &DaemonSet event: MODIFIED
May 17 06:40:12.043: INFO: Found daemon set daemon-set in namespace daemonsets-9539 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
May 17 06:40:12.043: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/17/23 06:40:12.048
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9539, will wait for the garbage collector to delete the pods 05/17/23 06:40:12.048
May 17 06:40:12.113: INFO: Deleting DaemonSet.extensions daemon-set took: 8.903762ms
May 17 06:40:12.214: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.058657ms
May 17 06:40:13.620: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 06:40:13.620: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 17 06:40:13.624: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1481421"},"items":null}

May 17 06:40:13.629: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1481421"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May 17 06:40:13.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9539" for this suite. 05/17/23 06:40:13.659
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":250,"skipped":4770,"failed":0}
------------------------------
â€¢ [4.802 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:40:08.865
    May 17 06:40:08.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename daemonsets 05/17/23 06:40:08.865
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:08.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:08.891
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 05/17/23 06:40:08.941
    STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 06:40:08.95
    May 17 06:40:08.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 06:40:08.967: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 06:40:10.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 06:40:10.007: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 06:40:10.984: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 06:40:10.984: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 06:40:11.987: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 17 06:40:11.987: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 05/17/23 06:40:11.993
    May 17 06:40:11.999: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 05/17/23 06:40:11.999
    May 17 06:40:12.027: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 05/17/23 06:40:12.027
    May 17 06:40:12.030: INFO: Observed &DaemonSet event: ADDED
    May 17 06:40:12.030: INFO: Observed &DaemonSet event: MODIFIED
    May 17 06:40:12.030: INFO: Observed &DaemonSet event: MODIFIED
    May 17 06:40:12.030: INFO: Observed &DaemonSet event: MODIFIED
    May 17 06:40:12.030: INFO: Observed &DaemonSet event: MODIFIED
    May 17 06:40:12.030: INFO: Found daemon set daemon-set in namespace daemonsets-9539 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May 17 06:40:12.030: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 05/17/23 06:40:12.03
    STEP: watching for the daemon set status to be patched 05/17/23 06:40:12.04
    May 17 06:40:12.042: INFO: Observed &DaemonSet event: ADDED
    May 17 06:40:12.042: INFO: Observed &DaemonSet event: MODIFIED
    May 17 06:40:12.043: INFO: Observed &DaemonSet event: MODIFIED
    May 17 06:40:12.043: INFO: Observed &DaemonSet event: MODIFIED
    May 17 06:40:12.043: INFO: Observed &DaemonSet event: MODIFIED
    May 17 06:40:12.043: INFO: Observed daemon set daemon-set in namespace daemonsets-9539 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May 17 06:40:12.043: INFO: Observed &DaemonSet event: MODIFIED
    May 17 06:40:12.043: INFO: Found daemon set daemon-set in namespace daemonsets-9539 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    May 17 06:40:12.043: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/17/23 06:40:12.048
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9539, will wait for the garbage collector to delete the pods 05/17/23 06:40:12.048
    May 17 06:40:12.113: INFO: Deleting DaemonSet.extensions daemon-set took: 8.903762ms
    May 17 06:40:12.214: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.058657ms
    May 17 06:40:13.620: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 06:40:13.620: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 17 06:40:13.624: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1481421"},"items":null}

    May 17 06:40:13.629: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1481421"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May 17 06:40:13.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9539" for this suite. 05/17/23 06:40:13.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:40:13.668
May 17 06:40:13.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename events 05/17/23 06:40:13.669
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:13.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:13.695
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 05/17/23 06:40:13.699
STEP: get a list of Events with a label in the current namespace 05/17/23 06:40:13.725
STEP: delete a list of events 05/17/23 06:40:13.731
May 17 06:40:13.731: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/17/23 06:40:13.753
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
May 17 06:40:13.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7230" for this suite. 05/17/23 06:40:13.766
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":251,"skipped":4798,"failed":0}
------------------------------
â€¢ [0.107 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:40:13.668
    May 17 06:40:13.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename events 05/17/23 06:40:13.669
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:13.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:13.695
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 05/17/23 06:40:13.699
    STEP: get a list of Events with a label in the current namespace 05/17/23 06:40:13.725
    STEP: delete a list of events 05/17/23 06:40:13.731
    May 17 06:40:13.731: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/17/23 06:40:13.753
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    May 17 06:40:13.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-7230" for this suite. 05/17/23 06:40:13.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:40:13.777
May 17 06:40:13.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename events 05/17/23 06:40:13.778
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:13.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:13.804
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 05/17/23 06:40:13.807
May 17 06:40:13.821: INFO: created test-event-1
May 17 06:40:13.830: INFO: created test-event-2
May 17 06:40:13.839: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 05/17/23 06:40:13.839
STEP: delete collection of events 05/17/23 06:40:13.845
May 17 06:40:13.845: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/17/23 06:40:13.867
May 17 06:40:13.867: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
May 17 06:40:13.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8185" for this suite. 05/17/23 06:40:13.88
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":252,"skipped":4837,"failed":0}
------------------------------
â€¢ [0.113 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:40:13.777
    May 17 06:40:13.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename events 05/17/23 06:40:13.778
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:13.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:13.804
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 05/17/23 06:40:13.807
    May 17 06:40:13.821: INFO: created test-event-1
    May 17 06:40:13.830: INFO: created test-event-2
    May 17 06:40:13.839: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 05/17/23 06:40:13.839
    STEP: delete collection of events 05/17/23 06:40:13.845
    May 17 06:40:13.845: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/17/23 06:40:13.867
    May 17 06:40:13.867: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    May 17 06:40:13.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8185" for this suite. 05/17/23 06:40:13.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:40:13.891
May 17 06:40:13.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename runtimeclass 05/17/23 06:40:13.891
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:13.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:13.914
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May 17 06:40:13.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1327" for this suite. 05/17/23 06:40:13.936
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":253,"skipped":4854,"failed":0}
------------------------------
â€¢ [0.055 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:40:13.891
    May 17 06:40:13.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename runtimeclass 05/17/23 06:40:13.891
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:13.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:13.914
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May 17 06:40:13.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1327" for this suite. 05/17/23 06:40:13.936
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:40:13.946
May 17 06:40:13.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/17/23 06:40:13.946
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:13.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:13.969
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 05/17/23 06:40:13.974
STEP: Creating hostNetwork=false pod 05/17/23 06:40:13.974
May 17 06:40:13.997: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-663" to be "running and ready"
May 17 06:40:14.001: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.512633ms
May 17 06:40:14.001: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May 17 06:40:16.008: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011393098s
May 17 06:40:16.008: INFO: The phase of Pod test-pod is Running (Ready = true)
May 17 06:40:16.008: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 05/17/23 06:40:16.03
May 17 06:40:16.040: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-663" to be "running and ready"
May 17 06:40:16.045: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.878553ms
May 17 06:40:16.045: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May 17 06:40:18.050: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010256055s
May 17 06:40:18.050: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
May 17 06:40:18.050: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 05/17/23 06:40:18.056
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/17/23 06:40:18.056
May 17 06:40:18.056: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:40:18.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:40:18.057: INFO: ExecWithOptions: Clientset creation
May 17 06:40:18.057: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May 17 06:40:18.210: INFO: Exec stderr: ""
May 17 06:40:18.210: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:40:18.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:40:18.211: INFO: ExecWithOptions: Clientset creation
May 17 06:40:18.211: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May 17 06:40:18.330: INFO: Exec stderr: ""
May 17 06:40:18.330: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:40:18.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:40:18.330: INFO: ExecWithOptions: Clientset creation
May 17 06:40:18.330: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May 17 06:40:18.434: INFO: Exec stderr: ""
May 17 06:40:18.434: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:40:18.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:40:18.435: INFO: ExecWithOptions: Clientset creation
May 17 06:40:18.435: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May 17 06:40:18.529: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/17/23 06:40:18.529
May 17 06:40:18.529: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:40:18.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:40:18.530: INFO: ExecWithOptions: Clientset creation
May 17 06:40:18.530: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May 17 06:40:18.666: INFO: Exec stderr: ""
May 17 06:40:18.666: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:40:18.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:40:18.666: INFO: ExecWithOptions: Clientset creation
May 17 06:40:18.666: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May 17 06:40:18.758: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/17/23 06:40:18.758
May 17 06:40:18.758: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:40:18.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:40:18.758: INFO: ExecWithOptions: Clientset creation
May 17 06:40:18.759: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May 17 06:40:18.858: INFO: Exec stderr: ""
May 17 06:40:18.858: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:40:18.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:40:18.858: INFO: ExecWithOptions: Clientset creation
May 17 06:40:18.858: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May 17 06:40:18.962: INFO: Exec stderr: ""
May 17 06:40:18.962: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:40:18.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:40:18.962: INFO: ExecWithOptions: Clientset creation
May 17 06:40:18.962: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May 17 06:40:19.070: INFO: Exec stderr: ""
May 17 06:40:19.070: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:40:19.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:40:19.071: INFO: ExecWithOptions: Clientset creation
May 17 06:40:19.071: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May 17 06:40:19.179: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
May 17 06:40:19.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-663" for this suite. 05/17/23 06:40:19.187
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":254,"skipped":4854,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.251 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:40:13.946
    May 17 06:40:13.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/17/23 06:40:13.946
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:13.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:13.969
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 05/17/23 06:40:13.974
    STEP: Creating hostNetwork=false pod 05/17/23 06:40:13.974
    May 17 06:40:13.997: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-663" to be "running and ready"
    May 17 06:40:14.001: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.512633ms
    May 17 06:40:14.001: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:40:16.008: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011393098s
    May 17 06:40:16.008: INFO: The phase of Pod test-pod is Running (Ready = true)
    May 17 06:40:16.008: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 05/17/23 06:40:16.03
    May 17 06:40:16.040: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-663" to be "running and ready"
    May 17 06:40:16.045: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.878553ms
    May 17 06:40:16.045: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:40:18.050: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010256055s
    May 17 06:40:18.050: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    May 17 06:40:18.050: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 05/17/23 06:40:18.056
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/17/23 06:40:18.056
    May 17 06:40:18.056: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:40:18.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:40:18.057: INFO: ExecWithOptions: Clientset creation
    May 17 06:40:18.057: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May 17 06:40:18.210: INFO: Exec stderr: ""
    May 17 06:40:18.210: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:40:18.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:40:18.211: INFO: ExecWithOptions: Clientset creation
    May 17 06:40:18.211: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May 17 06:40:18.330: INFO: Exec stderr: ""
    May 17 06:40:18.330: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:40:18.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:40:18.330: INFO: ExecWithOptions: Clientset creation
    May 17 06:40:18.330: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May 17 06:40:18.434: INFO: Exec stderr: ""
    May 17 06:40:18.434: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:40:18.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:40:18.435: INFO: ExecWithOptions: Clientset creation
    May 17 06:40:18.435: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May 17 06:40:18.529: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/17/23 06:40:18.529
    May 17 06:40:18.529: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:40:18.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:40:18.530: INFO: ExecWithOptions: Clientset creation
    May 17 06:40:18.530: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May 17 06:40:18.666: INFO: Exec stderr: ""
    May 17 06:40:18.666: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:40:18.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:40:18.666: INFO: ExecWithOptions: Clientset creation
    May 17 06:40:18.666: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May 17 06:40:18.758: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/17/23 06:40:18.758
    May 17 06:40:18.758: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:40:18.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:40:18.758: INFO: ExecWithOptions: Clientset creation
    May 17 06:40:18.759: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May 17 06:40:18.858: INFO: Exec stderr: ""
    May 17 06:40:18.858: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:40:18.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:40:18.858: INFO: ExecWithOptions: Clientset creation
    May 17 06:40:18.858: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May 17 06:40:18.962: INFO: Exec stderr: ""
    May 17 06:40:18.962: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:40:18.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:40:18.962: INFO: ExecWithOptions: Clientset creation
    May 17 06:40:18.962: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May 17 06:40:19.070: INFO: Exec stderr: ""
    May 17 06:40:19.070: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-663 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:40:19.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:40:19.071: INFO: ExecWithOptions: Clientset creation
    May 17 06:40:19.071: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-663/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May 17 06:40:19.179: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    May 17 06:40:19.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-663" for this suite. 05/17/23 06:40:19.187
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:40:19.197
May 17 06:40:19.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:40:19.198
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:19.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:19.223
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 05/17/23 06:40:19.227
May 17 06:40:19.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493" in namespace "projected-6173" to be "Succeeded or Failed"
May 17 06:40:19.246: INFO: Pod "downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493": Phase="Pending", Reason="", readiness=false. Elapsed: 4.246357ms
May 17 06:40:21.251: INFO: Pod "downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010070828s
May 17 06:40:23.252: INFO: Pod "downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011151038s
STEP: Saw pod success 05/17/23 06:40:23.252
May 17 06:40:23.253: INFO: Pod "downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493" satisfied condition "Succeeded or Failed"
May 17 06:40:23.259: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000d pod downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493 container client-container: <nil>
STEP: delete the pod 05/17/23 06:40:23.271
May 17 06:40:23.289: INFO: Waiting for pod downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493 to disappear
May 17 06:40:23.294: INFO: Pod downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May 17 06:40:23.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6173" for this suite. 05/17/23 06:40:23.303
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":255,"skipped":4854,"failed":0}
------------------------------
â€¢ [4.115 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:40:19.197
    May 17 06:40:19.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:40:19.198
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:19.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:19.223
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 05/17/23 06:40:19.227
    May 17 06:40:19.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493" in namespace "projected-6173" to be "Succeeded or Failed"
    May 17 06:40:19.246: INFO: Pod "downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493": Phase="Pending", Reason="", readiness=false. Elapsed: 4.246357ms
    May 17 06:40:21.251: INFO: Pod "downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010070828s
    May 17 06:40:23.252: INFO: Pod "downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011151038s
    STEP: Saw pod success 05/17/23 06:40:23.252
    May 17 06:40:23.253: INFO: Pod "downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493" satisfied condition "Succeeded or Failed"
    May 17 06:40:23.259: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000d pod downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493 container client-container: <nil>
    STEP: delete the pod 05/17/23 06:40:23.271
    May 17 06:40:23.289: INFO: Waiting for pod downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493 to disappear
    May 17 06:40:23.294: INFO: Pod downwardapi-volume-9a2a5c3b-e346-4547-b51b-f6e5ba843493 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May 17 06:40:23.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6173" for this suite. 05/17/23 06:40:23.303
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:40:23.313
May 17 06:40:23.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 06:40:23.313
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:23.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:23.341
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-1766255c-7b73-4aff-9e5c-468bb16a1365 05/17/23 06:40:23.345
STEP: Creating a pod to test consume secrets 05/17/23 06:40:23.351
May 17 06:40:23.369: INFO: Waiting up to 5m0s for pod "pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7" in namespace "secrets-8982" to be "Succeeded or Failed"
May 17 06:40:23.374: INFO: Pod "pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.63372ms
May 17 06:40:25.381: INFO: Pod "pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012009389s
May 17 06:40:27.381: INFO: Pod "pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012002917s
STEP: Saw pod success 05/17/23 06:40:27.381
May 17 06:40:27.381: INFO: Pod "pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7" satisfied condition "Succeeded or Failed"
May 17 06:40:27.385: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000d pod pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7 container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 06:40:27.395
May 17 06:40:27.412: INFO: Waiting for pod pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7 to disappear
May 17 06:40:27.417: INFO: Pod pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May 17 06:40:27.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8982" for this suite. 05/17/23 06:40:27.426
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":256,"skipped":4857,"failed":0}
------------------------------
â€¢ [4.122 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:40:23.313
    May 17 06:40:23.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 06:40:23.313
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:23.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:23.341
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-1766255c-7b73-4aff-9e5c-468bb16a1365 05/17/23 06:40:23.345
    STEP: Creating a pod to test consume secrets 05/17/23 06:40:23.351
    May 17 06:40:23.369: INFO: Waiting up to 5m0s for pod "pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7" in namespace "secrets-8982" to be "Succeeded or Failed"
    May 17 06:40:23.374: INFO: Pod "pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.63372ms
    May 17 06:40:25.381: INFO: Pod "pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012009389s
    May 17 06:40:27.381: INFO: Pod "pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012002917s
    STEP: Saw pod success 05/17/23 06:40:27.381
    May 17 06:40:27.381: INFO: Pod "pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7" satisfied condition "Succeeded or Failed"
    May 17 06:40:27.385: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000d pod pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7 container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 06:40:27.395
    May 17 06:40:27.412: INFO: Waiting for pod pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7 to disappear
    May 17 06:40:27.417: INFO: Pod pod-secrets-b88fb984-3d53-4c31-ac41-165087aee2f7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May 17 06:40:27.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8982" for this suite. 05/17/23 06:40:27.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:40:27.435
May 17 06:40:27.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename subpath 05/17/23 06:40:27.436
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:27.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:27.46
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/17/23 06:40:27.464
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-qnr5 05/17/23 06:40:27.475
STEP: Creating a pod to test atomic-volume-subpath 05/17/23 06:40:27.475
May 17 06:40:27.488: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qnr5" in namespace "subpath-5292" to be "Succeeded or Failed"
May 17 06:40:27.493: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272656ms
May 17 06:40:29.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009689048s
May 17 06:40:31.499: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 4.010492083s
May 17 06:40:33.499: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 6.011241795s
May 17 06:40:35.500: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 8.011968411s
May 17 06:40:37.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 10.009252065s
May 17 06:40:39.499: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 12.011071052s
May 17 06:40:41.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 14.009804024s
May 17 06:40:43.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 16.009940437s
May 17 06:40:45.499: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 18.010573585s
May 17 06:40:47.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 20.010083096s
May 17 06:40:49.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=false. Elapsed: 22.009572822s
May 17 06:40:51.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009563833s
STEP: Saw pod success 05/17/23 06:40:51.498
May 17 06:40:51.498: INFO: Pod "pod-subpath-test-secret-qnr5" satisfied condition "Succeeded or Failed"
May 17 06:40:51.502: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-subpath-test-secret-qnr5 container test-container-subpath-secret-qnr5: <nil>
STEP: delete the pod 05/17/23 06:40:51.513
May 17 06:40:51.528: INFO: Waiting for pod pod-subpath-test-secret-qnr5 to disappear
May 17 06:40:51.532: INFO: Pod pod-subpath-test-secret-qnr5 no longer exists
STEP: Deleting pod pod-subpath-test-secret-qnr5 05/17/23 06:40:51.532
May 17 06:40:51.532: INFO: Deleting pod "pod-subpath-test-secret-qnr5" in namespace "subpath-5292"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May 17 06:40:51.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5292" for this suite. 05/17/23 06:40:51.542
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":257,"skipped":4871,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.116 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:40:27.435
    May 17 06:40:27.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename subpath 05/17/23 06:40:27.436
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:27.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:27.46
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/17/23 06:40:27.464
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-qnr5 05/17/23 06:40:27.475
    STEP: Creating a pod to test atomic-volume-subpath 05/17/23 06:40:27.475
    May 17 06:40:27.488: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qnr5" in namespace "subpath-5292" to be "Succeeded or Failed"
    May 17 06:40:27.493: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272656ms
    May 17 06:40:29.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009689048s
    May 17 06:40:31.499: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 4.010492083s
    May 17 06:40:33.499: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 6.011241795s
    May 17 06:40:35.500: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 8.011968411s
    May 17 06:40:37.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 10.009252065s
    May 17 06:40:39.499: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 12.011071052s
    May 17 06:40:41.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 14.009804024s
    May 17 06:40:43.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 16.009940437s
    May 17 06:40:45.499: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 18.010573585s
    May 17 06:40:47.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=true. Elapsed: 20.010083096s
    May 17 06:40:49.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Running", Reason="", readiness=false. Elapsed: 22.009572822s
    May 17 06:40:51.498: INFO: Pod "pod-subpath-test-secret-qnr5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009563833s
    STEP: Saw pod success 05/17/23 06:40:51.498
    May 17 06:40:51.498: INFO: Pod "pod-subpath-test-secret-qnr5" satisfied condition "Succeeded or Failed"
    May 17 06:40:51.502: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-subpath-test-secret-qnr5 container test-container-subpath-secret-qnr5: <nil>
    STEP: delete the pod 05/17/23 06:40:51.513
    May 17 06:40:51.528: INFO: Waiting for pod pod-subpath-test-secret-qnr5 to disappear
    May 17 06:40:51.532: INFO: Pod pod-subpath-test-secret-qnr5 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-qnr5 05/17/23 06:40:51.532
    May 17 06:40:51.532: INFO: Deleting pod "pod-subpath-test-secret-qnr5" in namespace "subpath-5292"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May 17 06:40:51.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5292" for this suite. 05/17/23 06:40:51.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:40:51.552
May 17 06:40:51.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename daemonsets 05/17/23 06:40:51.552
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:51.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:51.575
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
May 17 06:40:51.609: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 05/17/23 06:40:51.615
May 17 06:40:51.619: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 06:40:51.619: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 05/17/23 06:40:51.619
May 17 06:40:51.661: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 06:40:51.661: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 06:40:52.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 06:40:52.667: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 06:40:53.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 06:40:53.667: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 05/17/23 06:40:53.672
May 17 06:40:53.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 06:40:53.717: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
May 17 06:40:54.722: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 06:40:54.722: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/17/23 06:40:54.722
May 17 06:40:54.736: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 06:40:54.736: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 06:40:55.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 06:40:55.742: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 06:40:56.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 06:40:56.743: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 06:40:57.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 06:40:57.742: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/17/23 06:40:57.753
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8840, will wait for the garbage collector to delete the pods 05/17/23 06:40:57.753
May 17 06:40:57.816: INFO: Deleting DaemonSet.extensions daemon-set took: 8.170917ms
May 17 06:40:57.917: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.426734ms
May 17 06:40:59.822: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 06:40:59.822: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 17 06:40:59.826: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1482040"},"items":null}

May 17 06:40:59.843: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1482040"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May 17 06:40:59.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8840" for this suite. 05/17/23 06:40:59.897
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":258,"skipped":4878,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.355 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:40:51.552
    May 17 06:40:51.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename daemonsets 05/17/23 06:40:51.552
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:51.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:51.575
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    May 17 06:40:51.609: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 05/17/23 06:40:51.615
    May 17 06:40:51.619: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 06:40:51.619: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 05/17/23 06:40:51.619
    May 17 06:40:51.661: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 06:40:51.661: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 06:40:52.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 06:40:52.667: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 06:40:53.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 06:40:53.667: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 05/17/23 06:40:53.672
    May 17 06:40:53.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 06:40:53.717: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    May 17 06:40:54.722: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 06:40:54.722: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/17/23 06:40:54.722
    May 17 06:40:54.736: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 06:40:54.736: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 06:40:55.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 06:40:55.742: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 06:40:56.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 06:40:56.743: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 06:40:57.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 06:40:57.742: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/17/23 06:40:57.753
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8840, will wait for the garbage collector to delete the pods 05/17/23 06:40:57.753
    May 17 06:40:57.816: INFO: Deleting DaemonSet.extensions daemon-set took: 8.170917ms
    May 17 06:40:57.917: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.426734ms
    May 17 06:40:59.822: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 06:40:59.822: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 17 06:40:59.826: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1482040"},"items":null}

    May 17 06:40:59.843: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1482040"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May 17 06:40:59.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8840" for this suite. 05/17/23 06:40:59.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:40:59.907
May 17 06:40:59.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename security-context-test 05/17/23 06:40:59.907
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:59.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:59.931
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
May 17 06:40:59.945: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-8cde500e-9e72-4cf4-a5b8-eaac76e56521" in namespace "security-context-test-6852" to be "Succeeded or Failed"
May 17 06:40:59.952: INFO: Pod "busybox-readonly-false-8cde500e-9e72-4cf4-a5b8-eaac76e56521": Phase="Pending", Reason="", readiness=false. Elapsed: 6.413744ms
May 17 06:41:01.957: INFO: Pod "busybox-readonly-false-8cde500e-9e72-4cf4-a5b8-eaac76e56521": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011579734s
May 17 06:41:03.958: INFO: Pod "busybox-readonly-false-8cde500e-9e72-4cf4-a5b8-eaac76e56521": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012558149s
May 17 06:41:03.958: INFO: Pod "busybox-readonly-false-8cde500e-9e72-4cf4-a5b8-eaac76e56521" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May 17 06:41:03.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6852" for this suite. 05/17/23 06:41:03.965
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":259,"skipped":4886,"failed":0}
------------------------------
â€¢ [4.066 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:40:59.907
    May 17 06:40:59.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename security-context-test 05/17/23 06:40:59.907
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:40:59.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:40:59.931
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    May 17 06:40:59.945: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-8cde500e-9e72-4cf4-a5b8-eaac76e56521" in namespace "security-context-test-6852" to be "Succeeded or Failed"
    May 17 06:40:59.952: INFO: Pod "busybox-readonly-false-8cde500e-9e72-4cf4-a5b8-eaac76e56521": Phase="Pending", Reason="", readiness=false. Elapsed: 6.413744ms
    May 17 06:41:01.957: INFO: Pod "busybox-readonly-false-8cde500e-9e72-4cf4-a5b8-eaac76e56521": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011579734s
    May 17 06:41:03.958: INFO: Pod "busybox-readonly-false-8cde500e-9e72-4cf4-a5b8-eaac76e56521": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012558149s
    May 17 06:41:03.958: INFO: Pod "busybox-readonly-false-8cde500e-9e72-4cf4-a5b8-eaac76e56521" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May 17 06:41:03.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-6852" for this suite. 05/17/23 06:41:03.965
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:41:03.974
May 17 06:41:03.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubelet-test 05/17/23 06:41:03.974
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:03.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:03.999
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
May 17 06:41:04.031: INFO: Waiting up to 5m0s for pod "busybox-scheduling-395d7a1d-4025-4861-9616-d0f90ea323cd" in namespace "kubelet-test-5312" to be "running and ready"
May 17 06:41:04.035: INFO: Pod "busybox-scheduling-395d7a1d-4025-4861-9616-d0f90ea323cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.400642ms
May 17 06:41:04.035: INFO: The phase of Pod busybox-scheduling-395d7a1d-4025-4861-9616-d0f90ea323cd is Pending, waiting for it to be Running (with Ready = true)
May 17 06:41:06.043: INFO: Pod "busybox-scheduling-395d7a1d-4025-4861-9616-d0f90ea323cd": Phase="Running", Reason="", readiness=true. Elapsed: 2.012785799s
May 17 06:41:06.043: INFO: The phase of Pod busybox-scheduling-395d7a1d-4025-4861-9616-d0f90ea323cd is Running (Ready = true)
May 17 06:41:06.043: INFO: Pod "busybox-scheduling-395d7a1d-4025-4861-9616-d0f90ea323cd" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May 17 06:41:06.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5312" for this suite. 05/17/23 06:41:06.071
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":260,"skipped":4890,"failed":0}
------------------------------
â€¢ [2.108 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:41:03.974
    May 17 06:41:03.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubelet-test 05/17/23 06:41:03.974
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:03.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:03.999
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    May 17 06:41:04.031: INFO: Waiting up to 5m0s for pod "busybox-scheduling-395d7a1d-4025-4861-9616-d0f90ea323cd" in namespace "kubelet-test-5312" to be "running and ready"
    May 17 06:41:04.035: INFO: Pod "busybox-scheduling-395d7a1d-4025-4861-9616-d0f90ea323cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.400642ms
    May 17 06:41:04.035: INFO: The phase of Pod busybox-scheduling-395d7a1d-4025-4861-9616-d0f90ea323cd is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:41:06.043: INFO: Pod "busybox-scheduling-395d7a1d-4025-4861-9616-d0f90ea323cd": Phase="Running", Reason="", readiness=true. Elapsed: 2.012785799s
    May 17 06:41:06.043: INFO: The phase of Pod busybox-scheduling-395d7a1d-4025-4861-9616-d0f90ea323cd is Running (Ready = true)
    May 17 06:41:06.043: INFO: Pod "busybox-scheduling-395d7a1d-4025-4861-9616-d0f90ea323cd" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May 17 06:41:06.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5312" for this suite. 05/17/23 06:41:06.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:41:06.082
May 17 06:41:06.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 06:41:06.082
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:06.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:06.108
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 06:41:06.13
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:41:06.805
STEP: Deploying the webhook pod 05/17/23 06:41:06.816
STEP: Wait for the deployment to be ready 05/17/23 06:41:06.829
May 17 06:41:06.841: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 06:41:08.857
STEP: Verifying the service has paired with the endpoint 05/17/23 06:41:08.87
May 17 06:41:09.871: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
May 17 06:41:09.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/17/23 06:41:10.388
STEP: Creating a custom resource that should be denied by the webhook 05/17/23 06:41:10.408
STEP: Creating a custom resource whose deletion would be denied by the webhook 05/17/23 06:41:12.508
STEP: Updating the custom resource with disallowed data should be denied 05/17/23 06:41:12.528
STEP: Deleting the custom resource should be denied 05/17/23 06:41:12.542
STEP: Remove the offending key and value from the custom resource data 05/17/23 06:41:12.553
STEP: Deleting the updated custom resource should be successful 05/17/23 06:41:12.567
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:41:13.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8835" for this suite. 05/17/23 06:41:13.106
STEP: Destroying namespace "webhook-8835-markers" for this suite. 05/17/23 06:41:13.114
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":261,"skipped":4900,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.089 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:41:06.082
    May 17 06:41:06.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 06:41:06.082
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:06.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:06.108
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 06:41:06.13
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:41:06.805
    STEP: Deploying the webhook pod 05/17/23 06:41:06.816
    STEP: Wait for the deployment to be ready 05/17/23 06:41:06.829
    May 17 06:41:06.841: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 06:41:08.857
    STEP: Verifying the service has paired with the endpoint 05/17/23 06:41:08.87
    May 17 06:41:09.871: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    May 17 06:41:09.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/17/23 06:41:10.388
    STEP: Creating a custom resource that should be denied by the webhook 05/17/23 06:41:10.408
    STEP: Creating a custom resource whose deletion would be denied by the webhook 05/17/23 06:41:12.508
    STEP: Updating the custom resource with disallowed data should be denied 05/17/23 06:41:12.528
    STEP: Deleting the custom resource should be denied 05/17/23 06:41:12.542
    STEP: Remove the offending key and value from the custom resource data 05/17/23 06:41:12.553
    STEP: Deleting the updated custom resource should be successful 05/17/23 06:41:12.567
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:41:13.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8835" for this suite. 05/17/23 06:41:13.106
    STEP: Destroying namespace "webhook-8835-markers" for this suite. 05/17/23 06:41:13.114
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:41:13.172
May 17 06:41:13.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 06:41:13.173
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:13.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:13.194
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-4f3681ce-e512-40f0-a0e6-13b14eda2b11 05/17/23 06:41:13.198
STEP: Creating a pod to test consume secrets 05/17/23 06:41:13.204
May 17 06:41:13.217: INFO: Waiting up to 5m0s for pod "pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d" in namespace "secrets-4483" to be "Succeeded or Failed"
May 17 06:41:13.222: INFO: Pod "pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.560895ms
May 17 06:41:15.228: INFO: Pod "pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010313043s
May 17 06:41:17.228: INFO: Pod "pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010279066s
STEP: Saw pod success 05/17/23 06:41:17.228
May 17 06:41:17.228: INFO: Pod "pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d" satisfied condition "Succeeded or Failed"
May 17 06:41:17.233: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d container secret-volume-test: <nil>
STEP: delete the pod 05/17/23 06:41:17.244
May 17 06:41:17.264: INFO: Waiting for pod pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d to disappear
May 17 06:41:17.269: INFO: Pod pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May 17 06:41:17.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4483" for this suite. 05/17/23 06:41:17.277
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":262,"skipped":4911,"failed":0}
------------------------------
â€¢ [4.114 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:41:13.172
    May 17 06:41:13.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 06:41:13.173
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:13.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:13.194
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-4f3681ce-e512-40f0-a0e6-13b14eda2b11 05/17/23 06:41:13.198
    STEP: Creating a pod to test consume secrets 05/17/23 06:41:13.204
    May 17 06:41:13.217: INFO: Waiting up to 5m0s for pod "pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d" in namespace "secrets-4483" to be "Succeeded or Failed"
    May 17 06:41:13.222: INFO: Pod "pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.560895ms
    May 17 06:41:15.228: INFO: Pod "pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010313043s
    May 17 06:41:17.228: INFO: Pod "pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010279066s
    STEP: Saw pod success 05/17/23 06:41:17.228
    May 17 06:41:17.228: INFO: Pod "pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d" satisfied condition "Succeeded or Failed"
    May 17 06:41:17.233: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d container secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 06:41:17.244
    May 17 06:41:17.264: INFO: Waiting for pod pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d to disappear
    May 17 06:41:17.269: INFO: Pod pod-secrets-8901db2f-ba46-4329-9ba6-4791496c8d4d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May 17 06:41:17.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4483" for this suite. 05/17/23 06:41:17.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:41:17.287
May 17 06:41:17.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename gc 05/17/23 06:41:17.287
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:17.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:17.312
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 05/17/23 06:41:17.317
STEP: Wait for the Deployment to create new ReplicaSet 05/17/23 06:41:17.324
STEP: delete the deployment 05/17/23 06:41:17.839
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/17/23 06:41:17.849
STEP: Gathering metrics 05/17/23 06:41:18.382
W0517 06:41:18.395557      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 17 06:41:18.395: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May 17 06:41:18.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8692" for this suite. 05/17/23 06:41:18.445
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":263,"skipped":4926,"failed":0}
------------------------------
â€¢ [1.169 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:41:17.287
    May 17 06:41:17.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename gc 05/17/23 06:41:17.287
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:17.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:17.312
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 05/17/23 06:41:17.317
    STEP: Wait for the Deployment to create new ReplicaSet 05/17/23 06:41:17.324
    STEP: delete the deployment 05/17/23 06:41:17.839
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/17/23 06:41:17.849
    STEP: Gathering metrics 05/17/23 06:41:18.382
    W0517 06:41:18.395557      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May 17 06:41:18.395: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May 17 06:41:18.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8692" for this suite. 05/17/23 06:41:18.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:41:18.457
May 17 06:41:18.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:41:18.458
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:18.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:18.486
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
May 17 06:41:18.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8470 version'
May 17 06:41:18.556: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
May 17 06:41:18.556: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"dc2f9dc64421983f0f7839da8ab4ab6d4673daad\", GitTreeState:\"clean\", BuildDate:\"2023-04-08T13:29:19Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:41:18.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8470" for this suite. 05/17/23 06:41:18.567
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":264,"skipped":4948,"failed":0}
------------------------------
â€¢ [0.118 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:41:18.457
    May 17 06:41:18.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:41:18.458
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:18.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:18.486
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    May 17 06:41:18.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8470 version'
    May 17 06:41:18.556: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    May 17 06:41:18.556: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"dc2f9dc64421983f0f7839da8ab4ab6d4673daad\", GitTreeState:\"clean\", BuildDate:\"2023-04-08T13:29:19Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:41:18.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8470" for this suite. 05/17/23 06:41:18.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:41:18.577
May 17 06:41:18.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 06:41:18.577
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:18.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:18.604
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 05/17/23 06:41:18.608
May 17 06:41:18.628: INFO: Waiting up to 5m0s for pod "labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00" in namespace "downward-api-1047" to be "running and ready"
May 17 06:41:18.634: INFO: Pod "labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00": Phase="Pending", Reason="", readiness=false. Elapsed: 5.194839ms
May 17 06:41:18.634: INFO: The phase of Pod labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00 is Pending, waiting for it to be Running (with Ready = true)
May 17 06:41:20.640: INFO: Pod "labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00": Phase="Running", Reason="", readiness=true. Elapsed: 2.011699051s
May 17 06:41:20.640: INFO: The phase of Pod labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00 is Running (Ready = true)
May 17 06:41:20.640: INFO: Pod "labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00" satisfied condition "running and ready"
May 17 06:41:21.171: INFO: Successfully updated pod "labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May 17 06:41:23.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1047" for this suite. 05/17/23 06:41:23.2
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":265,"skipped":4995,"failed":0}
------------------------------
â€¢ [4.633 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:41:18.577
    May 17 06:41:18.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 06:41:18.577
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:18.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:18.604
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 05/17/23 06:41:18.608
    May 17 06:41:18.628: INFO: Waiting up to 5m0s for pod "labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00" in namespace "downward-api-1047" to be "running and ready"
    May 17 06:41:18.634: INFO: Pod "labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00": Phase="Pending", Reason="", readiness=false. Elapsed: 5.194839ms
    May 17 06:41:18.634: INFO: The phase of Pod labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00 is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:41:20.640: INFO: Pod "labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00": Phase="Running", Reason="", readiness=true. Elapsed: 2.011699051s
    May 17 06:41:20.640: INFO: The phase of Pod labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00 is Running (Ready = true)
    May 17 06:41:20.640: INFO: Pod "labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00" satisfied condition "running and ready"
    May 17 06:41:21.171: INFO: Successfully updated pod "labelsupdatedbb08405-f2d3-4783-b2f0-a4b5c8876b00"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May 17 06:41:23.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1047" for this suite. 05/17/23 06:41:23.2
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:41:23.21
May 17 06:41:23.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 06:41:23.211
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:23.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:23.233
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-351cbadd-c2c2-477d-a8dd-07ada226b64f 05/17/23 06:41:23.236
STEP: Creating a pod to test consume configMaps 05/17/23 06:41:23.241
May 17 06:41:23.253: INFO: Waiting up to 5m0s for pod "pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978" in namespace "configmap-7387" to be "Succeeded or Failed"
May 17 06:41:23.257: INFO: Pod "pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978": Phase="Pending", Reason="", readiness=false. Elapsed: 3.981434ms
May 17 06:41:25.262: INFO: Pod "pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009287547s
May 17 06:41:27.267: INFO: Pod "pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014395996s
STEP: Saw pod success 05/17/23 06:41:27.267
May 17 06:41:27.267: INFO: Pod "pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978" satisfied condition "Succeeded or Failed"
May 17 06:41:27.272: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978 container configmap-volume-test: <nil>
STEP: delete the pod 05/17/23 06:41:27.289
May 17 06:41:27.306: INFO: Waiting for pod pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978 to disappear
May 17 06:41:27.310: INFO: Pod pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May 17 06:41:27.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7387" for this suite. 05/17/23 06:41:27.321
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":266,"skipped":4998,"failed":0}
------------------------------
â€¢ [4.119 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:41:23.21
    May 17 06:41:23.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 06:41:23.211
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:23.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:23.233
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-351cbadd-c2c2-477d-a8dd-07ada226b64f 05/17/23 06:41:23.236
    STEP: Creating a pod to test consume configMaps 05/17/23 06:41:23.241
    May 17 06:41:23.253: INFO: Waiting up to 5m0s for pod "pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978" in namespace "configmap-7387" to be "Succeeded or Failed"
    May 17 06:41:23.257: INFO: Pod "pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978": Phase="Pending", Reason="", readiness=false. Elapsed: 3.981434ms
    May 17 06:41:25.262: INFO: Pod "pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009287547s
    May 17 06:41:27.267: INFO: Pod "pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014395996s
    STEP: Saw pod success 05/17/23 06:41:27.267
    May 17 06:41:27.267: INFO: Pod "pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978" satisfied condition "Succeeded or Failed"
    May 17 06:41:27.272: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978 container configmap-volume-test: <nil>
    STEP: delete the pod 05/17/23 06:41:27.289
    May 17 06:41:27.306: INFO: Waiting for pod pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978 to disappear
    May 17 06:41:27.310: INFO: Pod pod-configmaps-0e0c0ae9-7c17-4984-9c41-5f24c1236978 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 06:41:27.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7387" for this suite. 05/17/23 06:41:27.321
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:41:27.329
May 17 06:41:27.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-probe 05/17/23 06:41:27.33
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:27.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:27.356
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-39ddbcbe-44bc-4368-82ba-98a23b599971 in namespace container-probe-8895 05/17/23 06:41:27.359
May 17 06:41:27.376: INFO: Waiting up to 5m0s for pod "busybox-39ddbcbe-44bc-4368-82ba-98a23b599971" in namespace "container-probe-8895" to be "not pending"
May 17 06:41:27.380: INFO: Pod "busybox-39ddbcbe-44bc-4368-82ba-98a23b599971": Phase="Pending", Reason="", readiness=false. Elapsed: 4.351043ms
May 17 06:41:29.385: INFO: Pod "busybox-39ddbcbe-44bc-4368-82ba-98a23b599971": Phase="Running", Reason="", readiness=true. Elapsed: 2.00982597s
May 17 06:41:29.385: INFO: Pod "busybox-39ddbcbe-44bc-4368-82ba-98a23b599971" satisfied condition "not pending"
May 17 06:41:29.385: INFO: Started pod busybox-39ddbcbe-44bc-4368-82ba-98a23b599971 in namespace container-probe-8895
STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 06:41:29.385
May 17 06:41:29.390: INFO: Initial restart count of pod busybox-39ddbcbe-44bc-4368-82ba-98a23b599971 is 0
STEP: deleting the pod 05/17/23 06:45:30.215
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May 17 06:45:30.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8895" for this suite. 05/17/23 06:45:30.246
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":267,"skipped":5002,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.926 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:41:27.329
    May 17 06:41:27.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-probe 05/17/23 06:41:27.33
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:41:27.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:41:27.356
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-39ddbcbe-44bc-4368-82ba-98a23b599971 in namespace container-probe-8895 05/17/23 06:41:27.359
    May 17 06:41:27.376: INFO: Waiting up to 5m0s for pod "busybox-39ddbcbe-44bc-4368-82ba-98a23b599971" in namespace "container-probe-8895" to be "not pending"
    May 17 06:41:27.380: INFO: Pod "busybox-39ddbcbe-44bc-4368-82ba-98a23b599971": Phase="Pending", Reason="", readiness=false. Elapsed: 4.351043ms
    May 17 06:41:29.385: INFO: Pod "busybox-39ddbcbe-44bc-4368-82ba-98a23b599971": Phase="Running", Reason="", readiness=true. Elapsed: 2.00982597s
    May 17 06:41:29.385: INFO: Pod "busybox-39ddbcbe-44bc-4368-82ba-98a23b599971" satisfied condition "not pending"
    May 17 06:41:29.385: INFO: Started pod busybox-39ddbcbe-44bc-4368-82ba-98a23b599971 in namespace container-probe-8895
    STEP: checking the pod's current state and verifying that restartCount is present 05/17/23 06:41:29.385
    May 17 06:41:29.390: INFO: Initial restart count of pod busybox-39ddbcbe-44bc-4368-82ba-98a23b599971 is 0
    STEP: deleting the pod 05/17/23 06:45:30.215
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May 17 06:45:30.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8895" for this suite. 05/17/23 06:45:30.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:45:30.257
May 17 06:45:30.257: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename sched-preemption 05/17/23 06:45:30.257
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:45:30.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:45:30.284
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May 17 06:45:30.306: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 06:46:30.362: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 05/17/23 06:46:30.368
May 17 06:46:30.436: INFO: Created pod: pod0-0-sched-preemption-low-priority
May 17 06:46:30.447: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May 17 06:46:30.500: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May 17 06:46:30.509: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May 17 06:46:30.545: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May 17 06:46:30.554: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/17/23 06:46:30.554
May 17 06:46:30.554: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8529" to be "running"
May 17 06:46:30.558: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.617249ms
May 17 06:46:32.565: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011626719s
May 17 06:46:34.566: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011990713s
May 17 06:46:36.564: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.010515896s
May 17 06:46:36.564: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May 17 06:46:36.564: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8529" to be "running"
May 17 06:46:36.569: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.247928ms
May 17 06:46:36.569: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May 17 06:46:36.569: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8529" to be "running"
May 17 06:46:36.575: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.084717ms
May 17 06:46:36.575: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May 17 06:46:36.575: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8529" to be "running"
May 17 06:46:36.580: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.183715ms
May 17 06:46:36.580: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May 17 06:46:36.580: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8529" to be "running"
May 17 06:46:36.586: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.892681ms
May 17 06:46:36.586: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May 17 06:46:36.586: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8529" to be "running"
May 17 06:46:36.592: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.837328ms
May 17 06:46:36.592: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 05/17/23 06:46:36.592
May 17 06:46:36.626: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
May 17 06:46:36.632: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.126766ms
May 17 06:46:38.638: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011542719s
May 17 06:46:40.639: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012603512s
May 17 06:46:40.639: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May 17 06:46:40.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8529" for this suite. 05/17/23 06:46:40.702
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":268,"skipped":5019,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.563 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:45:30.257
    May 17 06:45:30.257: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename sched-preemption 05/17/23 06:45:30.257
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:45:30.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:45:30.284
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May 17 06:45:30.306: INFO: Waiting up to 1m0s for all nodes to be ready
    May 17 06:46:30.362: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 05/17/23 06:46:30.368
    May 17 06:46:30.436: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May 17 06:46:30.447: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May 17 06:46:30.500: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May 17 06:46:30.509: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May 17 06:46:30.545: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May 17 06:46:30.554: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/17/23 06:46:30.554
    May 17 06:46:30.554: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8529" to be "running"
    May 17 06:46:30.558: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.617249ms
    May 17 06:46:32.565: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011626719s
    May 17 06:46:34.566: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011990713s
    May 17 06:46:36.564: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.010515896s
    May 17 06:46:36.564: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May 17 06:46:36.564: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8529" to be "running"
    May 17 06:46:36.569: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.247928ms
    May 17 06:46:36.569: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May 17 06:46:36.569: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8529" to be "running"
    May 17 06:46:36.575: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.084717ms
    May 17 06:46:36.575: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May 17 06:46:36.575: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8529" to be "running"
    May 17 06:46:36.580: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.183715ms
    May 17 06:46:36.580: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May 17 06:46:36.580: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8529" to be "running"
    May 17 06:46:36.586: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.892681ms
    May 17 06:46:36.586: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May 17 06:46:36.586: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8529" to be "running"
    May 17 06:46:36.592: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.837328ms
    May 17 06:46:36.592: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 05/17/23 06:46:36.592
    May 17 06:46:36.626: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    May 17 06:46:36.632: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.126766ms
    May 17 06:46:38.638: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011542719s
    May 17 06:46:40.639: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012603512s
    May 17 06:46:40.639: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May 17 06:46:40.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8529" for this suite. 05/17/23 06:46:40.702
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:46:40.821
May 17 06:46:40.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 06:46:40.821
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:46:40.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:46:40.849
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
May 17 06:46:40.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:46:44.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1254" for this suite. 05/17/23 06:46:44.268
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":269,"skipped":5032,"failed":0}
------------------------------
â€¢ [3.462 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:46:40.821
    May 17 06:46:40.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 06:46:40.821
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:46:40.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:46:40.849
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    May 17 06:46:40.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:46:44.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1254" for this suite. 05/17/23 06:46:44.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:46:44.283
May 17 06:46:44.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:46:44.284
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:46:44.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:46:44.31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-5689 05/17/23 06:46:44.314
May 17 06:46:44.330: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-5689" to be "running and ready"
May 17 06:46:44.335: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 4.708627ms
May 17 06:46:44.335: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 17 06:46:46.344: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.013588035s
May 17 06:46:46.344: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
May 17 06:46:46.344: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
May 17 06:46:46.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 17 06:46:46.588: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 17 06:46:46.588: INFO: stdout: "iptables"
May 17 06:46:46.588: INFO: proxyMode: iptables
May 17 06:46:46.604: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 17 06:46:46.609: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5689 05/17/23 06:46:46.609
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5689 05/17/23 06:46:46.639
I0517 06:46:46.646518      23 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5689, replica count: 3
I0517 06:46:49.698837      23 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 06:46:49.720: INFO: Creating new exec pod
May 17 06:46:49.729: INFO: Waiting up to 5m0s for pod "execpod-affinityxvvqb" in namespace "services-5689" to be "running"
May 17 06:46:49.735: INFO: Pod "execpod-affinityxvvqb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.157988ms
May 17 06:46:51.743: INFO: Pod "execpod-affinityxvvqb": Phase="Running", Reason="", readiness=true. Elapsed: 2.013226862s
May 17 06:46:51.743: INFO: Pod "execpod-affinityxvvqb" satisfied condition "running"
May 17 06:46:52.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
May 17 06:46:52.952: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 17 06:46:52.952: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:46:52.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.58.238 80'
May 17 06:46:53.146: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.58.238 80\nConnection to 10.0.58.238 80 port [tcp/http] succeeded!\n"
May 17 06:46:53.146: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:46:53.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.6 31483'
May 17 06:46:53.534: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.6 31483\nConnection to 10.224.0.6 31483 port [tcp/*] succeeded!\n"
May 17 06:46:53.534: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:46:53.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.4 31483'
May 17 06:46:53.710: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.4 31483\nConnection to 10.224.0.4 31483 port [tcp/*] succeeded!\n"
May 17 06:46:53.710: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:46:53.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.224.0.4:31483/ ; done'
May 17 06:46:53.971: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n"
May 17 06:46:53.971: INFO: stdout: "\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v"
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
May 17 06:46:53.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.224.0.4:31483/'
May 17 06:46:54.154: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n"
May 17 06:46:54.154: INFO: stdout: "affinity-nodeport-timeout-tsq6v"
May 17 06:47:14.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.224.0.4:31483/'
May 17 06:47:14.348: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n"
May 17 06:47:14.348: INFO: stdout: "affinity-nodeport-timeout-tsq6v"
May 17 06:47:34.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.224.0.4:31483/'
May 17 06:47:34.542: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n"
May 17 06:47:34.542: INFO: stdout: "affinity-nodeport-timeout-6k9wt"
May 17 06:47:34.542: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5689, will wait for the garbage collector to delete the pods 05/17/23 06:47:34.558
May 17 06:47:34.623: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.263127ms
May 17 06:47:34.723: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.462922ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:47:36.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5689" for this suite. 05/17/23 06:47:36.867
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":270,"skipped":5042,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.594 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:46:44.283
    May 17 06:46:44.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:46:44.284
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:46:44.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:46:44.31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-5689 05/17/23 06:46:44.314
    May 17 06:46:44.330: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-5689" to be "running and ready"
    May 17 06:46:44.335: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 4.708627ms
    May 17 06:46:44.335: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:46:46.344: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.013588035s
    May 17 06:46:46.344: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    May 17 06:46:46.344: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    May 17 06:46:46.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    May 17 06:46:46.588: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    May 17 06:46:46.588: INFO: stdout: "iptables"
    May 17 06:46:46.588: INFO: proxyMode: iptables
    May 17 06:46:46.604: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    May 17 06:46:46.609: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-5689 05/17/23 06:46:46.609
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-5689 05/17/23 06:46:46.639
    I0517 06:46:46.646518      23 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5689, replica count: 3
    I0517 06:46:49.698837      23 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 06:46:49.720: INFO: Creating new exec pod
    May 17 06:46:49.729: INFO: Waiting up to 5m0s for pod "execpod-affinityxvvqb" in namespace "services-5689" to be "running"
    May 17 06:46:49.735: INFO: Pod "execpod-affinityxvvqb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.157988ms
    May 17 06:46:51.743: INFO: Pod "execpod-affinityxvvqb": Phase="Running", Reason="", readiness=true. Elapsed: 2.013226862s
    May 17 06:46:51.743: INFO: Pod "execpod-affinityxvvqb" satisfied condition "running"
    May 17 06:46:52.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    May 17 06:46:52.952: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    May 17 06:46:52.952: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:46:52.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.58.238 80'
    May 17 06:46:53.146: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.58.238 80\nConnection to 10.0.58.238 80 port [tcp/http] succeeded!\n"
    May 17 06:46:53.146: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:46:53.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.6 31483'
    May 17 06:46:53.534: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.6 31483\nConnection to 10.224.0.6 31483 port [tcp/*] succeeded!\n"
    May 17 06:46:53.534: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:46:53.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.4 31483'
    May 17 06:46:53.710: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.4 31483\nConnection to 10.224.0.4 31483 port [tcp/*] succeeded!\n"
    May 17 06:46:53.710: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:46:53.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.224.0.4:31483/ ; done'
    May 17 06:46:53.971: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n"
    May 17 06:46:53.971: INFO: stdout: "\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v\naffinity-nodeport-timeout-tsq6v"
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Received response from host: affinity-nodeport-timeout-tsq6v
    May 17 06:46:53.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.224.0.4:31483/'
    May 17 06:46:54.154: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n"
    May 17 06:46:54.154: INFO: stdout: "affinity-nodeport-timeout-tsq6v"
    May 17 06:47:14.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.224.0.4:31483/'
    May 17 06:47:14.348: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n"
    May 17 06:47:14.348: INFO: stdout: "affinity-nodeport-timeout-tsq6v"
    May 17 06:47:34.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-5689 exec execpod-affinityxvvqb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.224.0.4:31483/'
    May 17 06:47:34.542: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.224.0.4:31483/\n"
    May 17 06:47:34.542: INFO: stdout: "affinity-nodeport-timeout-6k9wt"
    May 17 06:47:34.542: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5689, will wait for the garbage collector to delete the pods 05/17/23 06:47:34.558
    May 17 06:47:34.623: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.263127ms
    May 17 06:47:34.723: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.462922ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:47:36.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5689" for this suite. 05/17/23 06:47:36.867
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:47:36.88
May 17 06:47:36.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 06:47:36.881
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:47:36.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:47:36.91
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 06:47:36.932
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:47:37.146
STEP: Deploying the webhook pod 05/17/23 06:47:37.157
STEP: Wait for the deployment to be ready 05/17/23 06:47:37.17
May 17 06:47:37.181: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 06:47:39.197
STEP: Verifying the service has paired with the endpoint 05/17/23 06:47:39.211
May 17 06:47:40.211: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 05/17/23 06:47:40.303
STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 06:47:40.347
STEP: Deleting the collection of validation webhooks 05/17/23 06:47:40.386
STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 06:47:40.433
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:47:40.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6735" for this suite. 05/17/23 06:47:40.459
STEP: Destroying namespace "webhook-6735-markers" for this suite. 05/17/23 06:47:40.469
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":271,"skipped":5113,"failed":0}
------------------------------
â€¢ [3.645 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:47:36.88
    May 17 06:47:36.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 06:47:36.881
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:47:36.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:47:36.91
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 06:47:36.932
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:47:37.146
    STEP: Deploying the webhook pod 05/17/23 06:47:37.157
    STEP: Wait for the deployment to be ready 05/17/23 06:47:37.17
    May 17 06:47:37.181: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 06:47:39.197
    STEP: Verifying the service has paired with the endpoint 05/17/23 06:47:39.211
    May 17 06:47:40.211: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 05/17/23 06:47:40.303
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 06:47:40.347
    STEP: Deleting the collection of validation webhooks 05/17/23 06:47:40.386
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/17/23 06:47:40.433
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:47:40.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6735" for this suite. 05/17/23 06:47:40.459
    STEP: Destroying namespace "webhook-6735-markers" for this suite. 05/17/23 06:47:40.469
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:47:40.526
May 17 06:47:40.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pod-network-test 05/17/23 06:47:40.527
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:47:40.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:47:40.551
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-3691 05/17/23 06:47:40.555
STEP: creating a selector 05/17/23 06:47:40.556
STEP: Creating the service pods in kubernetes 05/17/23 06:47:40.556
May 17 06:47:40.556: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 17 06:47:40.596: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3691" to be "running and ready"
May 17 06:47:40.601: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.543107ms
May 17 06:47:40.601: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 06:47:42.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011531352s
May 17 06:47:42.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:47:44.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013018272s
May 17 06:47:44.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:47:46.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011782044s
May 17 06:47:46.608: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:47:48.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012503866s
May 17 06:47:48.608: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:47:50.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011551707s
May 17 06:47:50.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:47:52.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013309607s
May 17 06:47:52.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:47:54.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013011333s
May 17 06:47:54.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:47:56.618: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.022352354s
May 17 06:47:56.618: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:47:58.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013180459s
May 17 06:47:58.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:48:00.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.012830794s
May 17 06:48:00.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 17 06:48:02.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.012609093s
May 17 06:48:02.608: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May 17 06:48:02.608: INFO: Pod "netserver-0" satisfied condition "running and ready"
May 17 06:48:02.614: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3691" to be "running and ready"
May 17 06:48:02.619: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 5.109164ms
May 17 06:48:02.619: INFO: The phase of Pod netserver-1 is Running (Ready = false)
May 17 06:48:04.626: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.012406948s
May 17 06:48:04.626: INFO: The phase of Pod netserver-1 is Running (Ready = false)
May 17 06:48:06.634: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.019568856s
May 17 06:48:06.634: INFO: The phase of Pod netserver-1 is Running (Ready = false)
May 17 06:48:08.625: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.010901276s
May 17 06:48:08.625: INFO: The phase of Pod netserver-1 is Running (Ready = false)
May 17 06:48:10.626: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.012251646s
May 17 06:48:10.626: INFO: The phase of Pod netserver-1 is Running (Ready = false)
May 17 06:48:12.626: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 10.011914591s
May 17 06:48:12.626: INFO: The phase of Pod netserver-1 is Running (Ready = false)
May 17 06:48:14.626: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 12.012399447s
May 17 06:48:14.626: INFO: The phase of Pod netserver-1 is Running (Ready = false)
May 17 06:48:16.625: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.011119667s
May 17 06:48:16.625: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May 17 06:48:16.625: INFO: Pod "netserver-1" satisfied condition "running and ready"
May 17 06:48:16.631: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3691" to be "running and ready"
May 17 06:48:16.636: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.111819ms
May 17 06:48:16.636: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May 17 06:48:16.636: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/17/23 06:48:16.64
May 17 06:48:16.667: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3691" to be "running"
May 17 06:48:16.672: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.899389ms
May 17 06:48:18.678: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010645724s
May 17 06:48:18.678: INFO: Pod "test-container-pod" satisfied condition "running"
May 17 06:48:18.682: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3691" to be "running"
May 17 06:48:18.687: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.814108ms
May 17 06:48:18.687: INFO: Pod "host-test-container-pod" satisfied condition "running"
May 17 06:48:18.692: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 17 06:48:18.692: INFO: Going to poll 10.244.0.108 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 17 06:48:18.697: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.108 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3691 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:48:18.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:48:18.697: INFO: ExecWithOptions: Clientset creation
May 17 06:48:18.698: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-3691/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.108+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 17 06:48:19.828: INFO: Found all 1 expected endpoints: [netserver-0]
May 17 06:48:19.828: INFO: Going to poll 10.244.2.50 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 17 06:48:19.833: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.50 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3691 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:48:19.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:48:19.834: INFO: ExecWithOptions: Clientset creation
May 17 06:48:19.834: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-3691/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.50+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 17 06:48:20.976: INFO: Found all 1 expected endpoints: [netserver-1]
May 17 06:48:20.976: INFO: Going to poll 10.244.1.116 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 17 06:48:20.982: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.116 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3691 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 06:48:20.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 06:48:20.982: INFO: ExecWithOptions: Clientset creation
May 17 06:48:20.982: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-3691/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.116+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 17 06:48:22.133: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May 17 06:48:22.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3691" for this suite. 05/17/23 06:48:22.145
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":272,"skipped":5131,"failed":0}
------------------------------
â€¢ [SLOW TEST] [41.628 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:47:40.526
    May 17 06:47:40.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pod-network-test 05/17/23 06:47:40.527
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:47:40.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:47:40.551
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-3691 05/17/23 06:47:40.555
    STEP: creating a selector 05/17/23 06:47:40.556
    STEP: Creating the service pods in kubernetes 05/17/23 06:47:40.556
    May 17 06:47:40.556: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May 17 06:47:40.596: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3691" to be "running and ready"
    May 17 06:47:40.601: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.543107ms
    May 17 06:47:40.601: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:47:42.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011531352s
    May 17 06:47:42.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:47:44.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013018272s
    May 17 06:47:44.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:47:46.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011782044s
    May 17 06:47:46.608: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:47:48.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012503866s
    May 17 06:47:48.608: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:47:50.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011551707s
    May 17 06:47:50.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:47:52.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013309607s
    May 17 06:47:52.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:47:54.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013011333s
    May 17 06:47:54.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:47:56.618: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.022352354s
    May 17 06:47:56.618: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:47:58.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013180459s
    May 17 06:47:58.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:48:00.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.012830794s
    May 17 06:48:00.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 17 06:48:02.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.012609093s
    May 17 06:48:02.608: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May 17 06:48:02.608: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May 17 06:48:02.614: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3691" to be "running and ready"
    May 17 06:48:02.619: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 5.109164ms
    May 17 06:48:02.619: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    May 17 06:48:04.626: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.012406948s
    May 17 06:48:04.626: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    May 17 06:48:06.634: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.019568856s
    May 17 06:48:06.634: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    May 17 06:48:08.625: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.010901276s
    May 17 06:48:08.625: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    May 17 06:48:10.626: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.012251646s
    May 17 06:48:10.626: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    May 17 06:48:12.626: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 10.011914591s
    May 17 06:48:12.626: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    May 17 06:48:14.626: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 12.012399447s
    May 17 06:48:14.626: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    May 17 06:48:16.625: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.011119667s
    May 17 06:48:16.625: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May 17 06:48:16.625: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May 17 06:48:16.631: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3691" to be "running and ready"
    May 17 06:48:16.636: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.111819ms
    May 17 06:48:16.636: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May 17 06:48:16.636: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/17/23 06:48:16.64
    May 17 06:48:16.667: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3691" to be "running"
    May 17 06:48:16.672: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.899389ms
    May 17 06:48:18.678: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010645724s
    May 17 06:48:18.678: INFO: Pod "test-container-pod" satisfied condition "running"
    May 17 06:48:18.682: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3691" to be "running"
    May 17 06:48:18.687: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.814108ms
    May 17 06:48:18.687: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May 17 06:48:18.692: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May 17 06:48:18.692: INFO: Going to poll 10.244.0.108 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May 17 06:48:18.697: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.108 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3691 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:48:18.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:48:18.697: INFO: ExecWithOptions: Clientset creation
    May 17 06:48:18.698: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-3691/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.108+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 17 06:48:19.828: INFO: Found all 1 expected endpoints: [netserver-0]
    May 17 06:48:19.828: INFO: Going to poll 10.244.2.50 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May 17 06:48:19.833: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.50 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3691 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:48:19.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:48:19.834: INFO: ExecWithOptions: Clientset creation
    May 17 06:48:19.834: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-3691/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.50+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 17 06:48:20.976: INFO: Found all 1 expected endpoints: [netserver-1]
    May 17 06:48:20.976: INFO: Going to poll 10.244.1.116 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May 17 06:48:20.982: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.116 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3691 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 06:48:20.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 06:48:20.982: INFO: ExecWithOptions: Clientset creation
    May 17 06:48:20.982: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-3691/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.116+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 17 06:48:22.133: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May 17 06:48:22.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3691" for this suite. 05/17/23 06:48:22.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:48:22.155
May 17 06:48:22.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename replication-controller 05/17/23 06:48:22.156
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:22.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:22.181
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 05/17/23 06:48:22.186
STEP: When the matched label of one of its pods change 05/17/23 06:48:22.193
May 17 06:48:22.198: INFO: Pod name pod-release: Found 0 pods out of 1
May 17 06:48:27.205: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 05/17/23 06:48:27.218
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May 17 06:48:28.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4697" for this suite. 05/17/23 06:48:28.242
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":273,"skipped":5147,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.097 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:48:22.155
    May 17 06:48:22.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename replication-controller 05/17/23 06:48:22.156
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:22.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:22.181
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 05/17/23 06:48:22.186
    STEP: When the matched label of one of its pods change 05/17/23 06:48:22.193
    May 17 06:48:22.198: INFO: Pod name pod-release: Found 0 pods out of 1
    May 17 06:48:27.205: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/17/23 06:48:27.218
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May 17 06:48:28.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4697" for this suite. 05/17/23 06:48:28.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:48:28.253
May 17 06:48:28.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename cronjob 05/17/23 06:48:28.254
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:28.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:28.281
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 05/17/23 06:48:28.286
STEP: creating 05/17/23 06:48:28.286
STEP: getting 05/17/23 06:48:28.294
STEP: listing 05/17/23 06:48:28.3
STEP: watching 05/17/23 06:48:28.308
May 17 06:48:28.308: INFO: starting watch
STEP: cluster-wide listing 05/17/23 06:48:28.31
STEP: cluster-wide watching 05/17/23 06:48:28.318
May 17 06:48:28.318: INFO: starting watch
STEP: patching 05/17/23 06:48:28.32
STEP: updating 05/17/23 06:48:28.329
May 17 06:48:28.344: INFO: waiting for watch events with expected annotations
May 17 06:48:28.344: INFO: saw patched and updated annotations
STEP: patching /status 05/17/23 06:48:28.344
STEP: updating /status 05/17/23 06:48:28.353
STEP: get /status 05/17/23 06:48:28.366
STEP: deleting 05/17/23 06:48:28.372
STEP: deleting a collection 05/17/23 06:48:28.397
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May 17 06:48:28.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9883" for this suite. 05/17/23 06:48:28.424
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":274,"skipped":5171,"failed":0}
------------------------------
â€¢ [0.181 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:48:28.253
    May 17 06:48:28.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename cronjob 05/17/23 06:48:28.254
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:28.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:28.281
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 05/17/23 06:48:28.286
    STEP: creating 05/17/23 06:48:28.286
    STEP: getting 05/17/23 06:48:28.294
    STEP: listing 05/17/23 06:48:28.3
    STEP: watching 05/17/23 06:48:28.308
    May 17 06:48:28.308: INFO: starting watch
    STEP: cluster-wide listing 05/17/23 06:48:28.31
    STEP: cluster-wide watching 05/17/23 06:48:28.318
    May 17 06:48:28.318: INFO: starting watch
    STEP: patching 05/17/23 06:48:28.32
    STEP: updating 05/17/23 06:48:28.329
    May 17 06:48:28.344: INFO: waiting for watch events with expected annotations
    May 17 06:48:28.344: INFO: saw patched and updated annotations
    STEP: patching /status 05/17/23 06:48:28.344
    STEP: updating /status 05/17/23 06:48:28.353
    STEP: get /status 05/17/23 06:48:28.366
    STEP: deleting 05/17/23 06:48:28.372
    STEP: deleting a collection 05/17/23 06:48:28.397
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May 17 06:48:28.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9883" for this suite. 05/17/23 06:48:28.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:48:28.434
May 17 06:48:28.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename events 05/17/23 06:48:28.435
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:28.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:28.498
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 05/17/23 06:48:28.503
STEP: listing all events in all namespaces 05/17/23 06:48:28.514
STEP: patching the test event 05/17/23 06:48:28.525
STEP: fetching the test event 05/17/23 06:48:28.535
STEP: updating the test event 05/17/23 06:48:28.541
STEP: getting the test event 05/17/23 06:48:28.556
STEP: deleting the test event 05/17/23 06:48:28.562
STEP: listing all events in all namespaces 05/17/23 06:48:28.574
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
May 17 06:48:28.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1464" for this suite. 05/17/23 06:48:28.593
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":275,"skipped":5181,"failed":0}
------------------------------
â€¢ [0.168 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:48:28.434
    May 17 06:48:28.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename events 05/17/23 06:48:28.435
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:28.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:28.498
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 05/17/23 06:48:28.503
    STEP: listing all events in all namespaces 05/17/23 06:48:28.514
    STEP: patching the test event 05/17/23 06:48:28.525
    STEP: fetching the test event 05/17/23 06:48:28.535
    STEP: updating the test event 05/17/23 06:48:28.541
    STEP: getting the test event 05/17/23 06:48:28.556
    STEP: deleting the test event 05/17/23 06:48:28.562
    STEP: listing all events in all namespaces 05/17/23 06:48:28.574
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    May 17 06:48:28.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1464" for this suite. 05/17/23 06:48:28.593
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:48:28.603
May 17 06:48:28.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:48:28.603
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:28.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:28.628
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 05/17/23 06:48:28.637
May 17 06:48:28.650: INFO: Waiting up to 5m0s for pod "annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea" in namespace "projected-1889" to be "running and ready"
May 17 06:48:28.655: INFO: Pod "annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.990127ms
May 17 06:48:28.655: INFO: The phase of Pod annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea is Pending, waiting for it to be Running (with Ready = true)
May 17 06:48:30.660: INFO: Pod "annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.01063908s
May 17 06:48:30.660: INFO: The phase of Pod annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea is Running (Ready = true)
May 17 06:48:30.660: INFO: Pod "annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea" satisfied condition "running and ready"
May 17 06:48:31.196: INFO: Successfully updated pod "annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May 17 06:48:33.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1889" for this suite. 05/17/23 06:48:33.244
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":276,"skipped":5182,"failed":0}
------------------------------
â€¢ [4.651 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:48:28.603
    May 17 06:48:28.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:48:28.603
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:28.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:28.628
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 05/17/23 06:48:28.637
    May 17 06:48:28.650: INFO: Waiting up to 5m0s for pod "annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea" in namespace "projected-1889" to be "running and ready"
    May 17 06:48:28.655: INFO: Pod "annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.990127ms
    May 17 06:48:28.655: INFO: The phase of Pod annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:48:30.660: INFO: Pod "annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.01063908s
    May 17 06:48:30.660: INFO: The phase of Pod annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea is Running (Ready = true)
    May 17 06:48:30.660: INFO: Pod "annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea" satisfied condition "running and ready"
    May 17 06:48:31.196: INFO: Successfully updated pod "annotationupdate1c1d3bfc-a30c-4fe5-b740-b45aafc604ea"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May 17 06:48:33.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1889" for this suite. 05/17/23 06:48:33.244
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:48:33.254
May 17 06:48:33.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 06:48:33.255
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:33.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:33.284
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 05/17/23 06:48:33.288
May 17 06:48:33.303: INFO: Waiting up to 5m0s for pod "pod-0514142d-4951-4508-aaea-ef9d0dfcb574" in namespace "emptydir-5611" to be "Succeeded or Failed"
May 17 06:48:33.308: INFO: Pod "pod-0514142d-4951-4508-aaea-ef9d0dfcb574": Phase="Pending", Reason="", readiness=false. Elapsed: 5.015167ms
May 17 06:48:35.314: INFO: Pod "pod-0514142d-4951-4508-aaea-ef9d0dfcb574": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01097476s
May 17 06:48:37.316: INFO: Pod "pod-0514142d-4951-4508-aaea-ef9d0dfcb574": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012643441s
STEP: Saw pod success 05/17/23 06:48:37.316
May 17 06:48:37.316: INFO: Pod "pod-0514142d-4951-4508-aaea-ef9d0dfcb574" satisfied condition "Succeeded or Failed"
May 17 06:48:37.321: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-0514142d-4951-4508-aaea-ef9d0dfcb574 container test-container: <nil>
STEP: delete the pod 05/17/23 06:48:37.338
May 17 06:48:37.354: INFO: Waiting for pod pod-0514142d-4951-4508-aaea-ef9d0dfcb574 to disappear
May 17 06:48:37.359: INFO: Pod pod-0514142d-4951-4508-aaea-ef9d0dfcb574 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 06:48:37.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5611" for this suite. 05/17/23 06:48:37.369
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":277,"skipped":5183,"failed":0}
------------------------------
â€¢ [4.126 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:48:33.254
    May 17 06:48:33.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 06:48:33.255
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:33.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:33.284
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 05/17/23 06:48:33.288
    May 17 06:48:33.303: INFO: Waiting up to 5m0s for pod "pod-0514142d-4951-4508-aaea-ef9d0dfcb574" in namespace "emptydir-5611" to be "Succeeded or Failed"
    May 17 06:48:33.308: INFO: Pod "pod-0514142d-4951-4508-aaea-ef9d0dfcb574": Phase="Pending", Reason="", readiness=false. Elapsed: 5.015167ms
    May 17 06:48:35.314: INFO: Pod "pod-0514142d-4951-4508-aaea-ef9d0dfcb574": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01097476s
    May 17 06:48:37.316: INFO: Pod "pod-0514142d-4951-4508-aaea-ef9d0dfcb574": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012643441s
    STEP: Saw pod success 05/17/23 06:48:37.316
    May 17 06:48:37.316: INFO: Pod "pod-0514142d-4951-4508-aaea-ef9d0dfcb574" satisfied condition "Succeeded or Failed"
    May 17 06:48:37.321: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-0514142d-4951-4508-aaea-ef9d0dfcb574 container test-container: <nil>
    STEP: delete the pod 05/17/23 06:48:37.338
    May 17 06:48:37.354: INFO: Waiting for pod pod-0514142d-4951-4508-aaea-ef9d0dfcb574 to disappear
    May 17 06:48:37.359: INFO: Pod pod-0514142d-4951-4508-aaea-ef9d0dfcb574 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 06:48:37.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5611" for this suite. 05/17/23 06:48:37.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:48:37.381
May 17 06:48:37.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 06:48:37.382
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:37.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:37.418
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 05/17/23 06:48:37.422
May 17 06:48:37.438: INFO: Waiting up to 5m0s for pod "pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d" in namespace "emptydir-905" to be "Succeeded or Failed"
May 17 06:48:37.443: INFO: Pod "pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.960996ms
May 17 06:48:39.450: INFO: Pod "pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011903105s
May 17 06:48:41.449: INFO: Pod "pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010474921s
STEP: Saw pod success 05/17/23 06:48:41.449
May 17 06:48:41.449: INFO: Pod "pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d" satisfied condition "Succeeded or Failed"
May 17 06:48:41.454: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d container test-container: <nil>
STEP: delete the pod 05/17/23 06:48:41.466
May 17 06:48:41.482: INFO: Waiting for pod pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d to disappear
May 17 06:48:41.487: INFO: Pod pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 06:48:41.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-905" for this suite. 05/17/23 06:48:41.497
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":278,"skipped":5207,"failed":0}
------------------------------
â€¢ [4.125 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:48:37.381
    May 17 06:48:37.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 06:48:37.382
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:37.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:37.418
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 05/17/23 06:48:37.422
    May 17 06:48:37.438: INFO: Waiting up to 5m0s for pod "pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d" in namespace "emptydir-905" to be "Succeeded or Failed"
    May 17 06:48:37.443: INFO: Pod "pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.960996ms
    May 17 06:48:39.450: INFO: Pod "pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011903105s
    May 17 06:48:41.449: INFO: Pod "pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010474921s
    STEP: Saw pod success 05/17/23 06:48:41.449
    May 17 06:48:41.449: INFO: Pod "pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d" satisfied condition "Succeeded or Failed"
    May 17 06:48:41.454: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d container test-container: <nil>
    STEP: delete the pod 05/17/23 06:48:41.466
    May 17 06:48:41.482: INFO: Waiting for pod pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d to disappear
    May 17 06:48:41.487: INFO: Pod pod-bf7f19cc-0eda-444e-a643-f319d7d4d19d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 06:48:41.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-905" for this suite. 05/17/23 06:48:41.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:48:41.508
May 17 06:48:41.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 06:48:41.508
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:41.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:41.535
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 06:48:41.558
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:48:41.81
STEP: Deploying the webhook pod 05/17/23 06:48:41.822
STEP: Wait for the deployment to be ready 05/17/23 06:48:41.837
May 17 06:48:41.848: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 06:48:43.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 06:48:45.871: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 06:48:47.873: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 06:48:49.872: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 06:48:51.871: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/17/23 06:48:53.871
STEP: Verifying the service has paired with the endpoint 05/17/23 06:48:53.886
May 17 06:48:54.886: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/17/23 06:48:54.893
STEP: create a configmap that should be updated by the webhook 05/17/23 06:48:54.919
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:48:54.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5321" for this suite. 05/17/23 06:48:54.963
STEP: Destroying namespace "webhook-5321-markers" for this suite. 05/17/23 06:48:54.972
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":279,"skipped":5232,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.524 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:48:41.508
    May 17 06:48:41.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 06:48:41.508
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:41.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:41.535
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 06:48:41.558
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:48:41.81
    STEP: Deploying the webhook pod 05/17/23 06:48:41.822
    STEP: Wait for the deployment to be ready 05/17/23 06:48:41.837
    May 17 06:48:41.848: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May 17 06:48:43.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 06:48:45.871: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 06:48:47.873: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 06:48:49.872: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 06:48:51.871: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 48, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/17/23 06:48:53.871
    STEP: Verifying the service has paired with the endpoint 05/17/23 06:48:53.886
    May 17 06:48:54.886: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/17/23 06:48:54.893
    STEP: create a configmap that should be updated by the webhook 05/17/23 06:48:54.919
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:48:54.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5321" for this suite. 05/17/23 06:48:54.963
    STEP: Destroying namespace "webhook-5321-markers" for this suite. 05/17/23 06:48:54.972
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:48:55.033
May 17 06:48:55.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename deployment 05/17/23 06:48:55.034
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:55.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:55.061
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
May 17 06:48:55.065: INFO: Creating deployment "webserver-deployment"
May 17 06:48:55.073: INFO: Waiting for observed generation 1
May 17 06:48:57.084: INFO: Waiting for all required pods to come up
May 17 06:48:57.091: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 05/17/23 06:48:57.091
May 17 06:48:57.091: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-pgkrw" in namespace "deployment-6005" to be "running"
May 17 06:48:57.091: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-dthfm" in namespace "deployment-6005" to be "running"
May 17 06:48:57.091: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-slqbz" in namespace "deployment-6005" to be "running"
May 17 06:48:57.097: INFO: Pod "webserver-deployment-845c8977d9-pgkrw": Phase="Pending", Reason="", readiness=false. Elapsed: 5.375681ms
May 17 06:48:57.098: INFO: Pod "webserver-deployment-845c8977d9-slqbz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084245ms
May 17 06:48:57.098: INFO: Pod "webserver-deployment-845c8977d9-dthfm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.196036ms
May 17 06:48:59.103: INFO: Pod "webserver-deployment-845c8977d9-pgkrw": Phase="Running", Reason="", readiness=true. Elapsed: 2.011729152s
May 17 06:48:59.103: INFO: Pod "webserver-deployment-845c8977d9-pgkrw" satisfied condition "running"
May 17 06:48:59.104: INFO: Pod "webserver-deployment-845c8977d9-dthfm": Phase="Running", Reason="", readiness=true. Elapsed: 2.012398653s
May 17 06:48:59.104: INFO: Pod "webserver-deployment-845c8977d9-dthfm" satisfied condition "running"
May 17 06:48:59.104: INFO: Pod "webserver-deployment-845c8977d9-slqbz": Phase="Running", Reason="", readiness=true. Elapsed: 2.012385819s
May 17 06:48:59.104: INFO: Pod "webserver-deployment-845c8977d9-slqbz" satisfied condition "running"
May 17 06:48:59.104: INFO: Waiting for deployment "webserver-deployment" to complete
May 17 06:48:59.115: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 17 06:48:59.129: INFO: Updating deployment webserver-deployment
May 17 06:48:59.129: INFO: Waiting for observed generation 2
May 17 06:49:01.141: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 17 06:49:01.147: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 17 06:49:01.153: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 17 06:49:01.171: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 17 06:49:01.171: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 17 06:49:01.177: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 17 06:49:01.188: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 17 06:49:01.188: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 17 06:49:01.201: INFO: Updating deployment webserver-deployment
May 17 06:49:01.201: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 17 06:49:01.212: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 17 06:49:01.217: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 06:49:01.227: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6005  ee89fde6-46c8-4a28-a434-e8b2aa9dda88 1487803 3 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0086ca988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-05-17 06:48:59 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-17 06:49:01 +0000 UTC,LastTransitionTime:2023-05-17 06:49:01 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 17 06:49:01.233: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-6005  6e260efe-d9d3-43a5-b9ee-21705080d63e 1487801 3 2023-05-17 06:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ee89fde6-46c8-4a28-a434-e8b2aa9dda88 0xc003a92bc7 0xc003a92bc8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee89fde6-46c8-4a28-a434-e8b2aa9dda88\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a92c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 06:49:01.233: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 17 06:49:01.233: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-6005  9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 1487800 3 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ee89fde6-46c8-4a28-a434-e8b2aa9dda88 0xc003a92cc7 0xc003a92cc8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee89fde6-46c8-4a28-a434-e8b2aa9dda88\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a92d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 17 06:49:01.243: INFO: Pod "webserver-deployment-69b7448995-5bv9l" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5bv9l webserver-deployment-69b7448995- deployment-6005  3ce1f340-0133-4c3b-aedb-fd4ec175c6ef 1487781 0 2023-05-17 06:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93207 0xc003a93208}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sm4zl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sm4zl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.121,StartTime:2023-05-17 06:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.243: INFO: Pod "webserver-deployment-69b7448995-82mbb" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-82mbb webserver-deployment-69b7448995- deployment-6005  56466539-ce76-474b-bb50-863008140c63 1487809 0 2023-05-17 06:49:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93417 0xc003a93418}] [] [{kube-controller-manager Update v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xhtzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xhtzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.243: INFO: Pod "webserver-deployment-69b7448995-d6r7n" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-d6r7n webserver-deployment-69b7448995- deployment-6005  935e7b07-6717-4208-8566-2f36fc9bad1b 1487796 0 2023-05-17 06:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93570 0xc003a93571}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:49:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4259,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4259,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.62,StartTime:2023-05-17 06:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.244: INFO: Pod "webserver-deployment-69b7448995-dd4fl" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-dd4fl webserver-deployment-69b7448995- deployment-6005  d496b0f1-f6da-4a87-9604-59c64c0fa0fe 1487780 0 2023-05-17 06:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93777 0xc003a93778}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-428c6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-428c6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.122,StartTime:2023-05-17 06:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.244: INFO: Pod "webserver-deployment-69b7448995-tb8qg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tb8qg webserver-deployment-69b7448995- deployment-6005  f0e7cd72-1ed7-4d85-8fec-4c9912207590 1487808 0 2023-05-17 06:49:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93987 0xc003a93988}] [] [{kube-controller-manager Update v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtftj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtftj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:49:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.244: INFO: Pod "webserver-deployment-69b7448995-tzzgz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tzzgz webserver-deployment-69b7448995- deployment-6005  190032e4-4811-4d52-88e5-0dd30d72dba9 1487779 0 2023-05-17 06:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93af7 0xc003a93af8}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6ngrm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6ngrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.4,PodIP:10.244.0.111,StartTime:2023-05-17 06:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.244: INFO: Pod "webserver-deployment-69b7448995-znt2j" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-znt2j webserver-deployment-69b7448995- deployment-6005  17431058-8977-42a8-8815-80240daa2f50 1487765 0 2023-05-17 06:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93d07 0xc003a93d08}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hg9rx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hg9rx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:,StartTime:2023-05-17 06:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.244: INFO: Pod "webserver-deployment-845c8977d9-2h2wx" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-2h2wx webserver-deployment-845c8977d9- deployment-6005  991ac398-4a7a-4f07-98db-5bec3dc54d99 1487703 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc003a93ef7 0xc003a93ef8}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98wd6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98wd6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.120,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9713b9c2564bee4e382a1041b0713fcd0030dd1c828e848cb92475716db9f3a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.244: INFO: Pod "webserver-deployment-845c8977d9-6bxdf" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6bxdf webserver-deployment-845c8977d9- deployment-6005  2029ac89-cbcd-456f-bac7-d486914962f8 1487805 0 2023-05-17 06:49:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d20d7 0xc0079d20d8}] [] [{kube-controller-manager Update v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8nrtw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8nrtw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:49:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.244: INFO: Pod "webserver-deployment-845c8977d9-7j9xr" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7j9xr webserver-deployment-845c8977d9- deployment-6005  de66487b-ef63-4595-95c7-b2f4466b61fd 1487676 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2247 0xc0079d2248}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2cr4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2cr4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.118,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://81f98852e2ebccddae9ed021bd1a5467b7d7f7f47e3ea5e28b7bcf0876552465,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-k9dww" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-k9dww webserver-deployment-845c8977d9- deployment-6005  59aefd97-3e8e-4686-a6f0-f1974ca145c3 1487810 0 2023-05-17 06:49:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2437 0xc0079d2438}] [] [{kube-controller-manager Update v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w6fw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w6fw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:49:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-kxw56" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kxw56 webserver-deployment-845c8977d9- deployment-6005  ff8f7ece-8e7b-4045-9924-3686f1e14830 1487700 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2597 0xc0079d2598}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vbfjj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vbfjj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.4,PodIP:10.244.0.110,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://186f1a37092239f8b61fbf7b6dafa2cefbac98332533412406374c0d608b0fe0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-ms5wv" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ms5wv webserver-deployment-845c8977d9- deployment-6005  ab1ecace-be04-497a-9b8a-9ea3ccbce978 1487696 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2777 0xc0079d2778}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f2pdg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f2pdg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.59,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8a1666524df86261a32248ee3b7a9f69e77ee26bb34fb3944eb1b9b5f15fe348,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-n54lc" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-n54lc webserver-deployment-845c8977d9- deployment-6005  71fb7915-30d3-4a22-9972-b194a295bfa3 1487807 0 2023-05-17 06:49:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2957 0xc0079d2958}] [] [{kube-controller-manager Update v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5jbz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5jbz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-pgkrw" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pgkrw webserver-deployment-845c8977d9- deployment-6005  e86cb21d-2b4b-4027-9b8b-7c4dc36a0b04 1487713 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2aa0 0xc0079d2aa1}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khdf7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khdf7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.61,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5257b7cefe63067ffbd6740618acdf4543d2596010dc22032bfa44232f9a9d24,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-slqbz" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-slqbz webserver-deployment-845c8977d9- deployment-6005  05b8be64-8131-4c3f-a77b-1f7e58c96065 1487710 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2c77 0xc0079d2c78}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p566r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p566r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.119,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c421c89c925effb68a3958918afa0bf68a18341d1e367dfd11b8494cf81a3918,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-tqljt" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-tqljt webserver-deployment-845c8977d9- deployment-6005  55b97ce1-2ac5-4e53-98fd-0336b760d9a0 1487687 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2e57 0xc0079d2e58}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcvkg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcvkg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.117,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://98d9126ea8b139183a5510787bb7acb0f974d49c230e7a36c6fdd57f884ddd0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-x52tb" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-x52tb webserver-deployment-845c8977d9- deployment-6005  39306f44-5ab5-44ce-b74c-eaa72b755f7f 1487673 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d3037 0xc0079d3038}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bh49x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bh49x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.4,PodIP:10.244.0.109,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3ca83e18ee092554b1fdfb84977975e7af1fa985af9e25f6dceec6f015392ec9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May 17 06:49:01.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6005" for this suite. 05/17/23 06:49:01.259
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":280,"skipped":5252,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.237 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:48:55.033
    May 17 06:48:55.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename deployment 05/17/23 06:48:55.034
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:48:55.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:48:55.061
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    May 17 06:48:55.065: INFO: Creating deployment "webserver-deployment"
    May 17 06:48:55.073: INFO: Waiting for observed generation 1
    May 17 06:48:57.084: INFO: Waiting for all required pods to come up
    May 17 06:48:57.091: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 05/17/23 06:48:57.091
    May 17 06:48:57.091: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-pgkrw" in namespace "deployment-6005" to be "running"
    May 17 06:48:57.091: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-dthfm" in namespace "deployment-6005" to be "running"
    May 17 06:48:57.091: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-slqbz" in namespace "deployment-6005" to be "running"
    May 17 06:48:57.097: INFO: Pod "webserver-deployment-845c8977d9-pgkrw": Phase="Pending", Reason="", readiness=false. Elapsed: 5.375681ms
    May 17 06:48:57.098: INFO: Pod "webserver-deployment-845c8977d9-slqbz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084245ms
    May 17 06:48:57.098: INFO: Pod "webserver-deployment-845c8977d9-dthfm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.196036ms
    May 17 06:48:59.103: INFO: Pod "webserver-deployment-845c8977d9-pgkrw": Phase="Running", Reason="", readiness=true. Elapsed: 2.011729152s
    May 17 06:48:59.103: INFO: Pod "webserver-deployment-845c8977d9-pgkrw" satisfied condition "running"
    May 17 06:48:59.104: INFO: Pod "webserver-deployment-845c8977d9-dthfm": Phase="Running", Reason="", readiness=true. Elapsed: 2.012398653s
    May 17 06:48:59.104: INFO: Pod "webserver-deployment-845c8977d9-dthfm" satisfied condition "running"
    May 17 06:48:59.104: INFO: Pod "webserver-deployment-845c8977d9-slqbz": Phase="Running", Reason="", readiness=true. Elapsed: 2.012385819s
    May 17 06:48:59.104: INFO: Pod "webserver-deployment-845c8977d9-slqbz" satisfied condition "running"
    May 17 06:48:59.104: INFO: Waiting for deployment "webserver-deployment" to complete
    May 17 06:48:59.115: INFO: Updating deployment "webserver-deployment" with a non-existent image
    May 17 06:48:59.129: INFO: Updating deployment webserver-deployment
    May 17 06:48:59.129: INFO: Waiting for observed generation 2
    May 17 06:49:01.141: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    May 17 06:49:01.147: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    May 17 06:49:01.153: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May 17 06:49:01.171: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    May 17 06:49:01.171: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    May 17 06:49:01.177: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May 17 06:49:01.188: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    May 17 06:49:01.188: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    May 17 06:49:01.201: INFO: Updating deployment webserver-deployment
    May 17 06:49:01.201: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    May 17 06:49:01.212: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    May 17 06:49:01.217: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 06:49:01.227: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-6005  ee89fde6-46c8-4a28-a434-e8b2aa9dda88 1487803 3 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0086ca988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-05-17 06:48:59 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-17 06:49:01 +0000 UTC,LastTransitionTime:2023-05-17 06:49:01 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    May 17 06:49:01.233: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-6005  6e260efe-d9d3-43a5-b9ee-21705080d63e 1487801 3 2023-05-17 06:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ee89fde6-46c8-4a28-a434-e8b2aa9dda88 0xc003a92bc7 0xc003a92bc8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee89fde6-46c8-4a28-a434-e8b2aa9dda88\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a92c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 17 06:49:01.233: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    May 17 06:49:01.233: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-6005  9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 1487800 3 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ee89fde6-46c8-4a28-a434-e8b2aa9dda88 0xc003a92cc7 0xc003a92cc8}] [] [{kube-controller-manager Update apps/v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee89fde6-46c8-4a28-a434-e8b2aa9dda88\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a92d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    May 17 06:49:01.243: INFO: Pod "webserver-deployment-69b7448995-5bv9l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5bv9l webserver-deployment-69b7448995- deployment-6005  3ce1f340-0133-4c3b-aedb-fd4ec175c6ef 1487781 0 2023-05-17 06:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93207 0xc003a93208}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sm4zl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sm4zl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.121,StartTime:2023-05-17 06:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.243: INFO: Pod "webserver-deployment-69b7448995-82mbb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-82mbb webserver-deployment-69b7448995- deployment-6005  56466539-ce76-474b-bb50-863008140c63 1487809 0 2023-05-17 06:49:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93417 0xc003a93418}] [] [{kube-controller-manager Update v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xhtzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xhtzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.243: INFO: Pod "webserver-deployment-69b7448995-d6r7n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-d6r7n webserver-deployment-69b7448995- deployment-6005  935e7b07-6717-4208-8566-2f36fc9bad1b 1487796 0 2023-05-17 06:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93570 0xc003a93571}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:49:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4259,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4259,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.62,StartTime:2023-05-17 06:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.244: INFO: Pod "webserver-deployment-69b7448995-dd4fl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-dd4fl webserver-deployment-69b7448995- deployment-6005  d496b0f1-f6da-4a87-9604-59c64c0fa0fe 1487780 0 2023-05-17 06:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93777 0xc003a93778}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-428c6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-428c6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.122,StartTime:2023-05-17 06:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.244: INFO: Pod "webserver-deployment-69b7448995-tb8qg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tb8qg webserver-deployment-69b7448995- deployment-6005  f0e7cd72-1ed7-4d85-8fec-4c9912207590 1487808 0 2023-05-17 06:49:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93987 0xc003a93988}] [] [{kube-controller-manager Update v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtftj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtftj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:49:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.244: INFO: Pod "webserver-deployment-69b7448995-tzzgz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tzzgz webserver-deployment-69b7448995- deployment-6005  190032e4-4811-4d52-88e5-0dd30d72dba9 1487779 0 2023-05-17 06:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93af7 0xc003a93af8}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6ngrm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6ngrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.4,PodIP:10.244.0.111,StartTime:2023-05-17 06:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.244: INFO: Pod "webserver-deployment-69b7448995-znt2j" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-znt2j webserver-deployment-69b7448995- deployment-6005  17431058-8977-42a8-8815-80240daa2f50 1487765 0 2023-05-17 06:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 6e260efe-d9d3-43a5-b9ee-21705080d63e 0xc003a93d07 0xc003a93d08}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e260efe-d9d3-43a5-b9ee-21705080d63e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hg9rx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hg9rx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:,StartTime:2023-05-17 06:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.244: INFO: Pod "webserver-deployment-845c8977d9-2h2wx" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-2h2wx webserver-deployment-845c8977d9- deployment-6005  991ac398-4a7a-4f07-98db-5bec3dc54d99 1487703 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc003a93ef7 0xc003a93ef8}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98wd6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98wd6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.120,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9713b9c2564bee4e382a1041b0713fcd0030dd1c828e848cb92475716db9f3a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.244: INFO: Pod "webserver-deployment-845c8977d9-6bxdf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6bxdf webserver-deployment-845c8977d9- deployment-6005  2029ac89-cbcd-456f-bac7-d486914962f8 1487805 0 2023-05-17 06:49:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d20d7 0xc0079d20d8}] [] [{kube-controller-manager Update v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8nrtw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8nrtw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:49:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.244: INFO: Pod "webserver-deployment-845c8977d9-7j9xr" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7j9xr webserver-deployment-845c8977d9- deployment-6005  de66487b-ef63-4595-95c7-b2f4466b61fd 1487676 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2247 0xc0079d2248}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2cr4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2cr4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.118,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://81f98852e2ebccddae9ed021bd1a5467b7d7f7f47e3ea5e28b7bcf0876552465,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-k9dww" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-k9dww webserver-deployment-845c8977d9- deployment-6005  59aefd97-3e8e-4686-a6f0-f1974ca145c3 1487810 0 2023-05-17 06:49:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2437 0xc0079d2438}] [] [{kube-controller-manager Update v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w6fw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w6fw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:49:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-kxw56" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kxw56 webserver-deployment-845c8977d9- deployment-6005  ff8f7ece-8e7b-4045-9924-3686f1e14830 1487700 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2597 0xc0079d2598}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vbfjj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vbfjj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.4,PodIP:10.244.0.110,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://186f1a37092239f8b61fbf7b6dafa2cefbac98332533412406374c0d608b0fe0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-ms5wv" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ms5wv webserver-deployment-845c8977d9- deployment-6005  ab1ecace-be04-497a-9b8a-9ea3ccbce978 1487696 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2777 0xc0079d2778}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f2pdg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f2pdg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.59,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8a1666524df86261a32248ee3b7a9f69e77ee26bb34fb3944eb1b9b5f15fe348,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-n54lc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-n54lc webserver-deployment-845c8977d9- deployment-6005  71fb7915-30d3-4a22-9972-b194a295bfa3 1487807 0 2023-05-17 06:49:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2957 0xc0079d2958}] [] [{kube-controller-manager Update v1 2023-05-17 06:49:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5jbz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5jbz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-pgkrw" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pgkrw webserver-deployment-845c8977d9- deployment-6005  e86cb21d-2b4b-4027-9b8b-7c4dc36a0b04 1487713 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2aa0 0xc0079d2aa1}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khdf7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khdf7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.61,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5257b7cefe63067ffbd6740618acdf4543d2596010dc22032bfa44232f9a9d24,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-slqbz" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-slqbz webserver-deployment-845c8977d9- deployment-6005  05b8be64-8131-4c3f-a77b-1f7e58c96065 1487710 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2c77 0xc0079d2c78}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p566r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p566r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.119,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c421c89c925effb68a3958918afa0bf68a18341d1e367dfd11b8494cf81a3918,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-tqljt" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-tqljt webserver-deployment-845c8977d9- deployment-6005  55b97ce1-2ac5-4e53-98fd-0336b760d9a0 1487687 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d2e57 0xc0079d2e58}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcvkg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcvkg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.117,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://98d9126ea8b139183a5510787bb7acb0f974d49c230e7a36c6fdd57f884ddd0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 06:49:01.245: INFO: Pod "webserver-deployment-845c8977d9-x52tb" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-x52tb webserver-deployment-845c8977d9- deployment-6005  39306f44-5ab5-44ce-b74c-eaa72b755f7f 1487673 0 2023-05-17 06:48:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad 0xc0079d3037 0xc0079d3038}] [] [{kube-controller-manager Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a786cd3-8b7e-44a2-b3aa-8704e5ed6bad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:48:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bh49x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bh49x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000b,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.4,PodIP:10.244.0.109,StartTime:2023-05-17 06:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:48:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3ca83e18ee092554b1fdfb84977975e7af1fa985af9e25f6dceec6f015392ec9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May 17 06:49:01.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6005" for this suite. 05/17/23 06:49:01.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:49:01.272
May 17 06:49:01.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:49:01.273
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:01.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:01.299
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-e6c43589-8fb8-4e15-9a39-32b637415f2e 05/17/23 06:49:01.303
STEP: Creating a pod to test consume configMaps 05/17/23 06:49:01.31
May 17 06:49:01.323: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be" in namespace "projected-8422" to be "Succeeded or Failed"
May 17 06:49:01.328: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Pending", Reason="", readiness=false. Elapsed: 5.623554ms
May 17 06:49:03.335: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011859861s
May 17 06:49:05.335: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01208867s
May 17 06:49:07.334: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01171545s
May 17 06:49:09.335: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012125956s
May 17 06:49:11.334: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011573762s
May 17 06:49:13.335: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.012189734s
STEP: Saw pod success 05/17/23 06:49:13.335
May 17 06:49:13.335: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be" satisfied condition "Succeeded or Failed"
May 17 06:49:13.340: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be container agnhost-container: <nil>
STEP: delete the pod 05/17/23 06:49:13.354
May 17 06:49:13.366: INFO: Waiting for pod pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be to disappear
May 17 06:49:13.371: INFO: Pod pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May 17 06:49:13.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8422" for this suite. 05/17/23 06:49:13.381
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":281,"skipped":5266,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.119 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:49:01.272
    May 17 06:49:01.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:49:01.273
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:01.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:01.299
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-e6c43589-8fb8-4e15-9a39-32b637415f2e 05/17/23 06:49:01.303
    STEP: Creating a pod to test consume configMaps 05/17/23 06:49:01.31
    May 17 06:49:01.323: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be" in namespace "projected-8422" to be "Succeeded or Failed"
    May 17 06:49:01.328: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Pending", Reason="", readiness=false. Elapsed: 5.623554ms
    May 17 06:49:03.335: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011859861s
    May 17 06:49:05.335: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01208867s
    May 17 06:49:07.334: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01171545s
    May 17 06:49:09.335: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012125956s
    May 17 06:49:11.334: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011573762s
    May 17 06:49:13.335: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.012189734s
    STEP: Saw pod success 05/17/23 06:49:13.335
    May 17 06:49:13.335: INFO: Pod "pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be" satisfied condition "Succeeded or Failed"
    May 17 06:49:13.340: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 06:49:13.354
    May 17 06:49:13.366: INFO: Waiting for pod pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be to disappear
    May 17 06:49:13.371: INFO: Pod pod-projected-configmaps-c10de06c-30ed-4f33-abc7-ba0ca58852be no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May 17 06:49:13.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8422" for this suite. 05/17/23 06:49:13.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:49:13.392
May 17 06:49:13.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename replicaset 05/17/23 06:49:13.393
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:13.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:13.432
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 05/17/23 06:49:13.448
STEP: Verify that the required pods have come up. 05/17/23 06:49:13.456
May 17 06:49:13.461: INFO: Pod name sample-pod: Found 0 pods out of 1
May 17 06:49:18.469: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/17/23 06:49:18.469
STEP: Getting /status 05/17/23 06:49:18.469
May 17 06:49:18.476: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 05/17/23 06:49:18.476
May 17 06:49:18.489: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 05/17/23 06:49:18.489
May 17 06:49:18.492: INFO: Observed &ReplicaSet event: ADDED
May 17 06:49:18.492: INFO: Observed &ReplicaSet event: MODIFIED
May 17 06:49:18.492: INFO: Observed &ReplicaSet event: MODIFIED
May 17 06:49:18.492: INFO: Observed &ReplicaSet event: MODIFIED
May 17 06:49:18.492: INFO: Found replicaset test-rs in namespace replicaset-6969 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 17 06:49:18.492: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 05/17/23 06:49:18.492
May 17 06:49:18.492: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 17 06:49:18.501: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 05/17/23 06:49:18.501
May 17 06:49:18.503: INFO: Observed &ReplicaSet event: ADDED
May 17 06:49:18.503: INFO: Observed &ReplicaSet event: MODIFIED
May 17 06:49:18.503: INFO: Observed &ReplicaSet event: MODIFIED
May 17 06:49:18.503: INFO: Observed &ReplicaSet event: MODIFIED
May 17 06:49:18.503: INFO: Observed replicaset test-rs in namespace replicaset-6969 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 17 06:49:18.503: INFO: Observed &ReplicaSet event: MODIFIED
May 17 06:49:18.503: INFO: Found replicaset test-rs in namespace replicaset-6969 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
May 17 06:49:18.503: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May 17 06:49:18.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6969" for this suite. 05/17/23 06:49:18.514
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":282,"skipped":5292,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.132 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:49:13.392
    May 17 06:49:13.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename replicaset 05/17/23 06:49:13.393
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:13.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:13.432
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 05/17/23 06:49:13.448
    STEP: Verify that the required pods have come up. 05/17/23 06:49:13.456
    May 17 06:49:13.461: INFO: Pod name sample-pod: Found 0 pods out of 1
    May 17 06:49:18.469: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/17/23 06:49:18.469
    STEP: Getting /status 05/17/23 06:49:18.469
    May 17 06:49:18.476: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 05/17/23 06:49:18.476
    May 17 06:49:18.489: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 05/17/23 06:49:18.489
    May 17 06:49:18.492: INFO: Observed &ReplicaSet event: ADDED
    May 17 06:49:18.492: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 06:49:18.492: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 06:49:18.492: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 06:49:18.492: INFO: Found replicaset test-rs in namespace replicaset-6969 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May 17 06:49:18.492: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 05/17/23 06:49:18.492
    May 17 06:49:18.492: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May 17 06:49:18.501: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 05/17/23 06:49:18.501
    May 17 06:49:18.503: INFO: Observed &ReplicaSet event: ADDED
    May 17 06:49:18.503: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 06:49:18.503: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 06:49:18.503: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 06:49:18.503: INFO: Observed replicaset test-rs in namespace replicaset-6969 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 17 06:49:18.503: INFO: Observed &ReplicaSet event: MODIFIED
    May 17 06:49:18.503: INFO: Found replicaset test-rs in namespace replicaset-6969 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    May 17 06:49:18.503: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May 17 06:49:18.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6969" for this suite. 05/17/23 06:49:18.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:49:18.527
May 17 06:49:18.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename svcaccounts 05/17/23 06:49:18.528
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:18.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:18.555
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 05/17/23 06:49:18.56
STEP: watching for the ServiceAccount to be added 05/17/23 06:49:18.572
STEP: patching the ServiceAccount 05/17/23 06:49:18.575
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/17/23 06:49:18.583
STEP: deleting the ServiceAccount 05/17/23 06:49:18.592
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May 17 06:49:18.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3195" for this suite. 05/17/23 06:49:18.615
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":283,"skipped":5334,"failed":0}
------------------------------
â€¢ [0.099 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:49:18.527
    May 17 06:49:18.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 06:49:18.528
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:18.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:18.555
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 05/17/23 06:49:18.56
    STEP: watching for the ServiceAccount to be added 05/17/23 06:49:18.572
    STEP: patching the ServiceAccount 05/17/23 06:49:18.575
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/17/23 06:49:18.583
    STEP: deleting the ServiceAccount 05/17/23 06:49:18.592
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May 17 06:49:18.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3195" for this suite. 05/17/23 06:49:18.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:49:18.628
May 17 06:49:18.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename runtimeclass 05/17/23 06:49:18.628
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:18.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:18.655
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
May 17 06:49:18.678: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2128 to be scheduled
May 17 06:49:18.683: INFO: 1 pods are not scheduled: [runtimeclass-2128/test-runtimeclass-runtimeclass-2128-preconfigured-handler-kmrgf(cfb3b1f6-6d68-4656-9942-bfae0e578c6d)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May 17 06:49:20.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2128" for this suite. 05/17/23 06:49:20.712
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":284,"skipped":5352,"failed":0}
------------------------------
â€¢ [2.094 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:49:18.628
    May 17 06:49:18.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename runtimeclass 05/17/23 06:49:18.628
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:18.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:18.655
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    May 17 06:49:18.678: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2128 to be scheduled
    May 17 06:49:18.683: INFO: 1 pods are not scheduled: [runtimeclass-2128/test-runtimeclass-runtimeclass-2128-preconfigured-handler-kmrgf(cfb3b1f6-6d68-4656-9942-bfae0e578c6d)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May 17 06:49:20.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2128" for this suite. 05/17/23 06:49:20.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:49:20.722
May 17 06:49:20.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 06:49:20.722
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:20.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:20.747
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 05/17/23 06:49:20.751
May 17 06:49:20.764: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2" in namespace "downward-api-9799" to be "Succeeded or Failed"
May 17 06:49:20.769: INFO: Pod "downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.623395ms
May 17 06:49:22.775: INFO: Pod "downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010568467s
May 17 06:49:24.774: INFO: Pod "downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010358081s
STEP: Saw pod success 05/17/23 06:49:24.774
May 17 06:49:24.775: INFO: Pod "downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2" satisfied condition "Succeeded or Failed"
May 17 06:49:24.780: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2 container client-container: <nil>
STEP: delete the pod 05/17/23 06:49:24.793
May 17 06:49:24.809: INFO: Waiting for pod downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2 to disappear
May 17 06:49:24.814: INFO: Pod downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May 17 06:49:24.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9799" for this suite. 05/17/23 06:49:24.824
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":285,"skipped":5360,"failed":0}
------------------------------
â€¢ [4.115 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:49:20.722
    May 17 06:49:20.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 06:49:20.722
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:20.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:20.747
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 05/17/23 06:49:20.751
    May 17 06:49:20.764: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2" in namespace "downward-api-9799" to be "Succeeded or Failed"
    May 17 06:49:20.769: INFO: Pod "downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.623395ms
    May 17 06:49:22.775: INFO: Pod "downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010568467s
    May 17 06:49:24.774: INFO: Pod "downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010358081s
    STEP: Saw pod success 05/17/23 06:49:24.774
    May 17 06:49:24.775: INFO: Pod "downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2" satisfied condition "Succeeded or Failed"
    May 17 06:49:24.780: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2 container client-container: <nil>
    STEP: delete the pod 05/17/23 06:49:24.793
    May 17 06:49:24.809: INFO: Waiting for pod downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2 to disappear
    May 17 06:49:24.814: INFO: Pod downwardapi-volume-96a50673-ab19-4ae4-8c07-163088ec1cc2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May 17 06:49:24.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9799" for this suite. 05/17/23 06:49:24.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:49:24.838
May 17 06:49:24.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:49:24.839
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:24.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:24.865
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-99ea726f-10da-41fc-84ab-888aac0ba7d1 05/17/23 06:49:24.87
STEP: Creating a pod to test consume configMaps 05/17/23 06:49:24.876
May 17 06:49:24.889: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc" in namespace "projected-8591" to be "Succeeded or Failed"
May 17 06:49:24.893: INFO: Pod "pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.58076ms
May 17 06:49:26.899: INFO: Pod "pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010042642s
May 17 06:49:28.901: INFO: Pod "pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011668114s
STEP: Saw pod success 05/17/23 06:49:28.901
May 17 06:49:28.901: INFO: Pod "pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc" satisfied condition "Succeeded or Failed"
May 17 06:49:28.906: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc container agnhost-container: <nil>
STEP: delete the pod 05/17/23 06:49:28.919
May 17 06:49:28.931: INFO: Waiting for pod pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc to disappear
May 17 06:49:28.936: INFO: Pod pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May 17 06:49:28.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8591" for this suite. 05/17/23 06:49:28.946
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":286,"skipped":5375,"failed":0}
------------------------------
â€¢ [4.119 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:49:24.838
    May 17 06:49:24.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:49:24.839
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:24.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:24.865
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-99ea726f-10da-41fc-84ab-888aac0ba7d1 05/17/23 06:49:24.87
    STEP: Creating a pod to test consume configMaps 05/17/23 06:49:24.876
    May 17 06:49:24.889: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc" in namespace "projected-8591" to be "Succeeded or Failed"
    May 17 06:49:24.893: INFO: Pod "pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.58076ms
    May 17 06:49:26.899: INFO: Pod "pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010042642s
    May 17 06:49:28.901: INFO: Pod "pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011668114s
    STEP: Saw pod success 05/17/23 06:49:28.901
    May 17 06:49:28.901: INFO: Pod "pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc" satisfied condition "Succeeded or Failed"
    May 17 06:49:28.906: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 06:49:28.919
    May 17 06:49:28.931: INFO: Waiting for pod pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc to disappear
    May 17 06:49:28.936: INFO: Pod pod-projected-configmaps-88f03a81-8474-46a6-afe5-6344dbf232dc no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May 17 06:49:28.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8591" for this suite. 05/17/23 06:49:28.946
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:49:28.958
May 17 06:49:28.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename svcaccounts 05/17/23 06:49:28.958
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:28.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:28.981
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
May 17 06:49:28.991: INFO: Got root ca configmap in namespace "svcaccounts-3296"
May 17 06:49:29.000: INFO: Deleted root ca configmap in namespace "svcaccounts-3296"
STEP: waiting for a new root ca configmap created 05/17/23 06:49:29.501
May 17 06:49:29.507: INFO: Recreated root ca configmap in namespace "svcaccounts-3296"
May 17 06:49:29.515: INFO: Updated root ca configmap in namespace "svcaccounts-3296"
STEP: waiting for the root ca configmap reconciled 05/17/23 06:49:30.016
May 17 06:49:30.021: INFO: Reconciled root ca configmap in namespace "svcaccounts-3296"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May 17 06:49:30.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3296" for this suite. 05/17/23 06:49:30.033
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":287,"skipped":5377,"failed":0}
------------------------------
â€¢ [1.085 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:49:28.958
    May 17 06:49:28.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename svcaccounts 05/17/23 06:49:28.958
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:28.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:28.981
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    May 17 06:49:28.991: INFO: Got root ca configmap in namespace "svcaccounts-3296"
    May 17 06:49:29.000: INFO: Deleted root ca configmap in namespace "svcaccounts-3296"
    STEP: waiting for a new root ca configmap created 05/17/23 06:49:29.501
    May 17 06:49:29.507: INFO: Recreated root ca configmap in namespace "svcaccounts-3296"
    May 17 06:49:29.515: INFO: Updated root ca configmap in namespace "svcaccounts-3296"
    STEP: waiting for the root ca configmap reconciled 05/17/23 06:49:30.016
    May 17 06:49:30.021: INFO: Reconciled root ca configmap in namespace "svcaccounts-3296"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May 17 06:49:30.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3296" for this suite. 05/17/23 06:49:30.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:49:30.043
May 17 06:49:30.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename cronjob 05/17/23 06:49:30.044
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:30.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:30.068
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 05/17/23 06:49:30.072
STEP: Ensuring a job is scheduled 05/17/23 06:49:30.08
STEP: Ensuring exactly one is scheduled 05/17/23 06:50:02.087
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/17/23 06:50:02.093
STEP: Ensuring no more jobs are scheduled 05/17/23 06:50:02.098
STEP: Removing cronjob 05/17/23 06:55:02.111
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May 17 06:55:02.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9103" for this suite. 05/17/23 06:55:02.133
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":288,"skipped":5391,"failed":0}
------------------------------
â€¢ [SLOW TEST] [332.105 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:49:30.043
    May 17 06:49:30.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename cronjob 05/17/23 06:49:30.044
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:49:30.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:49:30.068
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 05/17/23 06:49:30.072
    STEP: Ensuring a job is scheduled 05/17/23 06:49:30.08
    STEP: Ensuring exactly one is scheduled 05/17/23 06:50:02.087
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/17/23 06:50:02.093
    STEP: Ensuring no more jobs are scheduled 05/17/23 06:50:02.098
    STEP: Removing cronjob 05/17/23 06:55:02.111
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May 17 06:55:02.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9103" for this suite. 05/17/23 06:55:02.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:55:02.148
May 17 06:55:02.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:55:02.149
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:55:02.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:55:02.191
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 05/17/23 06:55:02.196
May 17 06:55:02.215: INFO: Waiting up to 5m0s for pod "labelsupdateb8450458-7621-487a-990b-d78d1a7afdad" in namespace "projected-4529" to be "running and ready"
May 17 06:55:02.220: INFO: Pod "labelsupdateb8450458-7621-487a-990b-d78d1a7afdad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.100652ms
May 17 06:55:02.220: INFO: The phase of Pod labelsupdateb8450458-7621-487a-990b-d78d1a7afdad is Pending, waiting for it to be Running (with Ready = true)
May 17 06:55:04.227: INFO: Pod "labelsupdateb8450458-7621-487a-990b-d78d1a7afdad": Phase="Running", Reason="", readiness=true. Elapsed: 2.012621569s
May 17 06:55:04.227: INFO: The phase of Pod labelsupdateb8450458-7621-487a-990b-d78d1a7afdad is Running (Ready = true)
May 17 06:55:04.227: INFO: Pod "labelsupdateb8450458-7621-487a-990b-d78d1a7afdad" satisfied condition "running and ready"
May 17 06:55:04.764: INFO: Successfully updated pod "labelsupdateb8450458-7621-487a-990b-d78d1a7afdad"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May 17 06:55:06.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4529" for this suite. 05/17/23 06:55:06.803
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":289,"skipped":5404,"failed":0}
------------------------------
â€¢ [4.665 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:55:02.148
    May 17 06:55:02.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:55:02.149
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:55:02.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:55:02.191
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 05/17/23 06:55:02.196
    May 17 06:55:02.215: INFO: Waiting up to 5m0s for pod "labelsupdateb8450458-7621-487a-990b-d78d1a7afdad" in namespace "projected-4529" to be "running and ready"
    May 17 06:55:02.220: INFO: Pod "labelsupdateb8450458-7621-487a-990b-d78d1a7afdad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.100652ms
    May 17 06:55:02.220: INFO: The phase of Pod labelsupdateb8450458-7621-487a-990b-d78d1a7afdad is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:55:04.227: INFO: Pod "labelsupdateb8450458-7621-487a-990b-d78d1a7afdad": Phase="Running", Reason="", readiness=true. Elapsed: 2.012621569s
    May 17 06:55:04.227: INFO: The phase of Pod labelsupdateb8450458-7621-487a-990b-d78d1a7afdad is Running (Ready = true)
    May 17 06:55:04.227: INFO: Pod "labelsupdateb8450458-7621-487a-990b-d78d1a7afdad" satisfied condition "running and ready"
    May 17 06:55:04.764: INFO: Successfully updated pod "labelsupdateb8450458-7621-487a-990b-d78d1a7afdad"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May 17 06:55:06.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4529" for this suite. 05/17/23 06:55:06.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:55:06.814
May 17 06:55:06.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 06:55:06.814
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:55:06.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:55:06.843
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-b15ccdec-814a-45bb-a6ba-3d935f689e0a 05/17/23 06:55:06.848
STEP: Creating a pod to test consume configMaps 05/17/23 06:55:06.854
May 17 06:55:06.868: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f" in namespace "configmap-9882" to be "Succeeded or Failed"
May 17 06:55:06.872: INFO: Pod "pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.71201ms
May 17 06:55:08.879: INFO: Pod "pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011651891s
May 17 06:55:10.879: INFO: Pod "pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010907135s
STEP: Saw pod success 05/17/23 06:55:10.879
May 17 06:55:10.879: INFO: Pod "pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f" satisfied condition "Succeeded or Failed"
May 17 06:55:10.884: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f container agnhost-container: <nil>
STEP: delete the pod 05/17/23 06:55:10.897
May 17 06:55:10.914: INFO: Waiting for pod pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f to disappear
May 17 06:55:10.919: INFO: Pod pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May 17 06:55:10.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9882" for this suite. 05/17/23 06:55:10.93
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":290,"skipped":5410,"failed":0}
------------------------------
â€¢ [4.153 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:55:06.814
    May 17 06:55:06.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 06:55:06.814
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:55:06.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:55:06.843
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-b15ccdec-814a-45bb-a6ba-3d935f689e0a 05/17/23 06:55:06.848
    STEP: Creating a pod to test consume configMaps 05/17/23 06:55:06.854
    May 17 06:55:06.868: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f" in namespace "configmap-9882" to be "Succeeded or Failed"
    May 17 06:55:06.872: INFO: Pod "pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.71201ms
    May 17 06:55:08.879: INFO: Pod "pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011651891s
    May 17 06:55:10.879: INFO: Pod "pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010907135s
    STEP: Saw pod success 05/17/23 06:55:10.879
    May 17 06:55:10.879: INFO: Pod "pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f" satisfied condition "Succeeded or Failed"
    May 17 06:55:10.884: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 06:55:10.897
    May 17 06:55:10.914: INFO: Waiting for pod pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f to disappear
    May 17 06:55:10.919: INFO: Pod pod-configmaps-1ae18040-1668-46c6-9362-a42d123ded6f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 06:55:10.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9882" for this suite. 05/17/23 06:55:10.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:55:10.968
May 17 06:55:10.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename job 05/17/23 06:55:10.969
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:55:10.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:55:10.994
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 05/17/23 06:55:10.998
STEP: Ensuring job reaches completions 05/17/23 06:55:11.005
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May 17 06:55:21.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8600" for this suite. 05/17/23 06:55:21.017
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":291,"skipped":5435,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.058 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:55:10.968
    May 17 06:55:10.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename job 05/17/23 06:55:10.969
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:55:10.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:55:10.994
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 05/17/23 06:55:10.998
    STEP: Ensuring job reaches completions 05/17/23 06:55:11.005
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May 17 06:55:21.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8600" for this suite. 05/17/23 06:55:21.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:55:21.028
May 17 06:55:21.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename init-container 05/17/23 06:55:21.028
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:55:21.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:55:21.052
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 05/17/23 06:55:21.056
May 17 06:55:21.056: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May 17 06:55:24.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6023" for this suite. 05/17/23 06:55:24.216
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":292,"skipped":5483,"failed":0}
------------------------------
â€¢ [3.198 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:55:21.028
    May 17 06:55:21.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename init-container 05/17/23 06:55:21.028
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:55:21.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:55:21.052
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 05/17/23 06:55:21.056
    May 17 06:55:21.056: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May 17 06:55:24.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6023" for this suite. 05/17/23 06:55:24.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:55:24.227
May 17 06:55:24.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:55:24.227
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:55:24.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:55:24.255
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 05/17/23 06:55:24.265
May 17 06:55:24.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 create -f -'
May 17 06:55:24.467: INFO: stderr: ""
May 17 06:55:24.467: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 06:55:24.467
May 17 06:55:24.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 06:55:24.545: INFO: stderr: ""
May 17 06:55:24.545: INFO: stdout: "update-demo-nautilus-7mb2t update-demo-nautilus-x7hn6 "
May 17 06:55:24.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:55:24.621: INFO: stderr: ""
May 17 06:55:24.621: INFO: stdout: ""
May 17 06:55:24.621: INFO: update-demo-nautilus-7mb2t is created but not running
May 17 06:55:29.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 06:55:29.691: INFO: stderr: ""
May 17 06:55:29.691: INFO: stdout: "update-demo-nautilus-7mb2t update-demo-nautilus-x7hn6 "
May 17 06:55:29.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:55:29.752: INFO: stderr: ""
May 17 06:55:29.752: INFO: stdout: "true"
May 17 06:55:29.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 06:55:29.816: INFO: stderr: ""
May 17 06:55:29.816: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May 17 06:55:29.816: INFO: validating pod update-demo-nautilus-7mb2t
May 17 06:55:29.825: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 06:55:29.825: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 06:55:29.825: INFO: update-demo-nautilus-7mb2t is verified up and running
May 17 06:55:29.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-x7hn6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:55:30.117: INFO: stderr: ""
May 17 06:55:30.117: INFO: stdout: "true"
May 17 06:55:30.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-x7hn6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 06:55:30.187: INFO: stderr: ""
May 17 06:55:30.187: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May 17 06:55:30.187: INFO: validating pod update-demo-nautilus-x7hn6
May 17 06:55:35.202: INFO: update-demo-nautilus-x7hn6 is running right image but validator function failed: the server is currently unable to handle the request (get pods update-demo-nautilus-x7hn6)
May 17 06:55:40.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 06:55:40.269: INFO: stderr: ""
May 17 06:55:40.269: INFO: stdout: "update-demo-nautilus-7mb2t update-demo-nautilus-x7hn6 "
May 17 06:55:40.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:55:40.337: INFO: stderr: ""
May 17 06:55:40.337: INFO: stdout: "true"
May 17 06:55:40.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 06:55:40.401: INFO: stderr: ""
May 17 06:55:40.401: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May 17 06:55:40.401: INFO: validating pod update-demo-nautilus-7mb2t
May 17 06:55:40.408: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 06:55:40.408: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 06:55:40.408: INFO: update-demo-nautilus-7mb2t is verified up and running
May 17 06:55:40.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-x7hn6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:55:40.468: INFO: stderr: ""
May 17 06:55:40.468: INFO: stdout: "true"
May 17 06:55:40.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-x7hn6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 06:55:40.549: INFO: stderr: ""
May 17 06:55:40.549: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May 17 06:55:40.549: INFO: validating pod update-demo-nautilus-x7hn6
May 17 06:55:40.562: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 06:55:40.562: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 06:55:40.562: INFO: update-demo-nautilus-x7hn6 is verified up and running
STEP: scaling down the replication controller 05/17/23 06:55:40.562
May 17 06:55:40.563: INFO: scanned /root for discovery docs: <nil>
May 17 06:55:40.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 17 06:55:41.643: INFO: stderr: ""
May 17 06:55:41.643: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 06:55:41.643
May 17 06:55:41.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 06:55:41.722: INFO: stderr: ""
May 17 06:55:41.722: INFO: stdout: "update-demo-nautilus-7mb2t "
May 17 06:55:41.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:55:41.789: INFO: stderr: ""
May 17 06:55:41.790: INFO: stdout: "true"
May 17 06:55:41.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 06:55:41.857: INFO: stderr: ""
May 17 06:55:41.857: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May 17 06:55:41.857: INFO: validating pod update-demo-nautilus-7mb2t
May 17 06:55:41.864: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 06:55:41.864: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 06:55:41.864: INFO: update-demo-nautilus-7mb2t is verified up and running
STEP: scaling up the replication controller 05/17/23 06:55:41.864
May 17 06:55:41.865: INFO: scanned /root for discovery docs: <nil>
May 17 06:55:41.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 17 06:55:42.953: INFO: stderr: ""
May 17 06:55:42.953: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 06:55:42.953
May 17 06:55:42.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 06:55:43.021: INFO: stderr: ""
May 17 06:55:43.021: INFO: stdout: "update-demo-nautilus-7mb2t update-demo-nautilus-p2kr7 "
May 17 06:55:43.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:55:43.085: INFO: stderr: ""
May 17 06:55:43.085: INFO: stdout: "true"
May 17 06:55:43.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 06:55:43.146: INFO: stderr: ""
May 17 06:55:43.146: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May 17 06:55:43.146: INFO: validating pod update-demo-nautilus-7mb2t
May 17 06:55:43.153: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 06:55:43.153: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 06:55:43.153: INFO: update-demo-nautilus-7mb2t is verified up and running
May 17 06:55:43.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-p2kr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:55:43.213: INFO: stderr: ""
May 17 06:55:43.213: INFO: stdout: ""
May 17 06:55:43.213: INFO: update-demo-nautilus-p2kr7 is created but not running
May 17 06:55:48.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 06:55:48.283: INFO: stderr: ""
May 17 06:55:48.283: INFO: stdout: "update-demo-nautilus-7mb2t update-demo-nautilus-p2kr7 "
May 17 06:55:48.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:55:48.342: INFO: stderr: ""
May 17 06:55:48.342: INFO: stdout: "true"
May 17 06:55:48.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 06:55:48.404: INFO: stderr: ""
May 17 06:55:48.404: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May 17 06:55:48.404: INFO: validating pod update-demo-nautilus-7mb2t
May 17 06:55:48.415: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 06:55:48.415: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 06:55:48.415: INFO: update-demo-nautilus-7mb2t is verified up and running
May 17 06:55:48.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-p2kr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:55:48.482: INFO: stderr: ""
May 17 06:55:48.482: INFO: stdout: "true"
May 17 06:55:48.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-p2kr7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 06:55:48.555: INFO: stderr: ""
May 17 06:55:48.555: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May 17 06:55:48.555: INFO: validating pod update-demo-nautilus-p2kr7
May 17 06:55:53.564: INFO: update-demo-nautilus-p2kr7 is running right image but validator function failed: the server is currently unable to handle the request (get pods update-demo-nautilus-p2kr7)
May 17 06:55:58.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 06:55:58.630: INFO: stderr: ""
May 17 06:55:58.630: INFO: stdout: "update-demo-nautilus-7mb2t update-demo-nautilus-p2kr7 "
May 17 06:55:58.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:55:58.692: INFO: stderr: ""
May 17 06:55:58.692: INFO: stdout: "true"
May 17 06:55:58.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 06:55:58.756: INFO: stderr: ""
May 17 06:55:58.756: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May 17 06:55:58.756: INFO: validating pod update-demo-nautilus-7mb2t
May 17 06:55:58.766: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 06:55:58.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 06:55:58.766: INFO: update-demo-nautilus-7mb2t is verified up and running
May 17 06:55:58.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-p2kr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 06:55:58.826: INFO: stderr: ""
May 17 06:55:58.826: INFO: stdout: "true"
May 17 06:55:58.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-p2kr7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 06:55:58.886: INFO: stderr: ""
May 17 06:55:58.886: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May 17 06:55:58.886: INFO: validating pod update-demo-nautilus-p2kr7
May 17 06:55:58.896: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 06:55:58.896: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 06:55:58.896: INFO: update-demo-nautilus-p2kr7 is verified up and running
STEP: using delete to clean up resources 05/17/23 06:55:58.896
May 17 06:55:58.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 delete --grace-period=0 --force -f -'
May 17 06:55:58.965: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 06:55:58.965: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 17 06:55:58.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get rc,svc -l name=update-demo --no-headers'
May 17 06:55:59.035: INFO: stderr: "No resources found in kubectl-7834 namespace.\n"
May 17 06:55:59.035: INFO: stdout: ""
May 17 06:55:59.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 17 06:55:59.103: INFO: stderr: ""
May 17 06:55:59.103: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:55:59.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7834" for this suite. 05/17/23 06:55:59.111
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":293,"skipped":5502,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.893 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:55:24.227
    May 17 06:55:24.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:55:24.227
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:55:24.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:55:24.255
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 05/17/23 06:55:24.265
    May 17 06:55:24.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 create -f -'
    May 17 06:55:24.467: INFO: stderr: ""
    May 17 06:55:24.467: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 06:55:24.467
    May 17 06:55:24.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 06:55:24.545: INFO: stderr: ""
    May 17 06:55:24.545: INFO: stdout: "update-demo-nautilus-7mb2t update-demo-nautilus-x7hn6 "
    May 17 06:55:24.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:55:24.621: INFO: stderr: ""
    May 17 06:55:24.621: INFO: stdout: ""
    May 17 06:55:24.621: INFO: update-demo-nautilus-7mb2t is created but not running
    May 17 06:55:29.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 06:55:29.691: INFO: stderr: ""
    May 17 06:55:29.691: INFO: stdout: "update-demo-nautilus-7mb2t update-demo-nautilus-x7hn6 "
    May 17 06:55:29.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:55:29.752: INFO: stderr: ""
    May 17 06:55:29.752: INFO: stdout: "true"
    May 17 06:55:29.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 06:55:29.816: INFO: stderr: ""
    May 17 06:55:29.816: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May 17 06:55:29.816: INFO: validating pod update-demo-nautilus-7mb2t
    May 17 06:55:29.825: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 06:55:29.825: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 06:55:29.825: INFO: update-demo-nautilus-7mb2t is verified up and running
    May 17 06:55:29.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-x7hn6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:55:30.117: INFO: stderr: ""
    May 17 06:55:30.117: INFO: stdout: "true"
    May 17 06:55:30.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-x7hn6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 06:55:30.187: INFO: stderr: ""
    May 17 06:55:30.187: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May 17 06:55:30.187: INFO: validating pod update-demo-nautilus-x7hn6
    May 17 06:55:35.202: INFO: update-demo-nautilus-x7hn6 is running right image but validator function failed: the server is currently unable to handle the request (get pods update-demo-nautilus-x7hn6)
    May 17 06:55:40.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 06:55:40.269: INFO: stderr: ""
    May 17 06:55:40.269: INFO: stdout: "update-demo-nautilus-7mb2t update-demo-nautilus-x7hn6 "
    May 17 06:55:40.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:55:40.337: INFO: stderr: ""
    May 17 06:55:40.337: INFO: stdout: "true"
    May 17 06:55:40.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 06:55:40.401: INFO: stderr: ""
    May 17 06:55:40.401: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May 17 06:55:40.401: INFO: validating pod update-demo-nautilus-7mb2t
    May 17 06:55:40.408: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 06:55:40.408: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 06:55:40.408: INFO: update-demo-nautilus-7mb2t is verified up and running
    May 17 06:55:40.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-x7hn6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:55:40.468: INFO: stderr: ""
    May 17 06:55:40.468: INFO: stdout: "true"
    May 17 06:55:40.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-x7hn6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 06:55:40.549: INFO: stderr: ""
    May 17 06:55:40.549: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May 17 06:55:40.549: INFO: validating pod update-demo-nautilus-x7hn6
    May 17 06:55:40.562: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 06:55:40.562: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 06:55:40.562: INFO: update-demo-nautilus-x7hn6 is verified up and running
    STEP: scaling down the replication controller 05/17/23 06:55:40.562
    May 17 06:55:40.563: INFO: scanned /root for discovery docs: <nil>
    May 17 06:55:40.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    May 17 06:55:41.643: INFO: stderr: ""
    May 17 06:55:41.643: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 06:55:41.643
    May 17 06:55:41.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 06:55:41.722: INFO: stderr: ""
    May 17 06:55:41.722: INFO: stdout: "update-demo-nautilus-7mb2t "
    May 17 06:55:41.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:55:41.789: INFO: stderr: ""
    May 17 06:55:41.790: INFO: stdout: "true"
    May 17 06:55:41.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 06:55:41.857: INFO: stderr: ""
    May 17 06:55:41.857: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May 17 06:55:41.857: INFO: validating pod update-demo-nautilus-7mb2t
    May 17 06:55:41.864: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 06:55:41.864: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 06:55:41.864: INFO: update-demo-nautilus-7mb2t is verified up and running
    STEP: scaling up the replication controller 05/17/23 06:55:41.864
    May 17 06:55:41.865: INFO: scanned /root for discovery docs: <nil>
    May 17 06:55:41.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    May 17 06:55:42.953: INFO: stderr: ""
    May 17 06:55:42.953: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/17/23 06:55:42.953
    May 17 06:55:42.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 06:55:43.021: INFO: stderr: ""
    May 17 06:55:43.021: INFO: stdout: "update-demo-nautilus-7mb2t update-demo-nautilus-p2kr7 "
    May 17 06:55:43.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:55:43.085: INFO: stderr: ""
    May 17 06:55:43.085: INFO: stdout: "true"
    May 17 06:55:43.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 06:55:43.146: INFO: stderr: ""
    May 17 06:55:43.146: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May 17 06:55:43.146: INFO: validating pod update-demo-nautilus-7mb2t
    May 17 06:55:43.153: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 06:55:43.153: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 06:55:43.153: INFO: update-demo-nautilus-7mb2t is verified up and running
    May 17 06:55:43.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-p2kr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:55:43.213: INFO: stderr: ""
    May 17 06:55:43.213: INFO: stdout: ""
    May 17 06:55:43.213: INFO: update-demo-nautilus-p2kr7 is created but not running
    May 17 06:55:48.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 06:55:48.283: INFO: stderr: ""
    May 17 06:55:48.283: INFO: stdout: "update-demo-nautilus-7mb2t update-demo-nautilus-p2kr7 "
    May 17 06:55:48.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:55:48.342: INFO: stderr: ""
    May 17 06:55:48.342: INFO: stdout: "true"
    May 17 06:55:48.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 06:55:48.404: INFO: stderr: ""
    May 17 06:55:48.404: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May 17 06:55:48.404: INFO: validating pod update-demo-nautilus-7mb2t
    May 17 06:55:48.415: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 06:55:48.415: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 06:55:48.415: INFO: update-demo-nautilus-7mb2t is verified up and running
    May 17 06:55:48.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-p2kr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:55:48.482: INFO: stderr: ""
    May 17 06:55:48.482: INFO: stdout: "true"
    May 17 06:55:48.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-p2kr7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 06:55:48.555: INFO: stderr: ""
    May 17 06:55:48.555: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May 17 06:55:48.555: INFO: validating pod update-demo-nautilus-p2kr7
    May 17 06:55:53.564: INFO: update-demo-nautilus-p2kr7 is running right image but validator function failed: the server is currently unable to handle the request (get pods update-demo-nautilus-p2kr7)
    May 17 06:55:58.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 17 06:55:58.630: INFO: stderr: ""
    May 17 06:55:58.630: INFO: stdout: "update-demo-nautilus-7mb2t update-demo-nautilus-p2kr7 "
    May 17 06:55:58.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:55:58.692: INFO: stderr: ""
    May 17 06:55:58.692: INFO: stdout: "true"
    May 17 06:55:58.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-7mb2t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 06:55:58.756: INFO: stderr: ""
    May 17 06:55:58.756: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May 17 06:55:58.756: INFO: validating pod update-demo-nautilus-7mb2t
    May 17 06:55:58.766: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 06:55:58.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 06:55:58.766: INFO: update-demo-nautilus-7mb2t is verified up and running
    May 17 06:55:58.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-p2kr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 17 06:55:58.826: INFO: stderr: ""
    May 17 06:55:58.826: INFO: stdout: "true"
    May 17 06:55:58.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods update-demo-nautilus-p2kr7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 17 06:55:58.886: INFO: stderr: ""
    May 17 06:55:58.886: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May 17 06:55:58.886: INFO: validating pod update-demo-nautilus-p2kr7
    May 17 06:55:58.896: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 17 06:55:58.896: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 17 06:55:58.896: INFO: update-demo-nautilus-p2kr7 is verified up and running
    STEP: using delete to clean up resources 05/17/23 06:55:58.896
    May 17 06:55:58.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 delete --grace-period=0 --force -f -'
    May 17 06:55:58.965: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 17 06:55:58.965: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May 17 06:55:58.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get rc,svc -l name=update-demo --no-headers'
    May 17 06:55:59.035: INFO: stderr: "No resources found in kubectl-7834 namespace.\n"
    May 17 06:55:59.035: INFO: stdout: ""
    May 17 06:55:59.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-7834 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May 17 06:55:59.103: INFO: stderr: ""
    May 17 06:55:59.103: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:55:59.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7834" for this suite. 05/17/23 06:55:59.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:55:59.12
May 17 06:55:59.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename security-context-test 05/17/23 06:55:59.12
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:55:59.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:55:59.148
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
May 17 06:55:59.165: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b7477e27-e0ec-4b02-ab96-32ab8fe2a05d" in namespace "security-context-test-4530" to be "Succeeded or Failed"
May 17 06:55:59.171: INFO: Pod "alpine-nnp-false-b7477e27-e0ec-4b02-ab96-32ab8fe2a05d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.445308ms
May 17 06:56:01.176: INFO: Pod "alpine-nnp-false-b7477e27-e0ec-4b02-ab96-32ab8fe2a05d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010246009s
May 17 06:56:03.178: INFO: Pod "alpine-nnp-false-b7477e27-e0ec-4b02-ab96-32ab8fe2a05d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012437939s
May 17 06:56:05.176: INFO: Pod "alpine-nnp-false-b7477e27-e0ec-4b02-ab96-32ab8fe2a05d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010899208s
May 17 06:56:05.176: INFO: Pod "alpine-nnp-false-b7477e27-e0ec-4b02-ab96-32ab8fe2a05d" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May 17 06:56:05.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4530" for this suite. 05/17/23 06:56:05.196
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":294,"skipped":5508,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.085 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:55:59.12
    May 17 06:55:59.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename security-context-test 05/17/23 06:55:59.12
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:55:59.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:55:59.148
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    May 17 06:55:59.165: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b7477e27-e0ec-4b02-ab96-32ab8fe2a05d" in namespace "security-context-test-4530" to be "Succeeded or Failed"
    May 17 06:55:59.171: INFO: Pod "alpine-nnp-false-b7477e27-e0ec-4b02-ab96-32ab8fe2a05d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.445308ms
    May 17 06:56:01.176: INFO: Pod "alpine-nnp-false-b7477e27-e0ec-4b02-ab96-32ab8fe2a05d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010246009s
    May 17 06:56:03.178: INFO: Pod "alpine-nnp-false-b7477e27-e0ec-4b02-ab96-32ab8fe2a05d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012437939s
    May 17 06:56:05.176: INFO: Pod "alpine-nnp-false-b7477e27-e0ec-4b02-ab96-32ab8fe2a05d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010899208s
    May 17 06:56:05.176: INFO: Pod "alpine-nnp-false-b7477e27-e0ec-4b02-ab96-32ab8fe2a05d" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May 17 06:56:05.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4530" for this suite. 05/17/23 06:56:05.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:56:05.206
May 17 06:56:05.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 06:56:05.206
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:56:05.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:56:05.233
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-c3512b3e-8c35-4d4d-8fa1-060d1639bab1 05/17/23 06:56:05.236
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May 17 06:56:05.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2559" for this suite. 05/17/23 06:56:05.246
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":295,"skipped":5530,"failed":0}
------------------------------
â€¢ [0.049 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:56:05.206
    May 17 06:56:05.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 06:56:05.206
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:56:05.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:56:05.233
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-c3512b3e-8c35-4d4d-8fa1-060d1639bab1 05/17/23 06:56:05.236
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 06:56:05.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2559" for this suite. 05/17/23 06:56:05.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:56:05.255
May 17 06:56:05.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename job 05/17/23 06:56:05.256
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:56:05.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:56:05.28
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 05/17/23 06:56:05.284
STEP: Ensuring active pods == parallelism 05/17/23 06:56:05.291
STEP: delete a job 05/17/23 06:56:07.298
STEP: deleting Job.batch foo in namespace job-3478, will wait for the garbage collector to delete the pods 05/17/23 06:56:07.298
May 17 06:56:07.361: INFO: Deleting Job.batch foo took: 7.852856ms
May 17 06:56:07.461: INFO: Terminating Job.batch foo pods took: 100.550221ms
STEP: Ensuring job was deleted 05/17/23 06:56:39.462
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May 17 06:56:39.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3478" for this suite. 05/17/23 06:56:39.475
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":296,"skipped":5538,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.228 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:56:05.255
    May 17 06:56:05.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename job 05/17/23 06:56:05.256
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:56:05.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:56:05.28
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 05/17/23 06:56:05.284
    STEP: Ensuring active pods == parallelism 05/17/23 06:56:05.291
    STEP: delete a job 05/17/23 06:56:07.298
    STEP: deleting Job.batch foo in namespace job-3478, will wait for the garbage collector to delete the pods 05/17/23 06:56:07.298
    May 17 06:56:07.361: INFO: Deleting Job.batch foo took: 7.852856ms
    May 17 06:56:07.461: INFO: Terminating Job.batch foo pods took: 100.550221ms
    STEP: Ensuring job was deleted 05/17/23 06:56:39.462
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May 17 06:56:39.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3478" for this suite. 05/17/23 06:56:39.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:56:39.484
May 17 06:56:39.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:56:39.485
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:56:39.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:56:39.512
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-e819f6a4-2a00-453b-813f-10515cc57392 05/17/23 06:56:39.515
STEP: Creating a pod to test consume configMaps 05/17/23 06:56:39.52
May 17 06:56:39.550: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617" in namespace "projected-8004" to be "Succeeded or Failed"
May 17 06:56:39.555: INFO: Pod "pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617": Phase="Pending", Reason="", readiness=false. Elapsed: 5.140521ms
May 17 06:56:41.560: INFO: Pod "pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010134692s
May 17 06:56:43.562: INFO: Pod "pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012294461s
STEP: Saw pod success 05/17/23 06:56:43.562
May 17 06:56:43.562: INFO: Pod "pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617" satisfied condition "Succeeded or Failed"
May 17 06:56:43.567: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 06:56:43.577
May 17 06:56:43.594: INFO: Waiting for pod pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617 to disappear
May 17 06:56:43.598: INFO: Pod pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May 17 06:56:43.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8004" for this suite. 05/17/23 06:56:43.605
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":297,"skipped":5567,"failed":0}
------------------------------
â€¢ [4.131 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:56:39.484
    May 17 06:56:39.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:56:39.485
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:56:39.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:56:39.512
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-e819f6a4-2a00-453b-813f-10515cc57392 05/17/23 06:56:39.515
    STEP: Creating a pod to test consume configMaps 05/17/23 06:56:39.52
    May 17 06:56:39.550: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617" in namespace "projected-8004" to be "Succeeded or Failed"
    May 17 06:56:39.555: INFO: Pod "pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617": Phase="Pending", Reason="", readiness=false. Elapsed: 5.140521ms
    May 17 06:56:41.560: INFO: Pod "pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010134692s
    May 17 06:56:43.562: INFO: Pod "pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012294461s
    STEP: Saw pod success 05/17/23 06:56:43.562
    May 17 06:56:43.562: INFO: Pod "pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617" satisfied condition "Succeeded or Failed"
    May 17 06:56:43.567: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 06:56:43.577
    May 17 06:56:43.594: INFO: Waiting for pod pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617 to disappear
    May 17 06:56:43.598: INFO: Pod pod-projected-configmaps-09975d1a-e4ac-45e1-b198-43cc6156b617 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May 17 06:56:43.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8004" for this suite. 05/17/23 06:56:43.605
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:56:43.615
May 17 06:56:43.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename sched-preemption 05/17/23 06:56:43.616
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:56:43.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:56:43.638
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May 17 06:56:43.666: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 06:57:43.737: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 05/17/23 06:57:43.742
May 17 06:57:43.783: INFO: Created pod: pod0-0-sched-preemption-low-priority
May 17 06:57:43.794: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May 17 06:57:43.826: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May 17 06:57:43.840: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May 17 06:57:43.874: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May 17 06:57:43.882: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/17/23 06:57:43.882
May 17 06:57:43.882: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6740" to be "running"
May 17 06:57:43.886: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037749ms
May 17 06:57:45.892: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010052314s
May 17 06:57:45.892: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May 17 06:57:45.892: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6740" to be "running"
May 17 06:57:45.897: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.506684ms
May 17 06:57:45.897: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May 17 06:57:45.897: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6740" to be "running"
May 17 06:57:45.903: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.218183ms
May 17 06:57:45.903: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May 17 06:57:45.903: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6740" to be "running"
May 17 06:57:45.908: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.671139ms
May 17 06:57:45.908: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May 17 06:57:45.908: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-6740" to be "running"
May 17 06:57:45.912: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.354762ms
May 17 06:57:45.912: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May 17 06:57:45.912: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-6740" to be "running"
May 17 06:57:45.917: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.762033ms
May 17 06:57:45.917: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/17/23 06:57:45.917
May 17 06:57:45.926: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6740" to be "running"
May 17 06:57:45.931: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.59182ms
May 17 06:57:47.937: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010528494s
May 17 06:57:49.938: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011755535s
May 17 06:57:51.937: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.010663292s
May 17 06:57:51.937: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May 17 06:57:51.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6740" for this suite. 05/17/23 06:57:51.976
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":298,"skipped":5568,"failed":0}
------------------------------
â€¢ [SLOW TEST] [68.489 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:56:43.615
    May 17 06:56:43.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename sched-preemption 05/17/23 06:56:43.616
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:56:43.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:56:43.638
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May 17 06:56:43.666: INFO: Waiting up to 1m0s for all nodes to be ready
    May 17 06:57:43.737: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 05/17/23 06:57:43.742
    May 17 06:57:43.783: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May 17 06:57:43.794: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May 17 06:57:43.826: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May 17 06:57:43.840: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May 17 06:57:43.874: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May 17 06:57:43.882: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/17/23 06:57:43.882
    May 17 06:57:43.882: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6740" to be "running"
    May 17 06:57:43.886: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037749ms
    May 17 06:57:45.892: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010052314s
    May 17 06:57:45.892: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May 17 06:57:45.892: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6740" to be "running"
    May 17 06:57:45.897: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.506684ms
    May 17 06:57:45.897: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May 17 06:57:45.897: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6740" to be "running"
    May 17 06:57:45.903: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.218183ms
    May 17 06:57:45.903: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May 17 06:57:45.903: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6740" to be "running"
    May 17 06:57:45.908: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.671139ms
    May 17 06:57:45.908: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May 17 06:57:45.908: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-6740" to be "running"
    May 17 06:57:45.912: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.354762ms
    May 17 06:57:45.912: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May 17 06:57:45.912: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-6740" to be "running"
    May 17 06:57:45.917: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.762033ms
    May 17 06:57:45.917: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/17/23 06:57:45.917
    May 17 06:57:45.926: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6740" to be "running"
    May 17 06:57:45.931: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.59182ms
    May 17 06:57:47.937: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010528494s
    May 17 06:57:49.938: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011755535s
    May 17 06:57:51.937: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.010663292s
    May 17 06:57:51.937: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May 17 06:57:51.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6740" for this suite. 05/17/23 06:57:51.976
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:57:52.105
May 17 06:57:52.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename webhook 05/17/23 06:57:52.106
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:57:52.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:57:52.129
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/17/23 06:57:52.148
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:57:52.345
STEP: Deploying the webhook pod 05/17/23 06:57:52.356
STEP: Wait for the deployment to be ready 05/17/23 06:57:52.371
May 17 06:57:52.379: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/17/23 06:57:54.396
STEP: Verifying the service has paired with the endpoint 05/17/23 06:57:54.41
May 17 06:57:55.410: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
May 17 06:57:55.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4760-crds.webhook.example.com via the AdmissionRegistration API 05/17/23 06:57:55.929
STEP: Creating a custom resource that should be mutated by the webhook 05/17/23 06:57:55.952
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 06:57:58.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3216" for this suite. 05/17/23 06:57:58.691
STEP: Destroying namespace "webhook-3216-markers" for this suite. 05/17/23 06:57:58.701
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":299,"skipped":5587,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.670 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:57:52.105
    May 17 06:57:52.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename webhook 05/17/23 06:57:52.106
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:57:52.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:57:52.129
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/17/23 06:57:52.148
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/17/23 06:57:52.345
    STEP: Deploying the webhook pod 05/17/23 06:57:52.356
    STEP: Wait for the deployment to be ready 05/17/23 06:57:52.371
    May 17 06:57:52.379: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/17/23 06:57:54.396
    STEP: Verifying the service has paired with the endpoint 05/17/23 06:57:54.41
    May 17 06:57:55.410: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    May 17 06:57:55.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4760-crds.webhook.example.com via the AdmissionRegistration API 05/17/23 06:57:55.929
    STEP: Creating a custom resource that should be mutated by the webhook 05/17/23 06:57:55.952
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 06:57:58.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3216" for this suite. 05/17/23 06:57:58.691
    STEP: Destroying namespace "webhook-3216-markers" for this suite. 05/17/23 06:57:58.701
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:57:58.778
May 17 06:57:58.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename deployment 05/17/23 06:57:58.779
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:57:58.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:57:58.804
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
May 17 06:57:58.821: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 17 06:58:03.828: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/17/23 06:58:03.828
May 17 06:58:03.829: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 17 06:58:05.835: INFO: Creating deployment "test-rollover-deployment"
May 17 06:58:05.846: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 17 06:58:07.856: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 17 06:58:07.865: INFO: Ensure that both replica sets have 1 created replica
May 17 06:58:07.877: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 17 06:58:07.890: INFO: Updating deployment test-rollover-deployment
May 17 06:58:07.890: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 17 06:58:09.900: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 17 06:58:09.908: INFO: Make sure deployment "test-rollover-deployment" is complete
May 17 06:58:09.917: INFO: all replica sets need to contain the pod-template-hash label
May 17 06:58:09.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 06:58:11.937: INFO: all replica sets need to contain the pod-template-hash label
May 17 06:58:11.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 06:58:13.929: INFO: all replica sets need to contain the pod-template-hash label
May 17 06:58:13.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 06:58:15.933: INFO: all replica sets need to contain the pod-template-hash label
May 17 06:58:15.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 06:58:17.927: INFO: all replica sets need to contain the pod-template-hash label
May 17 06:58:17.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 06:58:19.932: INFO: 
May 17 06:58:19.932: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 06:58:19.946: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4659  e6f765a7-afcb-4b34-96c9-e7d8e3e32c22 1494267 2 2023-05-17 06:58:05 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-17 06:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:58:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003294f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 06:58:05 +0000 UTC,LastTransitionTime:2023-05-17 06:58:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-05-17 06:58:19 +0000 UTC,LastTransitionTime:2023-05-17 06:58:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 17 06:58:19.951: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-4659  5e936170-9dcf-4d48-b6f7-7752c5af8095 1494260 2 2023-05-17 06:58:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e6f765a7-afcb-4b34-96c9-e7d8e3e32c22 0xc003295567 0xc003295568}] [] [{kube-controller-manager Update apps/v1 2023-05-17 06:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6f765a7-afcb-4b34-96c9-e7d8e3e32c22\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:58:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003295618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 06:58:19.951: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 17 06:58:19.951: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4659  56da6346-9b17-4811-9cd8-9862a05592f2 1494266 2 2023-05-17 06:57:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e6f765a7-afcb-4b34-96c9-e7d8e3e32c22 0xc003295317 0xc003295318}] [] [{e2e.test Update apps/v1 2023-05-17 06:57:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:58:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6f765a7-afcb-4b34-96c9-e7d8e3e32c22\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:58:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0032953d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 06:58:19.952: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-4659  c3552a57-a609-47f3-a89a-5cf55a5f25c4 1494124 2 2023-05-17 06:58:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e6f765a7-afcb-4b34-96c9-e7d8e3e32c22 0xc003295447 0xc003295448}] [] [{kube-controller-manager Update apps/v1 2023-05-17 06:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6f765a7-afcb-4b34-96c9-e7d8e3e32c22\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:58:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032954f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 06:58:19.957: INFO: Pod "test-rollover-deployment-6d45fd857b-bmw8d" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-bmw8d test-rollover-deployment-6d45fd857b- deployment-4659  02b07b02-9958-4926-9f7a-5c448a1887a1 1494162 0 2023-05-17 06:58:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 5e936170-9dcf-4d48-b6f7-7752c5af8095 0xc002efaa97 0xc002efaa98}] [] [{kube-controller-manager Update v1 2023-05-17 06:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e936170-9dcf-4d48-b6f7-7752c5af8095\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:58:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ht9ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ht9ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:58:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:58:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:58:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:58:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.87,StartTime:2023-05-17 06:58:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:58:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://4e56b7052c6f1e44bb80647804a796a46f9a249b2de4f9a27465afac99f6e66d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May 17 06:58:19.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4659" for this suite. 05/17/23 06:58:19.966
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":300,"skipped":5650,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.199 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:57:58.778
    May 17 06:57:58.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename deployment 05/17/23 06:57:58.779
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:57:58.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:57:58.804
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    May 17 06:57:58.821: INFO: Pod name rollover-pod: Found 0 pods out of 1
    May 17 06:58:03.828: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/17/23 06:58:03.828
    May 17 06:58:03.829: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    May 17 06:58:05.835: INFO: Creating deployment "test-rollover-deployment"
    May 17 06:58:05.846: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    May 17 06:58:07.856: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    May 17 06:58:07.865: INFO: Ensure that both replica sets have 1 created replica
    May 17 06:58:07.877: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    May 17 06:58:07.890: INFO: Updating deployment test-rollover-deployment
    May 17 06:58:07.890: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    May 17 06:58:09.900: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    May 17 06:58:09.908: INFO: Make sure deployment "test-rollover-deployment" is complete
    May 17 06:58:09.917: INFO: all replica sets need to contain the pod-template-hash label
    May 17 06:58:09.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 06:58:11.937: INFO: all replica sets need to contain the pod-template-hash label
    May 17 06:58:11.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 06:58:13.929: INFO: all replica sets need to contain the pod-template-hash label
    May 17 06:58:13.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 06:58:15.933: INFO: all replica sets need to contain the pod-template-hash label
    May 17 06:58:15.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 06:58:17.927: INFO: all replica sets need to contain the pod-template-hash label
    May 17 06:58:17.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 6, 58, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 6, 58, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 06:58:19.932: INFO: 
    May 17 06:58:19.932: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 06:58:19.946: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4659  e6f765a7-afcb-4b34-96c9-e7d8e3e32c22 1494267 2 2023-05-17 06:58:05 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-17 06:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:58:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003294f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 06:58:05 +0000 UTC,LastTransitionTime:2023-05-17 06:58:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-05-17 06:58:19 +0000 UTC,LastTransitionTime:2023-05-17 06:58:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 17 06:58:19.951: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-4659  5e936170-9dcf-4d48-b6f7-7752c5af8095 1494260 2 2023-05-17 06:58:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e6f765a7-afcb-4b34-96c9-e7d8e3e32c22 0xc003295567 0xc003295568}] [] [{kube-controller-manager Update apps/v1 2023-05-17 06:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6f765a7-afcb-4b34-96c9-e7d8e3e32c22\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:58:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003295618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 17 06:58:19.951: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    May 17 06:58:19.951: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4659  56da6346-9b17-4811-9cd8-9862a05592f2 1494266 2 2023-05-17 06:57:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e6f765a7-afcb-4b34-96c9-e7d8e3e32c22 0xc003295317 0xc003295318}] [] [{e2e.test Update apps/v1 2023-05-17 06:57:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:58:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6f765a7-afcb-4b34-96c9-e7d8e3e32c22\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:58:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0032953d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 17 06:58:19.952: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-4659  c3552a57-a609-47f3-a89a-5cf55a5f25c4 1494124 2 2023-05-17 06:58:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e6f765a7-afcb-4b34-96c9-e7d8e3e32c22 0xc003295447 0xc003295448}] [] [{kube-controller-manager Update apps/v1 2023-05-17 06:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6f765a7-afcb-4b34-96c9-e7d8e3e32c22\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 06:58:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032954f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 17 06:58:19.957: INFO: Pod "test-rollover-deployment-6d45fd857b-bmw8d" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-bmw8d test-rollover-deployment-6d45fd857b- deployment-4659  02b07b02-9958-4926-9f7a-5c448a1887a1 1494162 0 2023-05-17 06:58:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 5e936170-9dcf-4d48-b6f7-7752c5af8095 0xc002efaa97 0xc002efaa98}] [] [{kube-controller-manager Update v1 2023-05-17 06:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e936170-9dcf-4d48-b6f7-7752c5af8095\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 06:58:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ht9ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ht9ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:58:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:58:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:58:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 06:58:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.87,StartTime:2023-05-17 06:58:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 06:58:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://4e56b7052c6f1e44bb80647804a796a46f9a249b2de4f9a27465afac99f6e66d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May 17 06:58:19.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4659" for this suite. 05/17/23 06:58:19.966
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:58:19.977
May 17 06:58:19.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 06:58:19.978
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:20
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:20.004
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 05/17/23 06:58:20.009
May 17 06:58:20.009: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-2981 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 05/17/23 06:58:20.038
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 06:58:20.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2981" for this suite. 05/17/23 06:58:20.081
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":301,"skipped":5650,"failed":0}
------------------------------
â€¢ [0.114 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:58:19.977
    May 17 06:58:19.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 06:58:19.978
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:20
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:20.004
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 05/17/23 06:58:20.009
    May 17 06:58:20.009: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-2981 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 05/17/23 06:58:20.038
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 06:58:20.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2981" for this suite. 05/17/23 06:58:20.081
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:58:20.092
May 17 06:58:20.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 06:58:20.093
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:20.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:20.117
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-4762 05/17/23 06:58:20.123
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4762 to expose endpoints map[] 05/17/23 06:58:20.137
May 17 06:58:20.141: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May 17 06:58:21.154: INFO: successfully validated that service endpoint-test2 in namespace services-4762 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4762 05/17/23 06:58:21.154
May 17 06:58:21.167: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4762" to be "running and ready"
May 17 06:58:21.172: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.627767ms
May 17 06:58:21.172: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 17 06:58:23.179: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011299208s
May 17 06:58:23.179: INFO: The phase of Pod pod1 is Running (Ready = true)
May 17 06:58:23.179: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4762 to expose endpoints map[pod1:[80]] 05/17/23 06:58:23.183
May 17 06:58:23.198: INFO: successfully validated that service endpoint-test2 in namespace services-4762 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 05/17/23 06:58:23.198
May 17 06:58:23.198: INFO: Creating new exec pod
May 17 06:58:23.209: INFO: Waiting up to 5m0s for pod "execpodkqrd7" in namespace "services-4762" to be "running"
May 17 06:58:23.214: INFO: Pod "execpodkqrd7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650742ms
May 17 06:58:25.219: INFO: Pod "execpodkqrd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009552401s
May 17 06:58:25.219: INFO: Pod "execpodkqrd7" satisfied condition "running"
May 17 06:58:26.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4762 exec execpodkqrd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May 17 06:58:26.394: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 17 06:58:26.394: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:58:26.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4762 exec execpodkqrd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.73.62 80'
May 17 06:58:26.596: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.73.62 80\nConnection to 10.0.73.62 80 port [tcp/http] succeeded!\n"
May 17 06:58:26.596: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-4762 05/17/23 06:58:26.596
May 17 06:58:26.605: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4762" to be "running and ready"
May 17 06:58:26.611: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.626485ms
May 17 06:58:26.611: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 17 06:58:28.617: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012482867s
May 17 06:58:28.617: INFO: The phase of Pod pod2 is Running (Ready = true)
May 17 06:58:28.617: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4762 to expose endpoints map[pod1:[80] pod2:[80]] 05/17/23 06:58:28.621
May 17 06:58:28.644: INFO: successfully validated that service endpoint-test2 in namespace services-4762 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 05/17/23 06:58:28.644
May 17 06:58:29.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4762 exec execpodkqrd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May 17 06:58:29.828: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 17 06:58:29.828: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:58:29.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4762 exec execpodkqrd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.73.62 80'
May 17 06:58:30.004: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.73.62 80\nConnection to 10.0.73.62 80 port [tcp/http] succeeded!\n"
May 17 06:58:30.004: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4762 05/17/23 06:58:30.004
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4762 to expose endpoints map[pod2:[80]] 05/17/23 06:58:30.021
May 17 06:58:30.035: INFO: successfully validated that service endpoint-test2 in namespace services-4762 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 05/17/23 06:58:30.035
May 17 06:58:31.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4762 exec execpodkqrd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May 17 06:58:31.231: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 17 06:58:31.231: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 06:58:31.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4762 exec execpodkqrd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.73.62 80'
May 17 06:58:31.426: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.73.62 80\nConnection to 10.0.73.62 80 port [tcp/http] succeeded!\n"
May 17 06:58:31.426: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-4762 05/17/23 06:58:31.426
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4762 to expose endpoints map[] 05/17/23 06:58:31.44
May 17 06:58:31.451: INFO: successfully validated that service endpoint-test2 in namespace services-4762 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 06:58:31.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4762" for this suite. 05/17/23 06:58:31.481
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":302,"skipped":5651,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.397 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:58:20.092
    May 17 06:58:20.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 06:58:20.093
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:20.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:20.117
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-4762 05/17/23 06:58:20.123
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4762 to expose endpoints map[] 05/17/23 06:58:20.137
    May 17 06:58:20.141: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    May 17 06:58:21.154: INFO: successfully validated that service endpoint-test2 in namespace services-4762 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4762 05/17/23 06:58:21.154
    May 17 06:58:21.167: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4762" to be "running and ready"
    May 17 06:58:21.172: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.627767ms
    May 17 06:58:21.172: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:58:23.179: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011299208s
    May 17 06:58:23.179: INFO: The phase of Pod pod1 is Running (Ready = true)
    May 17 06:58:23.179: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4762 to expose endpoints map[pod1:[80]] 05/17/23 06:58:23.183
    May 17 06:58:23.198: INFO: successfully validated that service endpoint-test2 in namespace services-4762 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 05/17/23 06:58:23.198
    May 17 06:58:23.198: INFO: Creating new exec pod
    May 17 06:58:23.209: INFO: Waiting up to 5m0s for pod "execpodkqrd7" in namespace "services-4762" to be "running"
    May 17 06:58:23.214: INFO: Pod "execpodkqrd7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650742ms
    May 17 06:58:25.219: INFO: Pod "execpodkqrd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009552401s
    May 17 06:58:25.219: INFO: Pod "execpodkqrd7" satisfied condition "running"
    May 17 06:58:26.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4762 exec execpodkqrd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    May 17 06:58:26.394: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May 17 06:58:26.394: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:58:26.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4762 exec execpodkqrd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.73.62 80'
    May 17 06:58:26.596: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.73.62 80\nConnection to 10.0.73.62 80 port [tcp/http] succeeded!\n"
    May 17 06:58:26.596: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-4762 05/17/23 06:58:26.596
    May 17 06:58:26.605: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4762" to be "running and ready"
    May 17 06:58:26.611: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.626485ms
    May 17 06:58:26.611: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May 17 06:58:28.617: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012482867s
    May 17 06:58:28.617: INFO: The phase of Pod pod2 is Running (Ready = true)
    May 17 06:58:28.617: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4762 to expose endpoints map[pod1:[80] pod2:[80]] 05/17/23 06:58:28.621
    May 17 06:58:28.644: INFO: successfully validated that service endpoint-test2 in namespace services-4762 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 05/17/23 06:58:28.644
    May 17 06:58:29.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4762 exec execpodkqrd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    May 17 06:58:29.828: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May 17 06:58:29.828: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:58:29.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4762 exec execpodkqrd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.73.62 80'
    May 17 06:58:30.004: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.73.62 80\nConnection to 10.0.73.62 80 port [tcp/http] succeeded!\n"
    May 17 06:58:30.004: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-4762 05/17/23 06:58:30.004
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4762 to expose endpoints map[pod2:[80]] 05/17/23 06:58:30.021
    May 17 06:58:30.035: INFO: successfully validated that service endpoint-test2 in namespace services-4762 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 05/17/23 06:58:30.035
    May 17 06:58:31.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4762 exec execpodkqrd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    May 17 06:58:31.231: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May 17 06:58:31.231: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 06:58:31.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-4762 exec execpodkqrd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.73.62 80'
    May 17 06:58:31.426: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.73.62 80\nConnection to 10.0.73.62 80 port [tcp/http] succeeded!\n"
    May 17 06:58:31.426: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-4762 05/17/23 06:58:31.426
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4762 to expose endpoints map[] 05/17/23 06:58:31.44
    May 17 06:58:31.451: INFO: successfully validated that service endpoint-test2 in namespace services-4762 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 06:58:31.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4762" for this suite. 05/17/23 06:58:31.481
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:58:31.49
May 17 06:58:31.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 06:58:31.491
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:31.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:31.515
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 05/17/23 06:58:31.518
May 17 06:58:31.531: INFO: Waiting up to 5m0s for pod "pod-d04eb983-90d9-4874-b5cb-444989e408b7" in namespace "emptydir-2508" to be "Succeeded or Failed"
May 17 06:58:31.536: INFO: Pod "pod-d04eb983-90d9-4874-b5cb-444989e408b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.609475ms
May 17 06:58:33.541: INFO: Pod "pod-d04eb983-90d9-4874-b5cb-444989e408b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009923132s
May 17 06:58:35.552: INFO: Pod "pod-d04eb983-90d9-4874-b5cb-444989e408b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02087368s
STEP: Saw pod success 05/17/23 06:58:35.552
May 17 06:58:35.552: INFO: Pod "pod-d04eb983-90d9-4874-b5cb-444989e408b7" satisfied condition "Succeeded or Failed"
May 17 06:58:35.556: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-d04eb983-90d9-4874-b5cb-444989e408b7 container test-container: <nil>
STEP: delete the pod 05/17/23 06:58:35.567
May 17 06:58:35.582: INFO: Waiting for pod pod-d04eb983-90d9-4874-b5cb-444989e408b7 to disappear
May 17 06:58:35.586: INFO: Pod pod-d04eb983-90d9-4874-b5cb-444989e408b7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 06:58:35.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2508" for this suite. 05/17/23 06:58:35.594
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":303,"skipped":5655,"failed":0}
------------------------------
â€¢ [4.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:58:31.49
    May 17 06:58:31.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 06:58:31.491
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:31.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:31.515
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/17/23 06:58:31.518
    May 17 06:58:31.531: INFO: Waiting up to 5m0s for pod "pod-d04eb983-90d9-4874-b5cb-444989e408b7" in namespace "emptydir-2508" to be "Succeeded or Failed"
    May 17 06:58:31.536: INFO: Pod "pod-d04eb983-90d9-4874-b5cb-444989e408b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.609475ms
    May 17 06:58:33.541: INFO: Pod "pod-d04eb983-90d9-4874-b5cb-444989e408b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009923132s
    May 17 06:58:35.552: INFO: Pod "pod-d04eb983-90d9-4874-b5cb-444989e408b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02087368s
    STEP: Saw pod success 05/17/23 06:58:35.552
    May 17 06:58:35.552: INFO: Pod "pod-d04eb983-90d9-4874-b5cb-444989e408b7" satisfied condition "Succeeded or Failed"
    May 17 06:58:35.556: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-d04eb983-90d9-4874-b5cb-444989e408b7 container test-container: <nil>
    STEP: delete the pod 05/17/23 06:58:35.567
    May 17 06:58:35.582: INFO: Waiting for pod pod-d04eb983-90d9-4874-b5cb-444989e408b7 to disappear
    May 17 06:58:35.586: INFO: Pod pod-d04eb983-90d9-4874-b5cb-444989e408b7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 06:58:35.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2508" for this suite. 05/17/23 06:58:35.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:58:35.603
May 17 06:58:35.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 06:58:35.603
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:35.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:35.628
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-70059c73-8bf5-4f16-8cfe-4866c777a1d3 05/17/23 06:58:35.631
STEP: Creating a pod to test consume secrets 05/17/23 06:58:35.637
May 17 06:58:35.649: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7" in namespace "projected-4142" to be "Succeeded or Failed"
May 17 06:58:35.655: INFO: Pod "pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.566203ms
May 17 06:58:37.662: INFO: Pod "pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012618719s
May 17 06:58:39.660: INFO: Pod "pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010810786s
STEP: Saw pod success 05/17/23 06:58:39.66
May 17 06:58:39.660: INFO: Pod "pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7" satisfied condition "Succeeded or Failed"
May 17 06:58:39.665: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/17/23 06:58:39.675
May 17 06:58:39.691: INFO: Waiting for pod pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7 to disappear
May 17 06:58:39.697: INFO: Pod pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May 17 06:58:39.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4142" for this suite. 05/17/23 06:58:39.704
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":304,"skipped":5676,"failed":0}
------------------------------
â€¢ [4.113 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:58:35.603
    May 17 06:58:35.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 06:58:35.603
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:35.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:35.628
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-70059c73-8bf5-4f16-8cfe-4866c777a1d3 05/17/23 06:58:35.631
    STEP: Creating a pod to test consume secrets 05/17/23 06:58:35.637
    May 17 06:58:35.649: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7" in namespace "projected-4142" to be "Succeeded or Failed"
    May 17 06:58:35.655: INFO: Pod "pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.566203ms
    May 17 06:58:37.662: INFO: Pod "pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012618719s
    May 17 06:58:39.660: INFO: Pod "pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010810786s
    STEP: Saw pod success 05/17/23 06:58:39.66
    May 17 06:58:39.660: INFO: Pod "pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7" satisfied condition "Succeeded or Failed"
    May 17 06:58:39.665: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/17/23 06:58:39.675
    May 17 06:58:39.691: INFO: Waiting for pod pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7 to disappear
    May 17 06:58:39.697: INFO: Pod pod-projected-secrets-8b51a0c9-4343-42ac-bc81-c0e2aaeee1b7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May 17 06:58:39.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4142" for this suite. 05/17/23 06:58:39.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:58:39.716
May 17 06:58:39.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-runtime 05/17/23 06:58:39.716
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:39.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:39.741
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 05/17/23 06:58:39.744
STEP: wait for the container to reach Failed 05/17/23 06:58:39.759
STEP: get the container status 05/17/23 06:58:42.782
STEP: the container should be terminated 05/17/23 06:58:42.787
STEP: the termination message should be set 05/17/23 06:58:42.787
May 17 06:58:42.787: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/17/23 06:58:42.787
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May 17 06:58:42.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8993" for this suite. 05/17/23 06:58:42.814
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":305,"skipped":5681,"failed":0}
------------------------------
â€¢ [3.106 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:58:39.716
    May 17 06:58:39.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-runtime 05/17/23 06:58:39.716
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:39.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:39.741
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 05/17/23 06:58:39.744
    STEP: wait for the container to reach Failed 05/17/23 06:58:39.759
    STEP: get the container status 05/17/23 06:58:42.782
    STEP: the container should be terminated 05/17/23 06:58:42.787
    STEP: the termination message should be set 05/17/23 06:58:42.787
    May 17 06:58:42.787: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/17/23 06:58:42.787
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May 17 06:58:42.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8993" for this suite. 05/17/23 06:58:42.814
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:58:42.822
May 17 06:58:42.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 06:58:42.823
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:42.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:42.847
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/17/23 06:58:42.85
May 17 06:58:42.862: INFO: Waiting up to 5m0s for pod "pod-4489f268-51b1-40b8-ad40-4aa28ca9837a" in namespace "emptydir-3817" to be "Succeeded or Failed"
May 17 06:58:42.866: INFO: Pod "pod-4489f268-51b1-40b8-ad40-4aa28ca9837a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293764ms
May 17 06:58:44.872: INFO: Pod "pod-4489f268-51b1-40b8-ad40-4aa28ca9837a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009873056s
May 17 06:58:46.873: INFO: Pod "pod-4489f268-51b1-40b8-ad40-4aa28ca9837a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011274551s
STEP: Saw pod success 05/17/23 06:58:46.873
May 17 06:58:46.873: INFO: Pod "pod-4489f268-51b1-40b8-ad40-4aa28ca9837a" satisfied condition "Succeeded or Failed"
May 17 06:58:46.879: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-4489f268-51b1-40b8-ad40-4aa28ca9837a container test-container: <nil>
STEP: delete the pod 05/17/23 06:58:46.889
May 17 06:58:46.903: INFO: Waiting for pod pod-4489f268-51b1-40b8-ad40-4aa28ca9837a to disappear
May 17 06:58:46.915: INFO: Pod pod-4489f268-51b1-40b8-ad40-4aa28ca9837a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 06:58:46.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3817" for this suite. 05/17/23 06:58:46.93
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":306,"skipped":5684,"failed":0}
------------------------------
â€¢ [4.117 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:58:42.822
    May 17 06:58:42.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 06:58:42.823
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:42.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:42.847
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/17/23 06:58:42.85
    May 17 06:58:42.862: INFO: Waiting up to 5m0s for pod "pod-4489f268-51b1-40b8-ad40-4aa28ca9837a" in namespace "emptydir-3817" to be "Succeeded or Failed"
    May 17 06:58:42.866: INFO: Pod "pod-4489f268-51b1-40b8-ad40-4aa28ca9837a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293764ms
    May 17 06:58:44.872: INFO: Pod "pod-4489f268-51b1-40b8-ad40-4aa28ca9837a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009873056s
    May 17 06:58:46.873: INFO: Pod "pod-4489f268-51b1-40b8-ad40-4aa28ca9837a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011274551s
    STEP: Saw pod success 05/17/23 06:58:46.873
    May 17 06:58:46.873: INFO: Pod "pod-4489f268-51b1-40b8-ad40-4aa28ca9837a" satisfied condition "Succeeded or Failed"
    May 17 06:58:46.879: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-4489f268-51b1-40b8-ad40-4aa28ca9837a container test-container: <nil>
    STEP: delete the pod 05/17/23 06:58:46.889
    May 17 06:58:46.903: INFO: Waiting for pod pod-4489f268-51b1-40b8-ad40-4aa28ca9837a to disappear
    May 17 06:58:46.915: INFO: Pod pod-4489f268-51b1-40b8-ad40-4aa28ca9837a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 06:58:46.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3817" for this suite. 05/17/23 06:58:46.93
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:58:46.939
May 17 06:58:46.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename endpointslice 05/17/23 06:58:46.94
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:46.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:46.964
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 05/17/23 06:58:52.042
STEP: referencing matching pods with named port 05/17/23 06:58:57.051
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/17/23 06:59:02.077
STEP: recreating EndpointSlices after they've been deleted 05/17/23 06:59:07.087
May 17 06:59:07.118: INFO: EndpointSlice for Service endpointslice-9613/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May 17 06:59:17.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9613" for this suite. 05/17/23 06:59:17.144
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":307,"skipped":5686,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.213 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:58:46.939
    May 17 06:58:46.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename endpointslice 05/17/23 06:58:46.94
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:58:46.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:58:46.964
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 05/17/23 06:58:52.042
    STEP: referencing matching pods with named port 05/17/23 06:58:57.051
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/17/23 06:59:02.077
    STEP: recreating EndpointSlices after they've been deleted 05/17/23 06:59:07.087
    May 17 06:59:07.118: INFO: EndpointSlice for Service endpointslice-9613/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May 17 06:59:17.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9613" for this suite. 05/17/23 06:59:17.144
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 06:59:17.152
May 17 06:59:17.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 06:59:17.153
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:59:17.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:59:17.179
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 05/17/23 06:59:17.183
May 17 06:59:17.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: rename a version 05/17/23 06:59:33.782
STEP: check the new version name is served 05/17/23 06:59:33.813
STEP: check the old version name is removed 05/17/23 06:59:43.92
STEP: check the other version is not changed 05/17/23 06:59:46.314
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 07:00:00.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-564" for this suite. 05/17/23 07:00:00.786
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":308,"skipped":5688,"failed":0}
------------------------------
â€¢ [SLOW TEST] [43.643 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 06:59:17.152
    May 17 06:59:17.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 06:59:17.153
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 06:59:17.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 06:59:17.179
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 05/17/23 06:59:17.183
    May 17 06:59:17.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: rename a version 05/17/23 06:59:33.782
    STEP: check the new version name is served 05/17/23 06:59:33.813
    STEP: check the old version name is removed 05/17/23 06:59:43.92
    STEP: check the other version is not changed 05/17/23 06:59:46.314
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 07:00:00.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-564" for this suite. 05/17/23 07:00:00.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:00:00.797
May 17 07:00:00.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename deployment 05/17/23 07:00:00.798
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:00:00.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:00:00.824
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 05/17/23 07:00:00.836
STEP: waiting for Deployment to be created 05/17/23 07:00:00.843
STEP: waiting for all Replicas to be Ready 05/17/23 07:00:00.845
May 17 07:00:00.847: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 07:00:00.847: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 07:00:00.860: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 07:00:00.860: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 07:00:00.878: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 07:00:00.878: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 07:00:00.895: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 07:00:00.895: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 07:00:01.733: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 17 07:00:01.733: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 17 07:00:02.049: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 05/17/23 07:00:02.049
W0517 07:00:02.059586      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May 17 07:00:02.061: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 05/17/23 07:00:02.061
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:02.074: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:02.074: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:02.104: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:02.104: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:02.125: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
May 17 07:00:02.125: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
May 17 07:00:02.139: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
May 17 07:00:02.139: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
May 17 07:00:03.056: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:03.056: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:03.102: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
STEP: listing Deployments 05/17/23 07:00:03.102
May 17 07:00:03.109: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 05/17/23 07:00:03.109
May 17 07:00:03.122: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 05/17/23 07:00:03.122
May 17 07:00:03.129: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 07:00:03.134: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 07:00:03.149: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 07:00:03.165: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 07:00:03.175: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 07:00:03.745: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 17 07:00:04.070: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
May 17 07:00:04.097: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 17 07:00:04.125: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 17 07:00:04.749: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 05/17/23 07:00:04.78
STEP: fetching the DeploymentStatus 05/17/23 07:00:04.788
May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 3
May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 3
STEP: deleting the Deployment 05/17/23 07:00:04.795
May 17 07:00:04.808: INFO: observed event type MODIFIED
May 17 07:00:04.808: INFO: observed event type MODIFIED
May 17 07:00:04.808: INFO: observed event type MODIFIED
May 17 07:00:04.808: INFO: observed event type MODIFIED
May 17 07:00:04.808: INFO: observed event type MODIFIED
May 17 07:00:04.809: INFO: observed event type MODIFIED
May 17 07:00:04.809: INFO: observed event type MODIFIED
May 17 07:00:04.809: INFO: observed event type MODIFIED
May 17 07:00:04.809: INFO: observed event type MODIFIED
May 17 07:00:04.809: INFO: observed event type MODIFIED
May 17 07:00:04.809: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 07:00:04.813: INFO: Log out all the ReplicaSets if there is no deployment created
May 17 07:00:04.817: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-2702  0513986e-0b9f-4837-9233-d049d5c5cd13 1495660 4 2023-05-17 07:00:02 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d6ae6598-518b-4a81-9650-86c4e3f1cc6e 0xc0055a4807 0xc0055a4808}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6ae6598-518b-4a81-9650-86c4e3f1cc6e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055a4900 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 17 07:00:04.826: INFO: pod: "test-deployment-54cc775c4b-jllzt":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-jllzt test-deployment-54cc775c4b- deployment-2702  01b14e15-a181-403f-9dd0-d7775e85c8c6 1495631 0 2023-05-17 07:00:02 +0000 UTC 2023-05-17 07:00:05 +0000 UTC 0xc0055a5268 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 0513986e-0b9f-4837-9233-d049d5c5cd13 0xc0055a52d7 0xc0055a52d8}] [] [{kube-controller-manager Update v1 2023-05-17 07:00:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0513986e-0b9f-4837-9233-d049d5c5cd13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:00:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mlwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mlwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.128,StartTime:2023-05-17 07:00:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:00:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://139b075a213d76abd02cbabed66e757da103722154f35fe3870b36d00684b2a0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.128,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 17 07:00:04.827: INFO: pod: "test-deployment-54cc775c4b-tqhg5":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-tqhg5 test-deployment-54cc775c4b- deployment-2702  db225e10-94d4-47c6-a362-77712df6d8d3 1495656 0 2023-05-17 07:00:03 +0000 UTC 2023-05-17 07:00:05 +0000 UTC 0xc0055a5678 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 0513986e-0b9f-4837-9233-d049d5c5cd13 0xc0055a5757 0xc0055a5758}] [] [{kube-controller-manager Update v1 2023-05-17 07:00:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0513986e-0b9f-4837-9233-d049d5c5cd13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:00:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qp8bx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qp8bx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.98,StartTime:2023-05-17 07:00:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:00:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://39912be168acd66f95d266a2d0ceb8767f6e61aa0ee9d98754f51e96297cee80,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 17 07:00:04.827: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2702  b0d5e77b-9b0f-4b1e-9e4e-03fb5981d24b 1495652 2 2023-05-17 07:00:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment d6ae6598-518b-4a81-9650-86c4e3f1cc6e 0xc0055a4977 0xc0055a4978}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6ae6598-518b-4a81-9650-86c4e3f1cc6e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055a4ab0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

May 17 07:00:04.832: INFO: pod: "test-deployment-7c7d8d58c8-8757c":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-8757c test-deployment-7c7d8d58c8- deployment-2702  7123c7f6-f32c-4d5b-a304-3c4885a9274a 1495651 0 2023-05-17 07:00:04 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b0d5e77b-9b0f-4b1e-9e4e-03fb5981d24b 0xc00561a067 0xc00561a068}] [] [{kube-controller-manager Update v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0d5e77b-9b0f-4b1e-9e4e-03fb5981d24b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5ltj4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5ltj4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.99,StartTime:2023-05-17 07:00:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:00:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://fa66109cd6de8e7d0f9d1c8b5556695d3da9163f31727c18bbe19195f4bd6854,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 17 07:00:04.832: INFO: pod: "test-deployment-7c7d8d58c8-q9pb2":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-q9pb2 test-deployment-7c7d8d58c8- deployment-2702  5f5e2806-e24b-4637-9c62-c9a6dbdda310 1495627 0 2023-05-17 07:00:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b0d5e77b-9b0f-4b1e-9e4e-03fb5981d24b 0xc00561a477 0xc00561a478}] [] [{kube-controller-manager Update v1 2023-05-17 07:00:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0d5e77b-9b0f-4b1e-9e4e-03fb5981d24b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mffgp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mffgp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.129,StartTime:2023-05-17 07:00:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:00:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7db59cc608a7dcb428c49125b27e5b47c29e21866c9db7e616649267bbdbb124,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 17 07:00:04.832: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-2702  eb31bf58-f4d8-4381-aba7-4321ada9ea9c 1495598 3 2023-05-17 07:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d6ae6598-518b-4a81-9650-86c4e3f1cc6e 0xc0055a4b37 0xc0055a4b38}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:00:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6ae6598-518b-4a81-9650-86c4e3f1cc6e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:00:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055a4bd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 17 07:00:04.838: INFO: pod: "test-deployment-8594bb6fdd-kcxbc":
&Pod{ObjectMeta:{test-deployment-8594bb6fdd-kcxbc test-deployment-8594bb6fdd- deployment-2702  3cc38420-5386-41ca-840a-e108f978f795 1495563 0 2023-05-17 07:00:00 +0000 UTC 2023-05-17 07:00:03 +0000 UTC 0xc00563c5b8 map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-8594bb6fdd eb31bf58-f4d8-4381-aba7-4321ada9ea9c 0xc00563c607 0xc00563c608}] [] [{kube-controller-manager Update v1 2023-05-17 07:00:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb31bf58-f4d8-4381-aba7-4321ada9ea9c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:00:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dm25d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dm25d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.127,StartTime:2023-05-17 07:00:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:00:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://d661498c242b83a774f4f000ac2df35bb200d60cf48dcbe4d918fe7855a5c74e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May 17 07:00:04.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2702" for this suite. 05/17/23 07:00:04.848
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":309,"skipped":5762,"failed":0}
------------------------------
â€¢ [4.060 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:00:00.797
    May 17 07:00:00.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename deployment 05/17/23 07:00:00.798
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:00:00.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:00:00.824
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 05/17/23 07:00:00.836
    STEP: waiting for Deployment to be created 05/17/23 07:00:00.843
    STEP: waiting for all Replicas to be Ready 05/17/23 07:00:00.845
    May 17 07:00:00.847: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 07:00:00.847: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 07:00:00.860: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 07:00:00.860: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 07:00:00.878: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 07:00:00.878: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 07:00:00.895: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 07:00:00.895: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 17 07:00:01.733: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May 17 07:00:01.733: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May 17 07:00:02.049: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 05/17/23 07:00:02.049
    W0517 07:00:02.059586      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May 17 07:00:02.061: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 05/17/23 07:00:02.061
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 0
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:02.063: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:02.074: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:02.074: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:02.104: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:02.104: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:02.125: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    May 17 07:00:02.125: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    May 17 07:00:02.139: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    May 17 07:00:02.139: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    May 17 07:00:03.056: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:03.056: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:03.102: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    STEP: listing Deployments 05/17/23 07:00:03.102
    May 17 07:00:03.109: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 05/17/23 07:00:03.109
    May 17 07:00:03.122: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 05/17/23 07:00:03.122
    May 17 07:00:03.129: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 07:00:03.134: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 07:00:03.149: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 07:00:03.165: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 07:00:03.175: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 07:00:03.745: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 07:00:04.070: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 07:00:04.097: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 07:00:04.125: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May 17 07:00:04.749: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 05/17/23 07:00:04.78
    STEP: fetching the DeploymentStatus 05/17/23 07:00:04.788
    May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 1
    May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 3
    May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 2
    May 17 07:00:04.795: INFO: observed Deployment test-deployment in namespace deployment-2702 with ReadyReplicas 3
    STEP: deleting the Deployment 05/17/23 07:00:04.795
    May 17 07:00:04.808: INFO: observed event type MODIFIED
    May 17 07:00:04.808: INFO: observed event type MODIFIED
    May 17 07:00:04.808: INFO: observed event type MODIFIED
    May 17 07:00:04.808: INFO: observed event type MODIFIED
    May 17 07:00:04.808: INFO: observed event type MODIFIED
    May 17 07:00:04.809: INFO: observed event type MODIFIED
    May 17 07:00:04.809: INFO: observed event type MODIFIED
    May 17 07:00:04.809: INFO: observed event type MODIFIED
    May 17 07:00:04.809: INFO: observed event type MODIFIED
    May 17 07:00:04.809: INFO: observed event type MODIFIED
    May 17 07:00:04.809: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 07:00:04.813: INFO: Log out all the ReplicaSets if there is no deployment created
    May 17 07:00:04.817: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-2702  0513986e-0b9f-4837-9233-d049d5c5cd13 1495660 4 2023-05-17 07:00:02 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d6ae6598-518b-4a81-9650-86c4e3f1cc6e 0xc0055a4807 0xc0055a4808}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6ae6598-518b-4a81-9650-86c4e3f1cc6e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055a4900 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    May 17 07:00:04.826: INFO: pod: "test-deployment-54cc775c4b-jllzt":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-jllzt test-deployment-54cc775c4b- deployment-2702  01b14e15-a181-403f-9dd0-d7775e85c8c6 1495631 0 2023-05-17 07:00:02 +0000 UTC 2023-05-17 07:00:05 +0000 UTC 0xc0055a5268 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 0513986e-0b9f-4837-9233-d049d5c5cd13 0xc0055a52d7 0xc0055a52d8}] [] [{kube-controller-manager Update v1 2023-05-17 07:00:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0513986e-0b9f-4837-9233-d049d5c5cd13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:00:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mlwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mlwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.128,StartTime:2023-05-17 07:00:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:00:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://139b075a213d76abd02cbabed66e757da103722154f35fe3870b36d00684b2a0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.128,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May 17 07:00:04.827: INFO: pod: "test-deployment-54cc775c4b-tqhg5":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-tqhg5 test-deployment-54cc775c4b- deployment-2702  db225e10-94d4-47c6-a362-77712df6d8d3 1495656 0 2023-05-17 07:00:03 +0000 UTC 2023-05-17 07:00:05 +0000 UTC 0xc0055a5678 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 0513986e-0b9f-4837-9233-d049d5c5cd13 0xc0055a5757 0xc0055a5758}] [] [{kube-controller-manager Update v1 2023-05-17 07:00:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0513986e-0b9f-4837-9233-d049d5c5cd13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:00:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qp8bx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qp8bx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.98,StartTime:2023-05-17 07:00:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:00:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://39912be168acd66f95d266a2d0ceb8767f6e61aa0ee9d98754f51e96297cee80,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May 17 07:00:04.827: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2702  b0d5e77b-9b0f-4b1e-9e4e-03fb5981d24b 1495652 2 2023-05-17 07:00:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment d6ae6598-518b-4a81-9650-86c4e3f1cc6e 0xc0055a4977 0xc0055a4978}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6ae6598-518b-4a81-9650-86c4e3f1cc6e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055a4ab0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    May 17 07:00:04.832: INFO: pod: "test-deployment-7c7d8d58c8-8757c":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-8757c test-deployment-7c7d8d58c8- deployment-2702  7123c7f6-f32c-4d5b-a304-3c4885a9274a 1495651 0 2023-05-17 07:00:04 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b0d5e77b-9b0f-4b1e-9e4e-03fb5981d24b 0xc00561a067 0xc00561a068}] [] [{kube-controller-manager Update v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0d5e77b-9b0f-4b1e-9e4e-03fb5981d24b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5ltj4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5ltj4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.99,StartTime:2023-05-17 07:00:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:00:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://fa66109cd6de8e7d0f9d1c8b5556695d3da9163f31727c18bbe19195f4bd6854,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May 17 07:00:04.832: INFO: pod: "test-deployment-7c7d8d58c8-q9pb2":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-q9pb2 test-deployment-7c7d8d58c8- deployment-2702  5f5e2806-e24b-4637-9c62-c9a6dbdda310 1495627 0 2023-05-17 07:00:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b0d5e77b-9b0f-4b1e-9e4e-03fb5981d24b 0xc00561a477 0xc00561a478}] [] [{kube-controller-manager Update v1 2023-05-17 07:00:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0d5e77b-9b0f-4b1e-9e4e-03fb5981d24b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:00:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mffgp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mffgp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.129,StartTime:2023-05-17 07:00:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:00:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7db59cc608a7dcb428c49125b27e5b47c29e21866c9db7e616649267bbdbb124,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May 17 07:00:04.832: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-2702  eb31bf58-f4d8-4381-aba7-4321ada9ea9c 1495598 3 2023-05-17 07:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d6ae6598-518b-4a81-9650-86c4e3f1cc6e 0xc0055a4b37 0xc0055a4b38}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:00:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6ae6598-518b-4a81-9650-86c4e3f1cc6e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:00:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055a4bd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    May 17 07:00:04.838: INFO: pod: "test-deployment-8594bb6fdd-kcxbc":
    &Pod{ObjectMeta:{test-deployment-8594bb6fdd-kcxbc test-deployment-8594bb6fdd- deployment-2702  3cc38420-5386-41ca-840a-e108f978f795 1495563 0 2023-05-17 07:00:00 +0000 UTC 2023-05-17 07:00:03 +0000 UTC 0xc00563c5b8 map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-8594bb6fdd eb31bf58-f4d8-4381-aba7-4321ada9ea9c 0xc00563c607 0xc00563c608}] [] [{kube-controller-manager Update v1 2023-05-17 07:00:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb31bf58-f4d8-4381-aba7-4321ada9ea9c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:00:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dm25d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dm25d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:00:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.5,PodIP:10.244.1.127,StartTime:2023-05-17 07:00:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:00:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://d661498c242b83a774f4f000ac2df35bb200d60cf48dcbe4d918fe7855a5c74e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May 17 07:00:04.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2702" for this suite. 05/17/23 07:00:04.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:00:04.859
May 17 07:00:04.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 07:00:04.86
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:00:04.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:00:04.882
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 05/17/23 07:00:04.886
May 17 07:00:04.904: INFO: Waiting up to 5m0s for pod "downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db" in namespace "downward-api-366" to be "Succeeded or Failed"
May 17 07:00:04.908: INFO: Pod "downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.635451ms
May 17 07:00:06.915: INFO: Pod "downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011079853s
May 17 07:00:08.915: INFO: Pod "downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011285703s
STEP: Saw pod success 05/17/23 07:00:08.915
May 17 07:00:08.915: INFO: Pod "downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db" satisfied condition "Succeeded or Failed"
May 17 07:00:08.920: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db container dapi-container: <nil>
STEP: delete the pod 05/17/23 07:00:08.931
May 17 07:00:08.949: INFO: Waiting for pod downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db to disappear
May 17 07:00:08.954: INFO: Pod downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May 17 07:00:08.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-366" for this suite. 05/17/23 07:00:08.962
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":310,"skipped":5795,"failed":0}
------------------------------
â€¢ [4.111 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:00:04.859
    May 17 07:00:04.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 07:00:04.86
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:00:04.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:00:04.882
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 05/17/23 07:00:04.886
    May 17 07:00:04.904: INFO: Waiting up to 5m0s for pod "downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db" in namespace "downward-api-366" to be "Succeeded or Failed"
    May 17 07:00:04.908: INFO: Pod "downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.635451ms
    May 17 07:00:06.915: INFO: Pod "downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011079853s
    May 17 07:00:08.915: INFO: Pod "downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011285703s
    STEP: Saw pod success 05/17/23 07:00:08.915
    May 17 07:00:08.915: INFO: Pod "downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db" satisfied condition "Succeeded or Failed"
    May 17 07:00:08.920: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db container dapi-container: <nil>
    STEP: delete the pod 05/17/23 07:00:08.931
    May 17 07:00:08.949: INFO: Waiting for pod downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db to disappear
    May 17 07:00:08.954: INFO: Pod downward-api-0be77fe6-aa08-4124-87c6-134beb5f91db no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May 17 07:00:08.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-366" for this suite. 05/17/23 07:00:08.962
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:00:08.97
May 17 07:00:08.970: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 07:00:08.971
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:00:08.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:00:08.993
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-99592171-febf-4ae1-8d17-92809380f536 05/17/23 07:00:08.997
STEP: Creating a pod to test consume configMaps 05/17/23 07:00:09.004
May 17 07:00:09.019: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad" in namespace "projected-6755" to be "Succeeded or Failed"
May 17 07:00:09.024: INFO: Pod "pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.913273ms
May 17 07:00:11.030: INFO: Pod "pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011146905s
May 17 07:00:13.031: INFO: Pod "pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012140196s
STEP: Saw pod success 05/17/23 07:00:13.031
May 17 07:00:13.031: INFO: Pod "pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad" satisfied condition "Succeeded or Failed"
May 17 07:00:13.037: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad container agnhost-container: <nil>
STEP: delete the pod 05/17/23 07:00:13.049
May 17 07:00:13.063: INFO: Waiting for pod pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad to disappear
May 17 07:00:13.070: INFO: Pod pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May 17 07:00:13.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6755" for this suite. 05/17/23 07:00:13.083
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":311,"skipped":5795,"failed":0}
------------------------------
â€¢ [4.123 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:00:08.97
    May 17 07:00:08.970: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 07:00:08.971
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:00:08.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:00:08.993
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-99592171-febf-4ae1-8d17-92809380f536 05/17/23 07:00:08.997
    STEP: Creating a pod to test consume configMaps 05/17/23 07:00:09.004
    May 17 07:00:09.019: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad" in namespace "projected-6755" to be "Succeeded or Failed"
    May 17 07:00:09.024: INFO: Pod "pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.913273ms
    May 17 07:00:11.030: INFO: Pod "pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011146905s
    May 17 07:00:13.031: INFO: Pod "pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012140196s
    STEP: Saw pod success 05/17/23 07:00:13.031
    May 17 07:00:13.031: INFO: Pod "pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad" satisfied condition "Succeeded or Failed"
    May 17 07:00:13.037: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 07:00:13.049
    May 17 07:00:13.063: INFO: Waiting for pod pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad to disappear
    May 17 07:00:13.070: INFO: Pod pod-projected-configmaps-9b4b210b-22df-4c5d-95ed-3ce6e80305ad no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May 17 07:00:13.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6755" for this suite. 05/17/23 07:00:13.083
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:00:13.093
May 17 07:00:13.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 07:00:13.094
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:00:13.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:00:13.119
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 05/17/23 07:00:13.123
May 17 07:00:13.136: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8" in namespace "downward-api-8811" to be "Succeeded or Failed"
May 17 07:00:13.141: INFO: Pod "downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.990307ms
May 17 07:00:15.147: INFO: Pod "downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8": Phase="Running", Reason="", readiness=false. Elapsed: 2.010969296s
May 17 07:00:17.148: INFO: Pod "downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012076698s
STEP: Saw pod success 05/17/23 07:00:17.148
May 17 07:00:17.148: INFO: Pod "downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8" satisfied condition "Succeeded or Failed"
May 17 07:00:17.154: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8 container client-container: <nil>
STEP: delete the pod 05/17/23 07:00:17.166
May 17 07:00:17.183: INFO: Waiting for pod downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8 to disappear
May 17 07:00:17.188: INFO: Pod downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May 17 07:00:17.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8811" for this suite. 05/17/23 07:00:17.195
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":312,"skipped":5798,"failed":0}
------------------------------
â€¢ [4.111 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:00:13.093
    May 17 07:00:13.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 07:00:13.094
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:00:13.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:00:13.119
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 05/17/23 07:00:13.123
    May 17 07:00:13.136: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8" in namespace "downward-api-8811" to be "Succeeded or Failed"
    May 17 07:00:13.141: INFO: Pod "downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.990307ms
    May 17 07:00:15.147: INFO: Pod "downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8": Phase="Running", Reason="", readiness=false. Elapsed: 2.010969296s
    May 17 07:00:17.148: INFO: Pod "downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012076698s
    STEP: Saw pod success 05/17/23 07:00:17.148
    May 17 07:00:17.148: INFO: Pod "downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8" satisfied condition "Succeeded or Failed"
    May 17 07:00:17.154: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8 container client-container: <nil>
    STEP: delete the pod 05/17/23 07:00:17.166
    May 17 07:00:17.183: INFO: Waiting for pod downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8 to disappear
    May 17 07:00:17.188: INFO: Pod downwardapi-volume-ec3eeabc-104a-4bc4-a716-7b6bc13025b8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May 17 07:00:17.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8811" for this suite. 05/17/23 07:00:17.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:00:17.205
May 17 07:00:17.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 07:00:17.206
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:00:17.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:00:17.23
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 05/17/23 07:00:17.233
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/17/23 07:00:17.235
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/17/23 07:00:17.235
STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/17/23 07:00:17.235
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/17/23 07:00:17.236
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/17/23 07:00:17.237
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/17/23 07:00:17.238
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 07:00:17.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1064" for this suite. 05/17/23 07:00:17.245
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":313,"skipped":5820,"failed":0}
------------------------------
â€¢ [0.053 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:00:17.205
    May 17 07:00:17.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 07:00:17.206
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:00:17.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:00:17.23
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 05/17/23 07:00:17.233
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/17/23 07:00:17.235
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/17/23 07:00:17.235
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/17/23 07:00:17.235
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/17/23 07:00:17.236
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/17/23 07:00:17.237
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/17/23 07:00:17.238
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 07:00:17.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1064" for this suite. 05/17/23 07:00:17.245
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:00:17.259
May 17 07:00:17.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename statefulset 05/17/23 07:00:17.259
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:00:17.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:00:17.29
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8430 05/17/23 07:00:17.294
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 05/17/23 07:00:17.301
STEP: Creating stateful set ss in namespace statefulset-8430 05/17/23 07:00:17.306
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8430 05/17/23 07:00:17.313
May 17 07:00:17.317: INFO: Found 0 stateful pods, waiting for 1
May 17 07:00:27.324: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/17/23 07:00:27.324
May 17 07:00:27.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 07:00:27.523: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 07:00:27.523: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 07:00:27.523: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 07:00:27.540: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 17 07:00:37.547: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 17 07:00:37.547: INFO: Waiting for statefulset status.replicas updated to 0
May 17 07:00:37.569: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999809s
May 17 07:00:38.574: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994573115s
May 17 07:00:39.581: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987961091s
May 17 07:00:40.586: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982524971s
May 17 07:00:41.592: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.975984835s
May 17 07:00:42.599: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.970017826s
May 17 07:00:43.605: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.964520011s
May 17 07:00:44.611: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.958397299s
May 17 07:00:45.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.952534089s
May 17 07:00:46.624: INFO: Verifying statefulset ss doesn't scale past 1 for another 946.84684ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8430 05/17/23 07:00:47.624
May 17 07:00:47.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 07:00:47.826: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 07:00:47.826: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 07:00:47.826: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 07:00:47.832: INFO: Found 1 stateful pods, waiting for 3
May 17 07:00:57.840: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 07:00:57.840: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 07:00:57.840: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 05/17/23 07:00:57.84
STEP: Scale down will halt with unhealthy stateful pod 05/17/23 07:00:57.84
May 17 07:00:57.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 07:00:58.051: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 07:00:58.051: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 07:00:58.051: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 07:00:58.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 07:00:58.230: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 07:00:58.230: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 07:00:58.230: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 07:00:58.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 07:00:58.413: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 07:00:58.413: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 07:00:58.413: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 07:00:58.413: INFO: Waiting for statefulset status.replicas updated to 0
May 17 07:00:58.418: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 17 07:01:08.431: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 17 07:01:08.431: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 17 07:01:08.431: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 17 07:01:08.464: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999979s
May 17 07:01:09.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994602051s
May 17 07:01:10.477: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987826478s
May 17 07:01:11.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980638191s
May 17 07:01:12.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974746473s
May 17 07:01:13.495: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969054595s
May 17 07:01:14.500: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963417194s
May 17 07:01:15.509: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957875377s
May 17 07:01:16.514: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.949599335s
May 17 07:01:17.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 944.11479ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8430 05/17/23 07:01:18.521
May 17 07:01:18.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 07:01:18.731: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 07:01:18.731: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 07:01:18.731: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 07:01:18.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 07:01:18.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 07:01:18.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 07:01:18.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 07:01:18.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 07:01:19.088: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 07:01:19.088: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 07:01:19.088: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 07:01:19.088: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 05/17/23 07:01:29.141
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May 17 07:01:29.141: INFO: Deleting all statefulset in ns statefulset-8430
May 17 07:01:29.146: INFO: Scaling statefulset ss to 0
May 17 07:01:29.161: INFO: Waiting for statefulset status.replicas updated to 0
May 17 07:01:29.166: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May 17 07:01:29.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8430" for this suite. 05/17/23 07:01:29.211
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":314,"skipped":5820,"failed":0}
------------------------------
â€¢ [SLOW TEST] [71.962 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:00:17.259
    May 17 07:00:17.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename statefulset 05/17/23 07:00:17.259
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:00:17.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:00:17.29
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8430 05/17/23 07:00:17.294
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 05/17/23 07:00:17.301
    STEP: Creating stateful set ss in namespace statefulset-8430 05/17/23 07:00:17.306
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8430 05/17/23 07:00:17.313
    May 17 07:00:17.317: INFO: Found 0 stateful pods, waiting for 1
    May 17 07:00:27.324: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/17/23 07:00:27.324
    May 17 07:00:27.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 07:00:27.523: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 07:00:27.523: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 07:00:27.523: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 07:00:27.540: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May 17 07:00:37.547: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May 17 07:00:37.547: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 07:00:37.569: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999809s
    May 17 07:00:38.574: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994573115s
    May 17 07:00:39.581: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987961091s
    May 17 07:00:40.586: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982524971s
    May 17 07:00:41.592: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.975984835s
    May 17 07:00:42.599: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.970017826s
    May 17 07:00:43.605: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.964520011s
    May 17 07:00:44.611: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.958397299s
    May 17 07:00:45.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.952534089s
    May 17 07:00:46.624: INFO: Verifying statefulset ss doesn't scale past 1 for another 946.84684ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8430 05/17/23 07:00:47.624
    May 17 07:00:47.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 07:00:47.826: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 07:00:47.826: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 07:00:47.826: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 07:00:47.832: INFO: Found 1 stateful pods, waiting for 3
    May 17 07:00:57.840: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May 17 07:00:57.840: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May 17 07:00:57.840: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 05/17/23 07:00:57.84
    STEP: Scale down will halt with unhealthy stateful pod 05/17/23 07:00:57.84
    May 17 07:00:57.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 07:00:58.051: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 07:00:58.051: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 07:00:58.051: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 07:00:58.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 07:00:58.230: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 07:00:58.230: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 07:00:58.230: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 07:00:58.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 17 07:00:58.413: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 17 07:00:58.413: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 17 07:00:58.413: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 17 07:00:58.413: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 07:00:58.418: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    May 17 07:01:08.431: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May 17 07:01:08.431: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May 17 07:01:08.431: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May 17 07:01:08.464: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999979s
    May 17 07:01:09.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994602051s
    May 17 07:01:10.477: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987826478s
    May 17 07:01:11.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980638191s
    May 17 07:01:12.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974746473s
    May 17 07:01:13.495: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969054595s
    May 17 07:01:14.500: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963417194s
    May 17 07:01:15.509: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957875377s
    May 17 07:01:16.514: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.949599335s
    May 17 07:01:17.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 944.11479ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8430 05/17/23 07:01:18.521
    May 17 07:01:18.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 07:01:18.731: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 07:01:18.731: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 07:01:18.731: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 07:01:18.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 07:01:18.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 07:01:18.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 07:01:18.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 07:01:18.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=statefulset-8430 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 17 07:01:19.088: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 17 07:01:19.088: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 17 07:01:19.088: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 17 07:01:19.088: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 05/17/23 07:01:29.141
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May 17 07:01:29.141: INFO: Deleting all statefulset in ns statefulset-8430
    May 17 07:01:29.146: INFO: Scaling statefulset ss to 0
    May 17 07:01:29.161: INFO: Waiting for statefulset status.replicas updated to 0
    May 17 07:01:29.166: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May 17 07:01:29.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8430" for this suite. 05/17/23 07:01:29.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:01:29.221
May 17 07:01:29.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 07:01:29.222
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:01:29.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:01:29.245
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/17/23 07:01:29.25
May 17 07:01:29.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/17/23 07:01:56.834
May 17 07:01:56.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 07:02:06.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 07:02:39.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2663" for this suite. 05/17/23 07:02:39.292
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":315,"skipped":5828,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.079 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:01:29.221
    May 17 07:01:29.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 07:01:29.222
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:01:29.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:01:29.245
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/17/23 07:01:29.25
    May 17 07:01:29.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/17/23 07:01:56.834
    May 17 07:01:56.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 07:02:06.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 07:02:39.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2663" for this suite. 05/17/23 07:02:39.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:02:39.302
May 17 07:02:39.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename containers 05/17/23 07:02:39.303
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:39.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:39.324
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
May 17 07:02:39.347: INFO: Waiting up to 5m0s for pod "client-containers-5353eac0-6513-484b-8bbf-1b69076141b4" in namespace "containers-5955" to be "running"
May 17 07:02:39.352: INFO: Pod "client-containers-5353eac0-6513-484b-8bbf-1b69076141b4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.025755ms
May 17 07:02:41.358: INFO: Pod "client-containers-5353eac0-6513-484b-8bbf-1b69076141b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010944946s
May 17 07:02:41.358: INFO: Pod "client-containers-5353eac0-6513-484b-8bbf-1b69076141b4" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May 17 07:02:41.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5955" for this suite. 05/17/23 07:02:41.384
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":316,"skipped":5843,"failed":0}
------------------------------
â€¢ [2.112 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:02:39.302
    May 17 07:02:39.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename containers 05/17/23 07:02:39.303
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:39.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:39.324
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    May 17 07:02:39.347: INFO: Waiting up to 5m0s for pod "client-containers-5353eac0-6513-484b-8bbf-1b69076141b4" in namespace "containers-5955" to be "running"
    May 17 07:02:39.352: INFO: Pod "client-containers-5353eac0-6513-484b-8bbf-1b69076141b4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.025755ms
    May 17 07:02:41.358: INFO: Pod "client-containers-5353eac0-6513-484b-8bbf-1b69076141b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010944946s
    May 17 07:02:41.358: INFO: Pod "client-containers-5353eac0-6513-484b-8bbf-1b69076141b4" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May 17 07:02:41.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5955" for this suite. 05/17/23 07:02:41.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:02:41.416
May 17 07:02:41.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 07:02:41.417
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:41.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:41.437
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 05/17/23 07:02:41.441
May 17 07:02:41.455: INFO: Waiting up to 5m0s for pod "downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae" in namespace "downward-api-2820" to be "Succeeded or Failed"
May 17 07:02:41.460: INFO: Pod "downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.172099ms
May 17 07:02:43.466: INFO: Pod "downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010409812s
May 17 07:02:45.465: INFO: Pod "downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009661523s
STEP: Saw pod success 05/17/23 07:02:45.465
May 17 07:02:45.465: INFO: Pod "downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae" satisfied condition "Succeeded or Failed"
May 17 07:02:45.470: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae container dapi-container: <nil>
STEP: delete the pod 05/17/23 07:02:45.481
May 17 07:02:45.495: INFO: Waiting for pod downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae to disappear
May 17 07:02:45.499: INFO: Pod downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May 17 07:02:45.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2820" for this suite. 05/17/23 07:02:45.508
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":317,"skipped":5897,"failed":0}
------------------------------
â€¢ [4.100 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:02:41.416
    May 17 07:02:41.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 07:02:41.417
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:41.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:41.437
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 05/17/23 07:02:41.441
    May 17 07:02:41.455: INFO: Waiting up to 5m0s for pod "downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae" in namespace "downward-api-2820" to be "Succeeded or Failed"
    May 17 07:02:41.460: INFO: Pod "downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.172099ms
    May 17 07:02:43.466: INFO: Pod "downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010409812s
    May 17 07:02:45.465: INFO: Pod "downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009661523s
    STEP: Saw pod success 05/17/23 07:02:45.465
    May 17 07:02:45.465: INFO: Pod "downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae" satisfied condition "Succeeded or Failed"
    May 17 07:02:45.470: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae container dapi-container: <nil>
    STEP: delete the pod 05/17/23 07:02:45.481
    May 17 07:02:45.495: INFO: Waiting for pod downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae to disappear
    May 17 07:02:45.499: INFO: Pod downward-api-f889f934-3e38-4d35-bb93-8ae159a909ae no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May 17 07:02:45.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2820" for this suite. 05/17/23 07:02:45.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:02:45.516
May 17 07:02:45.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 07:02:45.518
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:45.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:45.538
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 05/17/23 07:02:45.542
May 17 07:02:45.557: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811" in namespace "downward-api-1929" to be "Succeeded or Failed"
May 17 07:02:45.563: INFO: Pod "downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811": Phase="Pending", Reason="", readiness=false. Elapsed: 5.737612ms
May 17 07:02:47.572: INFO: Pod "downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014526101s
May 17 07:02:49.573: INFO: Pod "downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016089047s
STEP: Saw pod success 05/17/23 07:02:49.573
May 17 07:02:49.573: INFO: Pod "downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811" satisfied condition "Succeeded or Failed"
May 17 07:02:49.581: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811 container client-container: <nil>
STEP: delete the pod 05/17/23 07:02:49.594
May 17 07:02:49.610: INFO: Waiting for pod downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811 to disappear
May 17 07:02:49.616: INFO: Pod downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May 17 07:02:49.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1929" for this suite. 05/17/23 07:02:49.628
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":318,"skipped":5903,"failed":0}
------------------------------
â€¢ [4.121 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:02:45.516
    May 17 07:02:45.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 07:02:45.518
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:45.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:45.538
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 05/17/23 07:02:45.542
    May 17 07:02:45.557: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811" in namespace "downward-api-1929" to be "Succeeded or Failed"
    May 17 07:02:45.563: INFO: Pod "downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811": Phase="Pending", Reason="", readiness=false. Elapsed: 5.737612ms
    May 17 07:02:47.572: INFO: Pod "downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014526101s
    May 17 07:02:49.573: INFO: Pod "downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016089047s
    STEP: Saw pod success 05/17/23 07:02:49.573
    May 17 07:02:49.573: INFO: Pod "downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811" satisfied condition "Succeeded or Failed"
    May 17 07:02:49.581: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811 container client-container: <nil>
    STEP: delete the pod 05/17/23 07:02:49.594
    May 17 07:02:49.610: INFO: Waiting for pod downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811 to disappear
    May 17 07:02:49.616: INFO: Pod downwardapi-volume-fafe2fc7-5880-48c9-a79e-d406a64e3811 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May 17 07:02:49.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1929" for this suite. 05/17/23 07:02:49.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:02:49.638
May 17 07:02:49.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename daemonsets 05/17/23 07:02:49.638
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:49.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:49.658
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 05/17/23 07:02:49.694
STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 07:02:49.702
May 17 07:02:49.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 07:02:49.714: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 07:02:50.729: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 07:02:50.729: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 07:02:51.728: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 17 07:02:51.728: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 05/17/23 07:02:51.732
STEP: DeleteCollection of the DaemonSets 05/17/23 07:02:51.738
STEP: Verify that ReplicaSets have been deleted 05/17/23 07:02:51.746
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
May 17 07:02:51.759: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1497683"},"items":null}

May 17 07:02:51.765: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1497683"},"items":[{"metadata":{"name":"daemon-set-48pkl","generateName":"daemon-set-","namespace":"daemonsets-8714","uid":"66546f35-d839-4fe1-a64d-bc0a4d82a27f","resourceVersion":"1497676","creationTimestamp":"2023-05-17T07:02:49Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"02c7c4e8-d29b-47a1-8bb1-42a6d22c4bb9","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-17T07:02:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02c7c4e8-d29b-47a1-8bb1-42a6d22c4bb9\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-17T07:02:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9x5hs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9x5hs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aks-agentpool-72615086-vmss00000d","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aks-agentpool-72615086-vmss00000d"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:49Z"}],"hostIP":"10.224.0.5","podIP":"10.244.1.131","podIPs":[{"ip":"10.244.1.131"}],"startTime":"2023-05-17T07:02:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-17T07:02:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://aa69bfef9d88557f80a875c1b0226fb5016c9465705c8a64db0af7ecd197b37c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-c8m7x","generateName":"daemon-set-","namespace":"daemonsets-8714","uid":"a56b2700-bf44-4e35-8c6e-6845d07438c7","resourceVersion":"1497666","creationTimestamp":"2023-05-17T07:02:49Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"02c7c4e8-d29b-47a1-8bb1-42a6d22c4bb9","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-17T07:02:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02c7c4e8-d29b-47a1-8bb1-42a6d22c4bb9\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-17T07:02:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7xl6j","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7xl6j","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aks-agentpool-72615086-vmss00000b","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aks-agentpool-72615086-vmss00000b"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:50Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:50Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:49Z"}],"hostIP":"10.224.0.4","podIP":"10.244.0.115","podIPs":[{"ip":"10.244.0.115"}],"startTime":"2023-05-17T07:02:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-17T07:02:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://2d38d56ebc20f307a6d422a6b1ad5854170bc7658fcbda733aaed4505c1270a6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-z5rgg","generateName":"daemon-set-","namespace":"daemonsets-8714","uid":"838d3986-8487-47cf-9830-43304c45f138","resourceVersion":"1497672","creationTimestamp":"2023-05-17T07:02:49Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"02c7c4e8-d29b-47a1-8bb1-42a6d22c4bb9","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-17T07:02:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02c7c4e8-d29b-47a1-8bb1-42a6d22c4bb9\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-17T07:02:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qqs7p","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qqs7p","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aks-agentpool-72615086-vmss00000c","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aks-agentpool-72615086-vmss00000c"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:49Z"}],"hostIP":"10.224.0.6","podIP":"10.244.2.108","podIPs":[{"ip":"10.244.2.108"}],"startTime":"2023-05-17T07:02:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-17T07:02:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://1371da1ce8212fcdd8704d0227a04ecf1679d292fe57fdd375db372115f19821","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May 17 07:02:51.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8714" for this suite. 05/17/23 07:02:51.804
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":319,"skipped":5913,"failed":0}
------------------------------
â€¢ [2.175 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:02:49.638
    May 17 07:02:49.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename daemonsets 05/17/23 07:02:49.638
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:49.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:49.658
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 05/17/23 07:02:49.694
    STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 07:02:49.702
    May 17 07:02:49.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 07:02:49.714: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 07:02:50.729: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 07:02:50.729: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 07:02:51.728: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 17 07:02:51.728: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 05/17/23 07:02:51.732
    STEP: DeleteCollection of the DaemonSets 05/17/23 07:02:51.738
    STEP: Verify that ReplicaSets have been deleted 05/17/23 07:02:51.746
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    May 17 07:02:51.759: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1497683"},"items":null}

    May 17 07:02:51.765: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1497683"},"items":[{"metadata":{"name":"daemon-set-48pkl","generateName":"daemon-set-","namespace":"daemonsets-8714","uid":"66546f35-d839-4fe1-a64d-bc0a4d82a27f","resourceVersion":"1497676","creationTimestamp":"2023-05-17T07:02:49Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"02c7c4e8-d29b-47a1-8bb1-42a6d22c4bb9","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-17T07:02:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02c7c4e8-d29b-47a1-8bb1-42a6d22c4bb9\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-17T07:02:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9x5hs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9x5hs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aks-agentpool-72615086-vmss00000d","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aks-agentpool-72615086-vmss00000d"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:49Z"}],"hostIP":"10.224.0.5","podIP":"10.244.1.131","podIPs":[{"ip":"10.244.1.131"}],"startTime":"2023-05-17T07:02:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-17T07:02:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://aa69bfef9d88557f80a875c1b0226fb5016c9465705c8a64db0af7ecd197b37c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-c8m7x","generateName":"daemon-set-","namespace":"daemonsets-8714","uid":"a56b2700-bf44-4e35-8c6e-6845d07438c7","resourceVersion":"1497666","creationTimestamp":"2023-05-17T07:02:49Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"02c7c4e8-d29b-47a1-8bb1-42a6d22c4bb9","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-17T07:02:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02c7c4e8-d29b-47a1-8bb1-42a6d22c4bb9\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-17T07:02:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7xl6j","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7xl6j","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aks-agentpool-72615086-vmss00000b","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aks-agentpool-72615086-vmss00000b"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:50Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:50Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:49Z"}],"hostIP":"10.224.0.4","podIP":"10.244.0.115","podIPs":[{"ip":"10.244.0.115"}],"startTime":"2023-05-17T07:02:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-17T07:02:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://2d38d56ebc20f307a6d422a6b1ad5854170bc7658fcbda733aaed4505c1270a6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-z5rgg","generateName":"daemon-set-","namespace":"daemonsets-8714","uid":"838d3986-8487-47cf-9830-43304c45f138","resourceVersion":"1497672","creationTimestamp":"2023-05-17T07:02:49Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"02c7c4e8-d29b-47a1-8bb1-42a6d22c4bb9","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-17T07:02:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02c7c4e8-d29b-47a1-8bb1-42a6d22c4bb9\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-17T07:02:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qqs7p","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qqs7p","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aks-agentpool-72615086-vmss00000c","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aks-agentpool-72615086-vmss00000c"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-17T07:02:49Z"}],"hostIP":"10.224.0.6","podIP":"10.244.2.108","podIPs":[{"ip":"10.244.2.108"}],"startTime":"2023-05-17T07:02:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-17T07:02:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://1371da1ce8212fcdd8704d0227a04ecf1679d292fe57fdd375db372115f19821","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May 17 07:02:51.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8714" for this suite. 05/17/23 07:02:51.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:02:51.815
May 17 07:02:51.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename watch 05/17/23 07:02:51.815
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:51.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:51.836
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 05/17/23 07:02:51.839
STEP: creating a new configmap 05/17/23 07:02:51.842
STEP: modifying the configmap once 05/17/23 07:02:51.849
STEP: closing the watch once it receives two notifications 05/17/23 07:02:51.861
May 17 07:02:51.861: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8640  2371f1b7-b4b4-437e-8963-cc01a57ee14e 1497692 0 2023-05-17 07:02:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 07:02:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 07:02:51.861: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8640  2371f1b7-b4b4-437e-8963-cc01a57ee14e 1497693 0 2023-05-17 07:02:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 07:02:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 05/17/23 07:02:51.861
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/17/23 07:02:51.872
STEP: deleting the configmap 05/17/23 07:02:51.874
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/17/23 07:02:51.882
May 17 07:02:51.882: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8640  2371f1b7-b4b4-437e-8963-cc01a57ee14e 1497694 0 2023-05-17 07:02:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 07:02:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 07:02:51.882: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8640  2371f1b7-b4b4-437e-8963-cc01a57ee14e 1497695 0 2023-05-17 07:02:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 07:02:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May 17 07:02:51.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8640" for this suite. 05/17/23 07:02:51.889
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":320,"skipped":5968,"failed":0}
------------------------------
â€¢ [0.082 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:02:51.815
    May 17 07:02:51.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename watch 05/17/23 07:02:51.815
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:51.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:51.836
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 05/17/23 07:02:51.839
    STEP: creating a new configmap 05/17/23 07:02:51.842
    STEP: modifying the configmap once 05/17/23 07:02:51.849
    STEP: closing the watch once it receives two notifications 05/17/23 07:02:51.861
    May 17 07:02:51.861: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8640  2371f1b7-b4b4-437e-8963-cc01a57ee14e 1497692 0 2023-05-17 07:02:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 07:02:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 07:02:51.861: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8640  2371f1b7-b4b4-437e-8963-cc01a57ee14e 1497693 0 2023-05-17 07:02:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 07:02:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 05/17/23 07:02:51.861
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/17/23 07:02:51.872
    STEP: deleting the configmap 05/17/23 07:02:51.874
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/17/23 07:02:51.882
    May 17 07:02:51.882: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8640  2371f1b7-b4b4-437e-8963-cc01a57ee14e 1497694 0 2023-05-17 07:02:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 07:02:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 17 07:02:51.882: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8640  2371f1b7-b4b4-437e-8963-cc01a57ee14e 1497695 0 2023-05-17 07:02:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-17 07:02:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May 17 07:02:51.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8640" for this suite. 05/17/23 07:02:51.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:02:51.898
May 17 07:02:51.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename server-version 05/17/23 07:02:51.899
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:51.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:51.917
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 05/17/23 07:02:51.92
STEP: Confirm major version 05/17/23 07:02:51.922
May 17 07:02:51.922: INFO: Major version: 1
STEP: Confirm minor version 05/17/23 07:02:51.922
May 17 07:02:51.922: INFO: cleanMinorVersion: 25
May 17 07:02:51.922: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
May 17 07:02:51.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5901" for this suite. 05/17/23 07:02:51.928
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":321,"skipped":5985,"failed":0}
------------------------------
â€¢ [0.039 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:02:51.898
    May 17 07:02:51.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename server-version 05/17/23 07:02:51.899
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:51.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:51.917
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 05/17/23 07:02:51.92
    STEP: Confirm major version 05/17/23 07:02:51.922
    May 17 07:02:51.922: INFO: Major version: 1
    STEP: Confirm minor version 05/17/23 07:02:51.922
    May 17 07:02:51.922: INFO: cleanMinorVersion: 25
    May 17 07:02:51.922: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    May 17 07:02:51.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-5901" for this suite. 05/17/23 07:02:51.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:02:51.938
May 17 07:02:51.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename init-container 05/17/23 07:02:51.939
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:51.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:51.959
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 05/17/23 07:02:51.965
May 17 07:02:51.965: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May 17 07:02:57.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-593" for this suite. 05/17/23 07:02:57.089
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":322,"skipped":5997,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.162 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:02:51.938
    May 17 07:02:51.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename init-container 05/17/23 07:02:51.939
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:51.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:51.959
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 05/17/23 07:02:51.965
    May 17 07:02:51.965: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May 17 07:02:57.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-593" for this suite. 05/17/23 07:02:57.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:02:57.101
May 17 07:02:57.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename endpointslice 05/17/23 07:02:57.102
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:57.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:57.156
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
May 17 07:02:57.174: INFO: Endpoints addresses: [104.45.178.249] , ports: [443]
May 17 07:02:57.174: INFO: EndpointSlices addresses: [104.45.178.249] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May 17 07:02:57.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5490" for this suite. 05/17/23 07:02:57.182
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":323,"skipped":6022,"failed":0}
------------------------------
â€¢ [0.091 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:02:57.101
    May 17 07:02:57.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename endpointslice 05/17/23 07:02:57.102
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:57.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:57.156
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    May 17 07:02:57.174: INFO: Endpoints addresses: [104.45.178.249] , ports: [443]
    May 17 07:02:57.174: INFO: EndpointSlices addresses: [104.45.178.249] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May 17 07:02:57.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5490" for this suite. 05/17/23 07:02:57.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:02:57.193
May 17 07:02:57.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 07:02:57.194
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:57.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:57.218
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-9197 05/17/23 07:02:57.221
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9197 to expose endpoints map[] 05/17/23 07:02:57.238
May 17 07:02:57.255: INFO: successfully validated that service multi-endpoint-test in namespace services-9197 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9197 05/17/23 07:02:57.255
May 17 07:02:57.270: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9197" to be "running and ready"
May 17 07:02:57.274: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.524368ms
May 17 07:02:57.274: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 17 07:02:59.281: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011459255s
May 17 07:02:59.281: INFO: The phase of Pod pod1 is Running (Ready = true)
May 17 07:02:59.281: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9197 to expose endpoints map[pod1:[100]] 05/17/23 07:02:59.286
May 17 07:02:59.301: INFO: successfully validated that service multi-endpoint-test in namespace services-9197 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9197 05/17/23 07:02:59.301
May 17 07:02:59.310: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9197" to be "running and ready"
May 17 07:02:59.315: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.89795ms
May 17 07:02:59.315: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 17 07:03:01.322: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.01143089s
May 17 07:03:01.322: INFO: The phase of Pod pod2 is Running (Ready = true)
May 17 07:03:01.322: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9197 to expose endpoints map[pod1:[100] pod2:[101]] 05/17/23 07:03:01.326
May 17 07:03:01.347: INFO: successfully validated that service multi-endpoint-test in namespace services-9197 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 05/17/23 07:03:01.347
May 17 07:03:01.347: INFO: Creating new exec pod
May 17 07:03:01.357: INFO: Waiting up to 5m0s for pod "execpodjnd65" in namespace "services-9197" to be "running"
May 17 07:03:01.362: INFO: Pod "execpodjnd65": Phase="Pending", Reason="", readiness=false. Elapsed: 5.455684ms
May 17 07:03:03.370: INFO: Pod "execpodjnd65": Phase="Running", Reason="", readiness=true. Elapsed: 2.012818193s
May 17 07:03:03.370: INFO: Pod "execpodjnd65" satisfied condition "running"
May 17 07:03:04.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
May 17 07:03:06.555: INFO: rc: 1
May 17 07:03:06.555: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80:
Command stdout:

stderr:
+ echo hostName
+ nc -v -t -w 2 multi-endpoint-test 80
nc: connect to multi-endpoint-test port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
May 17 07:03:07.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
May 17 07:03:09.755: INFO: rc: 1
May 17 07:03:09.755: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80:
Command stdout:

stderr:
+ echo hostName
+ nc -v -t -w 2 multi-endpoint-test 80
nc: connect to multi-endpoint-test port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
May 17 07:03:10.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
May 17 07:03:12.748: INFO: rc: 1
May 17 07:03:12.748: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80:
Command stdout:

stderr:
+ echo hostName
+ nc -v -t -w 2 multi-endpoint-test 80
nc: connect to multi-endpoint-test port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
May 17 07:03:13.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
May 17 07:03:13.744: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
May 17 07:03:13.744: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 07:03:13.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.170.161 80'
May 17 07:03:13.939: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.170.161 80\nConnection to 10.0.170.161 80 port [tcp/http] succeeded!\n"
May 17 07:03:13.939: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 07:03:13.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
May 17 07:03:14.121: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
May 17 07:03:14.121: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 07:03:14.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.170.161 81'
May 17 07:03:14.310: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.170.161 81\nConnection to 10.0.170.161 81 port [tcp/*] succeeded!\n"
May 17 07:03:14.310: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9197 05/17/23 07:03:14.31
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9197 to expose endpoints map[pod2:[101]] 05/17/23 07:03:14.328
May 17 07:03:14.344: INFO: successfully validated that service multi-endpoint-test in namespace services-9197 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9197 05/17/23 07:03:14.344
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9197 to expose endpoints map[] 05/17/23 07:03:14.362
May 17 07:03:14.372: INFO: successfully validated that service multi-endpoint-test in namespace services-9197 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 07:03:14.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9197" for this suite. 05/17/23 07:03:14.402
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":324,"skipped":6047,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.219 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:02:57.193
    May 17 07:02:57.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 07:02:57.194
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:02:57.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:02:57.218
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-9197 05/17/23 07:02:57.221
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9197 to expose endpoints map[] 05/17/23 07:02:57.238
    May 17 07:02:57.255: INFO: successfully validated that service multi-endpoint-test in namespace services-9197 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9197 05/17/23 07:02:57.255
    May 17 07:02:57.270: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9197" to be "running and ready"
    May 17 07:02:57.274: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.524368ms
    May 17 07:02:57.274: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:02:59.281: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011459255s
    May 17 07:02:59.281: INFO: The phase of Pod pod1 is Running (Ready = true)
    May 17 07:02:59.281: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9197 to expose endpoints map[pod1:[100]] 05/17/23 07:02:59.286
    May 17 07:02:59.301: INFO: successfully validated that service multi-endpoint-test in namespace services-9197 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-9197 05/17/23 07:02:59.301
    May 17 07:02:59.310: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9197" to be "running and ready"
    May 17 07:02:59.315: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.89795ms
    May 17 07:02:59.315: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:03:01.322: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.01143089s
    May 17 07:03:01.322: INFO: The phase of Pod pod2 is Running (Ready = true)
    May 17 07:03:01.322: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9197 to expose endpoints map[pod1:[100] pod2:[101]] 05/17/23 07:03:01.326
    May 17 07:03:01.347: INFO: successfully validated that service multi-endpoint-test in namespace services-9197 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 05/17/23 07:03:01.347
    May 17 07:03:01.347: INFO: Creating new exec pod
    May 17 07:03:01.357: INFO: Waiting up to 5m0s for pod "execpodjnd65" in namespace "services-9197" to be "running"
    May 17 07:03:01.362: INFO: Pod "execpodjnd65": Phase="Pending", Reason="", readiness=false. Elapsed: 5.455684ms
    May 17 07:03:03.370: INFO: Pod "execpodjnd65": Phase="Running", Reason="", readiness=true. Elapsed: 2.012818193s
    May 17 07:03:03.370: INFO: Pod "execpodjnd65" satisfied condition "running"
    May 17 07:03:04.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    May 17 07:03:06.555: INFO: rc: 1
    May 17 07:03:06.555: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80:
    Command stdout:

    stderr:
    + echo hostName
    + nc -v -t -w 2 multi-endpoint-test 80
    nc: connect to multi-endpoint-test port 80 (tcp) timed out: Operation in progress
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    May 17 07:03:07.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    May 17 07:03:09.755: INFO: rc: 1
    May 17 07:03:09.755: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80:
    Command stdout:

    stderr:
    + echo hostName
    + nc -v -t -w 2 multi-endpoint-test 80
    nc: connect to multi-endpoint-test port 80 (tcp) timed out: Operation in progress
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    May 17 07:03:10.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    May 17 07:03:12.748: INFO: rc: 1
    May 17 07:03:12.748: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80:
    Command stdout:

    stderr:
    + echo hostName
    + nc -v -t -w 2 multi-endpoint-test 80
    nc: connect to multi-endpoint-test port 80 (tcp) timed out: Operation in progress
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    May 17 07:03:13.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    May 17 07:03:13.744: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    May 17 07:03:13.744: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 07:03:13.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.170.161 80'
    May 17 07:03:13.939: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.170.161 80\nConnection to 10.0.170.161 80 port [tcp/http] succeeded!\n"
    May 17 07:03:13.939: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 07:03:13.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    May 17 07:03:14.121: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    May 17 07:03:14.121: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 07:03:14.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-9197 exec execpodjnd65 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.170.161 81'
    May 17 07:03:14.310: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.170.161 81\nConnection to 10.0.170.161 81 port [tcp/*] succeeded!\n"
    May 17 07:03:14.310: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-9197 05/17/23 07:03:14.31
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9197 to expose endpoints map[pod2:[101]] 05/17/23 07:03:14.328
    May 17 07:03:14.344: INFO: successfully validated that service multi-endpoint-test in namespace services-9197 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-9197 05/17/23 07:03:14.344
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9197 to expose endpoints map[] 05/17/23 07:03:14.362
    May 17 07:03:14.372: INFO: successfully validated that service multi-endpoint-test in namespace services-9197 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 07:03:14.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9197" for this suite. 05/17/23 07:03:14.402
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:03:14.413
May 17 07:03:14.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename daemonsets 05/17/23 07:03:14.414
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:14.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:14.434
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
May 17 07:03:14.475: INFO: Create a RollingUpdate DaemonSet
May 17 07:03:14.481: INFO: Check that daemon pods launch on every node of the cluster
May 17 07:03:14.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 07:03:14.493: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 07:03:15.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 07:03:15.510: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 07:03:16.507: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 17 07:03:16.507: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
May 17 07:03:16.507: INFO: Update the DaemonSet to trigger a rollout
May 17 07:03:16.516: INFO: Updating DaemonSet daemon-set
May 17 07:03:18.541: INFO: Roll back the DaemonSet before rollout is complete
May 17 07:03:18.552: INFO: Updating DaemonSet daemon-set
May 17 07:03:18.552: INFO: Make sure DaemonSet rollback is complete
May 17 07:03:18.557: INFO: Wrong image for pod: daemon-set-49rwh. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
May 17 07:03:18.557: INFO: Pod daemon-set-49rwh is not available
May 17 07:03:20.572: INFO: Pod daemon-set-h6hfc is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/17/23 07:03:20.589
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-639, will wait for the garbage collector to delete the pods 05/17/23 07:03:20.589
May 17 07:03:20.652: INFO: Deleting DaemonSet.extensions daemon-set took: 7.512861ms
May 17 07:03:20.753: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.702462ms
May 17 07:03:22.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 07:03:22.459: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 17 07:03:22.463: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1498179"},"items":null}

May 17 07:03:22.468: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1498179"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May 17 07:03:22.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-639" for this suite. 05/17/23 07:03:22.5
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":325,"skipped":6084,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.096 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:03:14.413
    May 17 07:03:14.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename daemonsets 05/17/23 07:03:14.414
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:14.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:14.434
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    May 17 07:03:14.475: INFO: Create a RollingUpdate DaemonSet
    May 17 07:03:14.481: INFO: Check that daemon pods launch on every node of the cluster
    May 17 07:03:14.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 07:03:14.493: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 07:03:15.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 07:03:15.510: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 07:03:16.507: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 17 07:03:16.507: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    May 17 07:03:16.507: INFO: Update the DaemonSet to trigger a rollout
    May 17 07:03:16.516: INFO: Updating DaemonSet daemon-set
    May 17 07:03:18.541: INFO: Roll back the DaemonSet before rollout is complete
    May 17 07:03:18.552: INFO: Updating DaemonSet daemon-set
    May 17 07:03:18.552: INFO: Make sure DaemonSet rollback is complete
    May 17 07:03:18.557: INFO: Wrong image for pod: daemon-set-49rwh. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    May 17 07:03:18.557: INFO: Pod daemon-set-49rwh is not available
    May 17 07:03:20.572: INFO: Pod daemon-set-h6hfc is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/17/23 07:03:20.589
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-639, will wait for the garbage collector to delete the pods 05/17/23 07:03:20.589
    May 17 07:03:20.652: INFO: Deleting DaemonSet.extensions daemon-set took: 7.512861ms
    May 17 07:03:20.753: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.702462ms
    May 17 07:03:22.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 07:03:22.459: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 17 07:03:22.463: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1498179"},"items":null}

    May 17 07:03:22.468: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1498179"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May 17 07:03:22.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-639" for this suite. 05/17/23 07:03:22.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:03:22.512
May 17 07:03:22.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 07:03:22.513
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:22.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:22.532
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 05/17/23 07:03:22.536
May 17 07:03:22.550: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648" in namespace "projected-3136" to be "Succeeded or Failed"
May 17 07:03:22.556: INFO: Pod "downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648": Phase="Pending", Reason="", readiness=false. Elapsed: 5.982559ms
May 17 07:03:24.562: INFO: Pod "downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011940369s
May 17 07:03:26.562: INFO: Pod "downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012289888s
STEP: Saw pod success 05/17/23 07:03:26.562
May 17 07:03:26.562: INFO: Pod "downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648" satisfied condition "Succeeded or Failed"
May 17 07:03:26.567: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648 container client-container: <nil>
STEP: delete the pod 05/17/23 07:03:26.581
May 17 07:03:26.607: INFO: Waiting for pod downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648 to disappear
May 17 07:03:26.618: INFO: Pod downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May 17 07:03:26.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3136" for this suite. 05/17/23 07:03:26.626
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":326,"skipped":6126,"failed":0}
------------------------------
â€¢ [4.135 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:03:22.512
    May 17 07:03:22.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 07:03:22.513
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:22.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:22.532
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 05/17/23 07:03:22.536
    May 17 07:03:22.550: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648" in namespace "projected-3136" to be "Succeeded or Failed"
    May 17 07:03:22.556: INFO: Pod "downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648": Phase="Pending", Reason="", readiness=false. Elapsed: 5.982559ms
    May 17 07:03:24.562: INFO: Pod "downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011940369s
    May 17 07:03:26.562: INFO: Pod "downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012289888s
    STEP: Saw pod success 05/17/23 07:03:26.562
    May 17 07:03:26.562: INFO: Pod "downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648" satisfied condition "Succeeded or Failed"
    May 17 07:03:26.567: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648 container client-container: <nil>
    STEP: delete the pod 05/17/23 07:03:26.581
    May 17 07:03:26.607: INFO: Waiting for pod downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648 to disappear
    May 17 07:03:26.618: INFO: Pod downwardapi-volume-d42f9b5c-4128-4902-8e64-7dde330f0648 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May 17 07:03:26.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3136" for this suite. 05/17/23 07:03:26.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:03:26.647
May 17 07:03:26.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename daemonsets 05/17/23 07:03:26.648
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:26.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:26.67
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 05/17/23 07:03:26.714
STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 07:03:26.72
May 17 07:03:26.737: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 07:03:26.737: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 07:03:27.752: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 17 07:03:27.752: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
May 17 07:03:28.753: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 17 07:03:28.753: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/17/23 07:03:28.758
May 17 07:03:28.785: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 07:03:28.785: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 07:03:29.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 17 07:03:29.802: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
May 17 07:03:30.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 17 07:03:30.798: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 05/17/23 07:03:30.798
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/17/23 07:03:30.807
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5274, will wait for the garbage collector to delete the pods 05/17/23 07:03:30.807
May 17 07:03:30.870: INFO: Deleting DaemonSet.extensions daemon-set took: 8.354737ms
May 17 07:03:30.970: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.155373ms
May 17 07:03:33.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 17 07:03:33.176: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 17 07:03:33.180: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1498356"},"items":null}

May 17 07:03:33.184: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1498356"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May 17 07:03:33.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5274" for this suite. 05/17/23 07:03:33.222
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":327,"skipped":6151,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.583 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:03:26.647
    May 17 07:03:26.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename daemonsets 05/17/23 07:03:26.648
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:26.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:26.67
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 05/17/23 07:03:26.714
    STEP: Check that daemon pods launch on every node of the cluster. 05/17/23 07:03:26.72
    May 17 07:03:26.737: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 07:03:26.737: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 07:03:27.752: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 17 07:03:27.752: INFO: Node aks-agentpool-72615086-vmss00000b is running 0 daemon pod, expected 1
    May 17 07:03:28.753: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 17 07:03:28.753: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/17/23 07:03:28.758
    May 17 07:03:28.785: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 07:03:28.785: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 07:03:29.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 17 07:03:29.802: INFO: Node aks-agentpool-72615086-vmss00000c is running 0 daemon pod, expected 1
    May 17 07:03:30.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 17 07:03:30.798: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 05/17/23 07:03:30.798
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/17/23 07:03:30.807
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5274, will wait for the garbage collector to delete the pods 05/17/23 07:03:30.807
    May 17 07:03:30.870: INFO: Deleting DaemonSet.extensions daemon-set took: 8.354737ms
    May 17 07:03:30.970: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.155373ms
    May 17 07:03:33.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 17 07:03:33.176: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 17 07:03:33.180: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1498356"},"items":null}

    May 17 07:03:33.184: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1498356"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May 17 07:03:33.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5274" for this suite. 05/17/23 07:03:33.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:03:33.231
May 17 07:03:33.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 07:03:33.231
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:33.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:33.254
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 05/17/23 07:03:33.257
May 17 07:03:33.272: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-4f682de0-7f16-4099-8bad-a9f4769adbbb" in namespace "emptydir-7720" to be "running"
May 17 07:03:33.278: INFO: Pod "pod-sharedvolume-4f682de0-7f16-4099-8bad-a9f4769adbbb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.171161ms
May 17 07:03:35.283: INFO: Pod "pod-sharedvolume-4f682de0-7f16-4099-8bad-a9f4769adbbb": Phase="Running", Reason="", readiness=false. Elapsed: 2.01079573s
May 17 07:03:35.283: INFO: Pod "pod-sharedvolume-4f682de0-7f16-4099-8bad-a9f4769adbbb" satisfied condition "running"
STEP: Reading file content from the nginx-container 05/17/23 07:03:35.283
May 17 07:03:35.283: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7720 PodName:pod-sharedvolume-4f682de0-7f16-4099-8bad-a9f4769adbbb ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 07:03:35.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 07:03:35.284: INFO: ExecWithOptions: Clientset creation
May 17 07:03:35.284: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/emptydir-7720/pods/pod-sharedvolume-4f682de0-7f16-4099-8bad-a9f4769adbbb/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
May 17 07:03:35.410: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 07:03:35.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7720" for this suite. 05/17/23 07:03:35.418
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":328,"skipped":6161,"failed":0}
------------------------------
â€¢ [2.196 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:03:33.231
    May 17 07:03:33.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 07:03:33.231
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:33.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:33.254
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 05/17/23 07:03:33.257
    May 17 07:03:33.272: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-4f682de0-7f16-4099-8bad-a9f4769adbbb" in namespace "emptydir-7720" to be "running"
    May 17 07:03:33.278: INFO: Pod "pod-sharedvolume-4f682de0-7f16-4099-8bad-a9f4769adbbb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.171161ms
    May 17 07:03:35.283: INFO: Pod "pod-sharedvolume-4f682de0-7f16-4099-8bad-a9f4769adbbb": Phase="Running", Reason="", readiness=false. Elapsed: 2.01079573s
    May 17 07:03:35.283: INFO: Pod "pod-sharedvolume-4f682de0-7f16-4099-8bad-a9f4769adbbb" satisfied condition "running"
    STEP: Reading file content from the nginx-container 05/17/23 07:03:35.283
    May 17 07:03:35.283: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7720 PodName:pod-sharedvolume-4f682de0-7f16-4099-8bad-a9f4769adbbb ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 07:03:35.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 07:03:35.284: INFO: ExecWithOptions: Clientset creation
    May 17 07:03:35.284: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/emptydir-7720/pods/pod-sharedvolume-4f682de0-7f16-4099-8bad-a9f4769adbbb/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    May 17 07:03:35.410: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 07:03:35.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7720" for this suite. 05/17/23 07:03:35.418
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:03:35.427
May 17 07:03:35.427: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename ingress 05/17/23 07:03:35.428
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:35.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:35.45
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 05/17/23 07:03:35.453
STEP: getting /apis/networking.k8s.io 05/17/23 07:03:35.457
STEP: getting /apis/networking.k8s.iov1 05/17/23 07:03:35.458
STEP: creating 05/17/23 07:03:35.459
STEP: getting 05/17/23 07:03:35.495
STEP: listing 05/17/23 07:03:35.501
STEP: watching 05/17/23 07:03:35.506
May 17 07:03:35.507: INFO: starting watch
STEP: cluster-wide listing 05/17/23 07:03:35.508
STEP: cluster-wide watching 05/17/23 07:03:35.513
May 17 07:03:35.513: INFO: starting watch
STEP: patching 05/17/23 07:03:35.515
STEP: updating 05/17/23 07:03:35.537
May 17 07:03:35.550: INFO: waiting for watch events with expected annotations
May 17 07:03:35.550: INFO: saw patched and updated annotations
STEP: patching /status 05/17/23 07:03:35.55
STEP: updating /status 05/17/23 07:03:35.556
STEP: get /status 05/17/23 07:03:35.567
STEP: deleting 05/17/23 07:03:35.571
STEP: deleting a collection 05/17/23 07:03:35.589
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
May 17 07:03:35.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3210" for this suite. 05/17/23 07:03:35.613
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":329,"skipped":6163,"failed":0}
------------------------------
â€¢ [0.195 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:03:35.427
    May 17 07:03:35.427: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename ingress 05/17/23 07:03:35.428
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:35.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:35.45
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 05/17/23 07:03:35.453
    STEP: getting /apis/networking.k8s.io 05/17/23 07:03:35.457
    STEP: getting /apis/networking.k8s.iov1 05/17/23 07:03:35.458
    STEP: creating 05/17/23 07:03:35.459
    STEP: getting 05/17/23 07:03:35.495
    STEP: listing 05/17/23 07:03:35.501
    STEP: watching 05/17/23 07:03:35.506
    May 17 07:03:35.507: INFO: starting watch
    STEP: cluster-wide listing 05/17/23 07:03:35.508
    STEP: cluster-wide watching 05/17/23 07:03:35.513
    May 17 07:03:35.513: INFO: starting watch
    STEP: patching 05/17/23 07:03:35.515
    STEP: updating 05/17/23 07:03:35.537
    May 17 07:03:35.550: INFO: waiting for watch events with expected annotations
    May 17 07:03:35.550: INFO: saw patched and updated annotations
    STEP: patching /status 05/17/23 07:03:35.55
    STEP: updating /status 05/17/23 07:03:35.556
    STEP: get /status 05/17/23 07:03:35.567
    STEP: deleting 05/17/23 07:03:35.571
    STEP: deleting a collection 05/17/23 07:03:35.589
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    May 17 07:03:35.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-3210" for this suite. 05/17/23 07:03:35.613
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:03:35.622
May 17 07:03:35.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename gc 05/17/23 07:03:35.622
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:35.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:35.642
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 05/17/23 07:03:35.645
STEP: delete the rc 05/17/23 07:03:40.658
STEP: wait for all pods to be garbage collected 05/17/23 07:03:40.666
STEP: Gathering metrics 05/17/23 07:03:45.677
W0517 07:03:45.688731      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 17 07:03:45.688: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May 17 07:03:45.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9017" for this suite. 05/17/23 07:03:45.695
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":330,"skipped":6164,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.082 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:03:35.622
    May 17 07:03:35.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename gc 05/17/23 07:03:35.622
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:35.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:35.642
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 05/17/23 07:03:35.645
    STEP: delete the rc 05/17/23 07:03:40.658
    STEP: wait for all pods to be garbage collected 05/17/23 07:03:40.666
    STEP: Gathering metrics 05/17/23 07:03:45.677
    W0517 07:03:45.688731      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May 17 07:03:45.688: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May 17 07:03:45.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9017" for this suite. 05/17/23 07:03:45.695
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:03:45.704
May 17 07:03:45.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename services 05/17/23 07:03:45.705
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:45.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:45.738
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-3668 05/17/23 07:03:45.742
STEP: creating service affinity-nodeport-transition in namespace services-3668 05/17/23 07:03:45.742
STEP: creating replication controller affinity-nodeport-transition in namespace services-3668 05/17/23 07:03:45.761
I0517 07:03:45.767451      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3668, replica count: 3
I0517 07:03:48.818579      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 07:03:48.835: INFO: Creating new exec pod
May 17 07:03:48.848: INFO: Waiting up to 5m0s for pod "execpod-affinityw7hx5" in namespace "services-3668" to be "running"
May 17 07:03:48.855: INFO: Pod "execpod-affinityw7hx5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.607958ms
May 17 07:03:50.862: INFO: Pod "execpod-affinityw7hx5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013980847s
May 17 07:03:50.862: INFO: Pod "execpod-affinityw7hx5" satisfied condition "running"
May 17 07:03:51.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-3668 exec execpod-affinityw7hx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
May 17 07:03:52.074: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 17 07:03:52.074: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 07:03:52.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-3668 exec execpod-affinityw7hx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.97.183 80'
May 17 07:03:52.267: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.97.183 80\nConnection to 10.0.97.183 80 port [tcp/http] succeeded!\n"
May 17 07:03:52.267: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 07:03:52.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-3668 exec execpod-affinityw7hx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.6 30541'
May 17 07:03:52.460: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.6 30541\nConnection to 10.224.0.6 30541 port [tcp/*] succeeded!\n"
May 17 07:03:52.460: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 07:03:52.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-3668 exec execpod-affinityw7hx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.5 30541'
May 17 07:03:52.646: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.5 30541\nConnection to 10.224.0.5 30541 port [tcp/*] succeeded!\n"
May 17 07:03:52.646: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 17 07:03:52.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-3668 exec execpod-affinityw7hx5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.224.0.4:30541/ ; done'
May 17 07:03:52.928: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n"
May 17 07:03:52.928: INFO: stdout: "\naffinity-nodeport-transition-s9lzl\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-z95wl\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-s9lzl\naffinity-nodeport-transition-s9lzl\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-z95wl\naffinity-nodeport-transition-z95wl\naffinity-nodeport-transition-s9lzl\naffinity-nodeport-transition-s9lzl\naffinity-nodeport-transition-z95wl\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9"
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-s9lzl
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-z95wl
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-s9lzl
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-s9lzl
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-z95wl
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-z95wl
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-s9lzl
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-s9lzl
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-z95wl
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:52.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-3668 exec execpod-affinityw7hx5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.224.0.4:30541/ ; done'
May 17 07:03:53.217: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n"
May 17 07:03:53.217: INFO: stdout: "\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9"
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
May 17 07:03:53.217: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3668, will wait for the garbage collector to delete the pods 05/17/23 07:03:53.23
May 17 07:03:53.296: INFO: Deleting ReplicationController affinity-nodeport-transition took: 10.765996ms
May 17 07:03:53.397: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.037121ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May 17 07:03:55.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3668" for this suite. 05/17/23 07:03:55.542
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":331,"skipped":6167,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.846 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:03:45.704
    May 17 07:03:45.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename services 05/17/23 07:03:45.705
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:45.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:45.738
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-3668 05/17/23 07:03:45.742
    STEP: creating service affinity-nodeport-transition in namespace services-3668 05/17/23 07:03:45.742
    STEP: creating replication controller affinity-nodeport-transition in namespace services-3668 05/17/23 07:03:45.761
    I0517 07:03:45.767451      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3668, replica count: 3
    I0517 07:03:48.818579      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 17 07:03:48.835: INFO: Creating new exec pod
    May 17 07:03:48.848: INFO: Waiting up to 5m0s for pod "execpod-affinityw7hx5" in namespace "services-3668" to be "running"
    May 17 07:03:48.855: INFO: Pod "execpod-affinityw7hx5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.607958ms
    May 17 07:03:50.862: INFO: Pod "execpod-affinityw7hx5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013980847s
    May 17 07:03:50.862: INFO: Pod "execpod-affinityw7hx5" satisfied condition "running"
    May 17 07:03:51.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-3668 exec execpod-affinityw7hx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    May 17 07:03:52.074: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    May 17 07:03:52.074: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 07:03:52.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-3668 exec execpod-affinityw7hx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.97.183 80'
    May 17 07:03:52.267: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.97.183 80\nConnection to 10.0.97.183 80 port [tcp/http] succeeded!\n"
    May 17 07:03:52.267: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 07:03:52.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-3668 exec execpod-affinityw7hx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.6 30541'
    May 17 07:03:52.460: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.6 30541\nConnection to 10.224.0.6 30541 port [tcp/*] succeeded!\n"
    May 17 07:03:52.460: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 07:03:52.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-3668 exec execpod-affinityw7hx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.224.0.5 30541'
    May 17 07:03:52.646: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.224.0.5 30541\nConnection to 10.224.0.5 30541 port [tcp/*] succeeded!\n"
    May 17 07:03:52.646: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May 17 07:03:52.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-3668 exec execpod-affinityw7hx5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.224.0.4:30541/ ; done'
    May 17 07:03:52.928: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n"
    May 17 07:03:52.928: INFO: stdout: "\naffinity-nodeport-transition-s9lzl\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-z95wl\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-s9lzl\naffinity-nodeport-transition-s9lzl\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-z95wl\naffinity-nodeport-transition-z95wl\naffinity-nodeport-transition-s9lzl\naffinity-nodeport-transition-s9lzl\naffinity-nodeport-transition-z95wl\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9"
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-s9lzl
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-z95wl
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-s9lzl
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-s9lzl
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-z95wl
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-z95wl
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-s9lzl
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-s9lzl
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-z95wl
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:52.928: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:52.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=services-3668 exec execpod-affinityw7hx5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.224.0.4:30541/ ; done'
    May 17 07:03:53.217: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.224.0.4:30541/\n"
    May 17 07:03:53.217: INFO: stdout: "\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9\naffinity-nodeport-transition-dp9z9"
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Received response from host: affinity-nodeport-transition-dp9z9
    May 17 07:03:53.217: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3668, will wait for the garbage collector to delete the pods 05/17/23 07:03:53.23
    May 17 07:03:53.296: INFO: Deleting ReplicationController affinity-nodeport-transition took: 10.765996ms
    May 17 07:03:53.397: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.037121ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May 17 07:03:55.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3668" for this suite. 05/17/23 07:03:55.542
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:03:55.551
May 17 07:03:55.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename watch 05/17/23 07:03:55.552
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:55.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:55.578
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 05/17/23 07:03:55.582
STEP: starting a background goroutine to produce watch events 05/17/23 07:03:55.586
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/17/23 07:03:55.586
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May 17 07:03:58.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4626" for this suite. 05/17/23 07:03:58.411
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":332,"skipped":6197,"failed":0}
------------------------------
â€¢ [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:03:55.551
    May 17 07:03:55.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename watch 05/17/23 07:03:55.552
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:55.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:55.578
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 05/17/23 07:03:55.582
    STEP: starting a background goroutine to produce watch events 05/17/23 07:03:55.586
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/17/23 07:03:55.586
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May 17 07:03:58.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4626" for this suite. 05/17/23 07:03:58.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:03:58.462
May 17 07:03:58.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pods 05/17/23 07:03:58.463
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:58.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:58.486
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
May 17 07:03:58.505: INFO: Waiting up to 5m0s for pod "server-envvars-c1925d62-f066-4674-b7ed-9657d1166ca5" in namespace "pods-9606" to be "running and ready"
May 17 07:03:58.510: INFO: Pod "server-envvars-c1925d62-f066-4674-b7ed-9657d1166ca5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.570467ms
May 17 07:03:58.510: INFO: The phase of Pod server-envvars-c1925d62-f066-4674-b7ed-9657d1166ca5 is Pending, waiting for it to be Running (with Ready = true)
May 17 07:04:00.516: INFO: Pod "server-envvars-c1925d62-f066-4674-b7ed-9657d1166ca5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010868609s
May 17 07:04:00.516: INFO: The phase of Pod server-envvars-c1925d62-f066-4674-b7ed-9657d1166ca5 is Running (Ready = true)
May 17 07:04:00.516: INFO: Pod "server-envvars-c1925d62-f066-4674-b7ed-9657d1166ca5" satisfied condition "running and ready"
May 17 07:04:00.549: INFO: Waiting up to 5m0s for pod "client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c" in namespace "pods-9606" to be "Succeeded or Failed"
May 17 07:04:00.554: INFO: Pod "client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.836674ms
May 17 07:04:02.560: INFO: Pod "client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c": Phase="Running", Reason="", readiness=false. Elapsed: 2.011550082s
May 17 07:04:04.560: INFO: Pod "client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010817093s
STEP: Saw pod success 05/17/23 07:04:04.56
May 17 07:04:04.560: INFO: Pod "client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c" satisfied condition "Succeeded or Failed"
May 17 07:04:04.565: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c container env3cont: <nil>
STEP: delete the pod 05/17/23 07:04:04.582
May 17 07:04:04.601: INFO: Waiting for pod client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c to disappear
May 17 07:04:04.606: INFO: Pod client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May 17 07:04:04.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9606" for this suite. 05/17/23 07:04:04.616
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":333,"skipped":6209,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.163 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:03:58.462
    May 17 07:03:58.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pods 05/17/23 07:03:58.463
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:03:58.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:03:58.486
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    May 17 07:03:58.505: INFO: Waiting up to 5m0s for pod "server-envvars-c1925d62-f066-4674-b7ed-9657d1166ca5" in namespace "pods-9606" to be "running and ready"
    May 17 07:03:58.510: INFO: Pod "server-envvars-c1925d62-f066-4674-b7ed-9657d1166ca5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.570467ms
    May 17 07:03:58.510: INFO: The phase of Pod server-envvars-c1925d62-f066-4674-b7ed-9657d1166ca5 is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:04:00.516: INFO: Pod "server-envvars-c1925d62-f066-4674-b7ed-9657d1166ca5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010868609s
    May 17 07:04:00.516: INFO: The phase of Pod server-envvars-c1925d62-f066-4674-b7ed-9657d1166ca5 is Running (Ready = true)
    May 17 07:04:00.516: INFO: Pod "server-envvars-c1925d62-f066-4674-b7ed-9657d1166ca5" satisfied condition "running and ready"
    May 17 07:04:00.549: INFO: Waiting up to 5m0s for pod "client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c" in namespace "pods-9606" to be "Succeeded or Failed"
    May 17 07:04:00.554: INFO: Pod "client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.836674ms
    May 17 07:04:02.560: INFO: Pod "client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c": Phase="Running", Reason="", readiness=false. Elapsed: 2.011550082s
    May 17 07:04:04.560: INFO: Pod "client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010817093s
    STEP: Saw pod success 05/17/23 07:04:04.56
    May 17 07:04:04.560: INFO: Pod "client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c" satisfied condition "Succeeded or Failed"
    May 17 07:04:04.565: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c container env3cont: <nil>
    STEP: delete the pod 05/17/23 07:04:04.582
    May 17 07:04:04.601: INFO: Waiting for pod client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c to disappear
    May 17 07:04:04.606: INFO: Pod client-envvars-83f2db2f-89dc-41dc-b220-944177bbf33c no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May 17 07:04:04.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9606" for this suite. 05/17/23 07:04:04.616
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:04:04.625
May 17 07:04:04.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename deployment 05/17/23 07:04:04.626
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:04.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:04.651
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
May 17 07:04:04.655: INFO: Creating simple deployment test-new-deployment
May 17 07:04:04.673: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 05/17/23 07:04:06.689
STEP: updating a scale subresource 05/17/23 07:04:06.693
STEP: verifying the deployment Spec.Replicas was modified 05/17/23 07:04:06.698
STEP: Patch a scale subresource 05/17/23 07:04:06.702
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 07:04:06.718: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-7090  8c6d297e-4aa5-4f16-b844-7e4468ac7c5c 1498992 3 2023-05-17 07:04:04 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-17 07:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c19c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 07:04:06 +0000 UTC,LastTransitionTime:2023-05-17 07:04:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-05-17 07:04:06 +0000 UTC,LastTransitionTime:2023-05-17 07:04:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 17 07:04:06.722: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-7090  91a0c7a7-198c-4d91-b12b-9455f83701a8 1498993 2 2023-05-17 07:04:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 8c6d297e-4aa5-4f16-b844-7e4468ac7c5c 0xc00829b627 0xc00829b628}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c6d297e-4aa5-4f16-b844-7e4468ac7c5c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:04:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00829b6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 07:04:06.734: INFO: Pod "test-new-deployment-845c8977d9-l9lmn" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-l9lmn test-new-deployment-845c8977d9- deployment-7090  02ee3feb-52b2-4bf5-a9c3-e79b480ea34a 1498986 0 2023-05-17 07:04:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 91a0c7a7-198c-4d91-b12b-9455f83701a8 0xc00829ba97 0xc00829ba98}] [] [{kube-controller-manager Update v1 2023-05-17 07:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91a0c7a7-198c-4d91-b12b-9455f83701a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j5z8m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j5z8m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:04:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:04:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:04:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.123,StartTime:2023-05-17 07:04:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:04:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c748a51d6a714ab2aae3270e4dbc3d5315955d44a5b3a554c1c4810db1e72780,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 07:04:06.734: INFO: Pod "test-new-deployment-845c8977d9-rxfk4" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-rxfk4 test-new-deployment-845c8977d9- deployment-7090  3d9db27d-b35d-49ff-b1dc-60e2120ae531 1498996 0 2023-05-17 07:04:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 91a0c7a7-198c-4d91-b12b-9455f83701a8 0xc00829bcb7 0xc00829bcb8}] [] [{kube-controller-manager Update v1 2023-05-17 07:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91a0c7a7-198c-4d91-b12b-9455f83701a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hvtpj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hvtpj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:04:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May 17 07:04:06.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7090" for this suite. 05/17/23 07:04:06.746
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":334,"skipped":6211,"failed":0}
------------------------------
â€¢ [2.141 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:04:04.625
    May 17 07:04:04.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename deployment 05/17/23 07:04:04.626
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:04.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:04.651
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    May 17 07:04:04.655: INFO: Creating simple deployment test-new-deployment
    May 17 07:04:04.673: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 05/17/23 07:04:06.689
    STEP: updating a scale subresource 05/17/23 07:04:06.693
    STEP: verifying the deployment Spec.Replicas was modified 05/17/23 07:04:06.698
    STEP: Patch a scale subresource 05/17/23 07:04:06.702
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 07:04:06.718: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-7090  8c6d297e-4aa5-4f16-b844-7e4468ac7c5c 1498992 3 2023-05-17 07:04:04 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-17 07:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c19c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-17 07:04:06 +0000 UTC,LastTransitionTime:2023-05-17 07:04:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-05-17 07:04:06 +0000 UTC,LastTransitionTime:2023-05-17 07:04:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 17 07:04:06.722: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-7090  91a0c7a7-198c-4d91-b12b-9455f83701a8 1498993 2 2023-05-17 07:04:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 8c6d297e-4aa5-4f16-b844-7e4468ac7c5c 0xc00829b627 0xc00829b628}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c6d297e-4aa5-4f16-b844-7e4468ac7c5c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:04:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00829b6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 17 07:04:06.734: INFO: Pod "test-new-deployment-845c8977d9-l9lmn" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-l9lmn test-new-deployment-845c8977d9- deployment-7090  02ee3feb-52b2-4bf5-a9c3-e79b480ea34a 1498986 0 2023-05-17 07:04:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 91a0c7a7-198c-4d91-b12b-9455f83701a8 0xc00829ba97 0xc00829ba98}] [] [{kube-controller-manager Update v1 2023-05-17 07:04:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91a0c7a7-198c-4d91-b12b-9455f83701a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:04:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j5z8m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j5z8m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:04:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:04:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:04:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:04:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.123,StartTime:2023-05-17 07:04:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:04:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c748a51d6a714ab2aae3270e4dbc3d5315955d44a5b3a554c1c4810db1e72780,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 07:04:06.734: INFO: Pod "test-new-deployment-845c8977d9-rxfk4" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-rxfk4 test-new-deployment-845c8977d9- deployment-7090  3d9db27d-b35d-49ff-b1dc-60e2120ae531 1498996 0 2023-05-17 07:04:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 91a0c7a7-198c-4d91-b12b-9455f83701a8 0xc00829bcb7 0xc00829bcb8}] [] [{kube-controller-manager Update v1 2023-05-17 07:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91a0c7a7-198c-4d91-b12b-9455f83701a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hvtpj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hvtpj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:04:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May 17 07:04:06.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7090" for this suite. 05/17/23 07:04:06.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:04:06.767
May 17 07:04:06.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename discovery 05/17/23 07:04:06.767
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:06.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:06.79
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 05/17/23 07:04:06.795
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
May 17 07:04:06.984: INFO: Checking APIGroup: apiregistration.k8s.io
May 17 07:04:06.986: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 17 07:04:06.986: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
May 17 07:04:06.986: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 17 07:04:06.986: INFO: Checking APIGroup: apps
May 17 07:04:06.987: INFO: PreferredVersion.GroupVersion: apps/v1
May 17 07:04:06.987: INFO: Versions found [{apps/v1 v1}]
May 17 07:04:06.987: INFO: apps/v1 matches apps/v1
May 17 07:04:06.987: INFO: Checking APIGroup: events.k8s.io
May 17 07:04:06.988: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 17 07:04:06.988: INFO: Versions found [{events.k8s.io/v1 v1}]
May 17 07:04:06.988: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 17 07:04:06.988: INFO: Checking APIGroup: authentication.k8s.io
May 17 07:04:06.990: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 17 07:04:06.990: INFO: Versions found [{authentication.k8s.io/v1 v1}]
May 17 07:04:06.990: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 17 07:04:06.990: INFO: Checking APIGroup: authorization.k8s.io
May 17 07:04:06.991: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 17 07:04:06.991: INFO: Versions found [{authorization.k8s.io/v1 v1}]
May 17 07:04:06.991: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 17 07:04:06.991: INFO: Checking APIGroup: autoscaling
May 17 07:04:06.993: INFO: PreferredVersion.GroupVersion: autoscaling/v2
May 17 07:04:06.993: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
May 17 07:04:06.993: INFO: autoscaling/v2 matches autoscaling/v2
May 17 07:04:06.993: INFO: Checking APIGroup: batch
May 17 07:04:06.994: INFO: PreferredVersion.GroupVersion: batch/v1
May 17 07:04:06.994: INFO: Versions found [{batch/v1 v1}]
May 17 07:04:06.994: INFO: batch/v1 matches batch/v1
May 17 07:04:06.994: INFO: Checking APIGroup: certificates.k8s.io
May 17 07:04:06.995: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 17 07:04:06.995: INFO: Versions found [{certificates.k8s.io/v1 v1}]
May 17 07:04:06.995: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 17 07:04:06.995: INFO: Checking APIGroup: networking.k8s.io
May 17 07:04:06.997: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 17 07:04:06.997: INFO: Versions found [{networking.k8s.io/v1 v1}]
May 17 07:04:06.997: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 17 07:04:06.997: INFO: Checking APIGroup: policy
May 17 07:04:06.998: INFO: PreferredVersion.GroupVersion: policy/v1
May 17 07:04:06.998: INFO: Versions found [{policy/v1 v1}]
May 17 07:04:06.998: INFO: policy/v1 matches policy/v1
May 17 07:04:06.998: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 17 07:04:06.999: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 17 07:04:06.999: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
May 17 07:04:06.999: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 17 07:04:06.999: INFO: Checking APIGroup: storage.k8s.io
May 17 07:04:07.001: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 17 07:04:07.001: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 17 07:04:07.001: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 17 07:04:07.001: INFO: Checking APIGroup: admissionregistration.k8s.io
May 17 07:04:07.002: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 17 07:04:07.002: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
May 17 07:04:07.002: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 17 07:04:07.002: INFO: Checking APIGroup: apiextensions.k8s.io
May 17 07:04:07.004: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 17 07:04:07.004: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
May 17 07:04:07.004: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 17 07:04:07.004: INFO: Checking APIGroup: scheduling.k8s.io
May 17 07:04:07.005: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 17 07:04:07.005: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
May 17 07:04:07.005: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 17 07:04:07.005: INFO: Checking APIGroup: coordination.k8s.io
May 17 07:04:07.006: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 17 07:04:07.006: INFO: Versions found [{coordination.k8s.io/v1 v1}]
May 17 07:04:07.006: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 17 07:04:07.006: INFO: Checking APIGroup: node.k8s.io
May 17 07:04:07.008: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 17 07:04:07.008: INFO: Versions found [{node.k8s.io/v1 v1}]
May 17 07:04:07.008: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 17 07:04:07.008: INFO: Checking APIGroup: discovery.k8s.io
May 17 07:04:07.009: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May 17 07:04:07.009: INFO: Versions found [{discovery.k8s.io/v1 v1}]
May 17 07:04:07.009: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May 17 07:04:07.009: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 17 07:04:07.011: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
May 17 07:04:07.011: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May 17 07:04:07.011: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
May 17 07:04:07.011: INFO: Checking APIGroup: acme.cert-manager.io
May 17 07:04:07.012: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
May 17 07:04:07.012: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
May 17 07:04:07.012: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
May 17 07:04:07.012: INFO: Checking APIGroup: cert-manager.io
May 17 07:04:07.014: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
May 17 07:04:07.014: INFO: Versions found [{cert-manager.io/v1 v1}]
May 17 07:04:07.014: INFO: cert-manager.io/v1 matches cert-manager.io/v1
May 17 07:04:07.014: INFO: Checking APIGroup: kubevirt.io
May 17 07:04:07.015: INFO: PreferredVersion.GroupVersion: kubevirt.io/v1
May 17 07:04:07.015: INFO: Versions found [{kubevirt.io/v1 v1} {kubevirt.io/v1alpha3 v1alpha3}]
May 17 07:04:07.015: INFO: kubevirt.io/v1 matches kubevirt.io/v1
May 17 07:04:07.015: INFO: Checking APIGroup: snapshot.storage.k8s.io
May 17 07:04:07.017: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
May 17 07:04:07.017: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
May 17 07:04:07.017: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
May 17 07:04:07.017: INFO: Checking APIGroup: subresources.kubevirt.io
May 17 07:04:07.018: INFO: PreferredVersion.GroupVersion: subresources.kubevirt.io/v1
May 17 07:04:07.018: INFO: Versions found [{subresources.kubevirt.io/v1 v1} {subresources.kubevirt.io/v1alpha3 v1alpha3}]
May 17 07:04:07.018: INFO: subresources.kubevirt.io/v1 matches subresources.kubevirt.io/v1
May 17 07:04:07.018: INFO: Checking APIGroup: triliovault.trilio.io
May 17 07:04:07.019: INFO: PreferredVersion.GroupVersion: triliovault.trilio.io/v1
May 17 07:04:07.019: INFO: Versions found [{triliovault.trilio.io/v1 v1}]
May 17 07:04:07.019: INFO: triliovault.trilio.io/v1 matches triliovault.trilio.io/v1
May 17 07:04:07.019: INFO: Checking APIGroup: clone.kubevirt.io
May 17 07:04:07.021: INFO: PreferredVersion.GroupVersion: clone.kubevirt.io/v1alpha1
May 17 07:04:07.021: INFO: Versions found [{clone.kubevirt.io/v1alpha1 v1alpha1}]
May 17 07:04:07.021: INFO: clone.kubevirt.io/v1alpha1 matches clone.kubevirt.io/v1alpha1
May 17 07:04:07.021: INFO: Checking APIGroup: cluster.spectrocloud.com
May 17 07:04:07.022: INFO: PreferredVersion.GroupVersion: cluster.spectrocloud.com/v1alpha1
May 17 07:04:07.022: INFO: Versions found [{cluster.spectrocloud.com/v1alpha1 v1alpha1}]
May 17 07:04:07.022: INFO: cluster.spectrocloud.com/v1alpha1 matches cluster.spectrocloud.com/v1alpha1
May 17 07:04:07.022: INFO: Checking APIGroup: export.kubevirt.io
May 17 07:04:07.024: INFO: PreferredVersion.GroupVersion: export.kubevirt.io/v1alpha1
May 17 07:04:07.024: INFO: Versions found [{export.kubevirt.io/v1alpha1 v1alpha1}]
May 17 07:04:07.024: INFO: export.kubevirt.io/v1alpha1 matches export.kubevirt.io/v1alpha1
May 17 07:04:07.024: INFO: Checking APIGroup: instancetype.kubevirt.io
May 17 07:04:07.026: INFO: PreferredVersion.GroupVersion: instancetype.kubevirt.io/v1alpha2
May 17 07:04:07.026: INFO: Versions found [{instancetype.kubevirt.io/v1alpha2 v1alpha2} {instancetype.kubevirt.io/v1alpha1 v1alpha1}]
May 17 07:04:07.026: INFO: instancetype.kubevirt.io/v1alpha2 matches instancetype.kubevirt.io/v1alpha2
May 17 07:04:07.026: INFO: Checking APIGroup: migrations.kubevirt.io
May 17 07:04:07.027: INFO: PreferredVersion.GroupVersion: migrations.kubevirt.io/v1alpha1
May 17 07:04:07.027: INFO: Versions found [{migrations.kubevirt.io/v1alpha1 v1alpha1}]
May 17 07:04:07.027: INFO: migrations.kubevirt.io/v1alpha1 matches migrations.kubevirt.io/v1alpha1
May 17 07:04:07.027: INFO: Checking APIGroup: pool.kubevirt.io
May 17 07:04:07.029: INFO: PreferredVersion.GroupVersion: pool.kubevirt.io/v1alpha1
May 17 07:04:07.029: INFO: Versions found [{pool.kubevirt.io/v1alpha1 v1alpha1}]
May 17 07:04:07.029: INFO: pool.kubevirt.io/v1alpha1 matches pool.kubevirt.io/v1alpha1
May 17 07:04:07.029: INFO: Checking APIGroup: snapshot.kubevirt.io
May 17 07:04:07.031: INFO: PreferredVersion.GroupVersion: snapshot.kubevirt.io/v1alpha1
May 17 07:04:07.031: INFO: Versions found [{snapshot.kubevirt.io/v1alpha1 v1alpha1}]
May 17 07:04:07.031: INFO: snapshot.kubevirt.io/v1alpha1 matches snapshot.kubevirt.io/v1alpha1
May 17 07:04:07.031: INFO: Checking APIGroup: cdi.kubevirt.io
May 17 07:04:07.033: INFO: PreferredVersion.GroupVersion: cdi.kubevirt.io/v1beta1
May 17 07:04:07.033: INFO: Versions found [{cdi.kubevirt.io/v1beta1 v1beta1}]
May 17 07:04:07.033: INFO: cdi.kubevirt.io/v1beta1 matches cdi.kubevirt.io/v1beta1
May 17 07:04:07.033: INFO: Checking APIGroup: terraform.core.oam.dev
May 17 07:04:07.034: INFO: PreferredVersion.GroupVersion: terraform.core.oam.dev/v1beta1
May 17 07:04:07.034: INFO: Versions found [{terraform.core.oam.dev/v1beta1 v1beta1}]
May 17 07:04:07.034: INFO: terraform.core.oam.dev/v1beta1 matches terraform.core.oam.dev/v1beta1
May 17 07:04:07.034: INFO: Checking APIGroup: upload.cdi.kubevirt.io
May 17 07:04:07.036: INFO: PreferredVersion.GroupVersion: upload.cdi.kubevirt.io/v1beta1
May 17 07:04:07.036: INFO: Versions found [{upload.cdi.kubevirt.io/v1beta1 v1beta1}]
May 17 07:04:07.036: INFO: upload.cdi.kubevirt.io/v1beta1 matches upload.cdi.kubevirt.io/v1beta1
May 17 07:04:07.036: INFO: Checking APIGroup: metrics.k8s.io
May 17 07:04:07.037: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May 17 07:04:07.037: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May 17 07:04:07.037: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
May 17 07:04:07.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-743" for this suite. 05/17/23 07:04:07.051
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":335,"skipped":6231,"failed":0}
------------------------------
â€¢ [0.297 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:04:06.767
    May 17 07:04:06.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename discovery 05/17/23 07:04:06.767
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:06.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:06.79
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 05/17/23 07:04:06.795
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    May 17 07:04:06.984: INFO: Checking APIGroup: apiregistration.k8s.io
    May 17 07:04:06.986: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    May 17 07:04:06.986: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    May 17 07:04:06.986: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    May 17 07:04:06.986: INFO: Checking APIGroup: apps
    May 17 07:04:06.987: INFO: PreferredVersion.GroupVersion: apps/v1
    May 17 07:04:06.987: INFO: Versions found [{apps/v1 v1}]
    May 17 07:04:06.987: INFO: apps/v1 matches apps/v1
    May 17 07:04:06.987: INFO: Checking APIGroup: events.k8s.io
    May 17 07:04:06.988: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    May 17 07:04:06.988: INFO: Versions found [{events.k8s.io/v1 v1}]
    May 17 07:04:06.988: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    May 17 07:04:06.988: INFO: Checking APIGroup: authentication.k8s.io
    May 17 07:04:06.990: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    May 17 07:04:06.990: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    May 17 07:04:06.990: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    May 17 07:04:06.990: INFO: Checking APIGroup: authorization.k8s.io
    May 17 07:04:06.991: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    May 17 07:04:06.991: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    May 17 07:04:06.991: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    May 17 07:04:06.991: INFO: Checking APIGroup: autoscaling
    May 17 07:04:06.993: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    May 17 07:04:06.993: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    May 17 07:04:06.993: INFO: autoscaling/v2 matches autoscaling/v2
    May 17 07:04:06.993: INFO: Checking APIGroup: batch
    May 17 07:04:06.994: INFO: PreferredVersion.GroupVersion: batch/v1
    May 17 07:04:06.994: INFO: Versions found [{batch/v1 v1}]
    May 17 07:04:06.994: INFO: batch/v1 matches batch/v1
    May 17 07:04:06.994: INFO: Checking APIGroup: certificates.k8s.io
    May 17 07:04:06.995: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    May 17 07:04:06.995: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    May 17 07:04:06.995: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    May 17 07:04:06.995: INFO: Checking APIGroup: networking.k8s.io
    May 17 07:04:06.997: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    May 17 07:04:06.997: INFO: Versions found [{networking.k8s.io/v1 v1}]
    May 17 07:04:06.997: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    May 17 07:04:06.997: INFO: Checking APIGroup: policy
    May 17 07:04:06.998: INFO: PreferredVersion.GroupVersion: policy/v1
    May 17 07:04:06.998: INFO: Versions found [{policy/v1 v1}]
    May 17 07:04:06.998: INFO: policy/v1 matches policy/v1
    May 17 07:04:06.998: INFO: Checking APIGroup: rbac.authorization.k8s.io
    May 17 07:04:06.999: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    May 17 07:04:06.999: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    May 17 07:04:06.999: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    May 17 07:04:06.999: INFO: Checking APIGroup: storage.k8s.io
    May 17 07:04:07.001: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    May 17 07:04:07.001: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    May 17 07:04:07.001: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    May 17 07:04:07.001: INFO: Checking APIGroup: admissionregistration.k8s.io
    May 17 07:04:07.002: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    May 17 07:04:07.002: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    May 17 07:04:07.002: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    May 17 07:04:07.002: INFO: Checking APIGroup: apiextensions.k8s.io
    May 17 07:04:07.004: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    May 17 07:04:07.004: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    May 17 07:04:07.004: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    May 17 07:04:07.004: INFO: Checking APIGroup: scheduling.k8s.io
    May 17 07:04:07.005: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    May 17 07:04:07.005: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    May 17 07:04:07.005: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    May 17 07:04:07.005: INFO: Checking APIGroup: coordination.k8s.io
    May 17 07:04:07.006: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    May 17 07:04:07.006: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    May 17 07:04:07.006: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    May 17 07:04:07.006: INFO: Checking APIGroup: node.k8s.io
    May 17 07:04:07.008: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    May 17 07:04:07.008: INFO: Versions found [{node.k8s.io/v1 v1}]
    May 17 07:04:07.008: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    May 17 07:04:07.008: INFO: Checking APIGroup: discovery.k8s.io
    May 17 07:04:07.009: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    May 17 07:04:07.009: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    May 17 07:04:07.009: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    May 17 07:04:07.009: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    May 17 07:04:07.011: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    May 17 07:04:07.011: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    May 17 07:04:07.011: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    May 17 07:04:07.011: INFO: Checking APIGroup: acme.cert-manager.io
    May 17 07:04:07.012: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
    May 17 07:04:07.012: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
    May 17 07:04:07.012: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
    May 17 07:04:07.012: INFO: Checking APIGroup: cert-manager.io
    May 17 07:04:07.014: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
    May 17 07:04:07.014: INFO: Versions found [{cert-manager.io/v1 v1}]
    May 17 07:04:07.014: INFO: cert-manager.io/v1 matches cert-manager.io/v1
    May 17 07:04:07.014: INFO: Checking APIGroup: kubevirt.io
    May 17 07:04:07.015: INFO: PreferredVersion.GroupVersion: kubevirt.io/v1
    May 17 07:04:07.015: INFO: Versions found [{kubevirt.io/v1 v1} {kubevirt.io/v1alpha3 v1alpha3}]
    May 17 07:04:07.015: INFO: kubevirt.io/v1 matches kubevirt.io/v1
    May 17 07:04:07.015: INFO: Checking APIGroup: snapshot.storage.k8s.io
    May 17 07:04:07.017: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    May 17 07:04:07.017: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    May 17 07:04:07.017: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    May 17 07:04:07.017: INFO: Checking APIGroup: subresources.kubevirt.io
    May 17 07:04:07.018: INFO: PreferredVersion.GroupVersion: subresources.kubevirt.io/v1
    May 17 07:04:07.018: INFO: Versions found [{subresources.kubevirt.io/v1 v1} {subresources.kubevirt.io/v1alpha3 v1alpha3}]
    May 17 07:04:07.018: INFO: subresources.kubevirt.io/v1 matches subresources.kubevirt.io/v1
    May 17 07:04:07.018: INFO: Checking APIGroup: triliovault.trilio.io
    May 17 07:04:07.019: INFO: PreferredVersion.GroupVersion: triliovault.trilio.io/v1
    May 17 07:04:07.019: INFO: Versions found [{triliovault.trilio.io/v1 v1}]
    May 17 07:04:07.019: INFO: triliovault.trilio.io/v1 matches triliovault.trilio.io/v1
    May 17 07:04:07.019: INFO: Checking APIGroup: clone.kubevirt.io
    May 17 07:04:07.021: INFO: PreferredVersion.GroupVersion: clone.kubevirt.io/v1alpha1
    May 17 07:04:07.021: INFO: Versions found [{clone.kubevirt.io/v1alpha1 v1alpha1}]
    May 17 07:04:07.021: INFO: clone.kubevirt.io/v1alpha1 matches clone.kubevirt.io/v1alpha1
    May 17 07:04:07.021: INFO: Checking APIGroup: cluster.spectrocloud.com
    May 17 07:04:07.022: INFO: PreferredVersion.GroupVersion: cluster.spectrocloud.com/v1alpha1
    May 17 07:04:07.022: INFO: Versions found [{cluster.spectrocloud.com/v1alpha1 v1alpha1}]
    May 17 07:04:07.022: INFO: cluster.spectrocloud.com/v1alpha1 matches cluster.spectrocloud.com/v1alpha1
    May 17 07:04:07.022: INFO: Checking APIGroup: export.kubevirt.io
    May 17 07:04:07.024: INFO: PreferredVersion.GroupVersion: export.kubevirt.io/v1alpha1
    May 17 07:04:07.024: INFO: Versions found [{export.kubevirt.io/v1alpha1 v1alpha1}]
    May 17 07:04:07.024: INFO: export.kubevirt.io/v1alpha1 matches export.kubevirt.io/v1alpha1
    May 17 07:04:07.024: INFO: Checking APIGroup: instancetype.kubevirt.io
    May 17 07:04:07.026: INFO: PreferredVersion.GroupVersion: instancetype.kubevirt.io/v1alpha2
    May 17 07:04:07.026: INFO: Versions found [{instancetype.kubevirt.io/v1alpha2 v1alpha2} {instancetype.kubevirt.io/v1alpha1 v1alpha1}]
    May 17 07:04:07.026: INFO: instancetype.kubevirt.io/v1alpha2 matches instancetype.kubevirt.io/v1alpha2
    May 17 07:04:07.026: INFO: Checking APIGroup: migrations.kubevirt.io
    May 17 07:04:07.027: INFO: PreferredVersion.GroupVersion: migrations.kubevirt.io/v1alpha1
    May 17 07:04:07.027: INFO: Versions found [{migrations.kubevirt.io/v1alpha1 v1alpha1}]
    May 17 07:04:07.027: INFO: migrations.kubevirt.io/v1alpha1 matches migrations.kubevirt.io/v1alpha1
    May 17 07:04:07.027: INFO: Checking APIGroup: pool.kubevirt.io
    May 17 07:04:07.029: INFO: PreferredVersion.GroupVersion: pool.kubevirt.io/v1alpha1
    May 17 07:04:07.029: INFO: Versions found [{pool.kubevirt.io/v1alpha1 v1alpha1}]
    May 17 07:04:07.029: INFO: pool.kubevirt.io/v1alpha1 matches pool.kubevirt.io/v1alpha1
    May 17 07:04:07.029: INFO: Checking APIGroup: snapshot.kubevirt.io
    May 17 07:04:07.031: INFO: PreferredVersion.GroupVersion: snapshot.kubevirt.io/v1alpha1
    May 17 07:04:07.031: INFO: Versions found [{snapshot.kubevirt.io/v1alpha1 v1alpha1}]
    May 17 07:04:07.031: INFO: snapshot.kubevirt.io/v1alpha1 matches snapshot.kubevirt.io/v1alpha1
    May 17 07:04:07.031: INFO: Checking APIGroup: cdi.kubevirt.io
    May 17 07:04:07.033: INFO: PreferredVersion.GroupVersion: cdi.kubevirt.io/v1beta1
    May 17 07:04:07.033: INFO: Versions found [{cdi.kubevirt.io/v1beta1 v1beta1}]
    May 17 07:04:07.033: INFO: cdi.kubevirt.io/v1beta1 matches cdi.kubevirt.io/v1beta1
    May 17 07:04:07.033: INFO: Checking APIGroup: terraform.core.oam.dev
    May 17 07:04:07.034: INFO: PreferredVersion.GroupVersion: terraform.core.oam.dev/v1beta1
    May 17 07:04:07.034: INFO: Versions found [{terraform.core.oam.dev/v1beta1 v1beta1}]
    May 17 07:04:07.034: INFO: terraform.core.oam.dev/v1beta1 matches terraform.core.oam.dev/v1beta1
    May 17 07:04:07.034: INFO: Checking APIGroup: upload.cdi.kubevirt.io
    May 17 07:04:07.036: INFO: PreferredVersion.GroupVersion: upload.cdi.kubevirt.io/v1beta1
    May 17 07:04:07.036: INFO: Versions found [{upload.cdi.kubevirt.io/v1beta1 v1beta1}]
    May 17 07:04:07.036: INFO: upload.cdi.kubevirt.io/v1beta1 matches upload.cdi.kubevirt.io/v1beta1
    May 17 07:04:07.036: INFO: Checking APIGroup: metrics.k8s.io
    May 17 07:04:07.037: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    May 17 07:04:07.037: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    May 17 07:04:07.037: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    May 17 07:04:07.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-743" for this suite. 05/17/23 07:04:07.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:04:07.065
May 17 07:04:07.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename runtimeclass 05/17/23 07:04:07.066
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:07.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:07.089
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-7540-delete-me 05/17/23 07:04:07.099
STEP: Waiting for the RuntimeClass to disappear 05/17/23 07:04:07.108
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May 17 07:04:07.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7540" for this suite. 05/17/23 07:04:07.128
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":336,"skipped":6261,"failed":0}
------------------------------
â€¢ [0.073 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:04:07.065
    May 17 07:04:07.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename runtimeclass 05/17/23 07:04:07.066
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:07.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:07.089
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-7540-delete-me 05/17/23 07:04:07.099
    STEP: Waiting for the RuntimeClass to disappear 05/17/23 07:04:07.108
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May 17 07:04:07.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7540" for this suite. 05/17/23 07:04:07.128
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:04:07.138
May 17 07:04:07.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 07:04:07.139
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:07.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:07.166
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 05/17/23 07:04:07.169
May 17 07:04:07.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9" in namespace "downward-api-4387" to be "Succeeded or Failed"
May 17 07:04:07.188: INFO: Pod "downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.755272ms
May 17 07:04:09.195: INFO: Pod "downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012671126s
May 17 07:04:11.195: INFO: Pod "downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012661525s
STEP: Saw pod success 05/17/23 07:04:11.195
May 17 07:04:11.195: INFO: Pod "downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9" satisfied condition "Succeeded or Failed"
May 17 07:04:11.200: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9 container client-container: <nil>
STEP: delete the pod 05/17/23 07:04:11.212
May 17 07:04:11.235: INFO: Waiting for pod downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9 to disappear
May 17 07:04:11.239: INFO: Pod downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May 17 07:04:11.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4387" for this suite. 05/17/23 07:04:11.25
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":337,"skipped":6264,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:04:07.138
    May 17 07:04:07.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 07:04:07.139
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:07.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:07.166
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 05/17/23 07:04:07.169
    May 17 07:04:07.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9" in namespace "downward-api-4387" to be "Succeeded or Failed"
    May 17 07:04:07.188: INFO: Pod "downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.755272ms
    May 17 07:04:09.195: INFO: Pod "downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012671126s
    May 17 07:04:11.195: INFO: Pod "downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012661525s
    STEP: Saw pod success 05/17/23 07:04:11.195
    May 17 07:04:11.195: INFO: Pod "downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9" satisfied condition "Succeeded or Failed"
    May 17 07:04:11.200: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9 container client-container: <nil>
    STEP: delete the pod 05/17/23 07:04:11.212
    May 17 07:04:11.235: INFO: Waiting for pod downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9 to disappear
    May 17 07:04:11.239: INFO: Pod downwardapi-volume-2e07d0e2-3550-44a9-933b-9eb38270aef9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May 17 07:04:11.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4387" for this suite. 05/17/23 07:04:11.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:04:11.259
May 17 07:04:11.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename sched-pred 05/17/23 07:04:11.26
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:11.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:11.286
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May 17 07:04:11.290: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 07:04:11.308: INFO: Waiting for terminating namespaces to be deleted...
May 17 07:04:11.314: INFO: 
Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000b before test
May 17 07:04:11.334: INFO: cdi-apiserver-656bdddf7b-t4q89 from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container cdi-apiserver ready: true, restart count 0
May 17 07:04:11.334: INFO: cdi-deployment-59b94f4bcf-6b8js from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container cdi-controller ready: true, restart count 0
May 17 07:04:11.334: INFO: cdi-operator-7f58c444b8-cnf5v from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container cdi-operator ready: true, restart count 0
May 17 07:04:11.334: INFO: cdi-uploadproxy-7787fffb9b-g7dzg from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
May 17 07:04:11.334: INFO: cert-manager-96f7799d6-csgfn from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container cert-manager-controller ready: true, restart count 0
May 17 07:04:11.334: INFO: cert-manager-cainjector-6f79489644-w5ggx from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
May 17 07:04:11.334: INFO: cert-manager-webhook-7dfbf7bff6-zk657 from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container cert-manager-webhook ready: true, restart count 0
May 17 07:04:11.334: INFO: cluster-management-agent-lite-758b6c7957-74gwr from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container cluster-management-agent-lite ready: true, restart count 0
May 17 07:04:11.334: INFO: palette-lite-controller-manager-6cd9fb7bd-h4j4p from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container manager ready: true, restart count 0
May 17 07:04:11.334: INFO: terraform-controller-848679f679-6zmxw from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container terraform-controller ready: true, restart count 0
May 17 07:04:11.334: INFO: ama-logs-87zkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (2 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container ama-logs ready: true, restart count 0
May 17 07:04:11.334: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
May 17 07:04:11.334: INFO: ama-logs-rs-68c99469fb-bzwjp from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container ama-logs ready: true, restart count 0
May 17 07:04:11.334: INFO: azure-ip-masq-agent-928nn from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 17 07:04:11.334: INFO: cloud-node-manager-x7fjk from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container cloud-node-manager ready: true, restart count 0
May 17 07:04:11.334: INFO: coredns-75bbfcbc66-mkbkm from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container coredns ready: true, restart count 0
May 17 07:04:11.334: INFO: coredns-75bbfcbc66-vjv2g from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container coredns ready: true, restart count 0
May 17 07:04:11.334: INFO: coredns-autoscaler-7879846967-wvdn8 from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container autoscaler ready: true, restart count 0
May 17 07:04:11.334: INFO: csi-azuredisk-node-pccm7 from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container azuredisk ready: true, restart count 0
May 17 07:04:11.334: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 07:04:11.334: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 07:04:11.334: INFO: csi-azurefile-node-bzf7d from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container azurefile ready: true, restart count 0
May 17 07:04:11.334: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 07:04:11.334: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 07:04:11.334: INFO: konnectivity-agent-6859749db4-r746q from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container konnectivity-agent ready: true, restart count 0
May 17 07:04:11.334: INFO: konnectivity-agent-6859749db4-vbj9c from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container konnectivity-agent ready: true, restart count 0
May 17 07:04:11.334: INFO: kube-proxy-mzbkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 07:04:11.334: INFO: virt-api-6c4f849c9d-2jwc2 from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container virt-api ready: true, restart count 0
May 17 07:04:11.334: INFO: virt-api-6c4f849c9d-2x4kv from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container virt-api ready: true, restart count 0
May 17 07:04:11.334: INFO: virt-controller-67b95d99d5-2cnzp from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container virt-controller ready: true, restart count 0
May 17 07:04:11.334: INFO: virt-controller-67b95d99d5-8wv5s from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container virt-controller ready: true, restart count 0
May 17 07:04:11.334: INFO: virt-handler-hnwhl from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container virt-handler ready: true, restart count 0
May 17 07:04:11.334: INFO: virt-operator-798f64bdf6-gx4sg from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container virt-operator ready: true, restart count 0
May 17 07:04:11.334: INFO: virt-operator-798f64bdf6-x5z2b from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container virt-operator ready: true, restart count 0
May 17 07:04:11.334: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-5jttt from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:04:11.334: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 07:04:11.334: INFO: k8s-triliovault-admission-webhook-77b46b96cd-9lksn from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container triliovault-admission-webhook ready: true, restart count 0
May 17 07:04:11.334: INFO: k8s-triliovault-control-plane-6fcbd76b99-8kgx9 from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container triliovault-analyzer ready: true, restart count 0
May 17 07:04:11.334: INFO: 	Container triliovault-control-plane ready: true, restart count 0
May 17 07:04:11.334: INFO: k8s-triliovault-exporter-59c8c8dcdc-kwbxw from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container triliovault-exporter ready: true, restart count 0
May 17 07:04:11.334: INFO: k8s-triliovault-ingress-nginx-controller-76b7885d96-rr66q from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container controller ready: true, restart count 0
May 17 07:04:11.334: INFO: k8s-triliovault-operator-66747bd477-pmljl from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container k8s-triliovault-operator ready: true, restart count 0
May 17 07:04:11.334: INFO: k8s-triliovault-web-75bdc67d7d-l8t7z from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container triliovault-web ready: true, restart count 0
May 17 07:04:11.334: INFO: k8s-triliovault-web-backend-54b448979f-98rfz from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.334: INFO: 	Container triliovault-web-backend ready: true, restart count 0
May 17 07:04:11.334: INFO: 
Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000c before test
May 17 07:04:11.353: INFO: test-new-deployment-845c8977d9-l9lmn from deployment-7090 started at 2023-05-17 07:04:04 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.353: INFO: 	Container httpd ready: true, restart count 0
May 17 07:04:11.353: INFO: ama-logs-xntrk from kube-system started at 2023-05-17 04:32:30 +0000 UTC (2 container statuses recorded)
May 17 07:04:11.353: INFO: 	Container ama-logs ready: true, restart count 0
May 17 07:04:11.353: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
May 17 07:04:11.353: INFO: azure-ip-masq-agent-ns2hc from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.353: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 17 07:04:11.353: INFO: cloud-node-manager-hhrc6 from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.353: INFO: 	Container cloud-node-manager ready: true, restart count 0
May 17 07:04:11.353: INFO: csi-azuredisk-node-dms8v from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
May 17 07:04:11.353: INFO: 	Container azuredisk ready: true, restart count 0
May 17 07:04:11.353: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 07:04:11.353: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 07:04:11.353: INFO: csi-azurefile-node-whbbj from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
May 17 07:04:11.353: INFO: 	Container azurefile ready: true, restart count 0
May 17 07:04:11.353: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 07:04:11.353: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 07:04:11.353: INFO: kube-proxy-lpnpx from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.354: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 07:04:11.354: INFO: metrics-server-85c9977f87-495cd from kube-system started at 2023-05-17 06:19:50 +0000 UTC (2 container statuses recorded)
May 17 07:04:11.354: INFO: 	Container metrics-server ready: true, restart count 0
May 17 07:04:11.354: INFO: 	Container metrics-server-vpa ready: true, restart count 0
May 17 07:04:11.354: INFO: virt-handler-n79f2 from kubevirt started at 2023-05-17 06:11:14 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.354: INFO: 	Container virt-handler ready: true, restart count 0
May 17 07:04:11.354: INFO: sonobuoy from sonobuoy started at 2023-05-17 05:36:06 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.354: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 07:04:11.354: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-6hdss from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 07:04:11.354: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:04:11.354: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 07:04:11.354: INFO: k8s-triliovault-resource-cleaner-28071780-nqvc4 from trilio-system started at 2023-05-17 07:00:00 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.354: INFO: 	Container resource-cleaner ready: false, restart count 0
May 17 07:04:11.354: INFO: 
Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000d before test
May 17 07:04:11.377: INFO: virt-launcher-cirros-vm-x6gh2 from default started at 2023-05-17 05:47:31 +0000 UTC (2 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container compute ready: true, restart count 0
May 17 07:04:11.377: INFO: 	Container volumecontainerdisk ready: true, restart count 0
May 17 07:04:11.377: INFO: test-new-deployment-845c8977d9-rxfk4 from deployment-7090 started at 2023-05-17 07:04:06 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container httpd ready: false, restart count 0
May 17 07:04:11.377: INFO: ama-logs-bmxmm from kube-system started at 2023-05-17 04:32:24 +0000 UTC (2 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container ama-logs ready: true, restart count 0
May 17 07:04:11.377: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
May 17 07:04:11.377: INFO: azure-ip-masq-agent-vm9n2 from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 17 07:04:11.377: INFO: cloud-node-manager-6mv9z from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container cloud-node-manager ready: true, restart count 0
May 17 07:04:11.377: INFO: csi-azuredisk-node-rg26q from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container azuredisk ready: true, restart count 0
May 17 07:04:11.377: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 07:04:11.377: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 07:04:11.377: INFO: csi-azurefile-node-zlpwj from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container azurefile ready: true, restart count 0
May 17 07:04:11.377: INFO: 	Container liveness-probe ready: true, restart count 0
May 17 07:04:11.377: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 17 07:04:11.377: INFO: kube-proxy-nsx8v from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 07:04:11.377: INFO: metrics-server-85c9977f87-st4cg from kube-system started at 2023-05-17 06:19:50 +0000 UTC (2 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container metrics-server ready: true, restart count 0
May 17 07:04:11.377: INFO: 	Container metrics-server-vpa ready: true, restart count 0
May 17 07:04:11.377: INFO: virt-handler-9qq9z from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container virt-handler ready: true, restart count 0
May 17 07:04:11.377: INFO: mysql-qa-64f5b55869-877ll from mysql started at 2023-05-17 04:33:14 +0000 UTC (1 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container mysql-qa ready: true, restart count 0
May 17 07:04:11.377: INFO: sonobuoy-e2e-job-a0b627fa1b44421a from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container e2e ready: true, restart count 0
May 17 07:04:11.377: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:04:11.377: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-tnssv from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
May 17 07:04:11.377: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 07:04:11.377: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 05/17/23 07:04:11.377
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.175fdc575dfb73dc], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 05/17/23 07:04:13.462
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May 17 07:04:14.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1720" for this suite. 05/17/23 07:04:14.47
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":338,"skipped":6277,"failed":0}
------------------------------
â€¢ [3.219 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:04:11.259
    May 17 07:04:11.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename sched-pred 05/17/23 07:04:11.26
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:11.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:11.286
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May 17 07:04:11.290: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May 17 07:04:11.308: INFO: Waiting for terminating namespaces to be deleted...
    May 17 07:04:11.314: INFO: 
    Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000b before test
    May 17 07:04:11.334: INFO: cdi-apiserver-656bdddf7b-t4q89 from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container cdi-apiserver ready: true, restart count 0
    May 17 07:04:11.334: INFO: cdi-deployment-59b94f4bcf-6b8js from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container cdi-controller ready: true, restart count 0
    May 17 07:04:11.334: INFO: cdi-operator-7f58c444b8-cnf5v from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container cdi-operator ready: true, restart count 0
    May 17 07:04:11.334: INFO: cdi-uploadproxy-7787fffb9b-g7dzg from cdi started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container cdi-uploadproxy ready: true, restart count 0
    May 17 07:04:11.334: INFO: cert-manager-96f7799d6-csgfn from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container cert-manager-controller ready: true, restart count 0
    May 17 07:04:11.334: INFO: cert-manager-cainjector-6f79489644-w5ggx from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
    May 17 07:04:11.334: INFO: cert-manager-webhook-7dfbf7bff6-zk657 from cert-manager started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container cert-manager-webhook ready: true, restart count 0
    May 17 07:04:11.334: INFO: cluster-management-agent-lite-758b6c7957-74gwr from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container cluster-management-agent-lite ready: true, restart count 0
    May 17 07:04:11.334: INFO: palette-lite-controller-manager-6cd9fb7bd-h4j4p from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container manager ready: true, restart count 0
    May 17 07:04:11.334: INFO: terraform-controller-848679f679-6zmxw from cluster-6458ddaada42cb67ae77e285 started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container terraform-controller ready: true, restart count 0
    May 17 07:04:11.334: INFO: ama-logs-87zkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (2 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 07:04:11.334: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
    May 17 07:04:11.334: INFO: ama-logs-rs-68c99469fb-bzwjp from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 07:04:11.334: INFO: azure-ip-masq-agent-928nn from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    May 17 07:04:11.334: INFO: cloud-node-manager-x7fjk from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container cloud-node-manager ready: true, restart count 0
    May 17 07:04:11.334: INFO: coredns-75bbfcbc66-mkbkm from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container coredns ready: true, restart count 0
    May 17 07:04:11.334: INFO: coredns-75bbfcbc66-vjv2g from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container coredns ready: true, restart count 0
    May 17 07:04:11.334: INFO: coredns-autoscaler-7879846967-wvdn8 from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container autoscaler ready: true, restart count 0
    May 17 07:04:11.334: INFO: csi-azuredisk-node-pccm7 from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container azuredisk ready: true, restart count 0
    May 17 07:04:11.334: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 07:04:11.334: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 07:04:11.334: INFO: csi-azurefile-node-bzf7d from kube-system started at 2023-05-17 04:32:23 +0000 UTC (3 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container azurefile ready: true, restart count 0
    May 17 07:04:11.334: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 07:04:11.334: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 07:04:11.334: INFO: konnectivity-agent-6859749db4-r746q from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container konnectivity-agent ready: true, restart count 0
    May 17 07:04:11.334: INFO: konnectivity-agent-6859749db4-vbj9c from kube-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container konnectivity-agent ready: true, restart count 0
    May 17 07:04:11.334: INFO: kube-proxy-mzbkj from kube-system started at 2023-05-17 04:32:23 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 07:04:11.334: INFO: virt-api-6c4f849c9d-2jwc2 from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container virt-api ready: true, restart count 0
    May 17 07:04:11.334: INFO: virt-api-6c4f849c9d-2x4kv from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container virt-api ready: true, restart count 0
    May 17 07:04:11.334: INFO: virt-controller-67b95d99d5-2cnzp from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container virt-controller ready: true, restart count 0
    May 17 07:04:11.334: INFO: virt-controller-67b95d99d5-8wv5s from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container virt-controller ready: true, restart count 0
    May 17 07:04:11.334: INFO: virt-handler-hnwhl from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container virt-handler ready: true, restart count 0
    May 17 07:04:11.334: INFO: virt-operator-798f64bdf6-gx4sg from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container virt-operator ready: true, restart count 0
    May 17 07:04:11.334: INFO: virt-operator-798f64bdf6-x5z2b from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container virt-operator ready: true, restart count 0
    May 17 07:04:11.334: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-5jttt from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:04:11.334: INFO: 	Container systemd-logs ready: true, restart count 0
    May 17 07:04:11.334: INFO: k8s-triliovault-admission-webhook-77b46b96cd-9lksn from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container triliovault-admission-webhook ready: true, restart count 0
    May 17 07:04:11.334: INFO: k8s-triliovault-control-plane-6fcbd76b99-8kgx9 from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (2 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container triliovault-analyzer ready: true, restart count 0
    May 17 07:04:11.334: INFO: 	Container triliovault-control-plane ready: true, restart count 0
    May 17 07:04:11.334: INFO: k8s-triliovault-exporter-59c8c8dcdc-kwbxw from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container triliovault-exporter ready: true, restart count 0
    May 17 07:04:11.334: INFO: k8s-triliovault-ingress-nginx-controller-76b7885d96-rr66q from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container controller ready: true, restart count 0
    May 17 07:04:11.334: INFO: k8s-triliovault-operator-66747bd477-pmljl from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container k8s-triliovault-operator ready: true, restart count 0
    May 17 07:04:11.334: INFO: k8s-triliovault-web-75bdc67d7d-l8t7z from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container triliovault-web ready: true, restart count 0
    May 17 07:04:11.334: INFO: k8s-triliovault-web-backend-54b448979f-98rfz from trilio-system started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.334: INFO: 	Container triliovault-web-backend ready: true, restart count 0
    May 17 07:04:11.334: INFO: 
    Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000c before test
    May 17 07:04:11.353: INFO: test-new-deployment-845c8977d9-l9lmn from deployment-7090 started at 2023-05-17 07:04:04 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.353: INFO: 	Container httpd ready: true, restart count 0
    May 17 07:04:11.353: INFO: ama-logs-xntrk from kube-system started at 2023-05-17 04:32:30 +0000 UTC (2 container statuses recorded)
    May 17 07:04:11.353: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 07:04:11.353: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
    May 17 07:04:11.353: INFO: azure-ip-masq-agent-ns2hc from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.353: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    May 17 07:04:11.353: INFO: cloud-node-manager-hhrc6 from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.353: INFO: 	Container cloud-node-manager ready: true, restart count 0
    May 17 07:04:11.353: INFO: csi-azuredisk-node-dms8v from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
    May 17 07:04:11.353: INFO: 	Container azuredisk ready: true, restart count 0
    May 17 07:04:11.353: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 07:04:11.353: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 07:04:11.353: INFO: csi-azurefile-node-whbbj from kube-system started at 2023-05-17 04:32:30 +0000 UTC (3 container statuses recorded)
    May 17 07:04:11.353: INFO: 	Container azurefile ready: true, restart count 0
    May 17 07:04:11.353: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 07:04:11.353: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 07:04:11.353: INFO: kube-proxy-lpnpx from kube-system started at 2023-05-17 04:32:30 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.354: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 07:04:11.354: INFO: metrics-server-85c9977f87-495cd from kube-system started at 2023-05-17 06:19:50 +0000 UTC (2 container statuses recorded)
    May 17 07:04:11.354: INFO: 	Container metrics-server ready: true, restart count 0
    May 17 07:04:11.354: INFO: 	Container metrics-server-vpa ready: true, restart count 0
    May 17 07:04:11.354: INFO: virt-handler-n79f2 from kubevirt started at 2023-05-17 06:11:14 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.354: INFO: 	Container virt-handler ready: true, restart count 0
    May 17 07:04:11.354: INFO: sonobuoy from sonobuoy started at 2023-05-17 05:36:06 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.354: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May 17 07:04:11.354: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-6hdss from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 07:04:11.354: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:04:11.354: INFO: 	Container systemd-logs ready: true, restart count 0
    May 17 07:04:11.354: INFO: k8s-triliovault-resource-cleaner-28071780-nqvc4 from trilio-system started at 2023-05-17 07:00:00 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.354: INFO: 	Container resource-cleaner ready: false, restart count 0
    May 17 07:04:11.354: INFO: 
    Logging pods the apiserver thinks is on node aks-agentpool-72615086-vmss00000d before test
    May 17 07:04:11.377: INFO: virt-launcher-cirros-vm-x6gh2 from default started at 2023-05-17 05:47:31 +0000 UTC (2 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container compute ready: true, restart count 0
    May 17 07:04:11.377: INFO: 	Container volumecontainerdisk ready: true, restart count 0
    May 17 07:04:11.377: INFO: test-new-deployment-845c8977d9-rxfk4 from deployment-7090 started at 2023-05-17 07:04:06 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container httpd ready: false, restart count 0
    May 17 07:04:11.377: INFO: ama-logs-bmxmm from kube-system started at 2023-05-17 04:32:24 +0000 UTC (2 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container ama-logs ready: true, restart count 0
    May 17 07:04:11.377: INFO: 	Container ama-logs-prometheus ready: true, restart count 0
    May 17 07:04:11.377: INFO: azure-ip-masq-agent-vm9n2 from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    May 17 07:04:11.377: INFO: cloud-node-manager-6mv9z from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container cloud-node-manager ready: true, restart count 0
    May 17 07:04:11.377: INFO: csi-azuredisk-node-rg26q from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container azuredisk ready: true, restart count 0
    May 17 07:04:11.377: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 07:04:11.377: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 07:04:11.377: INFO: csi-azurefile-node-zlpwj from kube-system started at 2023-05-17 04:32:24 +0000 UTC (3 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container azurefile ready: true, restart count 0
    May 17 07:04:11.377: INFO: 	Container liveness-probe ready: true, restart count 0
    May 17 07:04:11.377: INFO: 	Container node-driver-registrar ready: true, restart count 0
    May 17 07:04:11.377: INFO: kube-proxy-nsx8v from kube-system started at 2023-05-17 04:32:24 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container kube-proxy ready: true, restart count 0
    May 17 07:04:11.377: INFO: metrics-server-85c9977f87-st4cg from kube-system started at 2023-05-17 06:19:50 +0000 UTC (2 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container metrics-server ready: true, restart count 0
    May 17 07:04:11.377: INFO: 	Container metrics-server-vpa ready: true, restart count 0
    May 17 07:04:11.377: INFO: virt-handler-9qq9z from kubevirt started at 2023-05-17 04:33:10 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container virt-handler ready: true, restart count 0
    May 17 07:04:11.377: INFO: mysql-qa-64f5b55869-877ll from mysql started at 2023-05-17 04:33:14 +0000 UTC (1 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container mysql-qa ready: true, restart count 0
    May 17 07:04:11.377: INFO: sonobuoy-e2e-job-a0b627fa1b44421a from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container e2e ready: true, restart count 0
    May 17 07:04:11.377: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:04:11.377: INFO: sonobuoy-systemd-logs-daemon-set-880aaedda6954705-tnssv from sonobuoy started at 2023-05-17 05:36:07 +0000 UTC (2 container statuses recorded)
    May 17 07:04:11.377: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 17 07:04:11.377: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 05/17/23 07:04:11.377
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.175fdc575dfb73dc], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 05/17/23 07:04:13.462
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May 17 07:04:14.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1720" for this suite. 05/17/23 07:04:14.47
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:04:14.478
May 17 07:04:14.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename emptydir 05/17/23 07:04:14.479
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:14.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:14.501
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 05/17/23 07:04:14.505
May 17 07:04:14.517: INFO: Waiting up to 5m0s for pod "pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568" in namespace "emptydir-5186" to be "Succeeded or Failed"
May 17 07:04:14.522: INFO: Pod "pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568": Phase="Pending", Reason="", readiness=false. Elapsed: 4.922889ms
May 17 07:04:16.528: INFO: Pod "pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010717667s
May 17 07:04:18.530: INFO: Pod "pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012319132s
STEP: Saw pod success 05/17/23 07:04:18.53
May 17 07:04:18.530: INFO: Pod "pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568" satisfied condition "Succeeded or Failed"
May 17 07:04:18.534: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568 container test-container: <nil>
STEP: delete the pod 05/17/23 07:04:18.546
May 17 07:04:18.562: INFO: Waiting for pod pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568 to disappear
May 17 07:04:18.567: INFO: Pod pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May 17 07:04:18.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5186" for this suite. 05/17/23 07:04:18.574
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":339,"skipped":6279,"failed":0}
------------------------------
â€¢ [4.104 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:04:14.478
    May 17 07:04:14.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename emptydir 05/17/23 07:04:14.479
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:14.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:14.501
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/17/23 07:04:14.505
    May 17 07:04:14.517: INFO: Waiting up to 5m0s for pod "pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568" in namespace "emptydir-5186" to be "Succeeded or Failed"
    May 17 07:04:14.522: INFO: Pod "pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568": Phase="Pending", Reason="", readiness=false. Elapsed: 4.922889ms
    May 17 07:04:16.528: INFO: Pod "pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010717667s
    May 17 07:04:18.530: INFO: Pod "pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012319132s
    STEP: Saw pod success 05/17/23 07:04:18.53
    May 17 07:04:18.530: INFO: Pod "pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568" satisfied condition "Succeeded or Failed"
    May 17 07:04:18.534: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568 container test-container: <nil>
    STEP: delete the pod 05/17/23 07:04:18.546
    May 17 07:04:18.562: INFO: Waiting for pod pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568 to disappear
    May 17 07:04:18.567: INFO: Pod pod-9ab1ce9c-2e5c-4717-8667-4fcd002e3568 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May 17 07:04:18.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5186" for this suite. 05/17/23 07:04:18.574
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:04:18.583
May 17 07:04:18.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename configmap 05/17/23 07:04:18.584
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:18.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:18.608
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-f22d3ad9-d62e-4808-a079-fdeb9f133be8 05/17/23 07:04:18.612
STEP: Creating a pod to test consume configMaps 05/17/23 07:04:18.618
May 17 07:04:18.631: INFO: Waiting up to 5m0s for pod "pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634" in namespace "configmap-6635" to be "Succeeded or Failed"
May 17 07:04:18.636: INFO: Pod "pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634": Phase="Pending", Reason="", readiness=false. Elapsed: 4.785079ms
May 17 07:04:20.642: INFO: Pod "pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634": Phase="Running", Reason="", readiness=false. Elapsed: 2.010534081s
May 17 07:04:22.642: INFO: Pod "pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010171016s
STEP: Saw pod success 05/17/23 07:04:22.642
May 17 07:04:22.642: INFO: Pod "pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634" satisfied condition "Succeeded or Failed"
May 17 07:04:22.648: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 07:04:22.659
May 17 07:04:22.676: INFO: Waiting for pod pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634 to disappear
May 17 07:04:22.683: INFO: Pod pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May 17 07:04:22.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6635" for this suite. 05/17/23 07:04:22.69
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":340,"skipped":6280,"failed":0}
------------------------------
â€¢ [4.116 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:04:18.583
    May 17 07:04:18.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename configmap 05/17/23 07:04:18.584
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:18.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:18.608
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-f22d3ad9-d62e-4808-a079-fdeb9f133be8 05/17/23 07:04:18.612
    STEP: Creating a pod to test consume configMaps 05/17/23 07:04:18.618
    May 17 07:04:18.631: INFO: Waiting up to 5m0s for pod "pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634" in namespace "configmap-6635" to be "Succeeded or Failed"
    May 17 07:04:18.636: INFO: Pod "pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634": Phase="Pending", Reason="", readiness=false. Elapsed: 4.785079ms
    May 17 07:04:20.642: INFO: Pod "pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634": Phase="Running", Reason="", readiness=false. Elapsed: 2.010534081s
    May 17 07:04:22.642: INFO: Pod "pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010171016s
    STEP: Saw pod success 05/17/23 07:04:22.642
    May 17 07:04:22.642: INFO: Pod "pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634" satisfied condition "Succeeded or Failed"
    May 17 07:04:22.648: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 07:04:22.659
    May 17 07:04:22.676: INFO: Waiting for pod pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634 to disappear
    May 17 07:04:22.683: INFO: Pod pod-configmaps-2b2854e1-5a07-4f3b-829e-d2f32f79d634 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May 17 07:04:22.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6635" for this suite. 05/17/23 07:04:22.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:04:22.7
May 17 07:04:22.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 07:04:22.701
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:22.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:22.724
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 05/17/23 07:04:22.728
May 17 07:04:22.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: mark a version not serverd 05/17/23 07:04:40.22
STEP: check the unserved version gets removed 05/17/23 07:04:40.273
STEP: check the other version is not changed 05/17/23 07:04:49.736
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 07:05:03.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9152" for this suite. 05/17/23 07:05:04.026
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":341,"skipped":6294,"failed":0}
------------------------------
â€¢ [SLOW TEST] [41.343 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:04:22.7
    May 17 07:04:22.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 07:04:22.701
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:04:22.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:04:22.724
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 05/17/23 07:04:22.728
    May 17 07:04:22.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: mark a version not serverd 05/17/23 07:04:40.22
    STEP: check the unserved version gets removed 05/17/23 07:04:40.273
    STEP: check the other version is not changed 05/17/23 07:04:49.736
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 07:05:03.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9152" for this suite. 05/17/23 07:05:04.026
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:05:04.044
May 17 07:05:04.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename secrets 05/17/23 07:05:04.045
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:05:04.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:05:04.069
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-db2e8a6e-0803-4347-ad90-e1572bab34c2 05/17/23 07:05:04.085
STEP: Creating secret with name s-test-opt-upd-da996b76-1729-4604-86d6-3f3049ea8e13 05/17/23 07:05:04.091
STEP: Creating the pod 05/17/23 07:05:04.097
May 17 07:05:04.113: INFO: Waiting up to 5m0s for pod "pod-secrets-6d39b500-e725-4256-b2ec-8274db7229b5" in namespace "secrets-2057" to be "running and ready"
May 17 07:05:04.118: INFO: Pod "pod-secrets-6d39b500-e725-4256-b2ec-8274db7229b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.070179ms
May 17 07:05:04.118: INFO: The phase of Pod pod-secrets-6d39b500-e725-4256-b2ec-8274db7229b5 is Pending, waiting for it to be Running (with Ready = true)
May 17 07:05:06.124: INFO: Pod "pod-secrets-6d39b500-e725-4256-b2ec-8274db7229b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011093311s
May 17 07:05:06.124: INFO: The phase of Pod pod-secrets-6d39b500-e725-4256-b2ec-8274db7229b5 is Running (Ready = true)
May 17 07:05:06.124: INFO: Pod "pod-secrets-6d39b500-e725-4256-b2ec-8274db7229b5" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-db2e8a6e-0803-4347-ad90-e1572bab34c2 05/17/23 07:05:06.168
STEP: Updating secret s-test-opt-upd-da996b76-1729-4604-86d6-3f3049ea8e13 05/17/23 07:05:06.176
STEP: Creating secret with name s-test-opt-create-46a150a6-376c-4e6e-a8a2-1d4c3a799dc7 05/17/23 07:05:06.183
STEP: waiting to observe update in volume 05/17/23 07:05:06.189
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May 17 07:05:08.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2057" for this suite. 05/17/23 07:05:08.259
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":342,"skipped":6298,"failed":0}
------------------------------
â€¢ [4.226 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:05:04.044
    May 17 07:05:04.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename secrets 05/17/23 07:05:04.045
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:05:04.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:05:04.069
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-db2e8a6e-0803-4347-ad90-e1572bab34c2 05/17/23 07:05:04.085
    STEP: Creating secret with name s-test-opt-upd-da996b76-1729-4604-86d6-3f3049ea8e13 05/17/23 07:05:04.091
    STEP: Creating the pod 05/17/23 07:05:04.097
    May 17 07:05:04.113: INFO: Waiting up to 5m0s for pod "pod-secrets-6d39b500-e725-4256-b2ec-8274db7229b5" in namespace "secrets-2057" to be "running and ready"
    May 17 07:05:04.118: INFO: Pod "pod-secrets-6d39b500-e725-4256-b2ec-8274db7229b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.070179ms
    May 17 07:05:04.118: INFO: The phase of Pod pod-secrets-6d39b500-e725-4256-b2ec-8274db7229b5 is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:05:06.124: INFO: Pod "pod-secrets-6d39b500-e725-4256-b2ec-8274db7229b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011093311s
    May 17 07:05:06.124: INFO: The phase of Pod pod-secrets-6d39b500-e725-4256-b2ec-8274db7229b5 is Running (Ready = true)
    May 17 07:05:06.124: INFO: Pod "pod-secrets-6d39b500-e725-4256-b2ec-8274db7229b5" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-db2e8a6e-0803-4347-ad90-e1572bab34c2 05/17/23 07:05:06.168
    STEP: Updating secret s-test-opt-upd-da996b76-1729-4604-86d6-3f3049ea8e13 05/17/23 07:05:06.176
    STEP: Creating secret with name s-test-opt-create-46a150a6-376c-4e6e-a8a2-1d4c3a799dc7 05/17/23 07:05:06.183
    STEP: waiting to observe update in volume 05/17/23 07:05:06.189
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May 17 07:05:08.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2057" for this suite. 05/17/23 07:05:08.259
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:05:08.27
May 17 07:05:08.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename downward-api 05/17/23 07:05:08.27
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:05:08.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:05:08.294
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 05/17/23 07:05:08.298
May 17 07:05:08.319: INFO: Waiting up to 5m0s for pod "annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e" in namespace "downward-api-4459" to be "running and ready"
May 17 07:05:08.325: INFO: Pod "annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.972334ms
May 17 07:05:08.325: INFO: The phase of Pod annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e is Pending, waiting for it to be Running (with Ready = true)
May 17 07:05:10.332: INFO: Pod "annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012740695s
May 17 07:05:10.332: INFO: The phase of Pod annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e is Running (Ready = true)
May 17 07:05:10.332: INFO: Pod "annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e" satisfied condition "running and ready"
May 17 07:05:10.867: INFO: Successfully updated pod "annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May 17 07:05:14.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4459" for this suite. 05/17/23 07:05:14.928
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":343,"skipped":6301,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.669 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:05:08.27
    May 17 07:05:08.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename downward-api 05/17/23 07:05:08.27
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:05:08.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:05:08.294
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 05/17/23 07:05:08.298
    May 17 07:05:08.319: INFO: Waiting up to 5m0s for pod "annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e" in namespace "downward-api-4459" to be "running and ready"
    May 17 07:05:08.325: INFO: Pod "annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.972334ms
    May 17 07:05:08.325: INFO: The phase of Pod annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:05:10.332: INFO: Pod "annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012740695s
    May 17 07:05:10.332: INFO: The phase of Pod annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e is Running (Ready = true)
    May 17 07:05:10.332: INFO: Pod "annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e" satisfied condition "running and ready"
    May 17 07:05:10.867: INFO: Successfully updated pod "annotationupdate5819c3d4-1f96-40b0-b8c8-98afb1614a5e"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May 17 07:05:14.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4459" for this suite. 05/17/23 07:05:14.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:05:14.939
May 17 07:05:14.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 07:05:14.94
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:05:14.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:05:14.965
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 05/17/23 07:05:14.97
May 17 07:05:14.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-1735 create -f -'
May 17 07:05:16.429: INFO: stderr: ""
May 17 07:05:16.429: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 05/17/23 07:05:16.429
May 17 07:05:16.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-1735 diff -f -'
May 17 07:05:17.593: INFO: rc: 1
May 17 07:05:17.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-1735 delete -f -'
May 17 07:05:17.663: INFO: stderr: ""
May 17 07:05:17.663: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 07:05:17.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1735" for this suite. 05/17/23 07:05:17.675
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":344,"skipped":6308,"failed":0}
------------------------------
â€¢ [2.752 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:05:14.939
    May 17 07:05:14.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 07:05:14.94
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:05:14.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:05:14.965
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 05/17/23 07:05:14.97
    May 17 07:05:14.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-1735 create -f -'
    May 17 07:05:16.429: INFO: stderr: ""
    May 17 07:05:16.429: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 05/17/23 07:05:16.429
    May 17 07:05:16.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-1735 diff -f -'
    May 17 07:05:17.593: INFO: rc: 1
    May 17 07:05:17.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-1735 delete -f -'
    May 17 07:05:17.663: INFO: stderr: ""
    May 17 07:05:17.663: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 07:05:17.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1735" for this suite. 05/17/23 07:05:17.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:05:17.691
May 17 07:05:17.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename cronjob 05/17/23 07:05:17.692
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:05:17.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:05:17.715
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 05/17/23 07:05:17.719
STEP: Ensuring no jobs are scheduled 05/17/23 07:05:17.728
STEP: Ensuring no job exists by listing jobs explicitly 05/17/23 07:10:17.739
STEP: Removing cronjob 05/17/23 07:10:17.746
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May 17 07:10:17.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6535" for this suite. 05/17/23 07:10:17.767
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":345,"skipped":6321,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.092 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:05:17.691
    May 17 07:05:17.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename cronjob 05/17/23 07:05:17.692
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:05:17.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:05:17.715
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 05/17/23 07:05:17.719
    STEP: Ensuring no jobs are scheduled 05/17/23 07:05:17.728
    STEP: Ensuring no job exists by listing jobs explicitly 05/17/23 07:10:17.739
    STEP: Removing cronjob 05/17/23 07:10:17.746
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May 17 07:10:17.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6535" for this suite. 05/17/23 07:10:17.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:10:17.784
May 17 07:10:17.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 07:10:17.785
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:10:17.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:10:17.81
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 05/17/23 07:10:17.814
May 17 07:10:17.833: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c" in namespace "projected-1451" to be "Succeeded or Failed"
May 17 07:10:17.838: INFO: Pod "downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.781186ms
May 17 07:10:19.844: INFO: Pod "downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011367521s
May 17 07:10:21.844: INFO: Pod "downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01104215s
STEP: Saw pod success 05/17/23 07:10:21.844
May 17 07:10:21.844: INFO: Pod "downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c" satisfied condition "Succeeded or Failed"
May 17 07:10:21.849: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c container client-container: <nil>
STEP: delete the pod 05/17/23 07:10:21.862
May 17 07:10:21.878: INFO: Waiting for pod downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c to disappear
May 17 07:10:21.883: INFO: Pod downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May 17 07:10:21.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1451" for this suite. 05/17/23 07:10:21.892
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":346,"skipped":6330,"failed":0}
------------------------------
â€¢ [4.118 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:10:17.784
    May 17 07:10:17.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 07:10:17.785
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:10:17.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:10:17.81
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 05/17/23 07:10:17.814
    May 17 07:10:17.833: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c" in namespace "projected-1451" to be "Succeeded or Failed"
    May 17 07:10:17.838: INFO: Pod "downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.781186ms
    May 17 07:10:19.844: INFO: Pod "downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011367521s
    May 17 07:10:21.844: INFO: Pod "downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01104215s
    STEP: Saw pod success 05/17/23 07:10:21.844
    May 17 07:10:21.844: INFO: Pod "downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c" satisfied condition "Succeeded or Failed"
    May 17 07:10:21.849: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c container client-container: <nil>
    STEP: delete the pod 05/17/23 07:10:21.862
    May 17 07:10:21.878: INFO: Waiting for pod downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c to disappear
    May 17 07:10:21.883: INFO: Pod downwardapi-volume-65f9d108-1c86-4529-be5e-b3116623be1c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May 17 07:10:21.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1451" for this suite. 05/17/23 07:10:21.892
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:10:21.903
May 17 07:10:21.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 07:10:21.904
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:10:21.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:10:21.932
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
May 17 07:10:21.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/17/23 07:10:31.651
May 17 07:10:31.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 create -f -'
May 17 07:10:33.099: INFO: stderr: ""
May 17 07:10:33.099: INFO: stdout: "e2e-test-crd-publish-openapi-486-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 17 07:10:33.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 delete e2e-test-crd-publish-openapi-486-crds test-foo'
May 17 07:10:33.252: INFO: stderr: ""
May 17 07:10:33.252: INFO: stdout: "e2e-test-crd-publish-openapi-486-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 17 07:10:33.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 apply -f -'
May 17 07:10:34.548: INFO: stderr: ""
May 17 07:10:34.548: INFO: stdout: "e2e-test-crd-publish-openapi-486-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 17 07:10:34.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 delete e2e-test-crd-publish-openapi-486-crds test-foo'
May 17 07:10:34.617: INFO: stderr: ""
May 17 07:10:34.617: INFO: stdout: "e2e-test-crd-publish-openapi-486-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/17/23 07:10:34.617
May 17 07:10:34.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 create -f -'
May 17 07:10:35.938: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/17/23 07:10:35.938
May 17 07:10:35.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 create -f -'
May 17 07:10:36.145: INFO: rc: 1
May 17 07:10:36.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 apply -f -'
May 17 07:10:37.718: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/17/23 07:10:37.718
May 17 07:10:37.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 create -f -'
May 17 07:10:37.924: INFO: rc: 1
May 17 07:10:37.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 apply -f -'
May 17 07:10:39.116: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 05/17/23 07:10:39.116
May 17 07:10:39.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 explain e2e-test-crd-publish-openapi-486-crds'
May 17 07:10:39.311: INFO: stderr: ""
May 17 07:10:39.311: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-486-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 05/17/23 07:10:39.311
May 17 07:10:39.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 explain e2e-test-crd-publish-openapi-486-crds.metadata'
May 17 07:10:39.509: INFO: stderr: ""
May 17 07:10:39.509: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-486-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 17 07:10:39.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 explain e2e-test-crd-publish-openapi-486-crds.spec'
May 17 07:10:39.707: INFO: stderr: ""
May 17 07:10:39.707: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-486-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 17 07:10:39.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 explain e2e-test-crd-publish-openapi-486-crds.spec.bars'
May 17 07:10:39.898: INFO: stderr: ""
May 17 07:10:39.898: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-486-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/17/23 07:10:39.898
May 17 07:10:39.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 explain e2e-test-crd-publish-openapi-486-crds.spec.bars2'
May 17 07:10:41.030: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 07:10:51.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6525" for this suite. 05/17/23 07:10:51.591
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":347,"skipped":6334,"failed":0}
------------------------------
â€¢ [SLOW TEST] [29.697 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:10:21.903
    May 17 07:10:21.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename crd-publish-openapi 05/17/23 07:10:21.904
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:10:21.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:10:21.932
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    May 17 07:10:21.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/17/23 07:10:31.651
    May 17 07:10:31.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 create -f -'
    May 17 07:10:33.099: INFO: stderr: ""
    May 17 07:10:33.099: INFO: stdout: "e2e-test-crd-publish-openapi-486-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May 17 07:10:33.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 delete e2e-test-crd-publish-openapi-486-crds test-foo'
    May 17 07:10:33.252: INFO: stderr: ""
    May 17 07:10:33.252: INFO: stdout: "e2e-test-crd-publish-openapi-486-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    May 17 07:10:33.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 apply -f -'
    May 17 07:10:34.548: INFO: stderr: ""
    May 17 07:10:34.548: INFO: stdout: "e2e-test-crd-publish-openapi-486-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May 17 07:10:34.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 delete e2e-test-crd-publish-openapi-486-crds test-foo'
    May 17 07:10:34.617: INFO: stderr: ""
    May 17 07:10:34.617: INFO: stdout: "e2e-test-crd-publish-openapi-486-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/17/23 07:10:34.617
    May 17 07:10:34.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 create -f -'
    May 17 07:10:35.938: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/17/23 07:10:35.938
    May 17 07:10:35.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 create -f -'
    May 17 07:10:36.145: INFO: rc: 1
    May 17 07:10:36.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 apply -f -'
    May 17 07:10:37.718: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/17/23 07:10:37.718
    May 17 07:10:37.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 create -f -'
    May 17 07:10:37.924: INFO: rc: 1
    May 17 07:10:37.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 --namespace=crd-publish-openapi-6525 apply -f -'
    May 17 07:10:39.116: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 05/17/23 07:10:39.116
    May 17 07:10:39.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 explain e2e-test-crd-publish-openapi-486-crds'
    May 17 07:10:39.311: INFO: stderr: ""
    May 17 07:10:39.311: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-486-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 05/17/23 07:10:39.311
    May 17 07:10:39.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 explain e2e-test-crd-publish-openapi-486-crds.metadata'
    May 17 07:10:39.509: INFO: stderr: ""
    May 17 07:10:39.509: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-486-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    May 17 07:10:39.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 explain e2e-test-crd-publish-openapi-486-crds.spec'
    May 17 07:10:39.707: INFO: stderr: ""
    May 17 07:10:39.707: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-486-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    May 17 07:10:39.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 explain e2e-test-crd-publish-openapi-486-crds.spec.bars'
    May 17 07:10:39.898: INFO: stderr: ""
    May 17 07:10:39.898: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-486-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/17/23 07:10:39.898
    May 17 07:10:39.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=crd-publish-openapi-6525 explain e2e-test-crd-publish-openapi-486-crds.spec.bars2'
    May 17 07:10:41.030: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 07:10:51.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6525" for this suite. 05/17/23 07:10:51.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:10:51.6
May 17 07:10:51.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename replicaset 05/17/23 07:10:51.601
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:10:51.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:10:51.624
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
May 17 07:10:51.648: INFO: Pod name sample-pod: Found 0 pods out of 1
May 17 07:10:56.654: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/17/23 07:10:56.654
STEP: Scaling up "test-rs" replicaset  05/17/23 07:10:56.654
May 17 07:10:56.664: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 05/17/23 07:10:56.664
W0517 07:10:56.671876      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May 17 07:10:56.674: INFO: observed ReplicaSet test-rs in namespace replicaset-5182 with ReadyReplicas 1, AvailableReplicas 1
May 17 07:10:56.702: INFO: observed ReplicaSet test-rs in namespace replicaset-5182 with ReadyReplicas 1, AvailableReplicas 1
May 17 07:10:56.729: INFO: observed ReplicaSet test-rs in namespace replicaset-5182 with ReadyReplicas 1, AvailableReplicas 1
May 17 07:10:56.736: INFO: observed ReplicaSet test-rs in namespace replicaset-5182 with ReadyReplicas 1, AvailableReplicas 1
May 17 07:10:57.931: INFO: observed ReplicaSet test-rs in namespace replicaset-5182 with ReadyReplicas 2, AvailableReplicas 2
May 17 07:10:58.187: INFO: observed Replicaset test-rs in namespace replicaset-5182 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May 17 07:10:58.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5182" for this suite. 05/17/23 07:10:58.195
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":348,"skipped":6349,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.604 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:10:51.6
    May 17 07:10:51.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename replicaset 05/17/23 07:10:51.601
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:10:51.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:10:51.624
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    May 17 07:10:51.648: INFO: Pod name sample-pod: Found 0 pods out of 1
    May 17 07:10:56.654: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/17/23 07:10:56.654
    STEP: Scaling up "test-rs" replicaset  05/17/23 07:10:56.654
    May 17 07:10:56.664: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 05/17/23 07:10:56.664
    W0517 07:10:56.671876      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May 17 07:10:56.674: INFO: observed ReplicaSet test-rs in namespace replicaset-5182 with ReadyReplicas 1, AvailableReplicas 1
    May 17 07:10:56.702: INFO: observed ReplicaSet test-rs in namespace replicaset-5182 with ReadyReplicas 1, AvailableReplicas 1
    May 17 07:10:56.729: INFO: observed ReplicaSet test-rs in namespace replicaset-5182 with ReadyReplicas 1, AvailableReplicas 1
    May 17 07:10:56.736: INFO: observed ReplicaSet test-rs in namespace replicaset-5182 with ReadyReplicas 1, AvailableReplicas 1
    May 17 07:10:57.931: INFO: observed ReplicaSet test-rs in namespace replicaset-5182 with ReadyReplicas 2, AvailableReplicas 2
    May 17 07:10:58.187: INFO: observed Replicaset test-rs in namespace replicaset-5182 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May 17 07:10:58.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5182" for this suite. 05/17/23 07:10:58.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:10:58.205
May 17 07:10:58.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename deployment 05/17/23 07:10:58.205
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:10:58.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:10:58.23
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 05/17/23 07:10:58.242
May 17 07:10:58.242: INFO: Creating simple deployment test-deployment-47j6z
May 17 07:10:58.258: INFO: deployment "test-deployment-47j6z" doesn't have the required revision set
STEP: Getting /status 05/17/23 07:11:00.281
May 17 07:11:00.285: INFO: Deployment test-deployment-47j6z has Conditions: [{Available True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47j6z-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 05/17/23 07:11:00.285
May 17 07:11:00.295: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 10, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 10, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 10, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 10, 58, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-47j6z-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 05/17/23 07:11:00.295
May 17 07:11:00.298: INFO: Observed &Deployment event: ADDED
May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47j6z-777898ffcc"}
May 17 07:11:00.298: INFO: Observed &Deployment event: MODIFIED
May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47j6z-777898ffcc"}
May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 17 07:11:00.298: INFO: Observed &Deployment event: MODIFIED
May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-47j6z-777898ffcc" is progressing.}
May 17 07:11:00.298: INFO: Observed &Deployment event: MODIFIED
May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47j6z-777898ffcc" has successfully progressed.}
May 17 07:11:00.298: INFO: Observed &Deployment event: MODIFIED
May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47j6z-777898ffcc" has successfully progressed.}
May 17 07:11:00.298: INFO: Found Deployment test-deployment-47j6z in namespace deployment-2664 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 17 07:11:00.298: INFO: Deployment test-deployment-47j6z has an updated status
STEP: patching the Statefulset Status 05/17/23 07:11:00.298
May 17 07:11:00.298: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 17 07:11:00.307: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 05/17/23 07:11:00.307
May 17 07:11:00.309: INFO: Observed &Deployment event: ADDED
May 17 07:11:00.309: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47j6z-777898ffcc"}
May 17 07:11:00.309: INFO: Observed &Deployment event: MODIFIED
May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47j6z-777898ffcc"}
May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 17 07:11:00.310: INFO: Observed &Deployment event: MODIFIED
May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-47j6z-777898ffcc" is progressing.}
May 17 07:11:00.310: INFO: Observed &Deployment event: MODIFIED
May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47j6z-777898ffcc" has successfully progressed.}
May 17 07:11:00.310: INFO: Observed &Deployment event: MODIFIED
May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47j6z-777898ffcc" has successfully progressed.}
May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 17 07:11:00.310: INFO: Observed &Deployment event: MODIFIED
May 17 07:11:00.310: INFO: Found deployment test-deployment-47j6z in namespace deployment-2664 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
May 17 07:11:00.310: INFO: Deployment test-deployment-47j6z has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 17 07:11:00.314: INFO: Deployment "test-deployment-47j6z":
&Deployment{ObjectMeta:{test-deployment-47j6z  deployment-2664  7f1158aa-a161-4f91-9f8b-ef2410b8aa66 1503562 1 2023-05-17 07:10:58 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-17 07:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-05-17 07:11:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d8c2dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 17 07:11:00.318: INFO: New ReplicaSet "test-deployment-47j6z-777898ffcc" of Deployment "test-deployment-47j6z":
&ReplicaSet{ObjectMeta:{test-deployment-47j6z-777898ffcc  deployment-2664  20c71671-f559-446a-894e-db28f0e8da0d 1503541 1 2023-05-17 07:10:58 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-47j6z 7f1158aa-a161-4f91-9f8b-ef2410b8aa66 0xc00cd6b867 0xc00cd6b868}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f1158aa-a161-4f91-9f8b-ef2410b8aa66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:10:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00cd6b918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 07:11:00.323: INFO: Pod "test-deployment-47j6z-777898ffcc-cz7mh" is available:
&Pod{ObjectMeta:{test-deployment-47j6z-777898ffcc-cz7mh test-deployment-47j6z-777898ffcc- deployment-2664  4e9e34f8-3174-48f6-a303-dd79d51a1a3b 1503540 0 2023-05-17 07:10:58 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-47j6z-777898ffcc 20c71671-f559-446a-894e-db28f0e8da0d 0xc00d9c8da7 0xc00d9c8da8}] [] [{kube-controller-manager Update v1 2023-05-17 07:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20c71671-f559-446a-894e-db28f0e8da0d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:10:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62tnq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62tnq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.134,StartTime:2023-05-17 07:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:10:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bc0d4804087538e4f9ce7c45b0ba74664a3f9b33d7f8e41fe13abdde6d29bd12,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May 17 07:11:00.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2664" for this suite. 05/17/23 07:11:00.331
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":349,"skipped":6355,"failed":0}
------------------------------
â€¢ [2.134 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:10:58.205
    May 17 07:10:58.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename deployment 05/17/23 07:10:58.205
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:10:58.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:10:58.23
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 05/17/23 07:10:58.242
    May 17 07:10:58.242: INFO: Creating simple deployment test-deployment-47j6z
    May 17 07:10:58.258: INFO: deployment "test-deployment-47j6z" doesn't have the required revision set
    STEP: Getting /status 05/17/23 07:11:00.281
    May 17 07:11:00.285: INFO: Deployment test-deployment-47j6z has Conditions: [{Available True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47j6z-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 05/17/23 07:11:00.285
    May 17 07:11:00.295: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 10, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 10, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 10, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 10, 58, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-47j6z-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 05/17/23 07:11:00.295
    May 17 07:11:00.298: INFO: Observed &Deployment event: ADDED
    May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47j6z-777898ffcc"}
    May 17 07:11:00.298: INFO: Observed &Deployment event: MODIFIED
    May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47j6z-777898ffcc"}
    May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May 17 07:11:00.298: INFO: Observed &Deployment event: MODIFIED
    May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-47j6z-777898ffcc" is progressing.}
    May 17 07:11:00.298: INFO: Observed &Deployment event: MODIFIED
    May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47j6z-777898ffcc" has successfully progressed.}
    May 17 07:11:00.298: INFO: Observed &Deployment event: MODIFIED
    May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May 17 07:11:00.298: INFO: Observed Deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47j6z-777898ffcc" has successfully progressed.}
    May 17 07:11:00.298: INFO: Found Deployment test-deployment-47j6z in namespace deployment-2664 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 17 07:11:00.298: INFO: Deployment test-deployment-47j6z has an updated status
    STEP: patching the Statefulset Status 05/17/23 07:11:00.298
    May 17 07:11:00.298: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May 17 07:11:00.307: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 05/17/23 07:11:00.307
    May 17 07:11:00.309: INFO: Observed &Deployment event: ADDED
    May 17 07:11:00.309: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47j6z-777898ffcc"}
    May 17 07:11:00.309: INFO: Observed &Deployment event: MODIFIED
    May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-47j6z-777898ffcc"}
    May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May 17 07:11:00.310: INFO: Observed &Deployment event: MODIFIED
    May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-47j6z-777898ffcc" is progressing.}
    May 17 07:11:00.310: INFO: Observed &Deployment event: MODIFIED
    May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47j6z-777898ffcc" has successfully progressed.}
    May 17 07:11:00.310: INFO: Observed &Deployment event: MODIFIED
    May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-17 07:10:58 +0000 UTC 2023-05-17 07:10:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-47j6z-777898ffcc" has successfully progressed.}
    May 17 07:11:00.310: INFO: Observed deployment test-deployment-47j6z in namespace deployment-2664 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 17 07:11:00.310: INFO: Observed &Deployment event: MODIFIED
    May 17 07:11:00.310: INFO: Found deployment test-deployment-47j6z in namespace deployment-2664 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    May 17 07:11:00.310: INFO: Deployment test-deployment-47j6z has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 17 07:11:00.314: INFO: Deployment "test-deployment-47j6z":
    &Deployment{ObjectMeta:{test-deployment-47j6z  deployment-2664  7f1158aa-a161-4f91-9f8b-ef2410b8aa66 1503562 1 2023-05-17 07:10:58 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-17 07:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-05-17 07:11:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d8c2dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 17 07:11:00.318: INFO: New ReplicaSet "test-deployment-47j6z-777898ffcc" of Deployment "test-deployment-47j6z":
    &ReplicaSet{ObjectMeta:{test-deployment-47j6z-777898ffcc  deployment-2664  20c71671-f559-446a-894e-db28f0e8da0d 1503541 1 2023-05-17 07:10:58 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-47j6z 7f1158aa-a161-4f91-9f8b-ef2410b8aa66 0xc00cd6b867 0xc00cd6b868}] [] [{kube-controller-manager Update apps/v1 2023-05-17 07:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f1158aa-a161-4f91-9f8b-ef2410b8aa66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-17 07:10:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00cd6b918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 17 07:11:00.323: INFO: Pod "test-deployment-47j6z-777898ffcc-cz7mh" is available:
    &Pod{ObjectMeta:{test-deployment-47j6z-777898ffcc-cz7mh test-deployment-47j6z-777898ffcc- deployment-2664  4e9e34f8-3174-48f6-a303-dd79d51a1a3b 1503540 0 2023-05-17 07:10:58 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-47j6z-777898ffcc 20c71671-f559-446a-894e-db28f0e8da0d 0xc00d9c8da7 0xc00d9c8da8}] [] [{kube-controller-manager Update v1 2023-05-17 07:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20c71671-f559-446a-894e-db28f0e8da0d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-17 07:10:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62tnq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62tnq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aks-agentpool-72615086-vmss00000c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-17 07:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.224.0.6,PodIP:10.244.2.134,StartTime:2023-05-17 07:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-17 07:10:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bc0d4804087538e4f9ce7c45b0ba74664a3f9b33d7f8e41fe13abdde6d29bd12,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May 17 07:11:00.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2664" for this suite. 05/17/23 07:11:00.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:11:00.348
May 17 07:11:00.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename kubectl 05/17/23 07:11:00.349
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:11:00.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:11:00.373
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/17/23 07:11:00.376
May 17 07:11:00.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8382 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May 17 07:11:00.450: INFO: stderr: ""
May 17 07:11:00.450: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 05/17/23 07:11:00.45
May 17 07:11:00.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8382 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
May 17 07:11:01.867: INFO: stderr: ""
May 17 07:11:01.867: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/17/23 07:11:01.867
May 17 07:11:01.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8382 delete pods e2e-test-httpd-pod'
May 17 07:11:04.960: INFO: stderr: ""
May 17 07:11:04.960: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May 17 07:11:04.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8382" for this suite. 05/17/23 07:11:04.968
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":350,"skipped":6394,"failed":0}
------------------------------
â€¢ [4.630 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:11:00.348
    May 17 07:11:00.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename kubectl 05/17/23 07:11:00.349
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:11:00.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:11:00.373
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/17/23 07:11:00.376
    May 17 07:11:00.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8382 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May 17 07:11:00.450: INFO: stderr: ""
    May 17 07:11:00.450: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 05/17/23 07:11:00.45
    May 17 07:11:00.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8382 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    May 17 07:11:01.867: INFO: stderr: ""
    May 17 07:11:01.867: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/17/23 07:11:01.867
    May 17 07:11:01.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1963122082 --namespace=kubectl-8382 delete pods e2e-test-httpd-pod'
    May 17 07:11:04.960: INFO: stderr: ""
    May 17 07:11:04.960: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May 17 07:11:04.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8382" for this suite. 05/17/23 07:11:04.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:11:04.979
May 17 07:11:04.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename dns 05/17/23 07:11:04.98
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:11:05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:11:05.004
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/17/23 07:11:05.007
May 17 07:11:05.025: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9474  7fac5ca6-2dd3-4d43-954b-d2fc6c667bfa 1503639 0 2023-05-17 07:11:05 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-17 07:11:05 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8ckq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8ckq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 07:11:05.025: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9474" to be "running and ready"
May 17 07:11:05.030: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.06619ms
May 17 07:11:05.030: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 17 07:11:07.037: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.011444242s
May 17 07:11:07.037: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
May 17 07:11:07.037: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 05/17/23 07:11:07.037
May 17 07:11:07.037: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9474 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 07:11:07.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 07:11:07.037: INFO: ExecWithOptions: Clientset creation
May 17 07:11:07.037: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/dns-9474/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 05/17/23 07:11:07.175
May 17 07:11:07.175: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9474 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 07:11:07.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
May 17 07:11:07.176: INFO: ExecWithOptions: Clientset creation
May 17 07:11:07.176: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/dns-9474/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 17 07:11:07.301: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May 17 07:11:07.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9474" for this suite. 05/17/23 07:11:07.331
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":351,"skipped":6422,"failed":0}
------------------------------
â€¢ [2.367 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:11:04.979
    May 17 07:11:04.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename dns 05/17/23 07:11:04.98
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:11:05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:11:05.004
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/17/23 07:11:05.007
    May 17 07:11:05.025: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9474  7fac5ca6-2dd3-4d43-954b-d2fc6c667bfa 1503639 0 2023-05-17 07:11:05 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-17 07:11:05 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8ckq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8ckq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 17 07:11:05.025: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9474" to be "running and ready"
    May 17 07:11:05.030: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.06619ms
    May 17 07:11:05.030: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:11:07.037: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.011444242s
    May 17 07:11:07.037: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    May 17 07:11:07.037: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 05/17/23 07:11:07.037
    May 17 07:11:07.037: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9474 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 07:11:07.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 07:11:07.037: INFO: ExecWithOptions: Clientset creation
    May 17 07:11:07.037: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/dns-9474/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 05/17/23 07:11:07.175
    May 17 07:11:07.175: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9474 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 17 07:11:07.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    May 17 07:11:07.176: INFO: ExecWithOptions: Clientset creation
    May 17 07:11:07.176: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/dns-9474/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 17 07:11:07.301: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May 17 07:11:07.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9474" for this suite. 05/17/23 07:11:07.331
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:11:07.346
May 17 07:11:07.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename job 05/17/23 07:11:07.347
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:11:07.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:11:07.373
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 05/17/23 07:11:07.384
STEP: Patching the Job 05/17/23 07:11:07.399
STEP: Watching for Job to be patched 05/17/23 07:11:07.413
May 17 07:11:07.415: INFO: Event ADDED observed for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4] and annotations: map[batch.kubernetes.io/job-tracking:]
May 17 07:11:07.415: INFO: Event MODIFIED found for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 05/17/23 07:11:07.415
STEP: Watching for Job to be updated 05/17/23 07:11:07.429
May 17 07:11:07.431: INFO: Event MODIFIED found for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 17 07:11:07.431: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 05/17/23 07:11:07.431
May 17 07:11:07.437: INFO: Job: e2e-l4kr4 as labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched]
STEP: Waiting for job to complete 05/17/23 07:11:07.437
STEP: Delete a job collection with a labelselector 05/17/23 07:11:17.443
STEP: Watching for Job to be deleted 05/17/23 07:11:17.455
May 17 07:11:17.457: INFO: Event MODIFIED observed for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 17 07:11:17.457: INFO: Event MODIFIED observed for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 17 07:11:17.457: INFO: Event MODIFIED observed for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 17 07:11:17.457: INFO: Event MODIFIED observed for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 17 07:11:17.457: INFO: Event MODIFIED observed for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 17 07:11:17.457: INFO: Event DELETED found for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 05/17/23 07:11:17.457
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May 17 07:11:17.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2175" for this suite. 05/17/23 07:11:17.472
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":352,"skipped":6426,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.134 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:11:07.346
    May 17 07:11:07.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename job 05/17/23 07:11:07.347
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:11:07.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:11:07.373
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 05/17/23 07:11:07.384
    STEP: Patching the Job 05/17/23 07:11:07.399
    STEP: Watching for Job to be patched 05/17/23 07:11:07.413
    May 17 07:11:07.415: INFO: Event ADDED observed for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4] and annotations: map[batch.kubernetes.io/job-tracking:]
    May 17 07:11:07.415: INFO: Event MODIFIED found for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 05/17/23 07:11:07.415
    STEP: Watching for Job to be updated 05/17/23 07:11:07.429
    May 17 07:11:07.431: INFO: Event MODIFIED found for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 17 07:11:07.431: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 05/17/23 07:11:07.431
    May 17 07:11:07.437: INFO: Job: e2e-l4kr4 as labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched]
    STEP: Waiting for job to complete 05/17/23 07:11:07.437
    STEP: Delete a job collection with a labelselector 05/17/23 07:11:17.443
    STEP: Watching for Job to be deleted 05/17/23 07:11:17.455
    May 17 07:11:17.457: INFO: Event MODIFIED observed for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 17 07:11:17.457: INFO: Event MODIFIED observed for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 17 07:11:17.457: INFO: Event MODIFIED observed for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 17 07:11:17.457: INFO: Event MODIFIED observed for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 17 07:11:17.457: INFO: Event MODIFIED observed for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 17 07:11:17.457: INFO: Event DELETED found for Job e2e-l4kr4 in namespace job-2175 with labels: map[e2e-job-label:e2e-l4kr4 e2e-l4kr4:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 05/17/23 07:11:17.457
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May 17 07:11:17.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2175" for this suite. 05/17/23 07:11:17.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:11:17.481
May 17 07:11:17.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 07:11:17.481
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:11:17.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:11:17.507
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
May 17 07:11:17.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May 17 07:11:18.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3701" for this suite. 05/17/23 07:11:18.095
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":353,"skipped":6434,"failed":0}
------------------------------
â€¢ [0.624 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:11:17.481
    May 17 07:11:17.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename custom-resource-definition 05/17/23 07:11:17.481
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:11:17.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:11:17.507
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    May 17 07:11:17.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May 17 07:11:18.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3701" for this suite. 05/17/23 07:11:18.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:11:18.105
May 17 07:11:18.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename limitrange 05/17/23 07:11:18.106
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:11:18.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:11:18.133
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 05/17/23 07:11:18.136
STEP: Setting up watch 05/17/23 07:11:18.136
STEP: Submitting a LimitRange 05/17/23 07:11:18.242
STEP: Verifying LimitRange creation was observed 05/17/23 07:11:18.252
STEP: Fetching the LimitRange to ensure it has proper values 05/17/23 07:11:18.252
May 17 07:11:18.257: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 17 07:11:18.257: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 05/17/23 07:11:18.257
STEP: Ensuring Pod has resource requirements applied from LimitRange 05/17/23 07:11:18.274
May 17 07:11:18.280: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 17 07:11:18.280: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 05/17/23 07:11:18.28
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/17/23 07:11:18.29
May 17 07:11:18.296: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 17 07:11:18.296: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 05/17/23 07:11:18.296
STEP: Failing to create a Pod with more than max resources 05/17/23 07:11:18.303
STEP: Updating a LimitRange 05/17/23 07:11:18.309
STEP: Verifying LimitRange updating is effective 05/17/23 07:11:18.317
STEP: Creating a Pod with less than former min resources 05/17/23 07:11:20.323
STEP: Failing to create a Pod with more than max resources 05/17/23 07:11:20.332
STEP: Deleting a LimitRange 05/17/23 07:11:20.336
STEP: Verifying the LimitRange was deleted 05/17/23 07:11:20.345
May 17 07:11:25.352: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 05/17/23 07:11:25.352
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
May 17 07:11:25.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8912" for this suite. 05/17/23 07:11:25.375
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":354,"skipped":6454,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.280 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:11:18.105
    May 17 07:11:18.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename limitrange 05/17/23 07:11:18.106
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:11:18.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:11:18.133
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 05/17/23 07:11:18.136
    STEP: Setting up watch 05/17/23 07:11:18.136
    STEP: Submitting a LimitRange 05/17/23 07:11:18.242
    STEP: Verifying LimitRange creation was observed 05/17/23 07:11:18.252
    STEP: Fetching the LimitRange to ensure it has proper values 05/17/23 07:11:18.252
    May 17 07:11:18.257: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May 17 07:11:18.257: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 05/17/23 07:11:18.257
    STEP: Ensuring Pod has resource requirements applied from LimitRange 05/17/23 07:11:18.274
    May 17 07:11:18.280: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May 17 07:11:18.280: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 05/17/23 07:11:18.28
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/17/23 07:11:18.29
    May 17 07:11:18.296: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    May 17 07:11:18.296: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 05/17/23 07:11:18.296
    STEP: Failing to create a Pod with more than max resources 05/17/23 07:11:18.303
    STEP: Updating a LimitRange 05/17/23 07:11:18.309
    STEP: Verifying LimitRange updating is effective 05/17/23 07:11:18.317
    STEP: Creating a Pod with less than former min resources 05/17/23 07:11:20.323
    STEP: Failing to create a Pod with more than max resources 05/17/23 07:11:20.332
    STEP: Deleting a LimitRange 05/17/23 07:11:20.336
    STEP: Verifying the LimitRange was deleted 05/17/23 07:11:20.345
    May 17 07:11:25.352: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 05/17/23 07:11:25.352
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    May 17 07:11:25.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-8912" for this suite. 05/17/23 07:11:25.375
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:11:25.386
May 17 07:11:25.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename gc 05/17/23 07:11:25.387
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:11:25.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:11:25.429
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 05/17/23 07:11:25.442
STEP: delete the rc 05/17/23 07:11:30.46
STEP: wait for the rc to be deleted 05/17/23 07:11:30.468
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/17/23 07:11:35.475
STEP: Gathering metrics 05/17/23 07:12:05.489
W0517 07:12:05.502296      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 17 07:12:05.502: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 17 07:12:05.502: INFO: Deleting pod "simpletest.rc-25zbx" in namespace "gc-9208"
May 17 07:12:05.514: INFO: Deleting pod "simpletest.rc-47rr6" in namespace "gc-9208"
May 17 07:12:05.531: INFO: Deleting pod "simpletest.rc-49rmg" in namespace "gc-9208"
May 17 07:12:05.549: INFO: Deleting pod "simpletest.rc-4qhww" in namespace "gc-9208"
May 17 07:12:05.569: INFO: Deleting pod "simpletest.rc-4tc7r" in namespace "gc-9208"
May 17 07:12:05.590: INFO: Deleting pod "simpletest.rc-4wxkv" in namespace "gc-9208"
May 17 07:12:05.621: INFO: Deleting pod "simpletest.rc-5gdnb" in namespace "gc-9208"
May 17 07:12:05.646: INFO: Deleting pod "simpletest.rc-5nqvr" in namespace "gc-9208"
May 17 07:12:05.667: INFO: Deleting pod "simpletest.rc-6bxlm" in namespace "gc-9208"
May 17 07:12:05.684: INFO: Deleting pod "simpletest.rc-6d5fc" in namespace "gc-9208"
May 17 07:12:05.701: INFO: Deleting pod "simpletest.rc-6lrp6" in namespace "gc-9208"
May 17 07:12:05.717: INFO: Deleting pod "simpletest.rc-6lvfl" in namespace "gc-9208"
May 17 07:12:05.733: INFO: Deleting pod "simpletest.rc-78wxd" in namespace "gc-9208"
May 17 07:12:05.746: INFO: Deleting pod "simpletest.rc-7fgps" in namespace "gc-9208"
May 17 07:12:05.759: INFO: Deleting pod "simpletest.rc-7lxpb" in namespace "gc-9208"
May 17 07:12:05.773: INFO: Deleting pod "simpletest.rc-7mmm2" in namespace "gc-9208"
May 17 07:12:05.787: INFO: Deleting pod "simpletest.rc-88q7s" in namespace "gc-9208"
May 17 07:12:05.799: INFO: Deleting pod "simpletest.rc-8g9x6" in namespace "gc-9208"
May 17 07:12:05.826: INFO: Deleting pod "simpletest.rc-8kmqq" in namespace "gc-9208"
May 17 07:12:05.842: INFO: Deleting pod "simpletest.rc-8lc6f" in namespace "gc-9208"
May 17 07:12:05.858: INFO: Deleting pod "simpletest.rc-8mjgt" in namespace "gc-9208"
May 17 07:12:05.872: INFO: Deleting pod "simpletest.rc-8plhf" in namespace "gc-9208"
May 17 07:12:05.886: INFO: Deleting pod "simpletest.rc-8vhrk" in namespace "gc-9208"
May 17 07:12:05.902: INFO: Deleting pod "simpletest.rc-9fhwr" in namespace "gc-9208"
May 17 07:12:05.922: INFO: Deleting pod "simpletest.rc-9gkzp" in namespace "gc-9208"
May 17 07:12:05.943: INFO: Deleting pod "simpletest.rc-9mcbp" in namespace "gc-9208"
May 17 07:12:05.961: INFO: Deleting pod "simpletest.rc-b2kx2" in namespace "gc-9208"
May 17 07:12:06.004: INFO: Deleting pod "simpletest.rc-bbz9q" in namespace "gc-9208"
May 17 07:12:06.026: INFO: Deleting pod "simpletest.rc-bkbdx" in namespace "gc-9208"
May 17 07:12:06.045: INFO: Deleting pod "simpletest.rc-br2vf" in namespace "gc-9208"
May 17 07:12:06.064: INFO: Deleting pod "simpletest.rc-c27xc" in namespace "gc-9208"
May 17 07:12:06.083: INFO: Deleting pod "simpletest.rc-c99mz" in namespace "gc-9208"
May 17 07:12:06.108: INFO: Deleting pod "simpletest.rc-c9m72" in namespace "gc-9208"
May 17 07:12:06.122: INFO: Deleting pod "simpletest.rc-cmhnt" in namespace "gc-9208"
May 17 07:12:06.135: INFO: Deleting pod "simpletest.rc-cnj4p" in namespace "gc-9208"
May 17 07:12:06.149: INFO: Deleting pod "simpletest.rc-cqxcz" in namespace "gc-9208"
May 17 07:12:06.165: INFO: Deleting pod "simpletest.rc-crk4f" in namespace "gc-9208"
May 17 07:12:06.178: INFO: Deleting pod "simpletest.rc-cz95n" in namespace "gc-9208"
May 17 07:12:06.193: INFO: Deleting pod "simpletest.rc-d7tdg" in namespace "gc-9208"
May 17 07:12:06.207: INFO: Deleting pod "simpletest.rc-djpgh" in namespace "gc-9208"
May 17 07:12:06.225: INFO: Deleting pod "simpletest.rc-drhnf" in namespace "gc-9208"
May 17 07:12:06.240: INFO: Deleting pod "simpletest.rc-dtxrn" in namespace "gc-9208"
May 17 07:12:06.257: INFO: Deleting pod "simpletest.rc-dzs5g" in namespace "gc-9208"
May 17 07:12:06.277: INFO: Deleting pod "simpletest.rc-f2265" in namespace "gc-9208"
May 17 07:12:06.295: INFO: Deleting pod "simpletest.rc-fh5gf" in namespace "gc-9208"
May 17 07:12:06.312: INFO: Deleting pod "simpletest.rc-fz8b9" in namespace "gc-9208"
May 17 07:12:06.328: INFO: Deleting pod "simpletest.rc-gfr5r" in namespace "gc-9208"
May 17 07:12:06.346: INFO: Deleting pod "simpletest.rc-gq6fx" in namespace "gc-9208"
May 17 07:12:06.361: INFO: Deleting pod "simpletest.rc-gzc7r" in namespace "gc-9208"
May 17 07:12:06.386: INFO: Deleting pod "simpletest.rc-hcs4x" in namespace "gc-9208"
May 17 07:12:06.402: INFO: Deleting pod "simpletest.rc-hkg4b" in namespace "gc-9208"
May 17 07:12:06.415: INFO: Deleting pod "simpletest.rc-hlv7z" in namespace "gc-9208"
May 17 07:12:06.432: INFO: Deleting pod "simpletest.rc-hqpsq" in namespace "gc-9208"
May 17 07:12:06.448: INFO: Deleting pod "simpletest.rc-j597j" in namespace "gc-9208"
May 17 07:12:06.463: INFO: Deleting pod "simpletest.rc-j7fn8" in namespace "gc-9208"
May 17 07:12:06.477: INFO: Deleting pod "simpletest.rc-jkslw" in namespace "gc-9208"
May 17 07:12:06.497: INFO: Deleting pod "simpletest.rc-jwwbw" in namespace "gc-9208"
May 17 07:12:06.513: INFO: Deleting pod "simpletest.rc-k27f2" in namespace "gc-9208"
May 17 07:12:06.529: INFO: Deleting pod "simpletest.rc-kbgpb" in namespace "gc-9208"
May 17 07:12:06.546: INFO: Deleting pod "simpletest.rc-kkmfd" in namespace "gc-9208"
May 17 07:12:06.563: INFO: Deleting pod "simpletest.rc-l2mds" in namespace "gc-9208"
May 17 07:12:06.582: INFO: Deleting pod "simpletest.rc-l8rcc" in namespace "gc-9208"
May 17 07:12:06.597: INFO: Deleting pod "simpletest.rc-ldcvw" in namespace "gc-9208"
May 17 07:12:06.616: INFO: Deleting pod "simpletest.rc-lp84s" in namespace "gc-9208"
May 17 07:12:06.656: INFO: Deleting pod "simpletest.rc-m96qn" in namespace "gc-9208"
May 17 07:12:06.688: INFO: Deleting pod "simpletest.rc-ml629" in namespace "gc-9208"
May 17 07:12:06.705: INFO: Deleting pod "simpletest.rc-mljpb" in namespace "gc-9208"
May 17 07:12:06.723: INFO: Deleting pod "simpletest.rc-msh7w" in namespace "gc-9208"
May 17 07:12:06.740: INFO: Deleting pod "simpletest.rc-mx652" in namespace "gc-9208"
May 17 07:12:06.758: INFO: Deleting pod "simpletest.rc-mxpjf" in namespace "gc-9208"
May 17 07:12:06.773: INFO: Deleting pod "simpletest.rc-n4xlk" in namespace "gc-9208"
May 17 07:12:06.791: INFO: Deleting pod "simpletest.rc-n8r5w" in namespace "gc-9208"
May 17 07:12:06.808: INFO: Deleting pod "simpletest.rc-p96lf" in namespace "gc-9208"
May 17 07:12:06.825: INFO: Deleting pod "simpletest.rc-p9ps8" in namespace "gc-9208"
May 17 07:12:06.838: INFO: Deleting pod "simpletest.rc-pdhn2" in namespace "gc-9208"
May 17 07:12:06.852: INFO: Deleting pod "simpletest.rc-plh4m" in namespace "gc-9208"
May 17 07:12:06.894: INFO: Deleting pod "simpletest.rc-pxf6p" in namespace "gc-9208"
May 17 07:12:06.940: INFO: Deleting pod "simpletest.rc-q6n9n" in namespace "gc-9208"
May 17 07:12:06.997: INFO: Deleting pod "simpletest.rc-qp78z" in namespace "gc-9208"
May 17 07:12:07.039: INFO: Deleting pod "simpletest.rc-r4pdt" in namespace "gc-9208"
May 17 07:12:07.093: INFO: Deleting pod "simpletest.rc-rbvch" in namespace "gc-9208"
May 17 07:12:07.146: INFO: Deleting pod "simpletest.rc-rkv44" in namespace "gc-9208"
May 17 07:12:07.198: INFO: Deleting pod "simpletest.rc-rn245" in namespace "gc-9208"
May 17 07:12:07.243: INFO: Deleting pod "simpletest.rc-rsdc9" in namespace "gc-9208"
May 17 07:12:07.294: INFO: Deleting pod "simpletest.rc-s56hc" in namespace "gc-9208"
May 17 07:12:07.348: INFO: Deleting pod "simpletest.rc-s6hqk" in namespace "gc-9208"
May 17 07:12:07.396: INFO: Deleting pod "simpletest.rc-shp4z" in namespace "gc-9208"
May 17 07:12:07.440: INFO: Deleting pod "simpletest.rc-sxj7k" in namespace "gc-9208"
May 17 07:12:07.492: INFO: Deleting pod "simpletest.rc-tr2b2" in namespace "gc-9208"
May 17 07:12:07.542: INFO: Deleting pod "simpletest.rc-tr9t6" in namespace "gc-9208"
May 17 07:12:07.595: INFO: Deleting pod "simpletest.rc-trpff" in namespace "gc-9208"
May 17 07:12:07.642: INFO: Deleting pod "simpletest.rc-vdpq6" in namespace "gc-9208"
May 17 07:12:07.690: INFO: Deleting pod "simpletest.rc-vnvkf" in namespace "gc-9208"
May 17 07:12:07.745: INFO: Deleting pod "simpletest.rc-w8trb" in namespace "gc-9208"
May 17 07:12:07.793: INFO: Deleting pod "simpletest.rc-x4ssd" in namespace "gc-9208"
May 17 07:12:07.842: INFO: Deleting pod "simpletest.rc-x99zp" in namespace "gc-9208"
May 17 07:12:07.891: INFO: Deleting pod "simpletest.rc-xhjhr" in namespace "gc-9208"
May 17 07:12:07.939: INFO: Deleting pod "simpletest.rc-xt6dx" in namespace "gc-9208"
May 17 07:12:07.995: INFO: Deleting pod "simpletest.rc-zbhc5" in namespace "gc-9208"
May 17 07:12:08.045: INFO: Deleting pod "simpletest.rc-zngbl" in namespace "gc-9208"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May 17 07:12:08.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9208" for this suite. 05/17/23 07:12:08.15
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":355,"skipped":6458,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.799 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:11:25.386
    May 17 07:11:25.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename gc 05/17/23 07:11:25.387
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:11:25.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:11:25.429
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 05/17/23 07:11:25.442
    STEP: delete the rc 05/17/23 07:11:30.46
    STEP: wait for the rc to be deleted 05/17/23 07:11:30.468
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/17/23 07:11:35.475
    STEP: Gathering metrics 05/17/23 07:12:05.489
    W0517 07:12:05.502296      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May 17 07:12:05.502: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May 17 07:12:05.502: INFO: Deleting pod "simpletest.rc-25zbx" in namespace "gc-9208"
    May 17 07:12:05.514: INFO: Deleting pod "simpletest.rc-47rr6" in namespace "gc-9208"
    May 17 07:12:05.531: INFO: Deleting pod "simpletest.rc-49rmg" in namespace "gc-9208"
    May 17 07:12:05.549: INFO: Deleting pod "simpletest.rc-4qhww" in namespace "gc-9208"
    May 17 07:12:05.569: INFO: Deleting pod "simpletest.rc-4tc7r" in namespace "gc-9208"
    May 17 07:12:05.590: INFO: Deleting pod "simpletest.rc-4wxkv" in namespace "gc-9208"
    May 17 07:12:05.621: INFO: Deleting pod "simpletest.rc-5gdnb" in namespace "gc-9208"
    May 17 07:12:05.646: INFO: Deleting pod "simpletest.rc-5nqvr" in namespace "gc-9208"
    May 17 07:12:05.667: INFO: Deleting pod "simpletest.rc-6bxlm" in namespace "gc-9208"
    May 17 07:12:05.684: INFO: Deleting pod "simpletest.rc-6d5fc" in namespace "gc-9208"
    May 17 07:12:05.701: INFO: Deleting pod "simpletest.rc-6lrp6" in namespace "gc-9208"
    May 17 07:12:05.717: INFO: Deleting pod "simpletest.rc-6lvfl" in namespace "gc-9208"
    May 17 07:12:05.733: INFO: Deleting pod "simpletest.rc-78wxd" in namespace "gc-9208"
    May 17 07:12:05.746: INFO: Deleting pod "simpletest.rc-7fgps" in namespace "gc-9208"
    May 17 07:12:05.759: INFO: Deleting pod "simpletest.rc-7lxpb" in namespace "gc-9208"
    May 17 07:12:05.773: INFO: Deleting pod "simpletest.rc-7mmm2" in namespace "gc-9208"
    May 17 07:12:05.787: INFO: Deleting pod "simpletest.rc-88q7s" in namespace "gc-9208"
    May 17 07:12:05.799: INFO: Deleting pod "simpletest.rc-8g9x6" in namespace "gc-9208"
    May 17 07:12:05.826: INFO: Deleting pod "simpletest.rc-8kmqq" in namespace "gc-9208"
    May 17 07:12:05.842: INFO: Deleting pod "simpletest.rc-8lc6f" in namespace "gc-9208"
    May 17 07:12:05.858: INFO: Deleting pod "simpletest.rc-8mjgt" in namespace "gc-9208"
    May 17 07:12:05.872: INFO: Deleting pod "simpletest.rc-8plhf" in namespace "gc-9208"
    May 17 07:12:05.886: INFO: Deleting pod "simpletest.rc-8vhrk" in namespace "gc-9208"
    May 17 07:12:05.902: INFO: Deleting pod "simpletest.rc-9fhwr" in namespace "gc-9208"
    May 17 07:12:05.922: INFO: Deleting pod "simpletest.rc-9gkzp" in namespace "gc-9208"
    May 17 07:12:05.943: INFO: Deleting pod "simpletest.rc-9mcbp" in namespace "gc-9208"
    May 17 07:12:05.961: INFO: Deleting pod "simpletest.rc-b2kx2" in namespace "gc-9208"
    May 17 07:12:06.004: INFO: Deleting pod "simpletest.rc-bbz9q" in namespace "gc-9208"
    May 17 07:12:06.026: INFO: Deleting pod "simpletest.rc-bkbdx" in namespace "gc-9208"
    May 17 07:12:06.045: INFO: Deleting pod "simpletest.rc-br2vf" in namespace "gc-9208"
    May 17 07:12:06.064: INFO: Deleting pod "simpletest.rc-c27xc" in namespace "gc-9208"
    May 17 07:12:06.083: INFO: Deleting pod "simpletest.rc-c99mz" in namespace "gc-9208"
    May 17 07:12:06.108: INFO: Deleting pod "simpletest.rc-c9m72" in namespace "gc-9208"
    May 17 07:12:06.122: INFO: Deleting pod "simpletest.rc-cmhnt" in namespace "gc-9208"
    May 17 07:12:06.135: INFO: Deleting pod "simpletest.rc-cnj4p" in namespace "gc-9208"
    May 17 07:12:06.149: INFO: Deleting pod "simpletest.rc-cqxcz" in namespace "gc-9208"
    May 17 07:12:06.165: INFO: Deleting pod "simpletest.rc-crk4f" in namespace "gc-9208"
    May 17 07:12:06.178: INFO: Deleting pod "simpletest.rc-cz95n" in namespace "gc-9208"
    May 17 07:12:06.193: INFO: Deleting pod "simpletest.rc-d7tdg" in namespace "gc-9208"
    May 17 07:12:06.207: INFO: Deleting pod "simpletest.rc-djpgh" in namespace "gc-9208"
    May 17 07:12:06.225: INFO: Deleting pod "simpletest.rc-drhnf" in namespace "gc-9208"
    May 17 07:12:06.240: INFO: Deleting pod "simpletest.rc-dtxrn" in namespace "gc-9208"
    May 17 07:12:06.257: INFO: Deleting pod "simpletest.rc-dzs5g" in namespace "gc-9208"
    May 17 07:12:06.277: INFO: Deleting pod "simpletest.rc-f2265" in namespace "gc-9208"
    May 17 07:12:06.295: INFO: Deleting pod "simpletest.rc-fh5gf" in namespace "gc-9208"
    May 17 07:12:06.312: INFO: Deleting pod "simpletest.rc-fz8b9" in namespace "gc-9208"
    May 17 07:12:06.328: INFO: Deleting pod "simpletest.rc-gfr5r" in namespace "gc-9208"
    May 17 07:12:06.346: INFO: Deleting pod "simpletest.rc-gq6fx" in namespace "gc-9208"
    May 17 07:12:06.361: INFO: Deleting pod "simpletest.rc-gzc7r" in namespace "gc-9208"
    May 17 07:12:06.386: INFO: Deleting pod "simpletest.rc-hcs4x" in namespace "gc-9208"
    May 17 07:12:06.402: INFO: Deleting pod "simpletest.rc-hkg4b" in namespace "gc-9208"
    May 17 07:12:06.415: INFO: Deleting pod "simpletest.rc-hlv7z" in namespace "gc-9208"
    May 17 07:12:06.432: INFO: Deleting pod "simpletest.rc-hqpsq" in namespace "gc-9208"
    May 17 07:12:06.448: INFO: Deleting pod "simpletest.rc-j597j" in namespace "gc-9208"
    May 17 07:12:06.463: INFO: Deleting pod "simpletest.rc-j7fn8" in namespace "gc-9208"
    May 17 07:12:06.477: INFO: Deleting pod "simpletest.rc-jkslw" in namespace "gc-9208"
    May 17 07:12:06.497: INFO: Deleting pod "simpletest.rc-jwwbw" in namespace "gc-9208"
    May 17 07:12:06.513: INFO: Deleting pod "simpletest.rc-k27f2" in namespace "gc-9208"
    May 17 07:12:06.529: INFO: Deleting pod "simpletest.rc-kbgpb" in namespace "gc-9208"
    May 17 07:12:06.546: INFO: Deleting pod "simpletest.rc-kkmfd" in namespace "gc-9208"
    May 17 07:12:06.563: INFO: Deleting pod "simpletest.rc-l2mds" in namespace "gc-9208"
    May 17 07:12:06.582: INFO: Deleting pod "simpletest.rc-l8rcc" in namespace "gc-9208"
    May 17 07:12:06.597: INFO: Deleting pod "simpletest.rc-ldcvw" in namespace "gc-9208"
    May 17 07:12:06.616: INFO: Deleting pod "simpletest.rc-lp84s" in namespace "gc-9208"
    May 17 07:12:06.656: INFO: Deleting pod "simpletest.rc-m96qn" in namespace "gc-9208"
    May 17 07:12:06.688: INFO: Deleting pod "simpletest.rc-ml629" in namespace "gc-9208"
    May 17 07:12:06.705: INFO: Deleting pod "simpletest.rc-mljpb" in namespace "gc-9208"
    May 17 07:12:06.723: INFO: Deleting pod "simpletest.rc-msh7w" in namespace "gc-9208"
    May 17 07:12:06.740: INFO: Deleting pod "simpletest.rc-mx652" in namespace "gc-9208"
    May 17 07:12:06.758: INFO: Deleting pod "simpletest.rc-mxpjf" in namespace "gc-9208"
    May 17 07:12:06.773: INFO: Deleting pod "simpletest.rc-n4xlk" in namespace "gc-9208"
    May 17 07:12:06.791: INFO: Deleting pod "simpletest.rc-n8r5w" in namespace "gc-9208"
    May 17 07:12:06.808: INFO: Deleting pod "simpletest.rc-p96lf" in namespace "gc-9208"
    May 17 07:12:06.825: INFO: Deleting pod "simpletest.rc-p9ps8" in namespace "gc-9208"
    May 17 07:12:06.838: INFO: Deleting pod "simpletest.rc-pdhn2" in namespace "gc-9208"
    May 17 07:12:06.852: INFO: Deleting pod "simpletest.rc-plh4m" in namespace "gc-9208"
    May 17 07:12:06.894: INFO: Deleting pod "simpletest.rc-pxf6p" in namespace "gc-9208"
    May 17 07:12:06.940: INFO: Deleting pod "simpletest.rc-q6n9n" in namespace "gc-9208"
    May 17 07:12:06.997: INFO: Deleting pod "simpletest.rc-qp78z" in namespace "gc-9208"
    May 17 07:12:07.039: INFO: Deleting pod "simpletest.rc-r4pdt" in namespace "gc-9208"
    May 17 07:12:07.093: INFO: Deleting pod "simpletest.rc-rbvch" in namespace "gc-9208"
    May 17 07:12:07.146: INFO: Deleting pod "simpletest.rc-rkv44" in namespace "gc-9208"
    May 17 07:12:07.198: INFO: Deleting pod "simpletest.rc-rn245" in namespace "gc-9208"
    May 17 07:12:07.243: INFO: Deleting pod "simpletest.rc-rsdc9" in namespace "gc-9208"
    May 17 07:12:07.294: INFO: Deleting pod "simpletest.rc-s56hc" in namespace "gc-9208"
    May 17 07:12:07.348: INFO: Deleting pod "simpletest.rc-s6hqk" in namespace "gc-9208"
    May 17 07:12:07.396: INFO: Deleting pod "simpletest.rc-shp4z" in namespace "gc-9208"
    May 17 07:12:07.440: INFO: Deleting pod "simpletest.rc-sxj7k" in namespace "gc-9208"
    May 17 07:12:07.492: INFO: Deleting pod "simpletest.rc-tr2b2" in namespace "gc-9208"
    May 17 07:12:07.542: INFO: Deleting pod "simpletest.rc-tr9t6" in namespace "gc-9208"
    May 17 07:12:07.595: INFO: Deleting pod "simpletest.rc-trpff" in namespace "gc-9208"
    May 17 07:12:07.642: INFO: Deleting pod "simpletest.rc-vdpq6" in namespace "gc-9208"
    May 17 07:12:07.690: INFO: Deleting pod "simpletest.rc-vnvkf" in namespace "gc-9208"
    May 17 07:12:07.745: INFO: Deleting pod "simpletest.rc-w8trb" in namespace "gc-9208"
    May 17 07:12:07.793: INFO: Deleting pod "simpletest.rc-x4ssd" in namespace "gc-9208"
    May 17 07:12:07.842: INFO: Deleting pod "simpletest.rc-x99zp" in namespace "gc-9208"
    May 17 07:12:07.891: INFO: Deleting pod "simpletest.rc-xhjhr" in namespace "gc-9208"
    May 17 07:12:07.939: INFO: Deleting pod "simpletest.rc-xt6dx" in namespace "gc-9208"
    May 17 07:12:07.995: INFO: Deleting pod "simpletest.rc-zbhc5" in namespace "gc-9208"
    May 17 07:12:08.045: INFO: Deleting pod "simpletest.rc-zngbl" in namespace "gc-9208"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May 17 07:12:08.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9208" for this suite. 05/17/23 07:12:08.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:12:08.19
May 17 07:12:08.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename projected 05/17/23 07:12:08.191
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:08.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:08.233
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 05/17/23 07:12:08.237
May 17 07:12:08.252: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423" in namespace "projected-485" to be "Succeeded or Failed"
May 17 07:12:08.258: INFO: Pod "downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423": Phase="Pending", Reason="", readiness=false. Elapsed: 6.825187ms
May 17 07:12:10.264: INFO: Pod "downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012771275s
May 17 07:12:12.267: INFO: Pod "downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015002244s
May 17 07:12:14.264: INFO: Pod "downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012293067s
STEP: Saw pod success 05/17/23 07:12:14.264
May 17 07:12:14.264: INFO: Pod "downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423" satisfied condition "Succeeded or Failed"
May 17 07:12:14.269: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423 container client-container: <nil>
STEP: delete the pod 05/17/23 07:12:14.28
May 17 07:12:14.298: INFO: Waiting for pod downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423 to disappear
May 17 07:12:14.303: INFO: Pod downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May 17 07:12:14.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-485" for this suite. 05/17/23 07:12:14.311
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":356,"skipped":6603,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.130 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:12:08.19
    May 17 07:12:08.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename projected 05/17/23 07:12:08.191
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:08.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:08.233
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 05/17/23 07:12:08.237
    May 17 07:12:08.252: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423" in namespace "projected-485" to be "Succeeded or Failed"
    May 17 07:12:08.258: INFO: Pod "downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423": Phase="Pending", Reason="", readiness=false. Elapsed: 6.825187ms
    May 17 07:12:10.264: INFO: Pod "downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012771275s
    May 17 07:12:12.267: INFO: Pod "downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015002244s
    May 17 07:12:14.264: INFO: Pod "downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012293067s
    STEP: Saw pod success 05/17/23 07:12:14.264
    May 17 07:12:14.264: INFO: Pod "downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423" satisfied condition "Succeeded or Failed"
    May 17 07:12:14.269: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423 container client-container: <nil>
    STEP: delete the pod 05/17/23 07:12:14.28
    May 17 07:12:14.298: INFO: Waiting for pod downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423 to disappear
    May 17 07:12:14.303: INFO: Pod downwardapi-volume-ef1dedc2-f3f2-427a-8ed1-78fafed0f423 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May 17 07:12:14.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-485" for this suite. 05/17/23 07:12:14.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:12:14.321
May 17 07:12:14.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename replicaset 05/17/23 07:12:14.322
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:14.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:14.351
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/17/23 07:12:14.355
May 17 07:12:14.366: INFO: Pod name sample-pod: Found 0 pods out of 1
May 17 07:12:19.372: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/17/23 07:12:19.372
STEP: getting scale subresource 05/17/23 07:12:19.372
STEP: updating a scale subresource 05/17/23 07:12:19.376
STEP: verifying the replicaset Spec.Replicas was modified 05/17/23 07:12:19.383
STEP: Patch a scale subresource 05/17/23 07:12:19.387
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May 17 07:12:19.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4778" for this suite. 05/17/23 07:12:19.404
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":357,"skipped":6621,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.090 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:12:14.321
    May 17 07:12:14.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename replicaset 05/17/23 07:12:14.322
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:14.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:14.351
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/17/23 07:12:14.355
    May 17 07:12:14.366: INFO: Pod name sample-pod: Found 0 pods out of 1
    May 17 07:12:19.372: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/17/23 07:12:19.372
    STEP: getting scale subresource 05/17/23 07:12:19.372
    STEP: updating a scale subresource 05/17/23 07:12:19.376
    STEP: verifying the replicaset Spec.Replicas was modified 05/17/23 07:12:19.383
    STEP: Patch a scale subresource 05/17/23 07:12:19.387
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May 17 07:12:19.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4778" for this suite. 05/17/23 07:12:19.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:12:19.414
May 17 07:12:19.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 07:12:19.414
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:19.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:19.437
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/17/23 07:12:19.448
May 17 07:12:19.466: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2504" to be "running and ready"
May 17 07:12:19.472: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.166486ms
May 17 07:12:19.472: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 17 07:12:21.478: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012310349s
May 17 07:12:21.478: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May 17 07:12:21.478: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 05/17/23 07:12:21.482
May 17 07:12:21.491: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-2504" to be "running and ready"
May 17 07:12:21.496: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.311198ms
May 17 07:12:21.496: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 17 07:12:23.503: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011495748s
May 17 07:12:23.503: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
May 17 07:12:23.503: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/17/23 07:12:23.515
STEP: delete the pod with lifecycle hook 05/17/23 07:12:23.536
May 17 07:12:23.545: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 07:12:23.550: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 07:12:25.551: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 07:12:25.557: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May 17 07:12:25.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2504" for this suite. 05/17/23 07:12:25.565
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":358,"skipped":6682,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.161 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:12:19.414
    May 17 07:12:19.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/17/23 07:12:19.414
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:19.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:19.437
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/17/23 07:12:19.448
    May 17 07:12:19.466: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2504" to be "running and ready"
    May 17 07:12:19.472: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.166486ms
    May 17 07:12:19.472: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:12:21.478: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012310349s
    May 17 07:12:21.478: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May 17 07:12:21.478: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 05/17/23 07:12:21.482
    May 17 07:12:21.491: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-2504" to be "running and ready"
    May 17 07:12:21.496: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.311198ms
    May 17 07:12:21.496: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:12:23.503: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011495748s
    May 17 07:12:23.503: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    May 17 07:12:23.503: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/17/23 07:12:23.515
    STEP: delete the pod with lifecycle hook 05/17/23 07:12:23.536
    May 17 07:12:23.545: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May 17 07:12:23.550: INFO: Pod pod-with-poststart-exec-hook still exists
    May 17 07:12:25.551: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May 17 07:12:25.557: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May 17 07:12:25.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-2504" for this suite. 05/17/23 07:12:25.565
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:12:25.575
May 17 07:12:25.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename containers 05/17/23 07:12:25.576
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:25.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:25.601
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 05/17/23 07:12:25.604
May 17 07:12:25.618: INFO: Waiting up to 5m0s for pod "client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15" in namespace "containers-2655" to be "Succeeded or Failed"
May 17 07:12:25.622: INFO: Pod "client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.348569ms
May 17 07:12:27.628: INFO: Pod "client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009601198s
May 17 07:12:29.628: INFO: Pod "client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009724974s
STEP: Saw pod success 05/17/23 07:12:29.628
May 17 07:12:29.628: INFO: Pod "client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15" satisfied condition "Succeeded or Failed"
May 17 07:12:29.633: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15 container agnhost-container: <nil>
STEP: delete the pod 05/17/23 07:12:29.644
May 17 07:12:29.662: INFO: Waiting for pod client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15 to disappear
May 17 07:12:29.667: INFO: Pod client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May 17 07:12:29.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2655" for this suite. 05/17/23 07:12:29.674
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":359,"skipped":6685,"failed":0}
------------------------------
â€¢ [4.108 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:12:25.575
    May 17 07:12:25.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename containers 05/17/23 07:12:25.576
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:25.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:25.601
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 05/17/23 07:12:25.604
    May 17 07:12:25.618: INFO: Waiting up to 5m0s for pod "client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15" in namespace "containers-2655" to be "Succeeded or Failed"
    May 17 07:12:25.622: INFO: Pod "client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.348569ms
    May 17 07:12:27.628: INFO: Pod "client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009601198s
    May 17 07:12:29.628: INFO: Pod "client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009724974s
    STEP: Saw pod success 05/17/23 07:12:29.628
    May 17 07:12:29.628: INFO: Pod "client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15" satisfied condition "Succeeded or Failed"
    May 17 07:12:29.633: INFO: Trying to get logs from node aks-agentpool-72615086-vmss00000c pod client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15 container agnhost-container: <nil>
    STEP: delete the pod 05/17/23 07:12:29.644
    May 17 07:12:29.662: INFO: Waiting for pod client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15 to disappear
    May 17 07:12:29.667: INFO: Pod client-containers-dcef65a0-7baa-4030-941d-0d8663aaec15 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May 17 07:12:29.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2655" for this suite. 05/17/23 07:12:29.674
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:12:29.683
May 17 07:12:29.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename aggregator 05/17/23 07:12:29.684
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:29.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:29.706
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
May 17 07:12:29.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 05/17/23 07:12:29.709
May 17 07:12:30.108: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 17 07:12:32.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:12:34.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:12:36.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:12:38.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:12:40.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:12:42.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:12:44.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:12:46.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:12:48.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:12:50.163: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:12:52.163: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 07:12:54.318: INFO: Waited 147.139609ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 05/17/23 07:12:54.549
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/17/23 07:12:54.582
STEP: List APIServices 05/17/23 07:12:54.635
May 17 07:12:54.687: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
May 17 07:12:55.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4779" for this suite. 05/17/23 07:12:55.535
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":360,"skipped":6687,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.903 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:12:29.683
    May 17 07:12:29.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename aggregator 05/17/23 07:12:29.684
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:29.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:29.706
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    May 17 07:12:29.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 05/17/23 07:12:29.709
    May 17 07:12:30.108: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    May 17 07:12:32.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:12:34.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:12:36.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:12:38.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:12:40.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:12:42.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:12:44.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:12:46.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:12:48.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:12:50.163: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:12:52.163: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 17, 7, 12, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 17 07:12:54.318: INFO: Waited 147.139609ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 05/17/23 07:12:54.549
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/17/23 07:12:54.582
    STEP: List APIServices 05/17/23 07:12:54.635
    May 17 07:12:54.687: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    May 17 07:12:55.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-4779" for this suite. 05/17/23 07:12:55.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:12:55.587
May 17 07:12:55.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename sysctl 05/17/23 07:12:55.588
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:55.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:55.636
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 05/17/23 07:12:55.64
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
May 17 07:12:55.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-604" for this suite. 05/17/23 07:12:55.656
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":361,"skipped":6704,"failed":0}
------------------------------
â€¢ [0.077 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:12:55.587
    May 17 07:12:55.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename sysctl 05/17/23 07:12:55.588
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:55.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:55.636
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 05/17/23 07:12:55.64
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    May 17 07:12:55.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-604" for this suite. 05/17/23 07:12:55.656
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/17/23 07:12:55.664
May 17 07:12:55.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
STEP: Building a namespace api object, basename pods 05/17/23 07:12:55.665
STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:55.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:55.682
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 05/17/23 07:12:55.686
STEP: submitting the pod to kubernetes 05/17/23 07:12:55.686
May 17 07:12:55.698: INFO: Waiting up to 5m0s for pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa" in namespace "pods-5837" to be "running and ready"
May 17 07:12:55.702: INFO: Pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.114516ms
May 17 07:12:55.702: INFO: The phase of Pod pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa is Pending, waiting for it to be Running (with Ready = true)
May 17 07:12:57.707: INFO: Pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00910757s
May 17 07:12:57.707: INFO: The phase of Pod pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa is Pending, waiting for it to be Running (with Ready = true)
May 17 07:12:59.708: INFO: Pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa": Phase="Running", Reason="", readiness=true. Elapsed: 4.009950715s
May 17 07:12:59.708: INFO: The phase of Pod pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa is Running (Ready = true)
May 17 07:12:59.708: INFO: Pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/17/23 07:12:59.712
STEP: updating the pod 05/17/23 07:12:59.717
May 17 07:13:00.229: INFO: Successfully updated pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa"
May 17 07:13:00.229: INFO: Waiting up to 5m0s for pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa" in namespace "pods-5837" to be "running"
May 17 07:13:00.233: INFO: Pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa": Phase="Running", Reason="", readiness=true. Elapsed: 4.094263ms
May 17 07:13:00.233: INFO: Pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 05/17/23 07:13:00.233
May 17 07:13:00.238: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May 17 07:13:00.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5837" for this suite. 05/17/23 07:13:00.245
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":362,"skipped":6704,"failed":0}
------------------------------
â€¢ [4.590 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/17/23 07:12:55.664
    May 17 07:12:55.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1963122082
    STEP: Building a namespace api object, basename pods 05/17/23 07:12:55.665
    STEP: Waiting for a default service account to be provisioned in namespace 05/17/23 07:12:55.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/17/23 07:12:55.682
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 05/17/23 07:12:55.686
    STEP: submitting the pod to kubernetes 05/17/23 07:12:55.686
    May 17 07:12:55.698: INFO: Waiting up to 5m0s for pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa" in namespace "pods-5837" to be "running and ready"
    May 17 07:12:55.702: INFO: Pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.114516ms
    May 17 07:12:55.702: INFO: The phase of Pod pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:12:57.707: INFO: Pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00910757s
    May 17 07:12:57.707: INFO: The phase of Pod pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa is Pending, waiting for it to be Running (with Ready = true)
    May 17 07:12:59.708: INFO: Pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa": Phase="Running", Reason="", readiness=true. Elapsed: 4.009950715s
    May 17 07:12:59.708: INFO: The phase of Pod pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa is Running (Ready = true)
    May 17 07:12:59.708: INFO: Pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/17/23 07:12:59.712
    STEP: updating the pod 05/17/23 07:12:59.717
    May 17 07:13:00.229: INFO: Successfully updated pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa"
    May 17 07:13:00.229: INFO: Waiting up to 5m0s for pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa" in namespace "pods-5837" to be "running"
    May 17 07:13:00.233: INFO: Pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa": Phase="Running", Reason="", readiness=true. Elapsed: 4.094263ms
    May 17 07:13:00.233: INFO: Pod "pod-update-fc5c724f-16c4-4190-ac23-d631f422f1fa" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 05/17/23 07:13:00.233
    May 17 07:13:00.238: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May 17 07:13:00.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5837" for this suite. 05/17/23 07:13:00.245
  << End Captured GinkgoWriter Output
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
May 17 07:13:00.255: INFO: Running AfterSuite actions on all nodes
May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
May 17 07:13:00.255: INFO: Running AfterSuite actions on node 1
May 17 07:13:00.255: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    May 17 07:13:00.255: INFO: Running AfterSuite actions on all nodes
    May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    May 17 07:13:00.255: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    May 17 07:13:00.255: INFO: Running AfterSuite actions on node 1
    May 17 07:13:00.255: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.040 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 5804.487 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h36m44.646165296s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

